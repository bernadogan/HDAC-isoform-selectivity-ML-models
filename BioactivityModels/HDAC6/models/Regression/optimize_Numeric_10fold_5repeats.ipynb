{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/anaconda3/envs/teachopencadd/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "levels_up = 3\n",
    "HDAC6= HERE.parents[levels_up-1]/'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>molecular_weight</th>\n",
       "      <th>n_rot</th>\n",
       "      <th>n_heavy</th>\n",
       "      <th>n_hba</th>\n",
       "      <th>n_hbd</th>\n",
       "      <th>logp</th>\n",
       "      <th>num_ar</th>\n",
       "      <th>num_sa</th>\n",
       "      <th>num_alip</th>\n",
       "      <th>pchembl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL356066</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[6670835, 7057043, 2678397, 6116087, 19849001,...</td>\n",
       "      <td>379.189592</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.72350</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3652228</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[35600637, 2478511, 10872982, 74375522, 394834...</td>\n",
       "      <td>244.096026</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.64762</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3939518</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[13333824, 2632634, 1991683, 218190, 16674131,...</td>\n",
       "      <td>408.128983</td>\n",
       "      <td>11.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.71980</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4281792</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1840283, 3913535, 8033062, 668868, 3328960, 4...</td>\n",
       "      <td>460.211055</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.87510</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4070232</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5922349, 1506859, 1823034, 2469425, 5363609, ...</td>\n",
       "      <td>516.189651</td>\n",
       "      <td>10.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.47360</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0       CHEMBL356066  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL3652228  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3939518  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL4281792  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL4070232  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  molecular_weight  n_rot  \\\n",
       "0  [6670835, 7057043, 2678397, 6116087, 19849001,...        379.189592    9.0   \n",
       "1  [35600637, 2478511, 10872982, 74375522, 394834...        244.096026    3.0   \n",
       "2  [13333824, 2632634, 1991683, 218190, 16674131,...        408.128983   11.0   \n",
       "3  [1840283, 3913535, 8033062, 668868, 3328960, 4...        460.211055    5.0   \n",
       "4  [5922349, 1506859, 1823034, 2469425, 5363609, ...        516.189651   10.0   \n",
       "\n",
       "   n_heavy  n_hba  n_hbd     logp  num_ar  num_sa  num_alip  pchembl  \n",
       "0     28.0    4.0    4.0  2.72350     3.0     0.0       0.0     8.02  \n",
       "1     18.0    5.0    3.0  1.64762     2.0     0.0       0.0     8.05  \n",
       "2     27.0    7.0    3.0  3.71980     2.0     1.0       1.0     6.87  \n",
       "3     34.0    5.0    2.0  2.87510     3.0     1.0       2.0     7.22  \n",
       "4     38.0    8.0    2.0  4.47360     4.0     0.0       0.0     7.15  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC6/\"HDAC6_2971compounds_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>pActivity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4082520</td>\n",
       "      <td>CN1C(=O)C2CN(Cc3c2c2cc(OCc4ccccc4)ccc2n3Cc2ccc...</td>\n",
       "      <td>10.10</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4098975</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...</td>\n",
       "      <td>9.85</td>\n",
       "      <td>hDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4100534</td>\n",
       "      <td>COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...</td>\n",
       "      <td>9.82</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4101480</td>\n",
       "      <td>COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...</td>\n",
       "      <td>9.80</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3912061</td>\n",
       "      <td>CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12</td>\n",
       "      <td>9.77</td>\n",
       "      <td>hDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL1798006</td>\n",
       "      <td>CC[C@H](C)[C@H](NC(=O)C1CCNCC1)C(=O)N1Cc2cc(OC...</td>\n",
       "      <td>6.77</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL470843</td>\n",
       "      <td>O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO</td>\n",
       "      <td>6.76</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL1798004</td>\n",
       "      <td>CC[C@H](C)[C@H](NC(=O)[C@@H]1CCCN1)C(=O)N1Cc2c...</td>\n",
       "      <td>6.72</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL3215861</td>\n",
       "      <td>CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC</td>\n",
       "      <td>6.61</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL3233708</td>\n",
       "      <td>O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...</td>\n",
       "      <td>6.53</td>\n",
       "      <td>Non-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                             smiles  \\\n",
       "0         CHEMBL4082520  CN1C(=O)C2CN(Cc3c2c2cc(OCc4ccccc4)ccc2n3Cc2ccc...   \n",
       "1         CHEMBL4098975  O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...   \n",
       "2         CHEMBL4100534  COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...   \n",
       "3         CHEMBL4101480  COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...   \n",
       "4         CHEMBL3912061     CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12   \n",
       "...                 ...                                                ...   \n",
       "2966      CHEMBL1798006  CC[C@H](C)[C@H](NC(=O)C1CCNCC1)C(=O)N1Cc2cc(OC...   \n",
       "2967       CHEMBL470843          O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO   \n",
       "2968      CHEMBL1798004  CC[C@H](C)[C@H](NC(=O)[C@@H]1CCCN1)C(=O)N1Cc2c...   \n",
       "2969      CHEMBL3215861           CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC   \n",
       "2970      CHEMBL3233708  O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...   \n",
       "\n",
       "      pActivity            label  \n",
       "0         10.10    Single points  \n",
       "1          9.85  hDAC6-selective  \n",
       "2          9.82    Single points  \n",
       "3          9.80    Single points  \n",
       "4          9.77  hDAC6-selective  \n",
       "...         ...              ...  \n",
       "2966       6.77    Single points  \n",
       "2967       6.76   Semi-selective  \n",
       "2968       6.72    Single points  \n",
       "2969       6.61      Dual-binder  \n",
       "2970       6.53       Non-binder  \n",
       "\n",
       "[2971 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC6/\"HDAC6_2971compounds_withTypes-Ki_newThreshold.csv\", index_col=0)\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>molecular_weight</th>\n",
       "      <th>n_rot</th>\n",
       "      <th>n_heavy</th>\n",
       "      <th>n_hba</th>\n",
       "      <th>n_hbd</th>\n",
       "      <th>logp</th>\n",
       "      <th>num_ar</th>\n",
       "      <th>num_sa</th>\n",
       "      <th>num_alip</th>\n",
       "      <th>pchembl</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL356066</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[6670835, 7057043, 2678397, 6116087, 19849001,...</td>\n",
       "      <td>379.189592</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.72350</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.02</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3652228</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[35600637, 2478511, 10872982, 74375522, 394834...</td>\n",
       "      <td>244.096026</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.64762</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.05</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3939518</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[13333824, 2632634, 1991683, 218190, 16674131,...</td>\n",
       "      <td>408.128983</td>\n",
       "      <td>11.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.71980</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.87</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4281792</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1840283, 3913535, 8033062, 668868, 3328960, 4...</td>\n",
       "      <td>460.211055</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.87510</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.22</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4070232</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5922349, 1506859, 1823034, 2469425, 5363609, ...</td>\n",
       "      <td>516.189651</td>\n",
       "      <td>10.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.47360</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.15</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0       CHEMBL356066  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL3652228  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3939518  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL4281792  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL4070232  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  molecular_weight  n_rot  \\\n",
       "0  [6670835, 7057043, 2678397, 6116087, 19849001,...        379.189592    9.0   \n",
       "1  [35600637, 2478511, 10872982, 74375522, 394834...        244.096026    3.0   \n",
       "2  [13333824, 2632634, 1991683, 218190, 16674131,...        408.128983   11.0   \n",
       "3  [1840283, 3913535, 8033062, 668868, 3328960, 4...        460.211055    5.0   \n",
       "4  [5922349, 1506859, 1823034, 2469425, 5363609, ...        516.189651   10.0   \n",
       "\n",
       "   n_heavy  n_hba  n_hbd     logp  num_ar  num_sa  num_alip  pchembl  \\\n",
       "0     28.0    4.0    4.0  2.72350     3.0     0.0       0.0     8.02   \n",
       "1     18.0    5.0    3.0  1.64762     2.0     0.0       0.0     8.05   \n",
       "2     27.0    7.0    3.0  3.71980     2.0     1.0       1.0     6.87   \n",
       "3     34.0    5.0    2.0  2.87510     3.0     1.0       2.0     7.22   \n",
       "4     38.0    8.0    2.0  4.47360     4.0     0.0       0.0     7.15   \n",
       "\n",
       "            label  \n",
       "0  Semi-selective  \n",
       "1  Semi-selective  \n",
       "2     Dual-binder  \n",
       "3   Single points  \n",
       "4     Dual-binder  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>molecular_weight</th>\n",
       "      <th>n_rot</th>\n",
       "      <th>n_heavy</th>\n",
       "      <th>n_hba</th>\n",
       "      <th>n_hbd</th>\n",
       "      <th>logp</th>\n",
       "      <th>num_ar</th>\n",
       "      <th>num_sa</th>\n",
       "      <th>num_alip</th>\n",
       "      <th>pchembl</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL356066</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[6670835, 7057043, 2678397, 6116087, 19849001,...</td>\n",
       "      <td>379.189592</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.72350</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.02</td>\n",
       "      <td>Semi-selective</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3652228</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[35600637, 2478511, 10872982, 74375522, 394834...</td>\n",
       "      <td>244.096026</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.64762</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.05</td>\n",
       "      <td>Semi-selective</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3939518</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[13333824, 2632634, 1991683, 218190, 16674131,...</td>\n",
       "      <td>408.128983</td>\n",
       "      <td>11.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.71980</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.87</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4281792</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1840283, 3913535, 8033062, 668868, 3328960, 4...</td>\n",
       "      <td>460.211055</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.87510</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.22</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0       CHEMBL356066  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL3652228  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3939518  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL4281792  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  molecular_weight  n_rot  \\\n",
       "0  [6670835, 7057043, 2678397, 6116087, 19849001,...        379.189592    9.0   \n",
       "1  [35600637, 2478511, 10872982, 74375522, 394834...        244.096026    3.0   \n",
       "2  [13333824, 2632634, 1991683, 218190, 16674131,...        408.128983   11.0   \n",
       "3  [1840283, 3913535, 8033062, 668868, 3328960, 4...        460.211055    5.0   \n",
       "\n",
       "   n_heavy  n_hba  n_hbd     logp  num_ar  num_sa  num_alip  pchembl  \\\n",
       "0     28.0    4.0    4.0  2.72350     3.0     0.0       0.0     8.02   \n",
       "1     18.0    5.0    3.0  1.64762     2.0     0.0       0.0     8.05   \n",
       "2     27.0    7.0    3.0  3.71980     2.0     1.0       1.0     6.87   \n",
       "3     34.0    5.0    2.0  2.87510     3.0     1.0       2.0     7.22   \n",
       "\n",
       "            label  Class  \n",
       "0  Semi-selective    5.0  \n",
       "1  Semi-selective    5.0  \n",
       "2     Dual-binder    3.0  \n",
       "3   Single points    0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'hDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'hDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'hDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "df.loc[df[df.label == 'Semi-selective'].index, \"Class\"] = 5.0\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for activity\n",
    "df[\"activity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as active if pchembl value is >=6.6 0 otherwise\n",
    "df.loc[df[df.pchembl >= 6.6].index, \"activity\"] = 1.0\n",
    "\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y = df[\"pchembl\"].values\n",
    "Y_cat =  df[\"activity\"].values\n",
    "Y_class = df['Class'].values\n",
    "indices =  np.array(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['pchembl'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_cat\"] = np.array(list(df.iloc[index_tr_all[i]]['activity'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['pchembl'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_cat\"] = np.array(list(df.iloc[index_te_all[i]]['activity'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.693683     0.042858\n",
      "1                    TP       166.200000     6.408328\n",
      "2                    TN        88.500000     7.989577\n",
      "3                    FP        24.900000     5.989806\n",
      "4                    FN        17.500000     5.562773\n",
      "5              Accuracy         0.857290     0.023371\n",
      "6             Precision         0.870614     0.025897\n",
      "7           Sensitivity         0.904813     0.029567\n",
      "8           Specificity         0.779790     0.054270\n",
      "9              F1 score         0.886916     0.017984\n",
      "10  F1 score (weighted)         0.856109     0.023678\n",
      "11     F1 score (macro)         0.846320     0.026168\n",
      "12    Balanced Accuracy         0.842301     0.027134\n",
      "13                  MCC         0.695561     0.050915\n",
      "14                  NPV         0.836320     0.041900\n",
      "15              ROC_AUC         0.842301     0.027134\n",
      "CPU times: user 1min 57s, sys: 62.8 ms, total: 1min 57s\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_reg =  RandomForestRegressor(random_state=1121218, max_features = None, n_jobs=8,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_reg.fit(x_train, y_train)\n",
    "        y_pred = rf_reg.predict(x_test)  \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6) , 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=8, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=8, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 10:12:36,863]\u001b[0m A new study created in memory with name: RFRegressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:13:32,328]\u001b[0m Trial 0 finished with value: 0.678994850425892 and parameters: {'n_estimators': 467}. Best is trial 0 with value: 0.678994850425892.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:15:33,872]\u001b[0m Trial 1 finished with value: 0.6796615212674724 and parameters: {'n_estimators': 923}. Best is trial 1 with value: 0.6796615212674724.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:16:56,056]\u001b[0m Trial 2 finished with value: 0.6792106057770427 and parameters: {'n_estimators': 536}. Best is trial 1 with value: 0.6796615212674724.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:18:20,576]\u001b[0m Trial 3 finished with value: 0.6793797660048344 and parameters: {'n_estimators': 542}. Best is trial 1 with value: 0.6796615212674724.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:20:52,039]\u001b[0m Trial 4 finished with value: 0.6797309732072196 and parameters: {'n_estimators': 990}. Best is trial 4 with value: 0.6797309732072196.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:23:09,256]\u001b[0m Trial 5 finished with value: 0.6796257951550755 and parameters: {'n_estimators': 897}. Best is trial 4 with value: 0.6797309732072196.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:24:56,136]\u001b[0m Trial 6 finished with value: 0.6796495109379774 and parameters: {'n_estimators': 714}. Best is trial 4 with value: 0.6797309732072196.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:27:07,939]\u001b[0m Trial 7 finished with value: 0.6796121288125436 and parameters: {'n_estimators': 887}. Best is trial 4 with value: 0.6797309732072196.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:27:41,007]\u001b[0m Trial 8 finished with value: 0.6776917783528329 and parameters: {'n_estimators': 210}. Best is trial 4 with value: 0.6797309732072196.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:28:41,122]\u001b[0m Trial 9 finished with value: 0.6784850303850618 and parameters: {'n_estimators': 391}. Best is trial 4 with value: 0.6797309732072196.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:30:26,782]\u001b[0m Trial 10 finished with value: 0.6796576117860778 and parameters: {'n_estimators': 696}. Best is trial 4 with value: 0.6797309732072196.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:32:57,833]\u001b[0m Trial 11 finished with value: 0.6797474359538459 and parameters: {'n_estimators': 987}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:35:21,478]\u001b[0m Trial 12 finished with value: 0.6796906573757981 and parameters: {'n_estimators': 972}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:37:16,300]\u001b[0m Trial 13 finished with value: 0.6796045347402028 and parameters: {'n_estimators': 734}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:39:18,685]\u001b[0m Trial 14 finished with value: 0.679566155200697 and parameters: {'n_estimators': 810}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:41:49,189]\u001b[0m Trial 15 finished with value: 0.6797309732072195 and parameters: {'n_estimators': 990}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:42:12,436]\u001b[0m Trial 16 finished with value: 0.6786245184945756 and parameters: {'n_estimators': 148}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:44:18,786]\u001b[0m Trial 17 finished with value: 0.6796660883326725 and parameters: {'n_estimators': 829}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:45:55,202]\u001b[0m Trial 18 finished with value: 0.6792508866740878 and parameters: {'n_estimators': 627}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:46:38,757]\u001b[0m Trial 19 finished with value: 0.6782351130900093 and parameters: {'n_estimators': 280}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:48:47,372]\u001b[0m Trial 20 finished with value: 0.6796660883326726 and parameters: {'n_estimators': 829}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:51:19,415]\u001b[0m Trial 21 finished with value: 0.6797309732072196 and parameters: {'n_estimators': 990}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:53:52,010]\u001b[0m Trial 22 finished with value: 0.6797254939050543 and parameters: {'n_estimators': 991}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:55:53,892]\u001b[0m Trial 23 finished with value: 0.6795151450260212 and parameters: {'n_estimators': 788}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 10:58:14,536]\u001b[0m Trial 24 finished with value: 0.6796426213548334 and parameters: {'n_estimators': 898}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:00:41,616]\u001b[0m Trial 25 finished with value: 0.6795858792931196 and parameters: {'n_estimators': 935}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:02:54,190]\u001b[0m Trial 26 finished with value: 0.6797310417362077 and parameters: {'n_estimators': 863}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:04:30,852]\u001b[0m Trial 27 finished with value: 0.6791768869085298 and parameters: {'n_estimators': 638}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:06:45,111]\u001b[0m Trial 28 finished with value: 0.6797006914971674 and parameters: {'n_estimators': 866}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:07:51,079]\u001b[0m Trial 29 finished with value: 0.678482951800356 and parameters: {'n_estimators': 428}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:09:53,417]\u001b[0m Trial 30 finished with value: 0.6795181196527723 and parameters: {'n_estimators': 794}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:12:25,901]\u001b[0m Trial 31 finished with value: 0.679704690301987 and parameters: {'n_estimators': 995}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:14:49,853]\u001b[0m Trial 32 finished with value: 0.6795705142609403 and parameters: {'n_estimators': 940}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:17:16,016]\u001b[0m Trial 33 finished with value: 0.679584169332119 and parameters: {'n_estimators': 951}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:19:28,221]\u001b[0m Trial 34 finished with value: 0.6797017872380454 and parameters: {'n_estimators': 865}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:21:49,821]\u001b[0m Trial 35 finished with value: 0.6796958415515456 and parameters: {'n_estimators': 920}. Best is trial 11 with value: 0.6797474359538459.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:23:49,442]\u001b[0m Trial 36 finished with value: 0.6797576113523447 and parameters: {'n_estimators': 765}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:25:46,269]\u001b[0m Trial 37 finished with value: 0.6796791946178427 and parameters: {'n_estimators': 758}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:27:24,558]\u001b[0m Trial 38 finished with value: 0.6791694155715803 and parameters: {'n_estimators': 631}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:28:42,375]\u001b[0m Trial 39 finished with value: 0.6790480031172157 and parameters: {'n_estimators': 506}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:30:58,980]\u001b[0m Trial 40 finished with value: 0.679673380083818 and parameters: {'n_estimators': 880}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:33:23,992]\u001b[0m Trial 41 finished with value: 0.6795705142609403 and parameters: {'n_estimators': 940}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:35:34,885]\u001b[0m Trial 42 finished with value: 0.6796672495791551 and parameters: {'n_estimators': 851}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:37:54,674]\u001b[0m Trial 43 finished with value: 0.6796327111929357 and parameters: {'n_estimators': 908}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:40:21,727]\u001b[0m Trial 44 finished with value: 0.6796179403301515 and parameters: {'n_estimators': 956}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:42:08,642]\u001b[0m Trial 45 finished with value: 0.6796866100701625 and parameters: {'n_estimators': 686}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 11:44:03,760]\u001b[0m Trial 46 finished with value: 0.6796204092034388 and parameters: {'n_estimators': 744}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:46:37,976]\u001b[0m Trial 47 finished with value: 0.6797359785963604 and parameters: {'n_estimators': 999}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:48:54,800]\u001b[0m Trial 48 finished with value: 0.6796397050292091 and parameters: {'n_estimators': 896}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:51:03,621]\u001b[0m Trial 49 finished with value: 0.6797512513727371 and parameters: {'n_estimators': 837}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6798\n",
      "\tBest params:\n",
      "\t\tn_estimators: 765\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFRegressor\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.672213\n",
      "1                    TP  313.000000\n",
      "2                    TN  178.000000\n",
      "3                    FP   59.000000\n",
      "4                    FN   45.000000\n",
      "5              Accuracy    0.825210\n",
      "6             Precision    0.841398\n",
      "7           Sensitivity    0.874302\n",
      "8           Specificity    0.751100\n",
      "9              F1 score    0.857534\n",
      "10  F1 score (weighted)    0.824226\n",
      "11     F1 score (macro)    0.815724\n",
      "12    Balanced Accuracy    0.812678\n",
      "13                  MCC    0.632440\n",
      "14                  NPV    0.798200\n",
      "15              ROC_AUC    0.812678\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_rf_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_0_cat = np.where((y_pred_rf_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_rf_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 11:53:15,730]\u001b[0m Trial 50 finished with value: 0.6684998088132157 and parameters: {'n_estimators': 769}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:55:20,223]\u001b[0m Trial 51 finished with value: 0.6688355567235549 and parameters: {'n_estimators': 824}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:56:50,781]\u001b[0m Trial 52 finished with value: 0.6689286519897982 and parameters: {'n_estimators': 578}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 11:59:25,959]\u001b[0m Trial 53 finished with value: 0.6686924785754835 and parameters: {'n_estimators': 999}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:01:38,726]\u001b[0m Trial 54 finished with value: 0.668876967662482 and parameters: {'n_estimators': 852}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:03:22,330]\u001b[0m Trial 55 finished with value: 0.6689355416781942 and parameters: {'n_estimators': 675}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:05:52,351]\u001b[0m Trial 56 finished with value: 0.6688429254870555 and parameters: {'n_estimators': 970}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:08:12,683]\u001b[0m Trial 57 finished with value: 0.6688698705686481 and parameters: {'n_estimators': 916}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:10:40,937]\u001b[0m Trial 58 finished with value: 0.6688749312773731 and parameters: {'n_estimators': 959}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:11:33,225]\u001b[0m Trial 59 finished with value: 0.6687157614452257 and parameters: {'n_estimators': 333}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:13:36,894]\u001b[0m Trial 60 finished with value: 0.6686048316807021 and parameters: {'n_estimators': 802}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:16:06,432]\u001b[0m Trial 61 finished with value: 0.6688574078572297 and parameters: {'n_estimators': 974}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:18:22,634]\u001b[0m Trial 62 finished with value: 0.668928000753722 and parameters: {'n_estimators': 888}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:20:31,622]\u001b[0m Trial 63 finished with value: 0.6689223373302142 and parameters: {'n_estimators': 838}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:23:04,674]\u001b[0m Trial 64 finished with value: 0.6687074217591841 and parameters: {'n_estimators': 1000}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:24:56,084]\u001b[0m Trial 65 finished with value: 0.6686262951221086 and parameters: {'n_estimators': 721}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:27:18,303]\u001b[0m Trial 66 finished with value: 0.6688808242929691 and parameters: {'n_estimators': 934}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:29:46,217]\u001b[0m Trial 67 finished with value: 0.6688560469031168 and parameters: {'n_estimators': 968}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:32:01,128]\u001b[0m Trial 68 finished with value: 0.6689466200000398 and parameters: {'n_estimators': 879}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:34:23,311]\u001b[0m Trial 69 finished with value: 0.6688462691807121 and parameters: {'n_estimators': 919}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:36:25,153]\u001b[0m Trial 70 finished with value: 0.6685836578805845 and parameters: {'n_estimators': 781}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:38:53,593]\u001b[0m Trial 71 finished with value: 0.6688528690928501 and parameters: {'n_estimators': 977}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:41:18,354]\u001b[0m Trial 72 finished with value: 0.6689415309747304 and parameters: {'n_estimators': 947}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:43:50,794]\u001b[0m Trial 73 finished with value: 0.6686773310712801 and parameters: {'n_estimators': 996}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:46:10,926]\u001b[0m Trial 74 finished with value: 0.6689107367478546 and parameters: {'n_estimators': 908}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:48:24,649]\u001b[0m Trial 75 finished with value: 0.668924624290019 and parameters: {'n_estimators': 865}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:50:48,990]\u001b[0m Trial 76 finished with value: 0.6689004835968035 and parameters: {'n_estimators': 933}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:53:18,111]\u001b[0m Trial 77 finished with value: 0.6688722756090361 and parameters: {'n_estimators': 975}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:55:24,717]\u001b[0m Trial 78 finished with value: 0.66882993866847 and parameters: {'n_estimators': 832}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:56:53,499]\u001b[0m Trial 79 finished with value: 0.6689286519897983 and parameters: {'n_estimators': 578}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 12:59:12,123]\u001b[0m Trial 80 finished with value: 0.6688983867601841 and parameters: {'n_estimators': 897}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:01:46,559]\u001b[0m Trial 81 finished with value: 0.6687074217591841 and parameters: {'n_estimators': 1000}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:04:12,144]\u001b[0m Trial 82 finished with value: 0.6689270787990658 and parameters: {'n_estimators': 949}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:06:43,182]\u001b[0m Trial 83 finished with value: 0.6688566490971625 and parameters: {'n_estimators': 980}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:09:08,214]\u001b[0m Trial 84 finished with value: 0.6688524383169911 and parameters: {'n_estimators': 930}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:09:30,556]\u001b[0m Trial 85 finished with value: 0.6676878029948987 and parameters: {'n_estimators': 143}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:11:57,616]\u001b[0m Trial 86 finished with value: 0.6688573480073371 and parameters: {'n_estimators': 958}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:14:12,261]\u001b[0m Trial 87 finished with value: 0.6689151550817085 and parameters: {'n_estimators': 881}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:15:53,698]\u001b[0m Trial 88 finished with value: 0.6687848737412981 and parameters: {'n_estimators': 662}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:17:06,671]\u001b[0m Trial 89 finished with value: 0.6693595885216966 and parameters: {'n_estimators': 466}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:19:09,698]\u001b[0m Trial 90 finished with value: 0.668829991496757 and parameters: {'n_estimators': 812}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:21:41,925]\u001b[0m Trial 91 finished with value: 0.6688171398282635 and parameters: {'n_estimators': 985}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:24:10,378]\u001b[0m Trial 92 finished with value: 0.6688749312773731 and parameters: {'n_estimators': 959}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:26:33,068]\u001b[0m Trial 93 finished with value: 0.6688201879229397 and parameters: {'n_estimators': 920}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:29:03,891]\u001b[0m Trial 94 finished with value: 0.6687552454324613 and parameters: {'n_estimators': 988}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:31:16,263]\u001b[0m Trial 95 finished with value: 0.6689239110698024 and parameters: {'n_estimators': 862}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:33:42,658]\u001b[0m Trial 96 finished with value: 0.6689707247399128 and parameters: {'n_estimators': 943}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 13:35:39,235]\u001b[0m Trial 97 finished with value: 0.6685933877550347 and parameters: {'n_estimators': 748}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:37:58,857]\u001b[0m Trial 98 finished with value: 0.6689107367478547 and parameters: {'n_estimators': 908}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:40:29,282]\u001b[0m Trial 99 finished with value: 0.6688574078572296 and parameters: {'n_estimators': 974}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6798\n",
      "\tBest params:\n",
      "\t\tn_estimators: 765\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.672213    0.730588\n",
      "1                    TP  313.000000  336.000000\n",
      "2                    TN  178.000000  174.000000\n",
      "3                    FP   59.000000   44.000000\n",
      "4                    FN   45.000000   41.000000\n",
      "5              Accuracy    0.825210    0.857143\n",
      "6             Precision    0.841398    0.884211\n",
      "7           Sensitivity    0.874302    0.891247\n",
      "8           Specificity    0.751100    0.798200\n",
      "9              F1 score    0.857534    0.887715\n",
      "10  F1 score (weighted)    0.824226    0.856931\n",
      "11     F1 score (macro)    0.815724    0.845705\n",
      "12    Balanced Accuracy    0.812678    0.844706\n",
      "13                  MCC    0.632440    0.691459\n",
      "14                  NPV    0.798200    0.809300\n",
      "15              ROC_AUC    0.812678    0.844706\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_rf_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_1_cat = np.where((y_pred_rf_1 >= 6.6), 1, 0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_rf_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 13:41:21,164]\u001b[0m Trial 100 finished with value: 0.6781035258159951 and parameters: {'n_estimators': 238}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:43:36,106]\u001b[0m Trial 101 finished with value: 0.6792712560048747 and parameters: {'n_estimators': 855}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:46:06,889]\u001b[0m Trial 102 finished with value: 0.6794616698497874 and parameters: {'n_estimators': 960}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:48:44,416]\u001b[0m Trial 103 finished with value: 0.6794721839583381 and parameters: {'n_estimators': 998}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:51:08,991]\u001b[0m Trial 104 finished with value: 0.6793646537014343 and parameters: {'n_estimators': 924}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:53:26,493]\u001b[0m Trial 105 finished with value: 0.6792569522600447 and parameters: {'n_estimators': 879}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:55:28,355]\u001b[0m Trial 106 finished with value: 0.6793402054803642 and parameters: {'n_estimators': 780}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 13:57:48,598]\u001b[0m Trial 107 finished with value: 0.6793530942006047 and parameters: {'n_estimators': 896}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:00:02,083]\u001b[0m Trial 108 finished with value: 0.6792443801973411 and parameters: {'n_estimators': 840}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:01:54,518]\u001b[0m Trial 109 finished with value: 0.679069566276117 and parameters: {'n_estimators': 704}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:04:24,074]\u001b[0m Trial 110 finished with value: 0.6794534835380688 and parameters: {'n_estimators': 944}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:06:59,474]\u001b[0m Trial 111 finished with value: 0.6795131700324608 and parameters: {'n_estimators': 983}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:09:09,998]\u001b[0m Trial 112 finished with value: 0.6792898152961396 and parameters: {'n_estimators': 813}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:11:27,954]\u001b[0m Trial 113 finished with value: 0.6792107203852618 and parameters: {'n_estimators': 875}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:14:01,302]\u001b[0m Trial 114 finished with value: 0.6794932713605396 and parameters: {'n_estimators': 962}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:16:26,098]\u001b[0m Trial 115 finished with value: 0.6794350478315757 and parameters: {'n_estimators': 906}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:18:32,203]\u001b[0m Trial 116 finished with value: 0.6793104117352142 and parameters: {'n_estimators': 794}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:21:01,035]\u001b[0m Trial 117 finished with value: 0.6794645532857945 and parameters: {'n_estimators': 942}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:23:37,364]\u001b[0m Trial 118 finished with value: 0.6795458067984262 and parameters: {'n_estimators': 981}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:26:16,420]\u001b[0m Trial 119 finished with value: 0.6794837983074121 and parameters: {'n_estimators': 999}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:28:31,596]\u001b[0m Trial 120 finished with value: 0.679245744316208 and parameters: {'n_estimators': 854}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:30:59,137]\u001b[0m Trial 121 finished with value: 0.6793566558915787 and parameters: {'n_estimators': 923}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:33:32,988]\u001b[0m Trial 122 finished with value: 0.6795042681920789 and parameters: {'n_estimators': 964}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:35:43,505]\u001b[0m Trial 123 finished with value: 0.6793619813778362 and parameters: {'n_estimators': 824}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:38:05,757]\u001b[0m Trial 124 finished with value: 0.6793072409745292 and parameters: {'n_estimators': 892}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:40:35,752]\u001b[0m Trial 125 finished with value: 0.6793211587553396 and parameters: {'n_estimators': 931}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:43:13,553]\u001b[0m Trial 126 finished with value: 0.6795458067984262 and parameters: {'n_estimators': 981}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:45:44,319]\u001b[0m Trial 127 finished with value: 0.6794135740724381 and parameters: {'n_estimators': 951}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:48:01,926]\u001b[0m Trial 128 finished with value: 0.6791740570449866 and parameters: {'n_estimators': 867}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:50:26,056]\u001b[0m Trial 129 finished with value: 0.6794405582692515 and parameters: {'n_estimators': 907}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:53:05,551]\u001b[0m Trial 130 finished with value: 0.6794766987833434 and parameters: {'n_estimators': 1000}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:55:38,602]\u001b[0m Trial 131 finished with value: 0.6794616698497874 and parameters: {'n_estimators': 960}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 14:58:12,421]\u001b[0m Trial 132 finished with value: 0.6795458874150908 and parameters: {'n_estimators': 973}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:00:40,400]\u001b[0m Trial 133 finished with value: 0.6794213268640603 and parameters: {'n_estimators': 936}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:03:18,688]\u001b[0m Trial 134 finished with value: 0.679522482073205 and parameters: {'n_estimators': 985}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:05:45,490]\u001b[0m Trial 135 finished with value: 0.6793475478261427 and parameters: {'n_estimators': 925}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:07:59,187]\u001b[0m Trial 136 finished with value: 0.6792656904102149 and parameters: {'n_estimators': 842}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:09:01,836]\u001b[0m Trial 137 finished with value: 0.6786662447215179 and parameters: {'n_estimators': 388}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:11:32,267]\u001b[0m Trial 138 finished with value: 0.6794066971881523 and parameters: {'n_estimators': 950}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:14:06,475]\u001b[0m Trial 139 finished with value: 0.6795035981749558 and parameters: {'n_estimators': 969}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:16:31,650]\u001b[0m Trial 140 finished with value: 0.6794233203170382 and parameters: {'n_estimators': 914}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:18:05,787]\u001b[0m Trial 141 finished with value: 0.6792388134207457 and parameters: {'n_estimators': 594}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:20:09,028]\u001b[0m Trial 142 finished with value: 0.679288692259708 and parameters: {'n_estimators': 770}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:21:54,992]\u001b[0m Trial 143 finished with value: 0.6792119396250066 and parameters: {'n_estimators': 674}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:23:51,938]\u001b[0m Trial 144 finished with value: 0.6789651723207267 and parameters: {'n_estimators': 729}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:26:29,288]\u001b[0m Trial 145 finished with value: 0.6794884356966258 and parameters: {'n_estimators': 986}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 15:28:39,838]\u001b[0m Trial 146 finished with value: 0.6792901132424142 and parameters: {'n_estimators': 809}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:31:02,500]\u001b[0m Trial 147 finished with value: 0.6793523035682376 and parameters: {'n_estimators': 887}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:33:36,291]\u001b[0m Trial 148 finished with value: 0.6795031260504559 and parameters: {'n_estimators': 966}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:36:07,983]\u001b[0m Trial 149 finished with value: 0.6794308422644677 and parameters: {'n_estimators': 946}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6798\n",
      "\tBest params:\n",
      "\t\tn_estimators: 765\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.672213    0.730588    0.709249\n",
      "1                    TP  313.000000  336.000000  319.000000\n",
      "2                    TN  178.000000  174.000000  192.000000\n",
      "3                    FP   59.000000   44.000000   45.000000\n",
      "4                    FN   45.000000   41.000000   39.000000\n",
      "5              Accuracy    0.825210    0.857143    0.858824\n",
      "6             Precision    0.841398    0.884211    0.876374\n",
      "7           Sensitivity    0.874302    0.891247    0.891061\n",
      "8           Specificity    0.751100    0.798200    0.810100\n",
      "9              F1 score    0.857534    0.887715    0.883657\n",
      "10  F1 score (weighted)    0.824226    0.856931    0.858505\n",
      "11     F1 score (macro)    0.815724    0.845705    0.852085\n",
      "12    Balanced Accuracy    0.812678    0.844706    0.850594\n",
      "13                  MCC    0.632440    0.691459    0.704358\n",
      "14                  NPV    0.798200    0.809300    0.831200\n",
      "15              ROC_AUC    0.812678    0.844706    0.850594\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_rf_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_2_cat = np.where((y_pred_rf_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_rf_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 15:38:03,037]\u001b[0m Trial 150 finished with value: 0.669809370020517 and parameters: {'n_estimators': 649}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:40:02,480]\u001b[0m Trial 151 finished with value: 0.6700734831398043 and parameters: {'n_estimators': 764}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:41:57,515]\u001b[0m Trial 152 finished with value: 0.6701685431628259 and parameters: {'n_estimators': 746}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:44:07,038]\u001b[0m Trial 153 finished with value: 0.6701164858670257 and parameters: {'n_estimators': 828}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:46:42,345]\u001b[0m Trial 154 finished with value: 0.6703037563353857 and parameters: {'n_estimators': 1000}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:48:35,730]\u001b[0m Trial 155 finished with value: 0.6703053114924886 and parameters: {'n_estimators': 721}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:50:24,503]\u001b[0m Trial 156 finished with value: 0.670154925366786 and parameters: {'n_estimators': 689}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:52:41,746]\u001b[0m Trial 157 finished with value: 0.6701928828243149 and parameters: {'n_estimators': 864}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:55:14,716]\u001b[0m Trial 158 finished with value: 0.670196214689752 and parameters: {'n_estimators': 977}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:56:28,969]\u001b[0m Trial 159 finished with value: 0.6702579803317723 and parameters: {'n_estimators': 531}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 15:58:19,673]\u001b[0m Trial 160 finished with value: 0.6700770325477425 and parameters: {'n_estimators': 796}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:00:18,192]\u001b[0m Trial 161 finished with value: 0.6700394029553272 and parameters: {'n_estimators': 847}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:02:21,781]\u001b[0m Trial 162 finished with value: 0.6701628344488204 and parameters: {'n_estimators': 891}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:04:24,939]\u001b[0m Trial 163 finished with value: 0.6700979061788083 and parameters: {'n_estimators': 874}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:06:37,320]\u001b[0m Trial 164 finished with value: 0.6702062969016331 and parameters: {'n_estimators': 961}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:08:02,707]\u001b[0m Trial 165 finished with value: 0.6701441012179868 and parameters: {'n_estimators': 619}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:10:13,393]\u001b[0m Trial 166 finished with value: 0.6702855707156232 and parameters: {'n_estimators': 934}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:12:33,434]\u001b[0m Trial 167 finished with value: 0.6703673601697447 and parameters: {'n_estimators': 998}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:14:40,905]\u001b[0m Trial 168 finished with value: 0.6702482107485807 and parameters: {'n_estimators': 910}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:16:34,818]\u001b[0m Trial 169 finished with value: 0.6700885086708748 and parameters: {'n_estimators': 822}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:18:50,517]\u001b[0m Trial 170 finished with value: 0.6702162363061422 and parameters: {'n_estimators': 981}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:20:48,849]\u001b[0m Trial 171 finished with value: 0.6700559135841768 and parameters: {'n_estimators': 850}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:22:46,118]\u001b[0m Trial 172 finished with value: 0.6700740570272832 and parameters: {'n_estimators': 846}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:24:45,126]\u001b[0m Trial 173 finished with value: 0.6701367984891695 and parameters: {'n_estimators': 865}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:26:46,606]\u001b[0m Trial 174 finished with value: 0.6700470202608249 and parameters: {'n_estimators': 884}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:28:57,599]\u001b[0m Trial 175 finished with value: 0.67032762652099 and parameters: {'n_estimators': 948}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:31:13,364]\u001b[0m Trial 176 finished with value: 0.6702838491383067 and parameters: {'n_estimators': 984}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:31:37,971]\u001b[0m Trial 177 finished with value: 0.6673468139923521 and parameters: {'n_estimators': 171}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:33:41,878]\u001b[0m Trial 178 finished with value: 0.6701816251959709 and parameters: {'n_estimators': 900}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:35:32,091]\u001b[0m Trial 179 finished with value: 0.6701041322179732 and parameters: {'n_estimators': 803}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:37:17,620]\u001b[0m Trial 180 finished with value: 0.6700998630051351 and parameters: {'n_estimators': 754}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:39:11,050]\u001b[0m Trial 181 finished with value: 0.6701039901321616 and parameters: {'n_estimators': 820}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:41:04,593]\u001b[0m Trial 182 finished with value: 0.670135613525842 and parameters: {'n_estimators': 831}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:42:51,444]\u001b[0m Trial 183 finished with value: 0.6700919916508693 and parameters: {'n_estimators': 785}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:45:08,419]\u001b[0m Trial 184 finished with value: 0.6703037563353855 and parameters: {'n_estimators': 1000}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:47:06,520]\u001b[0m Trial 185 finished with value: 0.6701993424297626 and parameters: {'n_estimators': 862}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:49:18,179]\u001b[0m Trial 186 finished with value: 0.6702483917574871 and parameters: {'n_estimators': 968}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:51:13,850]\u001b[0m Trial 187 finished with value: 0.6701343391315601 and parameters: {'n_estimators': 836}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:53:14,948]\u001b[0m Trial 188 finished with value: 0.670047020260825 and parameters: {'n_estimators': 884}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:55:22,459]\u001b[0m Trial 189 finished with value: 0.670277803234679 and parameters: {'n_estimators': 926}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:57:34,385]\u001b[0m Trial 190 finished with value: 0.6702483917574871 and parameters: {'n_estimators': 968}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 16:59:30,711]\u001b[0m Trial 191 finished with value: 0.6700635725308499 and parameters: {'n_estimators': 851}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:01:17,048]\u001b[0m Trial 192 finished with value: 0.6700885567204552 and parameters: {'n_estimators': 781}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:03:08,625]\u001b[0m Trial 193 finished with value: 0.6701471093276735 and parameters: {'n_estimators': 811}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:05:08,119]\u001b[0m Trial 194 finished with value: 0.6701351888449057 and parameters: {'n_estimators': 871}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:07:00,343]\u001b[0m Trial 195 finished with value: 0.6700995662202078 and parameters: {'n_estimators': 827}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 17:09:12,332]\u001b[0m Trial 196 finished with value: 0.6702162363061422 and parameters: {'n_estimators': 981}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:11:21,899]\u001b[0m Trial 197 finished with value: 0.67019761936223 and parameters: {'n_estimators': 954}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:13:17,951]\u001b[0m Trial 198 finished with value: 0.6701125512335588 and parameters: {'n_estimators': 841}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:15:36,047]\u001b[0m Trial 199 finished with value: 0.6703619816536939 and parameters: {'n_estimators': 999}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6798\n",
      "\tBest params:\n",
      "\t\tn_estimators: 765\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.672213    0.730588    0.709249    0.684037\n",
      "1                    TP  313.000000  336.000000  319.000000  325.000000\n",
      "2                    TN  178.000000  174.000000  192.000000  181.000000\n",
      "3                    FP   59.000000   44.000000   45.000000   50.000000\n",
      "4                    FN   45.000000   41.000000   39.000000   39.000000\n",
      "5              Accuracy    0.825210    0.857143    0.858824    0.850420\n",
      "6             Precision    0.841398    0.884211    0.876374    0.866667\n",
      "7           Sensitivity    0.874302    0.891247    0.891061    0.892857\n",
      "8           Specificity    0.751100    0.798200    0.810100    0.783500\n",
      "9              F1 score    0.857534    0.887715    0.883657    0.879567\n",
      "10  F1 score (weighted)    0.824226    0.856931    0.858505    0.849709\n",
      "11     F1 score (macro)    0.815724    0.845705    0.852085    0.841114\n",
      "12    Balanced Accuracy    0.812678    0.844706    0.850594    0.838203\n",
      "13                  MCC    0.632440    0.691459    0.704358    0.682870\n",
      "14                  NPV    0.798200    0.809300    0.831200    0.822700\n",
      "15              ROC_AUC    0.812678    0.844706    0.850594    0.838203\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_rf_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_3_cat = np.where((y_pred_rf_3 >= 6.6) , 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_rf_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 17:17:37,143]\u001b[0m Trial 200 finished with value: 0.6754595396025881 and parameters: {'n_estimators': 800}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:19:38,642]\u001b[0m Trial 201 finished with value: 0.6753028582236743 and parameters: {'n_estimators': 912}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:21:46,610]\u001b[0m Trial 202 finished with value: 0.6752905187103371 and parameters: {'n_estimators': 984}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:23:39,223]\u001b[0m Trial 203 finished with value: 0.6754453197549676 and parameters: {'n_estimators': 863}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:25:10,537]\u001b[0m Trial 204 finished with value: 0.6754510957332923 and parameters: {'n_estimators': 701}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:27:06,351]\u001b[0m Trial 205 finished with value: 0.6754323131787213 and parameters: {'n_estimators': 893}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:29:08,036]\u001b[0m Trial 206 finished with value: 0.6752881792780148 and parameters: {'n_estimators': 939}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:31:12,542]\u001b[0m Trial 207 finished with value: 0.675406890904414 and parameters: {'n_estimators': 961}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:32:59,344]\u001b[0m Trial 208 finished with value: 0.6754556982243812 and parameters: {'n_estimators': 872}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:34:59,207]\u001b[0m Trial 209 finished with value: 0.6752621034970142 and parameters: {'n_estimators': 976}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:36:43,698]\u001b[0m Trial 210 finished with value: 0.6754056528401928 and parameters: {'n_estimators': 850}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:38:13,254]\u001b[0m Trial 211 finished with value: 0.6756034171842173 and parameters: {'n_estimators': 731}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:40:15,442]\u001b[0m Trial 212 finished with value: 0.6752820228857622 and parameters: {'n_estimators': 998}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:41:39,421]\u001b[0m Trial 213 finished with value: 0.6753351947576848 and parameters: {'n_estimators': 685}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:42:54,072]\u001b[0m Trial 214 finished with value: 0.6753857755617507 and parameters: {'n_estimators': 609}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:44:34,887]\u001b[0m Trial 215 finished with value: 0.6755241974658233 and parameters: {'n_estimators': 818}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:46:02,218]\u001b[0m Trial 216 finished with value: 0.6755792031722644 and parameters: {'n_estimators': 711}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:47:59,592]\u001b[0m Trial 217 finished with value: 0.6754429097728607 and parameters: {'n_estimators': 956}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:49:52,548]\u001b[0m Trial 218 finished with value: 0.6752548675578132 and parameters: {'n_estimators': 920}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:51:28,044]\u001b[0m Trial 219 finished with value: 0.6755858565401063 and parameters: {'n_estimators': 777}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:52:46,278]\u001b[0m Trial 220 finished with value: 0.6755178864267184 and parameters: {'n_estimators': 637}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:54:14,757]\u001b[0m Trial 221 finished with value: 0.6755534704765896 and parameters: {'n_estimators': 722}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:55:48,480]\u001b[0m Trial 222 finished with value: 0.6756205036500003 and parameters: {'n_estimators': 764}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:57:19,874]\u001b[0m Trial 223 finished with value: 0.6756786250690014 and parameters: {'n_estimators': 743}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 17:58:47,054]\u001b[0m Trial 224 finished with value: 0.675462271536983 and parameters: {'n_estimators': 704}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:00:49,405]\u001b[0m Trial 225 finished with value: 0.675290518710337 and parameters: {'n_estimators': 984}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:02:32,776]\u001b[0m Trial 226 finished with value: 0.6755823396174568 and parameters: {'n_estimators': 836}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:03:56,607]\u001b[0m Trial 227 finished with value: 0.6753064884526581 and parameters: {'n_estimators': 678}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:05:56,720]\u001b[0m Trial 228 finished with value: 0.6752988662097187 and parameters: {'n_estimators': 973}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:07:45,871]\u001b[0m Trial 229 finished with value: 0.6754589235951368 and parameters: {'n_estimators': 883}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:09:06,786]\u001b[0m Trial 230 finished with value: 0.6755647417791093 and parameters: {'n_estimators': 652}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:10:57,598]\u001b[0m Trial 231 finished with value: 0.6754037509396325 and parameters: {'n_estimators': 896}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:12:45,054]\u001b[0m Trial 232 finished with value: 0.6754555591293436 and parameters: {'n_estimators': 875}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:14:36,663]\u001b[0m Trial 233 finished with value: 0.675318993709517 and parameters: {'n_estimators': 904}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:16:21,895]\u001b[0m Trial 234 finished with value: 0.6753999782891213 and parameters: {'n_estimators': 857}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:18:23,174]\u001b[0m Trial 235 finished with value: 0.6752962645379472 and parameters: {'n_estimators': 985}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:20:18,831]\u001b[0m Trial 236 finished with value: 0.6752881792780147 and parameters: {'n_estimators': 939}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:21:44,082]\u001b[0m Trial 237 finished with value: 0.6753449462179567 and parameters: {'n_estimators': 693}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:23:06,167]\u001b[0m Trial 238 finished with value: 0.6754016749955685 and parameters: {'n_estimators': 666}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:24:39,743]\u001b[0m Trial 239 finished with value: 0.675666105820475 and parameters: {'n_estimators': 758}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:26:42,670]\u001b[0m Trial 240 finished with value: 0.6752909126240889 and parameters: {'n_estimators': 996}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:28:32,517]\u001b[0m Trial 241 finished with value: 0.6754323131787212 and parameters: {'n_estimators': 893}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:30:19,899]\u001b[0m Trial 242 finished with value: 0.6754195070715834 and parameters: {'n_estimators': 871}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:32:11,361]\u001b[0m Trial 243 finished with value: 0.6752965776673584 and parameters: {'n_estimators': 911}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:33:55,645]\u001b[0m Trial 244 finished with value: 0.675444053352128 and parameters: {'n_estimators': 852}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:35:55,147]\u001b[0m Trial 245 finished with value: 0.6753669554404271 and parameters: {'n_estimators': 966}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 18:37:59,355]\u001b[0m Trial 246 finished with value: 0.675251709373301 and parameters: {'n_estimators': 1000}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:39:54,935]\u001b[0m Trial 247 finished with value: 0.6752270480282672 and parameters: {'n_estimators': 926}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:41:38,396]\u001b[0m Trial 248 finished with value: 0.6756014111184688 and parameters: {'n_estimators': 834}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:43:27,493]\u001b[0m Trial 249 finished with value: 0.6754304147925477 and parameters: {'n_estimators': 881}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6798\n",
      "\tBest params:\n",
      "\t\tn_estimators: 765\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77894dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.672213    0.730588    0.709249    0.684037   \n",
      "1                    TP  313.000000  336.000000  319.000000  325.000000   \n",
      "2                    TN  178.000000  174.000000  192.000000  181.000000   \n",
      "3                    FP   59.000000   44.000000   45.000000   50.000000   \n",
      "4                    FN   45.000000   41.000000   39.000000   39.000000   \n",
      "5              Accuracy    0.825210    0.857143    0.858824    0.850420   \n",
      "6             Precision    0.841398    0.884211    0.876374    0.866667   \n",
      "7           Sensitivity    0.874302    0.891247    0.891061    0.892857   \n",
      "8           Specificity    0.751100    0.798200    0.810100    0.783500   \n",
      "9              F1 score    0.857534    0.887715    0.883657    0.879567   \n",
      "10  F1 score (weighted)    0.824226    0.856931    0.858505    0.849709   \n",
      "11     F1 score (macro)    0.815724    0.845705    0.852085    0.841114   \n",
      "12    Balanced Accuracy    0.812678    0.844706    0.850594    0.838203   \n",
      "13                  MCC    0.632440    0.691459    0.704358    0.682870   \n",
      "14                  NPV    0.798200    0.809300    0.831200    0.822700   \n",
      "15              ROC_AUC    0.812678    0.844706    0.850594    0.838203   \n",
      "\n",
      "          Set4  \n",
      "0     0.706921  \n",
      "1   328.000000  \n",
      "2   181.000000  \n",
      "3    49.000000  \n",
      "4    37.000000  \n",
      "5     0.855462  \n",
      "6     0.870027  \n",
      "7     0.898630  \n",
      "8     0.787000  \n",
      "9     0.884097  \n",
      "10    0.854695  \n",
      "11    0.846066  \n",
      "12    0.842793  \n",
      "13    0.692905  \n",
      "14    0.830300  \n",
      "15    0.842793  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_rf_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_4_cat = np.where((y_pred_rf_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_rf_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 18:45:38,562]\u001b[0m Trial 250 finished with value: 0.664757730083468 and parameters: {'n_estimators': 953}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:47:28,050]\u001b[0m Trial 251 finished with value: 0.664641259077007 and parameters: {'n_estimators': 863}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:49:22,321]\u001b[0m Trial 252 finished with value: 0.6646778932133944 and parameters: {'n_estimators': 898}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:50:03,371]\u001b[0m Trial 253 finished with value: 0.6640853375859901 and parameters: {'n_estimators': 320}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:51:44,161]\u001b[0m Trial 254 finished with value: 0.6645412509984467 and parameters: {'n_estimators': 796}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:53:46,888]\u001b[0m Trial 255 finished with value: 0.6648469466528262 and parameters: {'n_estimators': 970}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:55:20,215]\u001b[0m Trial 256 finished with value: 0.6643751522095178 and parameters: {'n_estimators': 736}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:57:24,317]\u001b[0m Trial 257 finished with value: 0.6648932112853573 and parameters: {'n_estimators': 983}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 18:58:26,009]\u001b[0m Trial 258 finished with value: 0.6639666744781532 and parameters: {'n_estimators': 484}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:00:13,124]\u001b[0m Trial 259 finished with value: 0.6645563287690897 and parameters: {'n_estimators': 847}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:01:43,798]\u001b[0m Trial 260 finished with value: 0.6643221439818144 and parameters: {'n_estimators': 716}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:03:42,790]\u001b[0m Trial 261 finished with value: 0.6648222333893633 and parameters: {'n_estimators': 937}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:05:26,217]\u001b[0m Trial 262 finished with value: 0.6644707396404442 and parameters: {'n_estimators': 817}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:07:22,038]\u001b[0m Trial 263 finished with value: 0.6648491746096825 and parameters: {'n_estimators': 917}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:08:33,956]\u001b[0m Trial 264 finished with value: 0.6643150203899117 and parameters: {'n_estimators': 566}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:08:49,390]\u001b[0m Trial 265 finished with value: 0.6621658823236509 and parameters: {'n_estimators': 117}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:10:41,746]\u001b[0m Trial 266 finished with value: 0.6646749809086129 and parameters: {'n_estimators': 887}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:12:30,981]\u001b[0m Trial 267 finished with value: 0.6646190283276207 and parameters: {'n_estimators': 864}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:14:35,229]\u001b[0m Trial 268 finished with value: 0.6648892397348 and parameters: {'n_estimators': 984}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:16:41,682]\u001b[0m Trial 269 finished with value: 0.6649355860440506 and parameters: {'n_estimators': 1000}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:18:42,586]\u001b[0m Trial 270 finished with value: 0.6647564568090101 and parameters: {'n_estimators': 955}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:20:27,802]\u001b[0m Trial 271 finished with value: 0.66449460649036 and parameters: {'n_estimators': 832}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:22:30,837]\u001b[0m Trial 272 finished with value: 0.6648469466528261 and parameters: {'n_estimators': 970}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:24:21,690]\u001b[0m Trial 273 finished with value: 0.6646264297583461 and parameters: {'n_estimators': 878}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:26:16,149]\u001b[0m Trial 274 finished with value: 0.6647675631086856 and parameters: {'n_estimators': 909}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:28:15,327]\u001b[0m Trial 275 finished with value: 0.6648324142095807 and parameters: {'n_estimators': 945}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:29:57,915]\u001b[0m Trial 276 finished with value: 0.6644364125065074 and parameters: {'n_estimators': 809}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:31:46,267]\u001b[0m Trial 277 finished with value: 0.664667788094653 and parameters: {'n_estimators': 859}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:33:52,273]\u001b[0m Trial 278 finished with value: 0.6649355860440506 and parameters: {'n_estimators': 1000}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:35:55,172]\u001b[0m Trial 279 finished with value: 0.6649440711904193 and parameters: {'n_estimators': 974}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:37:41,656]\u001b[0m Trial 280 finished with value: 0.6645381493922414 and parameters: {'n_estimators': 842}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:39:34,577]\u001b[0m Trial 281 finished with value: 0.6646884261559866 and parameters: {'n_estimators': 895}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:41:30,940]\u001b[0m Trial 282 finished with value: 0.664783291603951 and parameters: {'n_estimators': 924}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:42:59,377]\u001b[0m Trial 283 finished with value: 0.664303938734612 and parameters: {'n_estimators': 701}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:44:39,445]\u001b[0m Trial 284 finished with value: 0.6645218727756517 and parameters: {'n_estimators': 795}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:46:16,911]\u001b[0m Trial 285 finished with value: 0.6644277266375145 and parameters: {'n_estimators': 776}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:48:20,557]\u001b[0m Trial 286 finished with value: 0.6648892397348 and parameters: {'n_estimators': 984}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:49:54,390]\u001b[0m Trial 287 finished with value: 0.6644325639061944 and parameters: {'n_estimators': 745}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:51:55,264]\u001b[0m Trial 288 finished with value: 0.6647711924194486 and parameters: {'n_estimators': 961}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:53:45,729]\u001b[0m Trial 289 finished with value: 0.6646700874535374 and parameters: {'n_estimators': 875}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:55:43,523]\u001b[0m Trial 290 finished with value: 0.6647927275534459 and parameters: {'n_estimators': 936}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:57:27,879]\u001b[0m Trial 291 finished with value: 0.6644611870984656 and parameters: {'n_estimators': 826}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 19:59:21,653]\u001b[0m Trial 292 finished with value: 0.6646761124140115 and parameters: {'n_estimators': 899}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:00:47,263]\u001b[0m Trial 293 finished with value: 0.6644593218197102 and parameters: {'n_estimators': 676}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:02:35,131]\u001b[0m Trial 294 finished with value: 0.6645897822728835 and parameters: {'n_estimators': 854}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:04:39,336]\u001b[0m Trial 295 finished with value: 0.6648957684157983 and parameters: {'n_estimators': 985}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 20:06:41,096]\u001b[0m Trial 296 finished with value: 0.6647889326990073 and parameters: {'n_estimators': 965}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:08:13,225]\u001b[0m Trial 297 finished with value: 0.6643460778270376 and parameters: {'n_estimators': 732}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:10:04,353]\u001b[0m Trial 298 finished with value: 0.6646572494304703 and parameters: {'n_estimators': 883}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:11:59,568]\u001b[0m Trial 299 finished with value: 0.6648491746096824 and parameters: {'n_estimators': 917}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6798\n",
      "\tBest params:\n",
      "\t\tn_estimators: 765\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.672213    0.730588    0.709249    0.684037   \n",
      "1                    TP  313.000000  336.000000  319.000000  325.000000   \n",
      "2                    TN  178.000000  174.000000  192.000000  181.000000   \n",
      "3                    FP   59.000000   44.000000   45.000000   50.000000   \n",
      "4                    FN   45.000000   41.000000   39.000000   39.000000   \n",
      "5              Accuracy    0.825210    0.857143    0.858824    0.850420   \n",
      "6             Precision    0.841398    0.884211    0.876374    0.866667   \n",
      "7           Sensitivity    0.874302    0.891247    0.891061    0.892857   \n",
      "8           Specificity    0.751100    0.798200    0.810100    0.783500   \n",
      "9              F1 score    0.857534    0.887715    0.883657    0.879567   \n",
      "10  F1 score (weighted)    0.824226    0.856931    0.858505    0.849709   \n",
      "11     F1 score (macro)    0.815724    0.845705    0.852085    0.841114   \n",
      "12    Balanced Accuracy    0.812678    0.844706    0.850594    0.838203   \n",
      "13                  MCC    0.632440    0.691459    0.704358    0.682870   \n",
      "14                  NPV    0.798200    0.809300    0.831200    0.822700   \n",
      "15              ROC_AUC    0.812678    0.844706    0.850594    0.838203   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.706921    0.706242  \n",
      "1   328.000000  336.000000  \n",
      "2   181.000000  181.000000  \n",
      "3    49.000000   41.000000  \n",
      "4    37.000000   37.000000  \n",
      "5     0.855462    0.868908  \n",
      "6     0.870027    0.891247  \n",
      "7     0.898630    0.900804  \n",
      "8     0.787000    0.815300  \n",
      "9     0.884097    0.896000  \n",
      "10    0.854695    0.868661  \n",
      "11    0.846066    0.859364  \n",
      "12    0.842793    0.858060  \n",
      "13    0.692905    0.718816  \n",
      "14    0.830300    0.830300  \n",
      "15    0.842793    0.858060  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_rf_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_5_cat = np.where((y_pred_rf_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_rf_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 20:13:56,120]\u001b[0m Trial 300 finished with value: 0.6792641951101225 and parameters: {'n_estimators': 842}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:15:54,614]\u001b[0m Trial 301 finished with value: 0.6793899240590666 and parameters: {'n_estimators': 948}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:17:58,991]\u001b[0m Trial 302 finished with value: 0.6796237187013292 and parameters: {'n_estimators': 985}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:19:45,803]\u001b[0m Trial 303 finished with value: 0.6792566818380027 and parameters: {'n_estimators': 865}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:21:51,762]\u001b[0m Trial 304 finished with value: 0.6796368403653309 and parameters: {'n_estimators': 999}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:23:33,746]\u001b[0m Trial 305 finished with value: 0.6791881281670182 and parameters: {'n_estimators': 820}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:25:10,398]\u001b[0m Trial 306 finished with value: 0.6787041236368874 and parameters: {'n_estimators': 765}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:27:12,623]\u001b[0m Trial 307 finished with value: 0.6795887728440779 and parameters: {'n_estimators': 969}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:28:35,082]\u001b[0m Trial 308 finished with value: 0.6782630796284526 and parameters: {'n_estimators': 653}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:30:04,422]\u001b[0m Trial 309 finished with value: 0.678479546855695 and parameters: {'n_estimators': 714}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:32:01,483]\u001b[0m Trial 310 finished with value: 0.6794053286597144 and parameters: {'n_estimators': 933}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:33:51,122]\u001b[0m Trial 311 finished with value: 0.679311365694034 and parameters: {'n_estimators': 890}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:35:37,158]\u001b[0m Trial 312 finished with value: 0.6792695935399193 and parameters: {'n_estimators': 856}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:37:29,503]\u001b[0m Trial 313 finished with value: 0.6794293501555315 and parameters: {'n_estimators': 904}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:38:56,793]\u001b[0m Trial 314 finished with value: 0.6783017113039875 and parameters: {'n_estimators': 690}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:40:57,262]\u001b[0m Trial 315 finished with value: 0.6795181018993144 and parameters: {'n_estimators': 957}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:42:40,207]\u001b[0m Trial 316 finished with value: 0.6792715878893836 and parameters: {'n_estimators': 835}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:44:43,697]\u001b[0m Trial 317 finished with value: 0.6796566192504694 and parameters: {'n_estimators': 1000}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:46:46,163]\u001b[0m Trial 318 finished with value: 0.67971737561141 and parameters: {'n_estimators': 981}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:48:48,412]\u001b[0m Trial 319 finished with value: 0.6796341385830715 and parameters: {'n_estimators': 984}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:50:48,368]\u001b[0m Trial 320 finished with value: 0.6796700489506675 and parameters: {'n_estimators': 976}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:52:53,649]\u001b[0m Trial 321 finished with value: 0.6796566192504694 and parameters: {'n_estimators': 1000}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:54:54,487]\u001b[0m Trial 322 finished with value: 0.6796700489506675 and parameters: {'n_estimators': 976}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:56:55,178]\u001b[0m Trial 323 finished with value: 0.6795861659983545 and parameters: {'n_estimators': 970}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 20:58:54,761]\u001b[0m Trial 324 finished with value: 0.6796536905849975 and parameters: {'n_estimators': 975}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:00:52,441]\u001b[0m Trial 325 finished with value: 0.6794177787733149 and parameters: {'n_estimators': 944}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:02:49,625]\u001b[0m Trial 326 finished with value: 0.679461176665772 and parameters: {'n_estimators': 954}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:04:50,814]\u001b[0m Trial 327 finished with value: 0.6797089720937728 and parameters: {'n_estimators': 979}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:06:51,431]\u001b[0m Trial 328 finished with value: 0.6795820528459131 and parameters: {'n_estimators': 971}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:08:55,281]\u001b[0m Trial 329 finished with value: 0.6796371035992218 and parameters: {'n_estimators': 986}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:10:55,190]\u001b[0m Trial 330 finished with value: 0.6795115177291154 and parameters: {'n_estimators': 956}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:12:56,362]\u001b[0m Trial 331 finished with value: 0.6797237530552065 and parameters: {'n_estimators': 980}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:15:00,007]\u001b[0m Trial 332 finished with value: 0.6796341385830715 and parameters: {'n_estimators': 984}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:16:59,863]\u001b[0m Trial 333 finished with value: 0.6796310416913681 and parameters: {'n_estimators': 974}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:19:00,285]\u001b[0m Trial 334 finished with value: 0.6795806751183437 and parameters: {'n_estimators': 963}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:21:01,746]\u001b[0m Trial 335 finished with value: 0.6796237187013293 and parameters: {'n_estimators': 985}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:23:02,774]\u001b[0m Trial 336 finished with value: 0.6796341385830715 and parameters: {'n_estimators': 984}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:25:02,711]\u001b[0m Trial 337 finished with value: 0.6795695822731196 and parameters: {'n_estimators': 962}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:25:56,173]\u001b[0m Trial 338 finished with value: 0.6784142252690357 and parameters: {'n_estimators': 421}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:28:00,934]\u001b[0m Trial 339 finished with value: 0.6796313111678461 and parameters: {'n_estimators': 996}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:29:58,467]\u001b[0m Trial 340 finished with value: 0.6794533115967641 and parameters: {'n_estimators': 943}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:32:03,757]\u001b[0m Trial 341 finished with value: 0.6796566192504694 and parameters: {'n_estimators': 1000}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:34:04,982]\u001b[0m Trial 342 finished with value: 0.6795899993918043 and parameters: {'n_estimators': 972}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:36:06,819]\u001b[0m Trial 343 finished with value: 0.6796536905849975 and parameters: {'n_estimators': 975}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:38:05,340]\u001b[0m Trial 344 finished with value: 0.6794486617275027 and parameters: {'n_estimators': 952}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:39:46,571]\u001b[0m Trial 345 finished with value: 0.6792095600129744 and parameters: {'n_estimators': 811}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 21:41:48,681]\u001b[0m Trial 346 finished with value: 0.6796574305344365 and parameters: {'n_estimators': 983}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:42:54,270]\u001b[0m Trial 347 finished with value: 0.6783603694799621 and parameters: {'n_estimators': 529}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:44:50,018]\u001b[0m Trial 348 finished with value: 0.6795115557596689 and parameters: {'n_estimators': 958}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:46:51,815]\u001b[0m Trial 349 finished with value: 0.6796237187013293 and parameters: {'n_estimators': 985}. Best is trial 36 with value: 0.6797576113523447.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6798\n",
      "\tBest params:\n",
      "\t\tn_estimators: 765\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.672213    0.730588    0.709249    0.684037   \n",
      "1                    TP  313.000000  336.000000  319.000000  325.000000   \n",
      "2                    TN  178.000000  174.000000  192.000000  181.000000   \n",
      "3                    FP   59.000000   44.000000   45.000000   50.000000   \n",
      "4                    FN   45.000000   41.000000   39.000000   39.000000   \n",
      "5              Accuracy    0.825210    0.857143    0.858824    0.850420   \n",
      "6             Precision    0.841398    0.884211    0.876374    0.866667   \n",
      "7           Sensitivity    0.874302    0.891247    0.891061    0.892857   \n",
      "8           Specificity    0.751100    0.798200    0.810100    0.783500   \n",
      "9              F1 score    0.857534    0.887715    0.883657    0.879567   \n",
      "10  F1 score (weighted)    0.824226    0.856931    0.858505    0.849709   \n",
      "11     F1 score (macro)    0.815724    0.845705    0.852085    0.841114   \n",
      "12    Balanced Accuracy    0.812678    0.844706    0.850594    0.838203   \n",
      "13                  MCC    0.632440    0.691459    0.704358    0.682870   \n",
      "14                  NPV    0.798200    0.809300    0.831200    0.822700   \n",
      "15              ROC_AUC    0.812678    0.844706    0.850594    0.838203   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.706921    0.706242    0.690513  \n",
      "1   328.000000  336.000000  326.000000  \n",
      "2   181.000000  181.000000  175.000000  \n",
      "3    49.000000   41.000000   57.000000  \n",
      "4    37.000000   37.000000   37.000000  \n",
      "5     0.855462    0.868908    0.842017  \n",
      "6     0.870027    0.891247    0.851175  \n",
      "7     0.898630    0.900804    0.898072  \n",
      "8     0.787000    0.815300    0.754300  \n",
      "9     0.884097    0.896000    0.873995  \n",
      "10    0.854695    0.868661    0.840576  \n",
      "11    0.846066    0.859364    0.831141  \n",
      "12    0.842793    0.858060    0.826191  \n",
      "13    0.692905    0.718816    0.664404  \n",
      "14    0.830300    0.830300    0.825500  \n",
      "15    0.842793    0.858060    0.826191  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_rf_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_6_cat = np.where((y_pred_rf_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_rf_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 21:49:01,091]\u001b[0m Trial 350 finished with value: 0.6832426004079739 and parameters: {'n_estimators': 968}. Best is trial 350 with value: 0.6832426004079739.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:50:56,161]\u001b[0m Trial 351 finished with value: 0.6832764018254944 and parameters: {'n_estimators': 935}. Best is trial 351 with value: 0.6832764018254944.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:52:52,335]\u001b[0m Trial 352 finished with value: 0.6832608625867873 and parameters: {'n_estimators': 944}. Best is trial 351 with value: 0.6832764018254944.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:54:46,333]\u001b[0m Trial 353 finished with value: 0.6833071653096249 and parameters: {'n_estimators': 929}. Best is trial 353 with value: 0.6833071653096249.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:56:41,594]\u001b[0m Trial 354 finished with value: 0.6832117217905085 and parameters: {'n_estimators': 937}. Best is trial 353 with value: 0.6833071653096249.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 21:58:37,016]\u001b[0m Trial 355 finished with value: 0.6832332766101084 and parameters: {'n_estimators': 943}. Best is trial 353 with value: 0.6833071653096249.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:00:31,608]\u001b[0m Trial 356 finished with value: 0.68328763206128 and parameters: {'n_estimators': 931}. Best is trial 353 with value: 0.6833071653096249.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:02:26,075]\u001b[0m Trial 357 finished with value: 0.6832683310944038 and parameters: {'n_estimators': 932}. Best is trial 353 with value: 0.6833071653096249.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:04:21,116]\u001b[0m Trial 358 finished with value: 0.6832117217905085 and parameters: {'n_estimators': 937}. Best is trial 353 with value: 0.6833071653096249.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:06:15,751]\u001b[0m Trial 359 finished with value: 0.6833071653096249 and parameters: {'n_estimators': 929}. Best is trial 353 with value: 0.6833071653096249.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:08:09,642]\u001b[0m Trial 360 finished with value: 0.6833071653096249 and parameters: {'n_estimators': 929}. Best is trial 353 with value: 0.6833071653096249.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:10:03,215]\u001b[0m Trial 361 finished with value: 0.6832826267461578 and parameters: {'n_estimators': 928}. Best is trial 353 with value: 0.6833071653096249.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:11:57,170]\u001b[0m Trial 362 finished with value: 0.6833157985299321 and parameters: {'n_estimators': 926}. Best is trial 362 with value: 0.6833157985299321.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:13:50,139]\u001b[0m Trial 363 finished with value: 0.683315798529932 and parameters: {'n_estimators': 926}. Best is trial 362 with value: 0.6833157985299321.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:15:44,539]\u001b[0m Trial 364 finished with value: 0.6832826267461579 and parameters: {'n_estimators': 928}. Best is trial 362 with value: 0.6833157985299321.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:17:38,708]\u001b[0m Trial 365 finished with value: 0.6832931325896219 and parameters: {'n_estimators': 925}. Best is trial 362 with value: 0.6833157985299321.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:19:32,887]\u001b[0m Trial 366 finished with value: 0.6832931325896219 and parameters: {'n_estimators': 925}. Best is trial 362 with value: 0.6833157985299321.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:21:27,131]\u001b[0m Trial 367 finished with value: 0.6832977465576594 and parameters: {'n_estimators': 927}. Best is trial 362 with value: 0.6833157985299321.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:23:20,331]\u001b[0m Trial 368 finished with value: 0.6833157985299321 and parameters: {'n_estimators': 926}. Best is trial 362 with value: 0.6833157985299321.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:25:13,611]\u001b[0m Trial 369 finished with value: 0.6833195260618311 and parameters: {'n_estimators': 923}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:27:07,305]\u001b[0m Trial 370 finished with value: 0.6832977465576594 and parameters: {'n_estimators': 927}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:29:01,442]\u001b[0m Trial 371 finished with value: 0.683315798529932 and parameters: {'n_estimators': 926}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:30:55,541]\u001b[0m Trial 372 finished with value: 0.6832826267461579 and parameters: {'n_estimators': 928}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:32:49,425]\u001b[0m Trial 373 finished with value: 0.6832931325896219 and parameters: {'n_estimators': 925}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:34:43,685]\u001b[0m Trial 374 finished with value: 0.6832931325896219 and parameters: {'n_estimators': 925}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:36:36,766]\u001b[0m Trial 375 finished with value: 0.6832813745197005 and parameters: {'n_estimators': 921}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:38:30,131]\u001b[0m Trial 376 finished with value: 0.6832813745197005 and parameters: {'n_estimators': 921}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:40:23,212]\u001b[0m Trial 377 finished with value: 0.68323144522982 and parameters: {'n_estimators': 920}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:42:16,590]\u001b[0m Trial 378 finished with value: 0.6832813745197005 and parameters: {'n_estimators': 921}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:44:09,597]\u001b[0m Trial 379 finished with value: 0.6832813745197006 and parameters: {'n_estimators': 921}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:46:03,094]\u001b[0m Trial 380 finished with value: 0.6832991303811324 and parameters: {'n_estimators': 922}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:47:56,521]\u001b[0m Trial 381 finished with value: 0.6832213107385219 and parameters: {'n_estimators': 919}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:49:49,081]\u001b[0m Trial 382 finished with value: 0.6832239347446877 and parameters: {'n_estimators': 918}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:51:41,808]\u001b[0m Trial 383 finished with value: 0.68323144522982 and parameters: {'n_estimators': 920}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:53:35,130]\u001b[0m Trial 384 finished with value: 0.6833195260618311 and parameters: {'n_estimators': 923}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:55:27,203]\u001b[0m Trial 385 finished with value: 0.6832374462247515 and parameters: {'n_estimators': 912}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:57:20,941]\u001b[0m Trial 386 finished with value: 0.6833195260618311 and parameters: {'n_estimators': 923}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 22:59:12,898]\u001b[0m Trial 387 finished with value: 0.6832318027772649 and parameters: {'n_estimators': 911}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:01:06,560]\u001b[0m Trial 388 finished with value: 0.6833186856709731 and parameters: {'n_estimators': 924}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:02:57,686]\u001b[0m Trial 389 finished with value: 0.6832375581672839 and parameters: {'n_estimators': 901}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:04:51,416]\u001b[0m Trial 390 finished with value: 0.6832931325896219 and parameters: {'n_estimators': 925}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:06:45,174]\u001b[0m Trial 391 finished with value: 0.6832977465576594 and parameters: {'n_estimators': 927}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:08:36,687]\u001b[0m Trial 392 finished with value: 0.683234166032383 and parameters: {'n_estimators': 909}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:10:30,816]\u001b[0m Trial 393 finished with value: 0.6832826267461579 and parameters: {'n_estimators': 928}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:12:25,161]\u001b[0m Trial 394 finished with value: 0.6833090694602996 and parameters: {'n_estimators': 930}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:14:15,701]\u001b[0m Trial 395 finished with value: 0.6832371931630852 and parameters: {'n_estimators': 903}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 23:16:10,092]\u001b[0m Trial 396 finished with value: 0.6832683310944038 and parameters: {'n_estimators': 932}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:18:04,506]\u001b[0m Trial 397 finished with value: 0.68328763206128 and parameters: {'n_estimators': 931}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:19:55,383]\u001b[0m Trial 398 finished with value: 0.6832371931630852 and parameters: {'n_estimators': 903}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:21:50,614]\u001b[0m Trial 399 finished with value: 0.6832683310944038 and parameters: {'n_estimators': 932}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6833\n",
      "\tBest params:\n",
      "\t\tn_estimators: 923\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.672213    0.730588    0.709249    0.684037   \n",
      "1                    TP  313.000000  336.000000  319.000000  325.000000   \n",
      "2                    TN  178.000000  174.000000  192.000000  181.000000   \n",
      "3                    FP   59.000000   44.000000   45.000000   50.000000   \n",
      "4                    FN   45.000000   41.000000   39.000000   39.000000   \n",
      "5              Accuracy    0.825210    0.857143    0.858824    0.850420   \n",
      "6             Precision    0.841398    0.884211    0.876374    0.866667   \n",
      "7           Sensitivity    0.874302    0.891247    0.891061    0.892857   \n",
      "8           Specificity    0.751100    0.798200    0.810100    0.783500   \n",
      "9              F1 score    0.857534    0.887715    0.883657    0.879567   \n",
      "10  F1 score (weighted)    0.824226    0.856931    0.858505    0.849709   \n",
      "11     F1 score (macro)    0.815724    0.845705    0.852085    0.841114   \n",
      "12    Balanced Accuracy    0.812678    0.844706    0.850594    0.838203   \n",
      "13                  MCC    0.632440    0.691459    0.704358    0.682870   \n",
      "14                  NPV    0.798200    0.809300    0.831200    0.822700   \n",
      "15              ROC_AUC    0.812678    0.844706    0.850594    0.838203   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.706921    0.706242    0.690513    0.666885  \n",
      "1   328.000000  336.000000  326.000000  320.000000  \n",
      "2   181.000000  181.000000  175.000000  182.000000  \n",
      "3    49.000000   41.000000   57.000000   46.000000  \n",
      "4    37.000000   37.000000   37.000000   47.000000  \n",
      "5     0.855462    0.868908    0.842017    0.843697  \n",
      "6     0.870027    0.891247    0.851175    0.874317  \n",
      "7     0.898630    0.900804    0.898072    0.871935  \n",
      "8     0.787000    0.815300    0.754300    0.798200  \n",
      "9     0.884097    0.896000    0.873995    0.873124  \n",
      "10    0.854695    0.868661    0.840576    0.843762  \n",
      "11    0.846066    0.859364    0.831141    0.834812  \n",
      "12    0.842793    0.858060    0.826191    0.835090  \n",
      "13    0.692905    0.718816    0.664404    0.669628  \n",
      "14    0.830300    0.830300    0.825500    0.794800  \n",
      "15    0.842793    0.858060    0.826191    0.835090  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_rf_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_7_cat = np.where((y_pred_rf_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_rf_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-20 23:23:52,244]\u001b[0m Trial 400 finished with value: 0.6720382155247081 and parameters: {'n_estimators': 905}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:25:43,685]\u001b[0m Trial 401 finished with value: 0.6721888153226371 and parameters: {'n_estimators': 932}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:27:31,998]\u001b[0m Trial 402 finished with value: 0.6720321381361563 and parameters: {'n_estimators': 906}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:29:24,796]\u001b[0m Trial 403 finished with value: 0.6721412903013819 and parameters: {'n_estimators': 941}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:31:12,304]\u001b[0m Trial 404 finished with value: 0.672088044254013 and parameters: {'n_estimators': 895}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:33:01,013]\u001b[0m Trial 405 finished with value: 0.6721118700168307 and parameters: {'n_estimators': 923}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:34:50,448]\u001b[0m Trial 406 finished with value: 0.6720644935022951 and parameters: {'n_estimators': 913}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:36:40,246]\u001b[0m Trial 407 finished with value: 0.6721754538351774 and parameters: {'n_estimators': 933}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:38:32,286]\u001b[0m Trial 408 finished with value: 0.6721188380286771 and parameters: {'n_estimators': 940}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:40:20,363]\u001b[0m Trial 409 finished with value: 0.6720707377599586 and parameters: {'n_estimators': 902}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:42:10,498]\u001b[0m Trial 410 finished with value: 0.6720801117218377 and parameters: {'n_estimators': 920}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:44:03,010]\u001b[0m Trial 411 finished with value: 0.6720955611434183 and parameters: {'n_estimators': 944}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:45:50,136]\u001b[0m Trial 412 finished with value: 0.6720674609085701 and parameters: {'n_estimators': 892}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:47:39,778]\u001b[0m Trial 413 finished with value: 0.672037763868136 and parameters: {'n_estimators': 914}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:49:31,769]\u001b[0m Trial 414 finished with value: 0.6721888153226371 and parameters: {'n_estimators': 932}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:51:24,695]\u001b[0m Trial 415 finished with value: 0.6721139628814123 and parameters: {'n_estimators': 943}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:53:10,748]\u001b[0m Trial 416 finished with value: 0.6720761015043307 and parameters: {'n_estimators': 893}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:54:59,649]\u001b[0m Trial 417 finished with value: 0.6720644935022951 and parameters: {'n_estimators': 913}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:56:50,653]\u001b[0m Trial 418 finished with value: 0.6721136377887513 and parameters: {'n_estimators': 928}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-20 23:58:42,672]\u001b[0m Trial 419 finished with value: 0.6721153007030986 and parameters: {'n_estimators': 939}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:00:30,651]\u001b[0m Trial 420 finished with value: 0.6720398716251822 and parameters: {'n_estimators': 904}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:02:23,063]\u001b[0m Trial 421 finished with value: 0.6720499074117405 and parameters: {'n_estimators': 949}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:04:12,382]\u001b[0m Trial 422 finished with value: 0.6721161200693425 and parameters: {'n_estimators': 924}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:05:55,832]\u001b[0m Trial 423 finished with value: 0.6720501446563458 and parameters: {'n_estimators': 890}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:07:45,894]\u001b[0m Trial 424 finished with value: 0.6720383776067236 and parameters: {'n_estimators': 915}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:09:39,184]\u001b[0m Trial 425 finished with value: 0.6720725211041201 and parameters: {'n_estimators': 947}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:11:29,080]\u001b[0m Trial 426 finished with value: 0.6721301791072815 and parameters: {'n_estimators': 927}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:13:15,163]\u001b[0m Trial 427 finished with value: 0.6720398716251822 and parameters: {'n_estimators': 904}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:15:06,503]\u001b[0m Trial 428 finished with value: 0.6720725211041201 and parameters: {'n_estimators': 947}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:16:52,859]\u001b[0m Trial 429 finished with value: 0.6720272929036664 and parameters: {'n_estimators': 888}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:17:22,769]\u001b[0m Trial 430 finished with value: 0.6716692931762838 and parameters: {'n_estimators': 249}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:19:13,918]\u001b[0m Trial 431 finished with value: 0.6721301791072815 and parameters: {'n_estimators': 927}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:21:02,912]\u001b[0m Trial 432 finished with value: 0.6720182783237729 and parameters: {'n_estimators': 910}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:22:54,814]\u001b[0m Trial 433 finished with value: 0.6721754538351774 and parameters: {'n_estimators': 933}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:24:47,489]\u001b[0m Trial 434 finished with value: 0.6720399645528193 and parameters: {'n_estimators': 950}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:26:36,566]\u001b[0m Trial 435 finished with value: 0.6720302575617074 and parameters: {'n_estimators': 908}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:28:25,962]\u001b[0m Trial 436 finished with value: 0.6721161200693424 and parameters: {'n_estimators': 924}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:30:16,933]\u001b[0m Trial 437 finished with value: 0.672139854472274 and parameters: {'n_estimators': 942}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:32:03,160]\u001b[0m Trial 438 finished with value: 0.6719726456592392 and parameters: {'n_estimators': 885}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:33:53,066]\u001b[0m Trial 439 finished with value: 0.6720377638681357 and parameters: {'n_estimators': 914}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:35:47,629]\u001b[0m Trial 440 finished with value: 0.6720324752095008 and parameters: {'n_estimators': 953}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:37:34,574]\u001b[0m Trial 441 finished with value: 0.672088044254013 and parameters: {'n_estimators': 895}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:39:26,522]\u001b[0m Trial 442 finished with value: 0.6721501222393204 and parameters: {'n_estimators': 931}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:41:15,959]\u001b[0m Trial 443 finished with value: 0.6721325027515366 and parameters: {'n_estimators': 922}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:43:07,769]\u001b[0m Trial 444 finished with value: 0.6720671678075422 and parameters: {'n_estimators': 952}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:44:55,662]\u001b[0m Trial 445 finished with value: 0.6720398716251822 and parameters: {'n_estimators': 904}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 00:46:47,470]\u001b[0m Trial 446 finished with value: 0.672171710240532 and parameters: {'n_estimators': 934}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:48:36,602]\u001b[0m Trial 447 finished with value: 0.6720508025512937 and parameters: {'n_estimators': 912}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:50:22,564]\u001b[0m Trial 448 finished with value: 0.6719251056543442 and parameters: {'n_estimators': 883}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:52:16,455]\u001b[0m Trial 449 finished with value: 0.6720725211041201 and parameters: {'n_estimators': 947}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6833\n",
      "\tBest params:\n",
      "\t\tn_estimators: 923\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.672213    0.730588    0.709249    0.684037   \n",
      "1                    TP  313.000000  336.000000  319.000000  325.000000   \n",
      "2                    TN  178.000000  174.000000  192.000000  181.000000   \n",
      "3                    FP   59.000000   44.000000   45.000000   50.000000   \n",
      "4                    FN   45.000000   41.000000   39.000000   39.000000   \n",
      "5              Accuracy    0.825210    0.857143    0.858824    0.850420   \n",
      "6             Precision    0.841398    0.884211    0.876374    0.866667   \n",
      "7           Sensitivity    0.874302    0.891247    0.891061    0.892857   \n",
      "8           Specificity    0.751100    0.798200    0.810100    0.783500   \n",
      "9              F1 score    0.857534    0.887715    0.883657    0.879567   \n",
      "10  F1 score (weighted)    0.824226    0.856931    0.858505    0.849709   \n",
      "11     F1 score (macro)    0.815724    0.845705    0.852085    0.841114   \n",
      "12    Balanced Accuracy    0.812678    0.844706    0.850594    0.838203   \n",
      "13                  MCC    0.632440    0.691459    0.704358    0.682870   \n",
      "14                  NPV    0.798200    0.809300    0.831200    0.822700   \n",
      "15              ROC_AUC    0.812678    0.844706    0.850594    0.838203   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.706921    0.706242    0.690513    0.666885    0.709272  \n",
      "1   328.000000  336.000000  326.000000  320.000000  315.000000  \n",
      "2   181.000000  181.000000  175.000000  182.000000  190.000000  \n",
      "3    49.000000   41.000000   57.000000   46.000000   54.000000  \n",
      "4    37.000000   37.000000   37.000000   47.000000   36.000000  \n",
      "5     0.855462    0.868908    0.842017    0.843697    0.848739  \n",
      "6     0.870027    0.891247    0.851175    0.874317    0.853659  \n",
      "7     0.898630    0.900804    0.898072    0.871935    0.897436  \n",
      "8     0.787000    0.815300    0.754300    0.798200    0.778700  \n",
      "9     0.884097    0.896000    0.873995    0.873124    0.875000  \n",
      "10    0.854695    0.868661    0.840576    0.843762    0.847734  \n",
      "11    0.846066    0.859364    0.831141    0.834812    0.841755  \n",
      "12    0.842793    0.858060    0.826191    0.835090    0.838062  \n",
      "13    0.692905    0.718816    0.664404    0.669628    0.685185  \n",
      "14    0.830300    0.830300    0.825500    0.794800    0.840700  \n",
      "15    0.842793    0.858060    0.826191    0.835090    0.838062  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_rf_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_8_cat = np.where((y_pred_rf_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_rf_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 00:54:21,489]\u001b[0m Trial 450 finished with value: 0.6695634450968291 and parameters: {'n_estimators': 929}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:56:10,486]\u001b[0m Trial 451 finished with value: 0.669641633616 and parameters: {'n_estimators': 897}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:58:05,137]\u001b[0m Trial 452 finished with value: 0.6696644095182296 and parameters: {'n_estimators': 948}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 00:59:56,396]\u001b[0m Trial 453 finished with value: 0.6696213771954324 and parameters: {'n_estimators': 918}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:01:48,898]\u001b[0m Trial 454 finished with value: 0.6695811883780202 and parameters: {'n_estimators': 933}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:02:33,452]\u001b[0m Trial 455 finished with value: 0.6687550419024808 and parameters: {'n_estimators': 363}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:04:24,040]\u001b[0m Trial 456 finished with value: 0.6696971969262154 and parameters: {'n_estimators': 911}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:06:10,775]\u001b[0m Trial 457 finished with value: 0.669589723137312 and parameters: {'n_estimators': 884}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:08:05,809]\u001b[0m Trial 458 finished with value: 0.669731297621626 and parameters: {'n_estimators': 951}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:09:58,190]\u001b[0m Trial 459 finished with value: 0.6695766489472057 and parameters: {'n_estimators': 926}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:11:47,115]\u001b[0m Trial 460 finished with value: 0.669629818961335 and parameters: {'n_estimators': 898}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:13:39,935]\u001b[0m Trial 461 finished with value: 0.6696186985021979 and parameters: {'n_estimators': 939}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:15:30,202]\u001b[0m Trial 462 finished with value: 0.669683262697659 and parameters: {'n_estimators': 912}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:17:25,194]\u001b[0m Trial 463 finished with value: 0.6697103915072564 and parameters: {'n_estimators': 952}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:19:17,556]\u001b[0m Trial 464 finished with value: 0.6695561645233743 and parameters: {'n_estimators': 927}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:21:06,672]\u001b[0m Trial 465 finished with value: 0.6697303542085586 and parameters: {'n_estimators': 902}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:23:00,661]\u001b[0m Trial 466 finished with value: 0.6695980017495076 and parameters: {'n_estimators': 940}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:24:51,980]\u001b[0m Trial 467 finished with value: 0.6696164874426567 and parameters: {'n_estimators': 917}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:26:38,836]\u001b[0m Trial 468 finished with value: 0.6695819006276384 and parameters: {'n_estimators': 883}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:28:34,643]\u001b[0m Trial 469 finished with value: 0.6697004729836044 and parameters: {'n_estimators': 954}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:30:26,299]\u001b[0m Trial 470 finished with value: 0.6695634450968291 and parameters: {'n_estimators': 929}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:32:15,366]\u001b[0m Trial 471 finished with value: 0.6697221216304099 and parameters: {'n_estimators': 901}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:34:10,578]\u001b[0m Trial 472 finished with value: 0.6697007169829232 and parameters: {'n_estimators': 956}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:36:01,737]\u001b[0m Trial 473 finished with value: 0.6696213771954324 and parameters: {'n_estimators': 918}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:37:55,520]\u001b[0m Trial 474 finished with value: 0.6696151310369377 and parameters: {'n_estimators': 938}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:39:41,913]\u001b[0m Trial 475 finished with value: 0.6695427096805738 and parameters: {'n_estimators': 880}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:41:31,234]\u001b[0m Trial 476 finished with value: 0.6697221216304099 and parameters: {'n_estimators': 901}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:43:22,903]\u001b[0m Trial 477 finished with value: 0.6696104899602411 and parameters: {'n_estimators': 920}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:45:16,661]\u001b[0m Trial 478 finished with value: 0.6696151310369377 and parameters: {'n_estimators': 938}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:47:07,126]\u001b[0m Trial 479 finished with value: 0.669683262697659 and parameters: {'n_estimators': 912}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:49:02,204]\u001b[0m Trial 480 finished with value: 0.6696901428125268 and parameters: {'n_estimators': 949}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:50:50,664]\u001b[0m Trial 481 finished with value: 0.6696226339826343 and parameters: {'n_estimators': 896}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:52:43,272]\u001b[0m Trial 482 finished with value: 0.6695855606326895 and parameters: {'n_estimators': 928}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:54:37,688]\u001b[0m Trial 483 finished with value: 0.6697007169829232 and parameters: {'n_estimators': 956}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:56:31,042]\u001b[0m Trial 484 finished with value: 0.6695841138930286 and parameters: {'n_estimators': 934}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 01:58:21,867]\u001b[0m Trial 485 finished with value: 0.6696475142466286 and parameters: {'n_estimators': 914}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:00:09,665]\u001b[0m Trial 486 finished with value: 0.6695604832191715 and parameters: {'n_estimators': 890}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:02:05,025]\u001b[0m Trial 487 finished with value: 0.6697103915072564 and parameters: {'n_estimators': 952}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:03:56,716]\u001b[0m Trial 488 finished with value: 0.6696068157252056 and parameters: {'n_estimators': 925}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:05:42,927]\u001b[0m Trial 489 finished with value: 0.6695979136370657 and parameters: {'n_estimators': 876}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:07:33,265]\u001b[0m Trial 490 finished with value: 0.6696832626976592 and parameters: {'n_estimators': 912}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:09:29,362]\u001b[0m Trial 491 finished with value: 0.6696893311299533 and parameters: {'n_estimators': 957}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:11:20,970]\u001b[0m Trial 492 finished with value: 0.6696054599424047 and parameters: {'n_estimators': 937}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:13:09,334]\u001b[0m Trial 493 finished with value: 0.6697472265194072 and parameters: {'n_estimators': 904}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:15:02,498]\u001b[0m Trial 494 finished with value: 0.6695700523506363 and parameters: {'n_estimators': 932}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:16:49,178]\u001b[0m Trial 495 finished with value: 0.6696190265803658 and parameters: {'n_estimators': 894}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 02:17:11,625]\u001b[0m Trial 496 finished with value: 0.6689151375149518 and parameters: {'n_estimators': 183}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:19:02,399]\u001b[0m Trial 497 finished with value: 0.6695990374033961 and parameters: {'n_estimators': 921}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:20:58,573]\u001b[0m Trial 498 finished with value: 0.6697057095168297 and parameters: {'n_estimators': 958}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:22:52,210]\u001b[0m Trial 499 finished with value: 0.6696005482880911 and parameters: {'n_estimators': 942}. Best is trial 369 with value: 0.6833195260618311.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6833\n",
      "\tBest params:\n",
      "\t\tn_estimators: 923\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.672213    0.730588    0.709249    0.684037   \n",
      "1                    TP  313.000000  336.000000  319.000000  325.000000   \n",
      "2                    TN  178.000000  174.000000  192.000000  181.000000   \n",
      "3                    FP   59.000000   44.000000   45.000000   50.000000   \n",
      "4                    FN   45.000000   41.000000   39.000000   39.000000   \n",
      "5              Accuracy    0.825210    0.857143    0.858824    0.850420   \n",
      "6             Precision    0.841398    0.884211    0.876374    0.866667   \n",
      "7           Sensitivity    0.874302    0.891247    0.891061    0.892857   \n",
      "8           Specificity    0.751100    0.798200    0.810100    0.783500   \n",
      "9              F1 score    0.857534    0.887715    0.883657    0.879567   \n",
      "10  F1 score (weighted)    0.824226    0.856931    0.858505    0.849709   \n",
      "11     F1 score (macro)    0.815724    0.845705    0.852085    0.841114   \n",
      "12    Balanced Accuracy    0.812678    0.844706    0.850594    0.838203   \n",
      "13                  MCC    0.632440    0.691459    0.704358    0.682870   \n",
      "14                  NPV    0.798200    0.809300    0.831200    0.822700   \n",
      "15              ROC_AUC    0.812678    0.844706    0.850594    0.838203   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.706921    0.706242    0.690513    0.666885    0.709272    0.716006  \n",
      "1   328.000000  336.000000  326.000000  320.000000  315.000000  337.000000  \n",
      "2   181.000000  181.000000  175.000000  182.000000  190.000000  173.000000  \n",
      "3    49.000000   41.000000   57.000000   46.000000   54.000000   47.000000  \n",
      "4    37.000000   37.000000   37.000000   47.000000   36.000000   38.000000  \n",
      "5     0.855462    0.868908    0.842017    0.843697    0.848739    0.857143  \n",
      "6     0.870027    0.891247    0.851175    0.874317    0.853659    0.877604  \n",
      "7     0.898630    0.900804    0.898072    0.871935    0.897436    0.898667  \n",
      "8     0.787000    0.815300    0.754300    0.798200    0.778700    0.786400  \n",
      "9     0.884097    0.896000    0.873995    0.873124    0.875000    0.888011  \n",
      "10    0.854695    0.868661    0.840576    0.843762    0.847734    0.856498  \n",
      "11    0.846066    0.859364    0.831141    0.834812    0.841755    0.845397  \n",
      "12    0.842793    0.858060    0.826191    0.835090    0.838062    0.842515  \n",
      "13    0.692905    0.718816    0.664404    0.669628    0.685185    0.691242  \n",
      "14    0.830300    0.830300    0.825500    0.794800    0.840700    0.819900  \n",
      "15    0.842793    0.858060    0.826191    0.835090    0.838062    0.842515  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_rf_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_9_cat = np.where((y_pred_rf_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_rf_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6833\n",
      "\tBest params:\n",
      "\t\tn_estimators: 923\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.695561</td>\n",
       "      <td>0.042602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>165.700000</td>\n",
       "      <td>7.227263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>89.200000</td>\n",
       "      <td>6.494442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>24.200000</td>\n",
       "      <td>4.894441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>6.446360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.857959</td>\n",
       "      <td>0.025195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.873015</td>\n",
       "      <td>0.022133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.902081</td>\n",
       "      <td>0.034330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.786320</td>\n",
       "      <td>0.043116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.886893</td>\n",
       "      <td>0.020808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.857067</td>\n",
       "      <td>0.024996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.847649</td>\n",
       "      <td>0.026681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.844199</td>\n",
       "      <td>0.025663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.697628</td>\n",
       "      <td>0.052533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.834150</td>\n",
       "      <td>0.047907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.844199</td>\n",
       "      <td>0.025663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.695561     0.042602\n",
       "1                    TP       165.700000     7.227263\n",
       "2                    TN        89.200000     6.494442\n",
       "3                    FP        24.200000     4.894441\n",
       "4                    FN        18.000000     6.446360\n",
       "5              Accuracy         0.857959     0.025195\n",
       "6             Precision         0.873015     0.022133\n",
       "7           Sensitivity         0.902081     0.034330\n",
       "8           Specificity         0.786320     0.043116\n",
       "9              F1 score         0.886893     0.020808\n",
       "10  F1 score (weighted)         0.857067     0.024996\n",
       "11     F1 score (macro)         0.847649     0.026681\n",
       "12    Balanced Accuracy         0.844199     0.025663\n",
       "13                  MCC         0.697628     0.052533\n",
       "14                  NPV         0.834150     0.047907\n",
       "15              ROC_AUC         0.844199     0.025663"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.672213</td>\n",
       "      <td>0.730588</td>\n",
       "      <td>0.709249</td>\n",
       "      <td>0.684037</td>\n",
       "      <td>0.706921</td>\n",
       "      <td>0.706242</td>\n",
       "      <td>0.690513</td>\n",
       "      <td>0.666885</td>\n",
       "      <td>0.709272</td>\n",
       "      <td>0.716006</td>\n",
       "      <td>0.699192</td>\n",
       "      <td>0.020158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>325.500000</td>\n",
       "      <td>8.809717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>180.700000</td>\n",
       "      <td>6.325434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>49.200000</td>\n",
       "      <td>5.846176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>39.600000</td>\n",
       "      <td>3.687818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.825210</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.850420</td>\n",
       "      <td>0.855462</td>\n",
       "      <td>0.868908</td>\n",
       "      <td>0.842017</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.850756</td>\n",
       "      <td>0.011932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.841398</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.876374</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.870027</td>\n",
       "      <td>0.891247</td>\n",
       "      <td>0.851175</td>\n",
       "      <td>0.874317</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.877604</td>\n",
       "      <td>0.868668</td>\n",
       "      <td>0.015646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.874302</td>\n",
       "      <td>0.891247</td>\n",
       "      <td>0.891061</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.898630</td>\n",
       "      <td>0.900804</td>\n",
       "      <td>0.898072</td>\n",
       "      <td>0.871935</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.898667</td>\n",
       "      <td>0.891501</td>\n",
       "      <td>0.010263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.751100</td>\n",
       "      <td>0.798200</td>\n",
       "      <td>0.810100</td>\n",
       "      <td>0.783500</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.815300</td>\n",
       "      <td>0.754300</td>\n",
       "      <td>0.798200</td>\n",
       "      <td>0.778700</td>\n",
       "      <td>0.786400</td>\n",
       "      <td>0.786280</td>\n",
       "      <td>0.021134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.857534</td>\n",
       "      <td>0.887715</td>\n",
       "      <td>0.883657</td>\n",
       "      <td>0.879567</td>\n",
       "      <td>0.884097</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.873995</td>\n",
       "      <td>0.873124</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.888011</td>\n",
       "      <td>0.879870</td>\n",
       "      <td>0.010661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.824226</td>\n",
       "      <td>0.856931</td>\n",
       "      <td>0.858505</td>\n",
       "      <td>0.849709</td>\n",
       "      <td>0.854695</td>\n",
       "      <td>0.868661</td>\n",
       "      <td>0.840576</td>\n",
       "      <td>0.843762</td>\n",
       "      <td>0.847734</td>\n",
       "      <td>0.856498</td>\n",
       "      <td>0.850130</td>\n",
       "      <td>0.012156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.815724</td>\n",
       "      <td>0.845705</td>\n",
       "      <td>0.852085</td>\n",
       "      <td>0.841114</td>\n",
       "      <td>0.846066</td>\n",
       "      <td>0.859364</td>\n",
       "      <td>0.831141</td>\n",
       "      <td>0.834812</td>\n",
       "      <td>0.841755</td>\n",
       "      <td>0.845397</td>\n",
       "      <td>0.841316</td>\n",
       "      <td>0.012024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.812678</td>\n",
       "      <td>0.844706</td>\n",
       "      <td>0.850594</td>\n",
       "      <td>0.838203</td>\n",
       "      <td>0.842793</td>\n",
       "      <td>0.858060</td>\n",
       "      <td>0.826191</td>\n",
       "      <td>0.835090</td>\n",
       "      <td>0.838062</td>\n",
       "      <td>0.842515</td>\n",
       "      <td>0.838889</td>\n",
       "      <td>0.012608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.632440</td>\n",
       "      <td>0.691459</td>\n",
       "      <td>0.704358</td>\n",
       "      <td>0.682870</td>\n",
       "      <td>0.692905</td>\n",
       "      <td>0.718816</td>\n",
       "      <td>0.664404</td>\n",
       "      <td>0.669628</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>0.691242</td>\n",
       "      <td>0.683331</td>\n",
       "      <td>0.023714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.798200</td>\n",
       "      <td>0.809300</td>\n",
       "      <td>0.831200</td>\n",
       "      <td>0.822700</td>\n",
       "      <td>0.830300</td>\n",
       "      <td>0.830300</td>\n",
       "      <td>0.825500</td>\n",
       "      <td>0.794800</td>\n",
       "      <td>0.840700</td>\n",
       "      <td>0.819900</td>\n",
       "      <td>0.820290</td>\n",
       "      <td>0.015010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.812678</td>\n",
       "      <td>0.844706</td>\n",
       "      <td>0.850594</td>\n",
       "      <td>0.838203</td>\n",
       "      <td>0.842793</td>\n",
       "      <td>0.858060</td>\n",
       "      <td>0.826191</td>\n",
       "      <td>0.835090</td>\n",
       "      <td>0.838062</td>\n",
       "      <td>0.842515</td>\n",
       "      <td>0.838889</td>\n",
       "      <td>0.012608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.672213    0.730588    0.709249    0.684037   \n",
       "1                    TP  313.000000  336.000000  319.000000  325.000000   \n",
       "2                    TN  178.000000  174.000000  192.000000  181.000000   \n",
       "3                    FP   59.000000   44.000000   45.000000   50.000000   \n",
       "4                    FN   45.000000   41.000000   39.000000   39.000000   \n",
       "5              Accuracy    0.825210    0.857143    0.858824    0.850420   \n",
       "6             Precision    0.841398    0.884211    0.876374    0.866667   \n",
       "7           Sensitivity    0.874302    0.891247    0.891061    0.892857   \n",
       "8           Specificity    0.751100    0.798200    0.810100    0.783500   \n",
       "9              F1 score    0.857534    0.887715    0.883657    0.879567   \n",
       "10  F1 score (weighted)    0.824226    0.856931    0.858505    0.849709   \n",
       "11     F1 score (macro)    0.815724    0.845705    0.852085    0.841114   \n",
       "12    Balanced Accuracy    0.812678    0.844706    0.850594    0.838203   \n",
       "13                  MCC    0.632440    0.691459    0.704358    0.682870   \n",
       "14                  NPV    0.798200    0.809300    0.831200    0.822700   \n",
       "15              ROC_AUC    0.812678    0.844706    0.850594    0.838203   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.706921    0.706242    0.690513    0.666885    0.709272    0.716006   \n",
       "1   328.000000  336.000000  326.000000  320.000000  315.000000  337.000000   \n",
       "2   181.000000  181.000000  175.000000  182.000000  190.000000  173.000000   \n",
       "3    49.000000   41.000000   57.000000   46.000000   54.000000   47.000000   \n",
       "4    37.000000   37.000000   37.000000   47.000000   36.000000   38.000000   \n",
       "5     0.855462    0.868908    0.842017    0.843697    0.848739    0.857143   \n",
       "6     0.870027    0.891247    0.851175    0.874317    0.853659    0.877604   \n",
       "7     0.898630    0.900804    0.898072    0.871935    0.897436    0.898667   \n",
       "8     0.787000    0.815300    0.754300    0.798200    0.778700    0.786400   \n",
       "9     0.884097    0.896000    0.873995    0.873124    0.875000    0.888011   \n",
       "10    0.854695    0.868661    0.840576    0.843762    0.847734    0.856498   \n",
       "11    0.846066    0.859364    0.831141    0.834812    0.841755    0.845397   \n",
       "12    0.842793    0.858060    0.826191    0.835090    0.838062    0.842515   \n",
       "13    0.692905    0.718816    0.664404    0.669628    0.685185    0.691242   \n",
       "14    0.830300    0.830300    0.825500    0.794800    0.840700    0.819900   \n",
       "15    0.842793    0.858060    0.826191    0.835090    0.838062    0.842515   \n",
       "\n",
       "           ave       std  \n",
       "0     0.699192  0.020158  \n",
       "1   325.500000  8.809717  \n",
       "2   180.700000  6.325434  \n",
       "3    49.200000  5.846176  \n",
       "4    39.600000  3.687818  \n",
       "5     0.850756  0.011932  \n",
       "6     0.868668  0.015646  \n",
       "7     0.891501  0.010263  \n",
       "8     0.786280  0.021134  \n",
       "9     0.879870  0.010661  \n",
       "10    0.850130  0.012156  \n",
       "11    0.841316  0.012024  \n",
       "12    0.838889  0.012608  \n",
       "13    0.683331  0.023714  \n",
       "14    0.820290  0.015010  \n",
       "15    0.838889  0.012608  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test['std'] = mat_met_rf_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_rf0</th>\n",
       "      <th>y_pred_rf1</th>\n",
       "      <th>y_pred_rf2</th>\n",
       "      <th>y_pred_rf3</th>\n",
       "      <th>y_pred_rf4</th>\n",
       "      <th>y_pred_rf_ave</th>\n",
       "      <th>y_pred_rf_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL356066</td>\n",
       "      <td>0</td>\n",
       "      <td>8.02</td>\n",
       "      <td>7.174476</td>\n",
       "      <td>7.267219</td>\n",
       "      <td>7.209938</td>\n",
       "      <td>7.264304</td>\n",
       "      <td>7.165649</td>\n",
       "      <td>7.350264</td>\n",
       "      <td>0.302078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3652228</td>\n",
       "      <td>1</td>\n",
       "      <td>8.05</td>\n",
       "      <td>8.153331</td>\n",
       "      <td>8.050878</td>\n",
       "      <td>8.049879</td>\n",
       "      <td>8.035496</td>\n",
       "      <td>8.023369</td>\n",
       "      <td>8.060492</td>\n",
       "      <td>0.042695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3939518</td>\n",
       "      <td>2</td>\n",
       "      <td>6.87</td>\n",
       "      <td>7.500500</td>\n",
       "      <td>7.443817</td>\n",
       "      <td>7.485595</td>\n",
       "      <td>7.504658</td>\n",
       "      <td>7.417742</td>\n",
       "      <td>7.370385</td>\n",
       "      <td>0.225925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4281792</td>\n",
       "      <td>3</td>\n",
       "      <td>7.22</td>\n",
       "      <td>8.188674</td>\n",
       "      <td>7.859492</td>\n",
       "      <td>7.526613</td>\n",
       "      <td>7.962881</td>\n",
       "      <td>7.868271</td>\n",
       "      <td>7.770989</td>\n",
       "      <td>0.314083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4070232</td>\n",
       "      <td>4</td>\n",
       "      <td>7.15</td>\n",
       "      <td>6.641647</td>\n",
       "      <td>6.772817</td>\n",
       "      <td>6.769940</td>\n",
       "      <td>6.760186</td>\n",
       "      <td>6.741241</td>\n",
       "      <td>6.805972</td>\n",
       "      <td>0.160230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL4202521</td>\n",
       "      <td>2966</td>\n",
       "      <td>7.43</td>\n",
       "      <td>6.692375</td>\n",
       "      <td>6.812465</td>\n",
       "      <td>6.850385</td>\n",
       "      <td>6.739560</td>\n",
       "      <td>6.659054</td>\n",
       "      <td>6.863973</td>\n",
       "      <td>0.261451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL216641</td>\n",
       "      <td>2967</td>\n",
       "      <td>7.35</td>\n",
       "      <td>7.530590</td>\n",
       "      <td>7.456516</td>\n",
       "      <td>7.580243</td>\n",
       "      <td>7.486778</td>\n",
       "      <td>7.406570</td>\n",
       "      <td>7.468449</td>\n",
       "      <td>0.076045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL3693750</td>\n",
       "      <td>2968</td>\n",
       "      <td>7.43</td>\n",
       "      <td>7.415298</td>\n",
       "      <td>7.417064</td>\n",
       "      <td>7.430293</td>\n",
       "      <td>7.405985</td>\n",
       "      <td>7.412671</td>\n",
       "      <td>7.418552</td>\n",
       "      <td>0.008890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL152665</td>\n",
       "      <td>2969</td>\n",
       "      <td>5.96</td>\n",
       "      <td>6.359609</td>\n",
       "      <td>6.401652</td>\n",
       "      <td>6.463235</td>\n",
       "      <td>6.393181</td>\n",
       "      <td>6.451435</td>\n",
       "      <td>6.338185</td>\n",
       "      <td>0.172723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL3693789</td>\n",
       "      <td>2970</td>\n",
       "      <td>8.52</td>\n",
       "      <td>8.097815</td>\n",
       "      <td>8.189812</td>\n",
       "      <td>8.116910</td>\n",
       "      <td>8.078846</td>\n",
       "      <td>8.150777</td>\n",
       "      <td>8.192360</td>\n",
       "      <td>0.150887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_rf0  y_pred_rf1  \\\n",
       "0          CHEMBL356066            0     8.02    7.174476    7.267219   \n",
       "1         CHEMBL3652228            1     8.05    8.153331    8.050878   \n",
       "2         CHEMBL3939518            2     6.87    7.500500    7.443817   \n",
       "3         CHEMBL4281792            3     7.22    8.188674    7.859492   \n",
       "4         CHEMBL4070232            4     7.15    6.641647    6.772817   \n",
       "...                 ...          ...      ...         ...         ...   \n",
       "2966      CHEMBL4202521         2966     7.43    6.692375    6.812465   \n",
       "2967       CHEMBL216641         2967     7.35    7.530590    7.456516   \n",
       "2968      CHEMBL3693750         2968     7.43    7.415298    7.417064   \n",
       "2969       CHEMBL152665         2969     5.96    6.359609    6.401652   \n",
       "2970      CHEMBL3693789         2970     8.52    8.097815    8.189812   \n",
       "\n",
       "      y_pred_rf2  y_pred_rf3  y_pred_rf4  y_pred_rf_ave  y_pred_rf_std  \n",
       "0       7.209938    7.264304    7.165649       7.350264       0.302078  \n",
       "1       8.049879    8.035496    8.023369       8.060492       0.042695  \n",
       "2       7.485595    7.504658    7.417742       7.370385       0.225925  \n",
       "3       7.526613    7.962881    7.868271       7.770989       0.314083  \n",
       "4       6.769940    6.760186    6.741241       6.805972       0.160230  \n",
       "...          ...         ...         ...            ...            ...  \n",
       "2966    6.850385    6.739560    6.659054       6.863973       0.261451  \n",
       "2967    7.580243    7.486778    7.406570       7.468449       0.076045  \n",
       "2968    7.430293    7.405985    7.412671       7.418552       0.008890  \n",
       "2969    6.463235    6.393181    6.451435       6.338185       0.172723  \n",
       "2970    8.116910    8.078846    8.150777       8.192360       0.150887  \n",
       "\n",
       "[2971 rows x 10 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_rf_cat = np.where((y_pred_optimized_rf >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_rf_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_rf))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "rf_run0 = data_rf[['y_test_idx0', 'y_test0', 'y_pred_rf0']]\n",
    "rf_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "rf_run0.reset_index(inplace=True, drop=True)\n",
    "rf_run1 = data_rf[['y_test_idx1', 'y_test1', 'y_pred_rf1']]\n",
    "rf_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "rf_run1.reset_index(inplace=True, drop=True)\n",
    "rf_run2 = data_rf[['y_test_idx2', 'y_test2', 'y_pred_rf2']]\n",
    "rf_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "rf_run2.reset_index(inplace=True, drop=True)\n",
    "rf_run3 = data_rf[['y_test_idx3', 'y_test3', 'y_pred_rf3']]\n",
    "rf_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "rf_run3.reset_index(inplace=True, drop=True)\n",
    "rf_run4 = data_rf[['y_test_idx4', 'y_test4', 'y_pred_rf4']]\n",
    "rf_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "rf_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "rf_5preds = pd.concat([chembl_id, rf_run0, rf_run1, rf_run2, rf_run3, rf_run4], axis=1)\n",
    "rf_5preds = rf_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_rf0', 'y_pred_rf1', 'y_pred_rf2', 'y_pred_rf3', 'y_pred_rf4']]\n",
    "rf_5preds['y_pred_rf_ave'] = rf_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "rf_5preds['y_pred_rf_std'] = rf_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "rf_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47203ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.696307</td>\n",
       "      <td>0.032259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.854523</td>\n",
       "      <td>0.015821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.871272</td>\n",
       "      <td>0.019052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.897729</td>\n",
       "      <td>0.020827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.784886</td>\n",
       "      <td>0.032495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.884051</td>\n",
       "      <td>0.013044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.853761</td>\n",
       "      <td>0.015912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.844103</td>\n",
       "      <td>0.017266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.841306</td>\n",
       "      <td>0.017698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.689750</td>\n",
       "      <td>0.034577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.825796</td>\n",
       "      <td>0.033412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.841306</td>\n",
       "      <td>0.017698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.696307     0.032259\n",
       "1              Accuracy         0.854523     0.015821\n",
       "2             Precision         0.871272     0.019052\n",
       "3           Sensitivity         0.897729     0.020827\n",
       "4           Specificity         0.784886     0.032495\n",
       "5              F1 score         0.884051     0.013044\n",
       "6   F1 score (weighted)         0.853761     0.015912\n",
       "7      F1 score (macro)         0.844103     0.017266\n",
       "8     Balanced Accuracy         0.841306     0.017698\n",
       "9                   MCC         0.689750     0.034577\n",
       "10                  NPV         0.825796     0.033412\n",
       "11              ROC_AUC         0.841306     0.017698"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_optimized_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d030d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_5preds.to_csv('rf_5test_CV_result.csv')\n",
    "mat_met_optimized_rf.to_csv('mat_met_rf_opt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bfc78124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (RF)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model r2_score 0.6922 with a standard deviation of 0.0462\n",
      "RF optimized model r2_score 0.6945 with a standard deviation of 0.0456\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"RF baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_rf.joblib']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf_reg, \"./rf_reg.joblib\")\n",
    "#joblib.dump(optimized_rf, \"./optimized_rf.joblib\") # fitted to whole training set with last random_state selected\n",
    "joblib.dump(optimizedCV_rf, \"./optimizedCV_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.694830     0.037032\n",
      "1                    TP       162.300000     6.583650\n",
      "2                    TN        90.300000     5.677441\n",
      "3                    FP        23.100000     4.629615\n",
      "4                    FN        21.400000     6.328068\n",
      "5              Accuracy         0.850221     0.028696\n",
      "6             Precision         0.875579     0.023610\n",
      "7           Sensitivity         0.883657     0.033737\n",
      "8           Specificity         0.796280     0.040199\n",
      "9              F1 score         0.879304     0.023700\n",
      "10  F1 score (weighted)         0.849998     0.028543\n",
      "11     F1 score (macro)         0.840752     0.030373\n",
      "12    Balanced Accuracy         0.839970     0.029310\n",
      "13                  MCC         0.682518     0.060699\n",
      "14                  NPV         0.809590     0.049992\n",
      "15              ROC_AUC         0.839970     0.029310\n",
      "CPU times: user 15.1 s, sys: 68 ms, total: 15.2 s\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_reg = lgbm.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=8,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_reg.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"rmse\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_reg.predict(X_test) \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "         # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 02:39:47,404]\u001b[0m A new study created in memory with name: lgbmRegressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:39:50,716]\u001b[0m Trial 0 finished with value: 0.6707583877598374 and parameters: {'n_estimators': 700, 'learning_rate': 0.04523684372753985, 'max_depth': 7, 'max_bin': 171, 'num_leaves': 306}. Best is trial 0 with value: 0.6707583877598374.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:39:52,911]\u001b[0m Trial 1 finished with value: 0.6762175993721412 and parameters: {'n_estimators': 240, 'learning_rate': 0.12495239559161461, 'max_depth': 9, 'max_bin': 227, 'num_leaves': 454}. Best is trial 1 with value: 0.6762175993721412.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:39:55,444]\u001b[0m Trial 2 finished with value: 0.6820215655677535 and parameters: {'n_estimators': 745, 'learning_rate': 0.1311719079706414, 'max_depth': 9, 'max_bin': 292, 'num_leaves': 664}. Best is trial 2 with value: 0.6820215655677535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:39:57,099]\u001b[0m Trial 3 finished with value: 0.6623311566447094 and parameters: {'n_estimators': 835, 'learning_rate': 0.14642550828077605, 'max_depth': 4, 'max_bin': 263, 'num_leaves': 459}. Best is trial 2 with value: 0.6820215655677535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:39:58,962]\u001b[0m Trial 4 finished with value: 0.6506963209135913 and parameters: {'n_estimators': 648, 'learning_rate': 0.1014503289084635, 'max_depth': 3, 'max_bin': 163, 'num_leaves': 149}. Best is trial 2 with value: 0.6820215655677535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:00,376]\u001b[0m Trial 5 finished with value: 0.6496776212400746 and parameters: {'n_estimators': 575, 'learning_rate': 0.18820876798269687, 'max_depth': 3, 'max_bin': 185, 'num_leaves': 53}. Best is trial 2 with value: 0.6820215655677535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:02,635]\u001b[0m Trial 6 finished with value: 0.6766354341681263 and parameters: {'n_estimators': 270, 'learning_rate': 0.07956240721069853, 'max_depth': 8, 'max_bin': 227, 'num_leaves': 566}. Best is trial 2 with value: 0.6820215655677535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:05,032]\u001b[0m Trial 7 finished with value: 0.43333218631643006 and parameters: {'n_estimators': 547, 'learning_rate': 0.0038748096072560925, 'max_depth': 3, 'max_bin': 286, 'num_leaves': 747}. Best is trial 2 with value: 0.6820215655677535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:08,546]\u001b[0m Trial 8 finished with value: 0.6640251338043918 and parameters: {'n_estimators': 363, 'learning_rate': 0.024571607895436488, 'max_depth': 8, 'max_bin': 196, 'num_leaves': 345}. Best is trial 2 with value: 0.6820215655677535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:10,217]\u001b[0m Trial 9 finished with value: 0.6696692942555563 and parameters: {'n_estimators': 320, 'learning_rate': 0.19867482682303003, 'max_depth': 7, 'max_bin': 222, 'num_leaves': 562}. Best is trial 2 with value: 0.6820215655677535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:11,386]\u001b[0m Trial 10 finished with value: 0.657753164686955 and parameters: {'n_estimators': 51, 'learning_rate': 0.15429818615436797, 'max_depth': 12, 'max_bin': 299, 'num_leaves': 746}. Best is trial 2 with value: 0.6820215655677535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:14,430]\u001b[0m Trial 11 finished with value: 0.6808655898122029 and parameters: {'n_estimators': 895, 'learning_rate': 0.07237156149687202, 'max_depth': 10, 'max_bin': 253, 'num_leaves': 603}. Best is trial 2 with value: 0.6820215655677535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:17,638]\u001b[0m Trial 12 finished with value: 0.6836554865657045 and parameters: {'n_estimators': 887, 'learning_rate': 0.0712718952854004, 'max_depth': 11, 'max_bin': 262, 'num_leaves': 625}. Best is trial 12 with value: 0.6836554865657045.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:20,319]\u001b[0m Trial 13 finished with value: 0.6798982253329358 and parameters: {'n_estimators': 775, 'learning_rate': 0.11025189110434296, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 646}. Best is trial 12 with value: 0.6836554865657045.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:23,734]\u001b[0m Trial 14 finished with value: 0.6824763209454259 and parameters: {'n_estimators': 754, 'learning_rate': 0.06923013813581616, 'max_depth': 10, 'max_bin': 281, 'num_leaves': 671}. Best is trial 12 with value: 0.6836554865657045.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:26,967]\u001b[0m Trial 15 finished with value: 0.6827926991234209 and parameters: {'n_estimators': 881, 'learning_rate': 0.061058863827585755, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 472}. Best is trial 12 with value: 0.6836554865657045.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:31,297]\u001b[0m Trial 16 finished with value: 0.6845919424024892 and parameters: {'n_estimators': 870, 'learning_rate': 0.043963684603517954, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 479}. Best is trial 16 with value: 0.6845919424024892.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:33,856]\u001b[0m Trial 17 finished with value: 0.6554235679722875 and parameters: {'n_estimators': 447, 'learning_rate': 0.037864013713539146, 'max_depth': 5, 'max_bin': 240, 'num_leaves': 227}. Best is trial 16 with value: 0.6845919424024892.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:39,180]\u001b[0m Trial 18 finished with value: 0.6855064700512495 and parameters: {'n_estimators': 639, 'learning_rate': 0.029190132034776092, 'max_depth': 11, 'max_bin': 210, 'num_leaves': 504}. Best is trial 18 with value: 0.6855064700512495.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:45,855]\u001b[0m Trial 19 finished with value: 0.18423197616743958 and parameters: {'n_estimators': 626, 'learning_rate': 0.0003831873262236628, 'max_depth': 6, 'max_bin': 210, 'num_leaves': 408}. Best is trial 18 with value: 0.6855064700512495.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:51,792]\u001b[0m Trial 20 finished with value: 0.6849412142206324 and parameters: {'n_estimators': 504, 'learning_rate': 0.023266225797023254, 'max_depth': 11, 'max_bin': 206, 'num_leaves': 516}. Best is trial 18 with value: 0.6855064700512495.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:40:57,559]\u001b[0m Trial 21 finished with value: 0.679642398382157 and parameters: {'n_estimators': 476, 'learning_rate': 0.021671843384282144, 'max_depth': 11, 'max_bin': 204, 'num_leaves': 517}. Best is trial 18 with value: 0.6855064700512495.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:41:01,873]\u001b[0m Trial 22 finished with value: 0.6846861587822571 and parameters: {'n_estimators': 461, 'learning_rate': 0.04638437442318237, 'max_depth': 10, 'max_bin': 183, 'num_leaves': 357}. Best is trial 18 with value: 0.6855064700512495.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:41:07,400]\u001b[0m Trial 23 finished with value: 0.6777757895555887 and parameters: {'n_estimators': 473, 'learning_rate': 0.019303372705885335, 'max_depth': 10, 'max_bin': 178, 'num_leaves': 366}. Best is trial 18 with value: 0.6855064700512495.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:41:11,992]\u001b[0m Trial 24 finished with value: 0.6862090103103351 and parameters: {'n_estimators': 409, 'learning_rate': 0.0428692677746269, 'max_depth': 12, 'max_bin': 190, 'num_leaves': 279}. Best is trial 24 with value: 0.6862090103103351.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:41:17,136]\u001b[0m Trial 25 finished with value: 0.6826718130110427 and parameters: {'n_estimators': 392, 'learning_rate': 0.026726379411520854, 'max_depth': 12, 'max_bin': 154, 'num_leaves': 268}. Best is trial 24 with value: 0.6862090103103351.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:41:20,333]\u001b[0m Trial 26 finished with value: 0.6860832550329732 and parameters: {'n_estimators': 549, 'learning_rate': 0.08807527994593109, 'max_depth': 12, 'max_bin': 196, 'num_leaves': 143}. Best is trial 24 with value: 0.6862090103103351.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:41:22,269]\u001b[0m Trial 27 finished with value: 0.6814118533251553 and parameters: {'n_estimators': 140, 'learning_rate': 0.08674451448067622, 'max_depth': 12, 'max_bin': 191, 'num_leaves': 174}. Best is trial 24 with value: 0.6862090103103351.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:41:25,705]\u001b[0m Trial 28 finished with value: 0.6854972565843435 and parameters: {'n_estimators': 604, 'learning_rate': 0.05463734573132847, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 66}. Best is trial 24 with value: 0.6862090103103351.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:41:28,382]\u001b[0m Trial 29 finished with value: 0.6786215337746487 and parameters: {'n_estimators': 688, 'learning_rate': 0.09348616719614054, 'max_depth': 9, 'max_bin': 170, 'num_leaves': 146}. Best is trial 24 with value: 0.6862090103103351.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:41:32,073]\u001b[0m Trial 30 finished with value: 0.6844783839625954 and parameters: {'n_estimators': 524, 'learning_rate': 0.055571278833054516, 'max_depth': 12, 'max_bin': 196, 'num_leaves': 295}. Best is trial 24 with value: 0.6862090103103351.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 02:41:36,018]\u001b[0m Trial 31 finished with value: 0.6854954887003599 and parameters: {'n_estimators': 602, 'learning_rate': 0.05226080547044741, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 44}. Best is trial 24 with value: 0.6862090103103351.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:41:39,203]\u001b[0m Trial 32 finished with value: 0.6838642768144745 and parameters: {'n_estimators': 408, 'learning_rate': 0.060505173029643755, 'max_depth': 11, 'max_bin': 235, 'num_leaves': 102}. Best is trial 24 with value: 0.6862090103103351.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:41:43,589]\u001b[0m Trial 33 finished with value: 0.6842944463980716 and parameters: {'n_estimators': 676, 'learning_rate': 0.03421623225650585, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 225}. Best is trial 24 with value: 0.6862090103103351.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:41:45,812]\u001b[0m Trial 34 finished with value: 0.6777797339600184 and parameters: {'n_estimators': 568, 'learning_rate': 0.11477082251772627, 'max_depth': 9, 'max_bin': 197, 'num_leaves': 101}. Best is trial 24 with value: 0.6862090103103351.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:41:54,299]\u001b[0m Trial 35 finished with value: 0.6707158438548566 and parameters: {'n_estimators': 722, 'learning_rate': 0.008393380841134869, 'max_depth': 10, 'max_bin': 233, 'num_leaves': 183}. Best is trial 24 with value: 0.6862090103103351.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:41:57,062]\u001b[0m Trial 36 finished with value: 0.6835621495545433 and parameters: {'n_estimators': 791, 'learning_rate': 0.08399113389393614, 'max_depth': 11, 'max_bin': 176, 'num_leaves': 81}. Best is trial 24 with value: 0.6862090103103351.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:41:59,904]\u001b[0m Trial 37 finished with value: 0.6875890318015646 and parameters: {'n_estimators': 644, 'learning_rate': 0.09985726080220583, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 233}. Best is trial 37 with value: 0.6875890318015646.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:02,168]\u001b[0m Trial 38 finished with value: 0.6822428763318709 and parameters: {'n_estimators': 653, 'learning_rate': 0.1314607163767972, 'max_depth': 11, 'max_bin': 187, 'num_leaves': 424}. Best is trial 37 with value: 0.6875890318015646.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:04,028]\u001b[0m Trial 39 finished with value: 0.6754836044354471 and parameters: {'n_estimators': 223, 'learning_rate': 0.15926699920386306, 'max_depth': 8, 'max_bin': 164, 'num_leaves': 244}. Best is trial 37 with value: 0.6875890318015646.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:06,083]\u001b[0m Trial 40 finished with value: 0.66991301647306 and parameters: {'n_estimators': 333, 'learning_rate': 0.10149699196295311, 'max_depth': 7, 'max_bin': 203, 'num_leaves': 304}. Best is trial 37 with value: 0.6875890318015646.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:08,588]\u001b[0m Trial 41 finished with value: 0.6820103466590078 and parameters: {'n_estimators': 602, 'learning_rate': 0.12032521914177421, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 150}. Best is trial 37 with value: 0.6875890318015646.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:11,535]\u001b[0m Trial 42 finished with value: 0.6860715966355726 and parameters: {'n_estimators': 543, 'learning_rate': 0.09404407488370313, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 195}. Best is trial 37 with value: 0.6875890318015646.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:14,602]\u001b[0m Trial 43 finished with value: 0.6889680803170803 and parameters: {'n_estimators': 524, 'learning_rate': 0.09986329043489525, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 188}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:17,596]\u001b[0m Trial 44 finished with value: 0.683866001941215 and parameters: {'n_estimators': 552, 'learning_rate': 0.1021756510019403, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 199}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:19,968]\u001b[0m Trial 45 finished with value: 0.679939225489427 and parameters: {'n_estimators': 421, 'learning_rate': 0.12777345265817805, 'max_depth': 10, 'max_bin': 230, 'num_leaves': 271}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:22,655]\u001b[0m Trial 46 finished with value: 0.6802122821168823 and parameters: {'n_estimators': 524, 'learning_rate': 0.13945064253096381, 'max_depth': 12, 'max_bin': 200, 'num_leaves': 128}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:24,654]\u001b[0m Trial 47 finished with value: 0.6620599357024335 and parameters: {'n_estimators': 499, 'learning_rate': 0.09656554117978501, 'max_depth': 4, 'max_bin': 241, 'num_leaves': 201}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:27,601]\u001b[0m Trial 48 finished with value: 0.6825495928970994 and parameters: {'n_estimators': 379, 'learning_rate': 0.10978348100713337, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 327}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:30,174]\u001b[0m Trial 49 finished with value: 0.6792484565985807 and parameters: {'n_estimators': 278, 'learning_rate': 0.07661737593052168, 'max_depth': 9, 'max_bin': 190, 'num_leaves': 249}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6890\n",
      "\tBest params:\n",
      "\t\tn_estimators: 524\n",
      "\t\tlearning_rate: 0.09986329043489525\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 228\n",
      "\t\tnum_leaves: 188\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.665689\n",
      "1                    TP  314.000000\n",
      "2                    TN  180.000000\n",
      "3                    FP   57.000000\n",
      "4                    FN   44.000000\n",
      "5              Accuracy    0.830252\n",
      "6             Precision    0.846361\n",
      "7           Sensitivity    0.877095\n",
      "8           Specificity    0.759500\n",
      "9              F1 score    0.861454\n",
      "10  F1 score (weighted)    0.829372\n",
      "11     F1 score (macro)    0.821183\n",
      "12    Balanced Accuracy    0.818294\n",
      "13                  MCC    0.643226\n",
      "14                  NPV    0.803600\n",
      "15              ROC_AUC    0.818294\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_lgbm_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_0_cat = np.where((y_pred_lgbm_0>= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 02:42:32,973]\u001b[0m Trial 50 finished with value: 0.663426572710603 and parameters: {'n_estimators': 570, 'learning_rate': 0.0894808759729541, 'max_depth': 6, 'max_bin': 245, 'num_leaves': 121}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:36,327]\u001b[0m Trial 51 finished with value: 0.6784029601033963 and parameters: {'n_estimators': 655, 'learning_rate': 0.06572247953466899, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 168}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:39,380]\u001b[0m Trial 52 finished with value: 0.6762639347984398 and parameters: {'n_estimators': 719, 'learning_rate': 0.08006960162383242, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 213}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:42,360]\u001b[0m Trial 53 finished with value: 0.6734983840962331 and parameters: {'n_estimators': 522, 'learning_rate': 0.10817819927483911, 'max_depth': 12, 'max_bin': 193, 'num_leaves': 271}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:45,066]\u001b[0m Trial 54 finished with value: 0.6707317571061948 and parameters: {'n_estimators': 616, 'learning_rate': 0.11636556524981063, 'max_depth': 11, 'max_bin': 212, 'num_leaves': 442}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:53,720]\u001b[0m Trial 55 finished with value: 0.6688414589439777 and parameters: {'n_estimators': 640, 'learning_rate': 0.011566377618509815, 'max_depth': 12, 'max_bin': 182, 'num_leaves': 379}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:55,969]\u001b[0m Trial 56 finished with value: 0.6658673682662151 and parameters: {'n_estimators': 446, 'learning_rate': 0.1678025667871688, 'max_depth': 11, 'max_bin': 229, 'num_leaves': 323}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:42:59,003]\u001b[0m Trial 57 finished with value: 0.6730927688374843 and parameters: {'n_estimators': 585, 'learning_rate': 0.07476663646999633, 'max_depth': 11, 'max_bin': 201, 'num_leaves': 534}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:43:03,668]\u001b[0m Trial 58 finished with value: 0.6756207300041074 and parameters: {'n_estimators': 550, 'learning_rate': 0.035896660676274776, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 243}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:43:06,674]\u001b[0m Trial 59 finished with value: 0.6697717114441909 and parameters: {'n_estimators': 823, 'learning_rate': 0.10446027092460115, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 701}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:43:09,478]\u001b[0m Trial 60 finished with value: 0.6760248868887972 and parameters: {'n_estimators': 495, 'learning_rate': 0.09400785297001817, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 163}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:43:13,745]\u001b[0m Trial 61 finished with value: 0.6789874015176812 and parameters: {'n_estimators': 672, 'learning_rate': 0.04826754680999729, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 62}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:43:19,063]\u001b[0m Trial 62 finished with value: 0.6723930207049615 and parameters: {'n_estimators': 594, 'learning_rate': 0.030919800700060646, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 30}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:43:23,326]\u001b[0m Trial 63 finished with value: 0.67614458953094 and parameters: {'n_estimators': 708, 'learning_rate': 0.043695576626222345, 'max_depth': 11, 'max_bin': 217, 'num_leaves': 77}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:43:26,926]\u001b[0m Trial 64 finished with value: 0.6756498979107936 and parameters: {'n_estimators': 438, 'learning_rate': 0.05903458590942613, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 191}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:43:30,698]\u001b[0m Trial 65 finished with value: 0.6786813274826254 and parameters: {'n_estimators': 636, 'learning_rate': 0.0667999144382441, 'max_depth': 11, 'max_bin': 209, 'num_leaves': 119}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:43:33,739]\u001b[0m Trial 66 finished with value: 0.6756136770047533 and parameters: {'n_estimators': 538, 'learning_rate': 0.08660931943224201, 'max_depth': 12, 'max_bin': 197, 'num_leaves': 579}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:43:38,033]\u001b[0m Trial 67 finished with value: 0.6766624067250571 and parameters: {'n_estimators': 482, 'learning_rate': 0.04223676437759756, 'max_depth': 11, 'max_bin': 205, 'num_leaves': 142}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:43:42,849]\u001b[0m Trial 68 finished with value: 0.6626116925418137 and parameters: {'n_estimators': 348, 'learning_rate': 0.018470960652292312, 'max_depth': 12, 'max_bin': 188, 'num_leaves': 95}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:43:47,037]\u001b[0m Trial 69 finished with value: 0.6775559771973579 and parameters: {'n_estimators': 751, 'learning_rate': 0.05243151405040708, 'max_depth': 11, 'max_bin': 237, 'num_leaves': 487}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:43:49,813]\u001b[0m Trial 70 finished with value: 0.6741487714101047 and parameters: {'n_estimators': 561, 'learning_rate': 0.08198583430962499, 'max_depth': 12, 'max_bin': 173, 'num_leaves': 167}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:43:53,877]\u001b[0m Trial 71 finished with value: 0.6738413303458335 and parameters: {'n_estimators': 594, 'learning_rate': 0.05161543070036448, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 37}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:43:57,226]\u001b[0m Trial 72 finished with value: 0.6766381300472315 and parameters: {'n_estimators': 628, 'learning_rate': 0.06359203652219897, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 63}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:44:01,852]\u001b[0m Trial 73 finished with value: 0.678309025312443 and parameters: {'n_estimators': 689, 'learning_rate': 0.03926064826041228, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 89}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:44:07,466]\u001b[0m Trial 74 finished with value: 0.6764137853025725 and parameters: {'n_estimators': 609, 'learning_rate': 0.030073627605524247, 'max_depth': 10, 'max_bin': 223, 'num_leaves': 214}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:44:10,136]\u001b[0m Trial 75 finished with value: 0.6720231901347984 and parameters: {'n_estimators': 519, 'learning_rate': 0.09101237315722062, 'max_depth': 11, 'max_bin': 194, 'num_leaves': 49}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:44:13,359]\u001b[0m Trial 76 finished with value: 0.675527809555984 and parameters: {'n_estimators': 572, 'learning_rate': 0.0711655515088902, 'max_depth': 12, 'max_bin': 199, 'num_leaves': 119}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:44:16,083]\u001b[0m Trial 77 finished with value: 0.6754237996234729 and parameters: {'n_estimators': 667, 'learning_rate': 0.09656930842803728, 'max_depth': 12, 'max_bin': 203, 'num_leaves': 289}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:44:22,101]\u001b[0m Trial 78 finished with value: 0.6640385168339897 and parameters: {'n_estimators': 471, 'learning_rate': 0.014489106522838219, 'max_depth': 11, 'max_bin': 180, 'num_leaves': 150}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:44:24,654]\u001b[0m Trial 79 finished with value: 0.6716421332796786 and parameters: {'n_estimators': 622, 'learning_rate': 0.12173527340417538, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 226}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:44:26,532]\u001b[0m Trial 80 finished with value: 0.6613516483933951 and parameters: {'n_estimators': 538, 'learning_rate': 0.13426411357029586, 'max_depth': 6, 'max_bin': 228, 'num_leaves': 396}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 02:44:30,481]\u001b[0m Trial 81 finished with value: 0.6552131284045654 and parameters: {'n_estimators': 421, 'learning_rate': 0.023769750049169334, 'max_depth': 8, 'max_bin': 207, 'num_leaves': 563}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:44:36,286]\u001b[0m Trial 82 finished with value: 0.6783033935372658 and parameters: {'n_estimators': 502, 'learning_rate': 0.029342638765390745, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 522}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:44:40,102]\u001b[0m Trial 83 finished with value: 0.6735412044009159 and parameters: {'n_estimators': 582, 'learning_rate': 0.057308218938345395, 'max_depth': 10, 'max_bin': 220, 'num_leaves': 493}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:44:48,776]\u001b[0m Trial 84 finished with value: 0.6196648958707331 and parameters: {'n_estimators': 502, 'learning_rate': 0.004529984802984437, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 178}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:44:55,512]\u001b[0m Trial 85 finished with value: 0.6670733842728493 and parameters: {'n_estimators': 540, 'learning_rate': 0.016855052061535827, 'max_depth': 11, 'max_bin': 185, 'num_leaves': 610}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:45:00,228]\u001b[0m Trial 86 finished with value: 0.6768633079303203 and parameters: {'n_estimators': 382, 'learning_rate': 0.035307397460492176, 'max_depth': 12, 'max_bin': 205, 'num_leaves': 443}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:45:02,368]\u001b[0m Trial 87 finished with value: 0.6634747753814951 and parameters: {'n_estimators': 729, 'learning_rate': 0.11331760132525723, 'max_depth': 9, 'max_bin': 210, 'num_leaves': 244}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:45:05,185]\u001b[0m Trial 88 finished with value: 0.6740265891246037 and parameters: {'n_estimators': 454, 'learning_rate': 0.10584010854749004, 'max_depth': 11, 'max_bin': 231, 'num_leaves': 547}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:45:08,117]\u001b[0m Trial 89 finished with value: 0.6702637116363728 and parameters: {'n_estimators': 646, 'learning_rate': 0.09932844970170854, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 74}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:45:14,268]\u001b[0m Trial 90 finished with value: 0.6749080685373197 and parameters: {'n_estimators': 561, 'learning_rate': 0.024149566213418523, 'max_depth': 11, 'max_bin': 201, 'num_leaves': 467}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:45:18,183]\u001b[0m Trial 91 finished with value: 0.6742212660650766 and parameters: {'n_estimators': 410, 'learning_rate': 0.04848247419350697, 'max_depth': 10, 'max_bin': 192, 'num_leaves': 353}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:45:23,282]\u001b[0m Trial 92 finished with value: 0.6784316537216912 and parameters: {'n_estimators': 464, 'learning_rate': 0.039788091879239304, 'max_depth': 12, 'max_bin': 168, 'num_leaves': 324}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:45:27,164]\u001b[0m Trial 93 finished with value: 0.6762638044600052 and parameters: {'n_estimators': 511, 'learning_rate': 0.048126950930428666, 'max_depth': 11, 'max_bin': 197, 'num_leaves': 264}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:45:30,818]\u001b[0m Trial 94 finished with value: 0.6737449301929497 and parameters: {'n_estimators': 438, 'learning_rate': 0.05411125600452947, 'max_depth': 12, 'max_bin': 189, 'num_leaves': 286}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:45:35,954]\u001b[0m Trial 95 finished with value: 0.6771537973666029 and parameters: {'n_estimators': 606, 'learning_rate': 0.03348088081124516, 'max_depth': 12, 'max_bin': 184, 'num_leaves': 207}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:45:39,352]\u001b[0m Trial 96 finished with value: 0.6754704297373507 and parameters: {'n_estimators': 485, 'learning_rate': 0.0771168693018092, 'max_depth': 11, 'max_bin': 177, 'num_leaves': 504}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:45:40,490]\u001b[0m Trial 97 finished with value: 0.6406062248306365 and parameters: {'n_estimators': 58, 'learning_rate': 0.0866031286847962, 'max_depth': 10, 'max_bin': 227, 'num_leaves': 410}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:45:45,004]\u001b[0m Trial 98 finished with value: 0.6769869814418008 and parameters: {'n_estimators': 525, 'learning_rate': 0.04344850535668618, 'max_depth': 12, 'max_bin': 155, 'num_leaves': 133}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:45:48,449]\u001b[0m Trial 99 finished with value: 0.6769426975483221 and parameters: {'n_estimators': 364, 'learning_rate': 0.06241972996463901, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 188}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6890\n",
      "\tBest params:\n",
      "\t\tn_estimators: 524\n",
      "\t\tlearning_rate: 0.09986329043489525\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 228\n",
      "\t\tnum_leaves: 188\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.665689    0.729387\n",
      "1                    TP  314.000000  332.000000\n",
      "2                    TN  180.000000  168.000000\n",
      "3                    FP   57.000000   50.000000\n",
      "4                    FN   44.000000   45.000000\n",
      "5              Accuracy    0.830252    0.840336\n",
      "6             Precision    0.846361    0.869110\n",
      "7           Sensitivity    0.877095    0.880637\n",
      "8           Specificity    0.759500    0.770600\n",
      "9              F1 score    0.861454    0.874835\n",
      "10  F1 score (weighted)    0.829372    0.839936\n",
      "11     F1 score (macro)    0.821183    0.827209\n",
      "12    Balanced Accuracy    0.818294    0.825639\n",
      "13                  MCC    0.643226    0.654552\n",
      "14                  NPV    0.803600    0.788700\n",
      "15              ROC_AUC    0.818294    0.825639\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_lgbm_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    " \n",
    "y_pred_lgbm_1_cat = np.where((y_pred_lgbm_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 02:45:51,544]\u001b[0m Trial 100 finished with value: 0.6746622513731355 and parameters: {'n_estimators': 306, 'learning_rate': 0.1115940910192676, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 352}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:45:56,087]\u001b[0m Trial 101 finished with value: 0.6754994787543072 and parameters: {'n_estimators': 591, 'learning_rate': 0.048045377859374384, 'max_depth': 11, 'max_bin': 297, 'num_leaves': 581}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:46:01,422]\u001b[0m Trial 102 finished with value: 0.6844813006365474 and parameters: {'n_estimators': 546, 'learning_rate': 0.04208828596647454, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 455}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:46:05,378]\u001b[0m Trial 103 finished with value: 0.674803927407144 and parameters: {'n_estimators': 830, 'learning_rate': 0.055738108747720505, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 431}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:46:10,818]\u001b[0m Trial 104 finished with value: 0.6758880637368863 and parameters: {'n_estimators': 852, 'learning_rate': 0.02684475005242518, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 106}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:46:18,034]\u001b[0m Trial 105 finished with value: 0.67566934135059 and parameters: {'n_estimators': 698, 'learning_rate': 0.020688099838442404, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 516}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:46:21,380]\u001b[0m Trial 106 finished with value: 0.6807957118172518 and parameters: {'n_estimators': 492, 'learning_rate': 0.0984894201288414, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 474}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:46:24,908]\u001b[0m Trial 107 finished with value: 0.6756858430895004 and parameters: {'n_estimators': 576, 'learning_rate': 0.09211197386915068, 'max_depth': 11, 'max_bin': 194, 'num_leaves': 544}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:46:28,762]\u001b[0m Trial 108 finished with value: 0.6848003156778597 and parameters: {'n_estimators': 796, 'learning_rate': 0.06978578686144786, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 259}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:46:32,732]\u001b[0m Trial 109 finished with value: 0.679140042950946 and parameters: {'n_estimators': 797, 'learning_rate': 0.06654218055810637, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 309}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:46:36,484]\u001b[0m Trial 110 finished with value: 0.6821561212575694 and parameters: {'n_estimators': 769, 'learning_rate': 0.07307258100832191, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 256}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:46:40,401]\u001b[0m Trial 111 finished with value: 0.6790040448282685 and parameters: {'n_estimators': 889, 'learning_rate': 0.05929893955018724, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 221}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:46:43,505]\u001b[0m Trial 112 finished with value: 0.6416628533789742 and parameters: {'n_estimators': 865, 'learning_rate': 0.03781150400961978, 'max_depth': 4, 'max_bin': 259, 'num_leaves': 200}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:46:46,742]\u001b[0m Trial 113 finished with value: 0.6779302924736486 and parameters: {'n_estimators': 795, 'learning_rate': 0.0840511436059626, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 230}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:46:50,942]\u001b[0m Trial 114 finished with value: 0.6764813579459423 and parameters: {'n_estimators': 866, 'learning_rate': 0.05022462375818621, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 234}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:46:55,366]\u001b[0m Trial 115 finished with value: 0.6785403229763316 and parameters: {'n_estimators': 814, 'learning_rate': 0.045176650433084116, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 162}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:46:57,681]\u001b[0m Trial 116 finished with value: 0.671375207834008 and parameters: {'n_estimators': 669, 'learning_rate': 0.11891030174482503, 'max_depth': 11, 'max_bin': 257, 'num_leaves': 41}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:47:00,790]\u001b[0m Trial 117 finished with value: 0.676613701844089 and parameters: {'n_estimators': 613, 'learning_rate': 0.10258864743446344, 'max_depth': 9, 'max_bin': 233, 'num_leaves': 278}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:47:03,265]\u001b[0m Trial 118 finished with value: 0.6655872826942996 and parameters: {'n_estimators': 530, 'learning_rate': 0.07981339095158366, 'max_depth': 7, 'max_bin': 180, 'num_leaves': 374}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:47:06,266]\u001b[0m Trial 119 finished with value: 0.6771204153244631 and parameters: {'n_estimators': 426, 'learning_rate': 0.10727058581594234, 'max_depth': 12, 'max_bin': 199, 'num_leaves': 65}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:47:12,196]\u001b[0m Trial 120 finished with value: 0.6811285596455342 and parameters: {'n_estimators': 632, 'learning_rate': 0.02965379582778959, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 307}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:47:17,536]\u001b[0m Trial 121 finished with value: 0.6809835397073098 and parameters: {'n_estimators': 551, 'learning_rate': 0.04287113624057019, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 448}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:47:23,042]\u001b[0m Trial 122 finished with value: 0.6773304562026926 and parameters: {'n_estimators': 464, 'learning_rate': 0.033671628175967534, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 484}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:47:28,810]\u001b[0m Trial 123 finished with value: 0.6842368549054847 and parameters: {'n_estimators': 561, 'learning_rate': 0.040487306782742156, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 510}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:47:32,974]\u001b[0m Trial 124 finished with value: 0.6771980828325783 and parameters: {'n_estimators': 396, 'learning_rate': 0.05291783322506183, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 459}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:47:37,442]\u001b[0m Trial 125 finished with value: 0.6762139581576274 and parameters: {'n_estimators': 507, 'learning_rate': 0.04624882479330986, 'max_depth': 11, 'max_bin': 265, 'num_leaves': 531}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:47:42,970]\u001b[0m Trial 126 finished with value: 0.6784577877445211 and parameters: {'n_estimators': 550, 'learning_rate': 0.03735239543326101, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 494}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:47:47,199]\u001b[0m Trial 127 finished with value: 0.6802074339357485 and parameters: {'n_estimators': 586, 'learning_rate': 0.07006249086537816, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 403}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:47:50,259]\u001b[0m Trial 128 finished with value: 0.6787844139553162 and parameters: {'n_estimators': 600, 'learning_rate': 0.08920532498606303, 'max_depth': 11, 'max_bin': 222, 'num_leaves': 186}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:47:54,045]\u001b[0m Trial 129 finished with value: 0.6770094864752209 and parameters: {'n_estimators': 899, 'learning_rate': 0.06418033380417565, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 417}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:48:01,072]\u001b[0m Trial 130 finished with value: 0.665576709327292 and parameters: {'n_estimators': 651, 'learning_rate': 0.012946762106747505, 'max_depth': 10, 'max_bin': 288, 'num_leaves': 156}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 02:48:05,418]\u001b[0m Trial 131 finished with value: 0.6804114224854073 and parameters: {'n_estimators': 525, 'learning_rate': 0.05726205518638411, 'max_depth': 12, 'max_bin': 195, 'num_leaves': 337}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:48:08,485]\u001b[0m Trial 132 finished with value: 0.6783601609252169 and parameters: {'n_estimators': 487, 'learning_rate': 0.0941201803445504, 'max_depth': 12, 'max_bin': 186, 'num_leaves': 263}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:48:12,605]\u001b[0m Trial 133 finished with value: 0.6808870872546839 and parameters: {'n_estimators': 537, 'learning_rate': 0.05337900863707235, 'max_depth': 12, 'max_bin': 191, 'num_leaves': 300}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:48:16,573]\u001b[0m Trial 134 finished with value: 0.6811938104978819 and parameters: {'n_estimators': 571, 'learning_rate': 0.061128601923166256, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 248}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:48:18,921]\u001b[0m Trial 135 finished with value: 0.6628594004922685 and parameters: {'n_estimators': 510, 'learning_rate': 0.1921243437228417, 'max_depth': 12, 'max_bin': 204, 'num_leaves': 283}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:48:23,135]\u001b[0m Trial 136 finished with value: 0.673522312408622 and parameters: {'n_estimators': 453, 'learning_rate': 0.044656189967387, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 111}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:48:26,789]\u001b[0m Trial 137 finished with value: 0.676980064715854 and parameters: {'n_estimators': 474, 'learning_rate': 0.05127821901506833, 'max_depth': 11, 'max_bin': 198, 'num_leaves': 175}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:48:32,238]\u001b[0m Trial 138 finished with value: 0.6793420533485952 and parameters: {'n_estimators': 553, 'learning_rate': 0.0327343518465123, 'max_depth': 12, 'max_bin': 188, 'num_leaves': 208}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:48:37,193]\u001b[0m Trial 139 finished with value: 0.6794631185821911 and parameters: {'n_estimators': 743, 'learning_rate': 0.04156444672393345, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 431}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:48:39,553]\u001b[0m Trial 140 finished with value: 0.6749715135388537 and parameters: {'n_estimators': 620, 'learning_rate': 0.17302518205879328, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 385}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:48:45,384]\u001b[0m Trial 141 finished with value: 0.6805664619215986 and parameters: {'n_estimators': 673, 'learning_rate': 0.02654743816824196, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 229}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:48:50,802]\u001b[0m Trial 142 finished with value: 0.6816062358478026 and parameters: {'n_estimators': 681, 'learning_rate': 0.039120445661553385, 'max_depth': 12, 'max_bin': 183, 'num_leaves': 256}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:48:57,087]\u001b[0m Trial 143 finished with value: 0.6756310090997037 and parameters: {'n_estimators': 578, 'learning_rate': 0.021291498537856654, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 135}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:49:02,539]\u001b[0m Trial 144 finished with value: 0.6808070266960697 and parameters: {'n_estimators': 714, 'learning_rate': 0.03562648719892986, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 479}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:49:06,558]\u001b[0m Trial 145 finished with value: 0.679650428067402 and parameters: {'n_estimators': 655, 'learning_rate': 0.05678053685455288, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 295}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:49:11,037]\u001b[0m Trial 146 finished with value: 0.676413863608176 and parameters: {'n_estimators': 604, 'learning_rate': 0.047476709303809005, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 87}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:49:14,305]\u001b[0m Trial 147 finished with value: 0.6761425549787088 and parameters: {'n_estimators': 638, 'learning_rate': 0.10032780934333962, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 213}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:49:18,084]\u001b[0m Trial 148 finished with value: 0.6823136977289076 and parameters: {'n_estimators': 521, 'learning_rate': 0.0757109379899693, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 238}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:49:23,201]\u001b[0m Trial 149 finished with value: 0.6781404527133211 and parameters: {'n_estimators': 540, 'learning_rate': 0.030064648967808697, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 196}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6890\n",
      "\tBest params:\n",
      "\t\tn_estimators: 524\n",
      "\t\tlearning_rate: 0.09986329043489525\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 228\n",
      "\t\tnum_leaves: 188\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.665689    0.729387    0.729845\n",
      "1                    TP  314.000000  332.000000  324.000000\n",
      "2                    TN  180.000000  168.000000  195.000000\n",
      "3                    FP   57.000000   50.000000   42.000000\n",
      "4                    FN   44.000000   45.000000   34.000000\n",
      "5              Accuracy    0.830252    0.840336    0.872269\n",
      "6             Precision    0.846361    0.869110    0.885246\n",
      "7           Sensitivity    0.877095    0.880637    0.905028\n",
      "8           Specificity    0.759500    0.770600    0.822800\n",
      "9              F1 score    0.861454    0.874835    0.895028\n",
      "10  F1 score (weighted)    0.829372    0.839936    0.871878\n",
      "11     F1 score (macro)    0.821183    0.827209    0.865969\n",
      "12    Balanced Accuracy    0.818294    0.825639    0.863906\n",
      "13                  MCC    0.643226    0.654552    0.732280\n",
      "14                  NPV    0.803600    0.788700    0.851500\n",
      "15              ROC_AUC    0.818294    0.825639    0.863906\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_lgbm_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_2_cat = np.where((y_pred_lgbm_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 02:49:26,599]\u001b[0m Trial 150 finished with value: 0.6791068407406373 and parameters: {'n_estimators': 589, 'learning_rate': 0.09512631966654449, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 502}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:49:31,050]\u001b[0m Trial 151 finished with value: 0.6820010527345486 and parameters: {'n_estimators': 558, 'learning_rate': 0.04174622657749043, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 505}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:49:36,033]\u001b[0m Trial 152 finished with value: 0.6843958196154005 and parameters: {'n_estimators': 564, 'learning_rate': 0.03867582058587586, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 523}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:49:40,514]\u001b[0m Trial 153 finished with value: 0.6811699046149131 and parameters: {'n_estimators': 846, 'learning_rate': 0.036031003812839996, 'max_depth': 12, 'max_bin': 201, 'num_leaves': 51}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:49:44,844]\u001b[0m Trial 154 finished with value: 0.6802183146980195 and parameters: {'n_estimators': 496, 'learning_rate': 0.049951867398348435, 'max_depth': 12, 'max_bin': 196, 'num_leaves': 526}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:49:50,247]\u001b[0m Trial 155 finished with value: 0.6777483410292363 and parameters: {'n_estimators': 618, 'learning_rate': 0.028084205908215504, 'max_depth': 12, 'max_bin': 205, 'num_leaves': 541}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:49:54,630]\u001b[0m Trial 156 finished with value: 0.6813086403006141 and parameters: {'n_estimators': 517, 'learning_rate': 0.04422460290844616, 'max_depth': 11, 'max_bin': 192, 'num_leaves': 274}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:49:59,784]\u001b[0m Trial 157 finished with value: 0.6809028343739997 and parameters: {'n_estimators': 577, 'learning_rate': 0.032359006099971936, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 464}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:50:04,958]\u001b[0m Trial 158 finished with value: 0.6690970859834798 and parameters: {'n_estimators': 542, 'learning_rate': 0.022682231853002333, 'max_depth': 9, 'max_bin': 203, 'num_leaves': 572}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:50:08,054]\u001b[0m Trial 159 finished with value: 0.6788602442121459 and parameters: {'n_estimators': 766, 'learning_rate': 0.10453196650042787, 'max_depth': 12, 'max_bin': 200, 'num_leaves': 313}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:50:15,725]\u001b[0m Trial 160 finished with value: 0.657474012729124 and parameters: {'n_estimators': 815, 'learning_rate': 0.00884917738101415, 'max_depth': 8, 'max_bin': 218, 'num_leaves': 223}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:50:20,398]\u001b[0m Trial 161 finished with value: 0.6791797393369035 and parameters: {'n_estimators': 600, 'learning_rate': 0.03968421991390234, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 519}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:50:24,677]\u001b[0m Trial 162 finished with value: 0.6792724525532339 and parameters: {'n_estimators': 564, 'learning_rate': 0.05469098766234897, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 33}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:50:29,198]\u001b[0m Trial 163 finished with value: 0.6812692830289502 and parameters: {'n_estimators': 557, 'learning_rate': 0.046111622159676104, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 491}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:50:33,926]\u001b[0m Trial 164 finished with value: 0.6789536828250046 and parameters: {'n_estimators': 531, 'learning_rate': 0.03882097731928149, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 560}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:50:36,864]\u001b[0m Trial 165 finished with value: 0.6718109849637928 and parameters: {'n_estimators': 627, 'learning_rate': 0.08886888229806778, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 477}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:50:41,301]\u001b[0m Trial 166 finished with value: 0.6817968464808084 and parameters: {'n_estimators': 509, 'learning_rate': 0.04985679449521933, 'max_depth': 12, 'max_bin': 196, 'num_leaves': 508}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:50:47,481]\u001b[0m Trial 167 finished with value: 0.6714081517378891 and parameters: {'n_estimators': 569, 'learning_rate': 0.017140951044020764, 'max_depth': 10, 'max_bin': 258, 'num_leaves': 550}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:50:51,255]\u001b[0m Trial 168 finished with value: 0.6782426197702293 and parameters: {'n_estimators': 432, 'learning_rate': 0.060009656932836455, 'max_depth': 11, 'max_bin': 175, 'num_leaves': 513}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:50:56,177]\u001b[0m Trial 169 finished with value: 0.6803376208879939 and parameters: {'n_estimators': 481, 'learning_rate': 0.03335717285928744, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 364}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:00,506]\u001b[0m Trial 170 finished with value: 0.6794660305558453 and parameters: {'n_estimators': 650, 'learning_rate': 0.04193969998364342, 'max_depth': 11, 'max_bin': 189, 'num_leaves': 241}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:03,237]\u001b[0m Trial 171 finished with value: 0.6777396940738518 and parameters: {'n_estimators': 541, 'learning_rate': 0.1099281810254615, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 215}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:06,242]\u001b[0m Trial 172 finished with value: 0.6756628976856394 and parameters: {'n_estimators': 586, 'learning_rate': 0.09919389897929676, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 192}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:09,175]\u001b[0m Trial 173 finished with value: 0.6794147100939572 and parameters: {'n_estimators': 556, 'learning_rate': 0.10189935389429272, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 266}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:11,825]\u001b[0m Trial 174 finished with value: 0.6749946533782946 and parameters: {'n_estimators': 215, 'learning_rate': 0.08314129549985703, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 176}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:15,550]\u001b[0m Trial 175 finished with value: 0.6786961139081116 and parameters: {'n_estimators': 518, 'learning_rate': 0.06689267990093359, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 202}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:18,852]\u001b[0m Trial 176 finished with value: 0.6788290229231067 and parameters: {'n_estimators': 603, 'learning_rate': 0.09643256399724162, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 532}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:24,173]\u001b[0m Trial 177 finished with value: 0.6747489834889588 and parameters: {'n_estimators': 496, 'learning_rate': 0.0252952112302515, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 248}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:26,101]\u001b[0m Trial 178 finished with value: 0.6426803081397997 and parameters: {'n_estimators': 693, 'learning_rate': 0.09263403574801014, 'max_depth': 3, 'max_bin': 207, 'num_leaves': 70}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:29,163]\u001b[0m Trial 179 finished with value: 0.6793585740487172 and parameters: {'n_estimators': 569, 'learning_rate': 0.10639853757050008, 'max_depth': 11, 'max_bin': 180, 'num_leaves': 450}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:31,578]\u001b[0m Trial 180 finished with value: 0.677204833070878 and parameters: {'n_estimators': 532, 'learning_rate': 0.1150426101929892, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 153}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 02:51:35,067]\u001b[0m Trial 181 finished with value: 0.6800839027621218 and parameters: {'n_estimators': 363, 'learning_rate': 0.05430697324036963, 'max_depth': 11, 'max_bin': 239, 'num_leaves': 103}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:37,232]\u001b[0m Trial 182 finished with value: 0.6529242669912996 and parameters: {'n_estimators': 390, 'learning_rate': 0.06207095268052241, 'max_depth': 5, 'max_bin': 233, 'num_leaves': 56}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:40,976]\u001b[0m Trial 183 finished with value: 0.678156130635076 and parameters: {'n_estimators': 445, 'learning_rate': 0.04772243502531782, 'max_depth': 11, 'max_bin': 227, 'num_leaves': 77}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:44,747]\u001b[0m Trial 184 finished with value: 0.6807479719284627 and parameters: {'n_estimators': 399, 'learning_rate': 0.05211382264266278, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 127}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:49,433]\u001b[0m Trial 185 finished with value: 0.6785902615538331 and parameters: {'n_estimators': 873, 'learning_rate': 0.03659375198084139, 'max_depth': 12, 'max_bin': 194, 'num_leaves': 496}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:53,371]\u001b[0m Trial 186 finished with value: 0.68239089078417 and parameters: {'n_estimators': 471, 'learning_rate': 0.05731808328311909, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 92}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:51:57,376]\u001b[0m Trial 187 finished with value: 0.6809859321961145 and parameters: {'n_estimators': 404, 'learning_rate': 0.04480971096473693, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 142}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:52:01,596]\u001b[0m Trial 188 finished with value: 0.6811830293152852 and parameters: {'n_estimators': 550, 'learning_rate': 0.040834884621262724, 'max_depth': 11, 'max_bin': 210, 'num_leaves': 219}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:52:05,110]\u001b[0m Trial 189 finished with value: 0.6786592403357192 and parameters: {'n_estimators': 422, 'learning_rate': 0.0694964464931829, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 331}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:52:09,124]\u001b[0m Trial 190 finished with value: 0.6795303340339726 and parameters: {'n_estimators': 588, 'learning_rate': 0.049899236970163, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 184}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:52:13,048]\u001b[0m Trial 191 finished with value: 0.6793851523349242 and parameters: {'n_estimators': 880, 'learning_rate': 0.06545380236888644, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 646}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:52:16,533]\u001b[0m Trial 192 finished with value: 0.6790818091861435 and parameters: {'n_estimators': 631, 'learning_rate': 0.07662957915525519, 'max_depth': 10, 'max_bin': 218, 'num_leaves': 665}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:52:20,675]\u001b[0m Trial 193 finished with value: 0.681052981151754 and parameters: {'n_estimators': 783, 'learning_rate': 0.05946747080610634, 'max_depth': 11, 'max_bin': 254, 'num_leaves': 720}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:52:23,822]\u001b[0m Trial 194 finished with value: 0.6786739338985563 and parameters: {'n_estimators': 897, 'learning_rate': 0.0868302926963572, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 468}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:52:28,237]\u001b[0m Trial 195 finished with value: 0.6790396543960587 and parameters: {'n_estimators': 846, 'learning_rate': 0.03592193927351984, 'max_depth': 11, 'max_bin': 264, 'num_leaves': 290}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:52:30,921]\u001b[0m Trial 196 finished with value: 0.6782854208990633 and parameters: {'n_estimators': 525, 'learning_rate': 0.10207821283269841, 'max_depth': 12, 'max_bin': 203, 'num_leaves': 46}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:52:35,896]\u001b[0m Trial 197 finished with value: 0.6770884398556566 and parameters: {'n_estimators': 860, 'learning_rate': 0.03090964502024401, 'max_depth': 10, 'max_bin': 260, 'num_leaves': 682}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:52:39,100]\u001b[0m Trial 198 finished with value: 0.6756978114249379 and parameters: {'n_estimators': 570, 'learning_rate': 0.07328670662777359, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 257}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:52:43,700]\u001b[0m Trial 199 finished with value: 0.6805647345982451 and parameters: {'n_estimators': 831, 'learning_rate': 0.043729143744199804, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 481}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6890\n",
      "\tBest params:\n",
      "\t\tn_estimators: 524\n",
      "\t\tlearning_rate: 0.09986329043489525\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 228\n",
      "\t\tnum_leaves: 188\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.665689    0.729387    0.729845    0.683805\n",
      "1                    TP  314.000000  332.000000  324.000000  323.000000\n",
      "2                    TN  180.000000  168.000000  195.000000  186.000000\n",
      "3                    FP   57.000000   50.000000   42.000000   45.000000\n",
      "4                    FN   44.000000   45.000000   34.000000   41.000000\n",
      "5              Accuracy    0.830252    0.840336    0.872269    0.855462\n",
      "6             Precision    0.846361    0.869110    0.885246    0.877717\n",
      "7           Sensitivity    0.877095    0.880637    0.905028    0.887363\n",
      "8           Specificity    0.759500    0.770600    0.822800    0.805200\n",
      "9              F1 score    0.861454    0.874835    0.895028    0.882514\n",
      "10  F1 score (weighted)    0.829372    0.839936    0.871878    0.855226\n",
      "11     F1 score (macro)    0.821183    0.827209    0.865969    0.847370\n",
      "12    Balanced Accuracy    0.818294    0.825639    0.863906    0.846279\n",
      "13                  MCC    0.643226    0.654552    0.732280    0.694825\n",
      "14                  NPV    0.803600    0.788700    0.851500    0.819400\n",
      "15              ROC_AUC    0.818294    0.825639    0.863906    0.846279\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_lgbm_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_3_cat = np.where((y_pred_lgbm_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 02:52:48,679]\u001b[0m Trial 200 finished with value: 0.685995068768177 and parameters: {'n_estimators': 617, 'learning_rate': 0.05475570946855269, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 587}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:52:52,733]\u001b[0m Trial 201 finished with value: 0.6859473182037192 and parameters: {'n_estimators': 662, 'learning_rate': 0.05696594810619406, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 651}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:52:57,176]\u001b[0m Trial 202 finished with value: 0.6865856849823337 and parameters: {'n_estimators': 652, 'learning_rate': 0.05236045359891655, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 646}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:53:01,046]\u001b[0m Trial 203 finished with value: 0.6812182404658771 and parameters: {'n_estimators': 646, 'learning_rate': 0.05295762732997496, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 625}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:53:05,808]\u001b[0m Trial 204 finished with value: 0.6868442177289322 and parameters: {'n_estimators': 671, 'learning_rate': 0.046793734252520994, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 593}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:53:10,227]\u001b[0m Trial 205 finished with value: 0.6853456239101384 and parameters: {'n_estimators': 651, 'learning_rate': 0.047974749353297395, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 613}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:53:15,037]\u001b[0m Trial 206 finished with value: 0.6872986220105237 and parameters: {'n_estimators': 661, 'learning_rate': 0.048095966784280596, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 636}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:53:19,748]\u001b[0m Trial 207 finished with value: 0.6829349149818941 and parameters: {'n_estimators': 676, 'learning_rate': 0.04736621804662241, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 600}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:53:24,047]\u001b[0m Trial 208 finished with value: 0.6845210485317544 and parameters: {'n_estimators': 657, 'learning_rate': 0.05599871201682768, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 641}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:53:28,678]\u001b[0m Trial 209 finished with value: 0.6851524935355282 and parameters: {'n_estimators': 693, 'learning_rate': 0.05457875452531106, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 630}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:53:33,150]\u001b[0m Trial 210 finished with value: 0.686059208895068 and parameters: {'n_estimators': 667, 'learning_rate': 0.055389340340757015, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 642}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:53:36,848]\u001b[0m Trial 211 finished with value: 0.6837906697609084 and parameters: {'n_estimators': 661, 'learning_rate': 0.05772625271880792, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 644}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:53:41,392]\u001b[0m Trial 212 finished with value: 0.6863685424496009 and parameters: {'n_estimators': 699, 'learning_rate': 0.05425642118229744, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 639}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:53:45,686]\u001b[0m Trial 213 finished with value: 0.6848113236179862 and parameters: {'n_estimators': 703, 'learning_rate': 0.05422523469921203, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 629}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:53:50,021]\u001b[0m Trial 214 finished with value: 0.681712447925148 and parameters: {'n_estimators': 704, 'learning_rate': 0.05325775636853225, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 617}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:53:53,426]\u001b[0m Trial 215 finished with value: 0.6853927534878868 and parameters: {'n_estimators': 688, 'learning_rate': 0.06239946563733209, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 591}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:53:56,793]\u001b[0m Trial 216 finished with value: 0.6843077869035643 and parameters: {'n_estimators': 725, 'learning_rate': 0.06241546523237877, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 591}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:54:00,537]\u001b[0m Trial 217 finished with value: 0.6832118170868856 and parameters: {'n_estimators': 685, 'learning_rate': 0.06123735583893651, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 633}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:54:04,339]\u001b[0m Trial 218 finished with value: 0.6799338001892455 and parameters: {'n_estimators': 705, 'learning_rate': 0.05674024975814257, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 656}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:54:08,820]\u001b[0m Trial 219 finished with value: 0.6849208858198101 and parameters: {'n_estimators': 669, 'learning_rate': 0.051105895641375736, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 607}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:54:12,893]\u001b[0m Trial 220 finished with value: 0.6840043394874359 and parameters: {'n_estimators': 669, 'learning_rate': 0.051901166960910315, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 609}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:54:16,928]\u001b[0m Trial 221 finished with value: 0.684414121133229 and parameters: {'n_estimators': 738, 'learning_rate': 0.05028566472562476, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 625}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:54:21,365]\u001b[0m Trial 222 finished with value: 0.688518266723333 and parameters: {'n_estimators': 682, 'learning_rate': 0.05464620418589947, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 595}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:54:25,881]\u001b[0m Trial 223 finished with value: 0.6873694840897021 and parameters: {'n_estimators': 689, 'learning_rate': 0.05501365618662157, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 591}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:54:30,080]\u001b[0m Trial 224 finished with value: 0.6851111848444997 and parameters: {'n_estimators': 694, 'learning_rate': 0.05513228643246293, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 598}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:54:33,961]\u001b[0m Trial 225 finished with value: 0.6814650617225158 and parameters: {'n_estimators': 688, 'learning_rate': 0.057187864273644376, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 591}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:54:38,027]\u001b[0m Trial 226 finished with value: 0.6828524396219855 and parameters: {'n_estimators': 667, 'learning_rate': 0.04898803405499744, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 598}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:54:41,754]\u001b[0m Trial 227 finished with value: 0.6842455004968669 and parameters: {'n_estimators': 685, 'learning_rate': 0.060004778256502744, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 679}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:54:46,194]\u001b[0m Trial 228 finished with value: 0.6867908677846327 and parameters: {'n_estimators': 638, 'learning_rate': 0.054451236890348614, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 584}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:54:49,706]\u001b[0m Trial 229 finished with value: 0.6850424985922979 and parameters: {'n_estimators': 639, 'learning_rate': 0.06472533839601995, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 577}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:54:53,053]\u001b[0m Trial 230 finished with value: 0.6836074266280757 and parameters: {'n_estimators': 641, 'learning_rate': 0.06368263671252372, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 582}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 02:54:56,739]\u001b[0m Trial 231 finished with value: 0.6806829562079809 and parameters: {'n_estimators': 635, 'learning_rate': 0.055237923641474056, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 580}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:00,364]\u001b[0m Trial 232 finished with value: 0.6824848165259096 and parameters: {'n_estimators': 650, 'learning_rate': 0.05927312666384485, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 614}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:04,145]\u001b[0m Trial 233 finished with value: 0.687566547496455 and parameters: {'n_estimators': 722, 'learning_rate': 0.06460965539234732, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 563}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:07,843]\u001b[0m Trial 234 finished with value: 0.685288645903029 and parameters: {'n_estimators': 717, 'learning_rate': 0.06431047929634509, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 653}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:11,960]\u001b[0m Trial 235 finished with value: 0.6871408369479842 and parameters: {'n_estimators': 722, 'learning_rate': 0.05596779089596707, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 658}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:15,661]\u001b[0m Trial 236 finished with value: 0.6868706217452955 and parameters: {'n_estimators': 725, 'learning_rate': 0.06835032057875887, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 651}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:19,327]\u001b[0m Trial 237 finished with value: 0.6866156135072476 and parameters: {'n_estimators': 718, 'learning_rate': 0.06663488970144489, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 657}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:22,879]\u001b[0m Trial 238 finished with value: 0.684984019382352 and parameters: {'n_estimators': 728, 'learning_rate': 0.06693210997868558, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 669}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:26,924]\u001b[0m Trial 239 finished with value: 0.686906348657058 and parameters: {'n_estimators': 757, 'learning_rate': 0.06007227449118172, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 702}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:30,255]\u001b[0m Trial 240 finished with value: 0.6842888413931132 and parameters: {'n_estimators': 743, 'learning_rate': 0.0682404973067725, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 686}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:34,276]\u001b[0m Trial 241 finished with value: 0.6887525229065188 and parameters: {'n_estimators': 747, 'learning_rate': 0.05995263450345328, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 699}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:38,181]\u001b[0m Trial 242 finished with value: 0.6873579194446372 and parameters: {'n_estimators': 759, 'learning_rate': 0.060438616799115434, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 705}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:41,991]\u001b[0m Trial 243 finished with value: 0.6833464701726816 and parameters: {'n_estimators': 759, 'learning_rate': 0.059401675647862834, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 709}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:45,634]\u001b[0m Trial 244 finished with value: 0.6811899742062513 and parameters: {'n_estimators': 759, 'learning_rate': 0.059050541146416835, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 689}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:49,798]\u001b[0m Trial 245 finished with value: 0.6887746689012026 and parameters: {'n_estimators': 731, 'learning_rate': 0.06264879583685615, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 696}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:53,525]\u001b[0m Trial 246 finished with value: 0.6841407694290803 and parameters: {'n_estimators': 739, 'learning_rate': 0.06933361980280445, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 711}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:55:57,408]\u001b[0m Trial 247 finished with value: 0.688452067951417 and parameters: {'n_estimators': 723, 'learning_rate': 0.06246170082730437, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 738}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:56:01,445]\u001b[0m Trial 248 finished with value: 0.6839083861825453 and parameters: {'n_estimators': 724, 'learning_rate': 0.06400197987584506, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 746}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:56:05,325]\u001b[0m Trial 249 finished with value: 0.6858944725179269 and parameters: {'n_estimators': 715, 'learning_rate': 0.061059069016615015, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 720}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6890\n",
      "\tBest params:\n",
      "\t\tn_estimators: 524\n",
      "\t\tlearning_rate: 0.09986329043489525\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 228\n",
      "\t\tnum_leaves: 188\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.665689    0.729387    0.729845    0.683805   \n",
      "1                    TP  314.000000  332.000000  324.000000  323.000000   \n",
      "2                    TN  180.000000  168.000000  195.000000  186.000000   \n",
      "3                    FP   57.000000   50.000000   42.000000   45.000000   \n",
      "4                    FN   44.000000   45.000000   34.000000   41.000000   \n",
      "5              Accuracy    0.830252    0.840336    0.872269    0.855462   \n",
      "6             Precision    0.846361    0.869110    0.885246    0.877717   \n",
      "7           Sensitivity    0.877095    0.880637    0.905028    0.887363   \n",
      "8           Specificity    0.759500    0.770600    0.822800    0.805200   \n",
      "9              F1 score    0.861454    0.874835    0.895028    0.882514   \n",
      "10  F1 score (weighted)    0.829372    0.839936    0.871878    0.855226   \n",
      "11     F1 score (macro)    0.821183    0.827209    0.865969    0.847370   \n",
      "12    Balanced Accuracy    0.818294    0.825639    0.863906    0.846279   \n",
      "13                  MCC    0.643226    0.654552    0.732280    0.694825   \n",
      "14                  NPV    0.803600    0.788700    0.851500    0.819400   \n",
      "15              ROC_AUC    0.818294    0.825639    0.863906    0.846279   \n",
      "\n",
      "          Set4  \n",
      "0     0.714233  \n",
      "1   329.000000  \n",
      "2   185.000000  \n",
      "3    45.000000  \n",
      "4    36.000000  \n",
      "5     0.863866  \n",
      "6     0.879679  \n",
      "7     0.901370  \n",
      "8     0.804300  \n",
      "9     0.890392  \n",
      "10    0.863336  \n",
      "11    0.855396  \n",
      "12    0.852859  \n",
      "13    0.711229  \n",
      "14    0.837100  \n",
      "15    0.852859  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_lgbm_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    " \n",
    "y_pred_lgbm_4_cat = np.where((y_pred_lgbm_4 >= 6.6) , 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 02:56:09,308]\u001b[0m Trial 250 finished with value: 0.6673430718892026 and parameters: {'n_estimators': 750, 'learning_rate': 0.06221287221692448, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 722}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:56:12,514]\u001b[0m Trial 251 finished with value: 0.6669574287888441 and parameters: {'n_estimators': 718, 'learning_rate': 0.06762393246788131, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 742}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:56:15,825]\u001b[0m Trial 252 finished with value: 0.6687721257987581 and parameters: {'n_estimators': 710, 'learning_rate': 0.05795043373711863, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 693}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:56:20,082]\u001b[0m Trial 253 finished with value: 0.6720341943714623 and parameters: {'n_estimators': 733, 'learning_rate': 0.0607621396825651, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 726}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:56:23,604]\u001b[0m Trial 254 finished with value: 0.6723480000493168 and parameters: {'n_estimators': 779, 'learning_rate': 0.07104847780221436, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 658}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:56:27,085]\u001b[0m Trial 255 finished with value: 0.6666482646384122 and parameters: {'n_estimators': 752, 'learning_rate': 0.0645652717378833, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 700}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:56:30,845]\u001b[0m Trial 256 finished with value: 0.6713415957579839 and parameters: {'n_estimators': 713, 'learning_rate': 0.056192075852831, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 665}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:56:34,712]\u001b[0m Trial 257 finished with value: 0.6718493465550773 and parameters: {'n_estimators': 769, 'learning_rate': 0.06036404131271038, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 706}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:56:38,819]\u001b[0m Trial 258 finished with value: 0.6690432263966865 and parameters: {'n_estimators': 731, 'learning_rate': 0.05181499644057178, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 735}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:56:42,385]\u001b[0m Trial 259 finished with value: 0.6697984583390261 and parameters: {'n_estimators': 714, 'learning_rate': 0.07193709488185272, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 676}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:56:46,130]\u001b[0m Trial 260 finished with value: 0.667483807290618 and parameters: {'n_estimators': 698, 'learning_rate': 0.056666107048898, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 644}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:56:49,605]\u001b[0m Trial 261 finished with value: 0.6697539550673554 and parameters: {'n_estimators': 739, 'learning_rate': 0.06500979247605852, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 670}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:56:54,072]\u001b[0m Trial 262 finished with value: 0.6711863524854143 and parameters: {'n_estimators': 672, 'learning_rate': 0.05304726969730405, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 721}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:56:58,234]\u001b[0m Trial 263 finished with value: 0.6701162355851531 and parameters: {'n_estimators': 681, 'learning_rate': 0.05944059335174109, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 639}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:01,338]\u001b[0m Trial 264 finished with value: 0.6632662496980088 and parameters: {'n_estimators': 705, 'learning_rate': 0.09656055637143741, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 697}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:04,122]\u001b[0m Trial 265 finished with value: 0.6519321264184926 and parameters: {'n_estimators': 724, 'learning_rate': 0.04968809167862685, 'max_depth': 5, 'max_bin': 267, 'num_leaves': 564}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:07,587]\u001b[0m Trial 266 finished with value: 0.6684836194214914 and parameters: {'n_estimators': 755, 'learning_rate': 0.06702766478400524, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 667}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:11,213]\u001b[0m Trial 267 finished with value: 0.6711626404304141 and parameters: {'n_estimators': 661, 'learning_rate': 0.0620994741568313, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 650}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:15,305]\u001b[0m Trial 268 finished with value: 0.6687119010061146 and parameters: {'n_estimators': 702, 'learning_rate': 0.05594014837619904, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 685}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:19,816]\u001b[0m Trial 269 finished with value: 0.6735682378842069 and parameters: {'n_estimators': 680, 'learning_rate': 0.047216629401354765, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 621}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:23,916]\u001b[0m Trial 270 finished with value: 0.6701810219962038 and parameters: {'n_estimators': 740, 'learning_rate': 0.05364438765225428, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 736}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:27,595]\u001b[0m Trial 271 finished with value: 0.6672033574739907 and parameters: {'n_estimators': 720, 'learning_rate': 0.05882997260682846, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 714}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:31,770]\u001b[0m Trial 272 finished with value: 0.6750412751356947 and parameters: {'n_estimators': 696, 'learning_rate': 0.0516953146580025, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 635}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:35,250]\u001b[0m Trial 273 finished with value: 0.6692785496347078 and parameters: {'n_estimators': 659, 'learning_rate': 0.07402683823449653, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 655}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:37,328]\u001b[0m Trial 274 finished with value: 0.6581406994575405 and parameters: {'n_estimators': 760, 'learning_rate': 0.09109500026079846, 'max_depth': 7, 'max_bin': 260, 'num_leaves': 696}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:40,803]\u001b[0m Trial 275 finished with value: 0.6679782390885235 and parameters: {'n_estimators': 774, 'learning_rate': 0.06366830300860322, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 677}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:44,034]\u001b[0m Trial 276 finished with value: 0.6648837994735718 and parameters: {'n_estimators': 679, 'learning_rate': 0.056884656918224384, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 730}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:47,033]\u001b[0m Trial 277 finished with value: 0.657590299910946 and parameters: {'n_estimators': 740, 'learning_rate': 0.04664302811043378, 'max_depth': 6, 'max_bin': 263, 'num_leaves': 656}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:50,568]\u001b[0m Trial 278 finished with value: 0.6672015811715395 and parameters: {'n_estimators': 620, 'learning_rate': 0.06857498135184273, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 750}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:54,840]\u001b[0m Trial 279 finished with value: 0.6723783424846781 and parameters: {'n_estimators': 721, 'learning_rate': 0.06009432426654149, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 708}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:57:58,476]\u001b[0m Trial 280 finished with value: 0.6696474617571486 and parameters: {'n_estimators': 706, 'learning_rate': 0.05325734198408451, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 637}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 02:58:01,829]\u001b[0m Trial 281 finished with value: 0.6652871152470701 and parameters: {'n_estimators': 662, 'learning_rate': 0.06315784312031925, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 607}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:58:06,246]\u001b[0m Trial 282 finished with value: 0.6722628027781898 and parameters: {'n_estimators': 690, 'learning_rate': 0.04935053205875553, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 620}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:58:09,163]\u001b[0m Trial 283 finished with value: 0.6655259832160713 and parameters: {'n_estimators': 751, 'learning_rate': 0.09939658439812217, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 574}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:58:13,215]\u001b[0m Trial 284 finished with value: 0.670843237845755 and parameters: {'n_estimators': 726, 'learning_rate': 0.05589129415185335, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 590}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:58:16,893]\u001b[0m Trial 285 finished with value: 0.6673826303808787 and parameters: {'n_estimators': 789, 'learning_rate': 0.0668862600344829, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 559}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:58:20,672]\u001b[0m Trial 286 finished with value: 0.6665205002217018 and parameters: {'n_estimators': 673, 'learning_rate': 0.06078861079902692, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 668}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:58:25,355]\u001b[0m Trial 287 finished with value: 0.6725742851041392 and parameters: {'n_estimators': 640, 'learning_rate': 0.04577978642686337, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 690}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:58:29,053]\u001b[0m Trial 288 finished with value: 0.6710960145492901 and parameters: {'n_estimators': 704, 'learning_rate': 0.08109082676819601, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 638}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:58:33,121]\u001b[0m Trial 289 finished with value: 0.6729370486205273 and parameters: {'n_estimators': 654, 'learning_rate': 0.05166950145667696, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 703}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:58:36,659]\u001b[0m Trial 290 finished with value: 0.6676490566250578 and parameters: {'n_estimators': 734, 'learning_rate': 0.05787315960880439, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 650}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:58:40,230]\u001b[0m Trial 291 finished with value: 0.6655122467093932 and parameters: {'n_estimators': 624, 'learning_rate': 0.07066707693480145, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 626}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:58:43,875]\u001b[0m Trial 292 finished with value: 0.667844765033365 and parameters: {'n_estimators': 685, 'learning_rate': 0.06520902696543644, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 681}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:58:47,463]\u001b[0m Trial 293 finished with value: 0.6668185001814613 and parameters: {'n_estimators': 333, 'learning_rate': 0.052810882450137674, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 602}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:58:49,848]\u001b[0m Trial 294 finished with value: 0.6664728861966559 and parameters: {'n_estimators': 765, 'learning_rate': 0.15204779744894348, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 721}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:58:52,887]\u001b[0m Trial 295 finished with value: 0.6684384224787182 and parameters: {'n_estimators': 707, 'learning_rate': 0.10478168359438199, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 658}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:58:56,518]\u001b[0m Trial 296 finished with value: 0.6662100407292751 and parameters: {'n_estimators': 674, 'learning_rate': 0.06118225683917458, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 586}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:00,687]\u001b[0m Trial 297 finished with value: 0.6693181863598116 and parameters: {'n_estimators': 723, 'learning_rate': 0.05555995859280358, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 614}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:05,123]\u001b[0m Trial 298 finished with value: 0.671317941619811 and parameters: {'n_estimators': 745, 'learning_rate': 0.04796845240701265, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 676}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:08,963]\u001b[0m Trial 299 finished with value: 0.6713547027264652 and parameters: {'n_estimators': 693, 'learning_rate': 0.05796760532676442, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 643}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6890\n",
      "\tBest params:\n",
      "\t\tn_estimators: 524\n",
      "\t\tlearning_rate: 0.09986329043489525\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 228\n",
      "\t\tnum_leaves: 188\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.665689    0.729387    0.729845    0.683805   \n",
      "1                    TP  314.000000  332.000000  324.000000  323.000000   \n",
      "2                    TN  180.000000  168.000000  195.000000  186.000000   \n",
      "3                    FP   57.000000   50.000000   42.000000   45.000000   \n",
      "4                    FN   44.000000   45.000000   34.000000   41.000000   \n",
      "5              Accuracy    0.830252    0.840336    0.872269    0.855462   \n",
      "6             Precision    0.846361    0.869110    0.885246    0.877717   \n",
      "7           Sensitivity    0.877095    0.880637    0.905028    0.887363   \n",
      "8           Specificity    0.759500    0.770600    0.822800    0.805200   \n",
      "9              F1 score    0.861454    0.874835    0.895028    0.882514   \n",
      "10  F1 score (weighted)    0.829372    0.839936    0.871878    0.855226   \n",
      "11     F1 score (macro)    0.821183    0.827209    0.865969    0.847370   \n",
      "12    Balanced Accuracy    0.818294    0.825639    0.863906    0.846279   \n",
      "13                  MCC    0.643226    0.654552    0.732280    0.694825   \n",
      "14                  NPV    0.803600    0.788700    0.851500    0.819400   \n",
      "15              ROC_AUC    0.818294    0.825639    0.863906    0.846279   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.714233    0.733553  \n",
      "1   329.000000  329.000000  \n",
      "2   185.000000  187.000000  \n",
      "3    45.000000   35.000000  \n",
      "4    36.000000   44.000000  \n",
      "5     0.863866    0.867227  \n",
      "6     0.879679    0.903846  \n",
      "7     0.901370    0.882038  \n",
      "8     0.804300    0.842300  \n",
      "9     0.890392    0.892809  \n",
      "10    0.863336    0.867735  \n",
      "11    0.855396    0.859208  \n",
      "12    0.852859    0.862190  \n",
      "13    0.711229    0.718854  \n",
      "14    0.837100    0.809500  \n",
      "15    0.852859    0.862190  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_lgbm_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_5_cat = np.where((y_pred_lgbm_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 02:59:12,507]\u001b[0m Trial 300 finished with value: 0.683366146164053 and parameters: {'n_estimators': 640, 'learning_rate': 0.09725576212564652, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 167}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:17,303]\u001b[0m Trial 301 finished with value: 0.681395871800866 and parameters: {'n_estimators': 660, 'learning_rate': 0.044755903118128576, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 570}. Best is trial 43 with value: 0.6889680803170803.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:21,193]\u001b[0m Trial 302 finished with value: 0.6907864632713362 and parameters: {'n_estimators': 714, 'learning_rate': 0.07750017736109742, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 732}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:24,667]\u001b[0m Trial 303 finished with value: 0.6846892432205406 and parameters: {'n_estimators': 737, 'learning_rate': 0.08925676315741368, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 733}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:28,225]\u001b[0m Trial 304 finished with value: 0.6879909729752456 and parameters: {'n_estimators': 680, 'learning_rate': 0.07966983434252246, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 663}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:31,652]\u001b[0m Trial 305 finished with value: 0.687020781527768 and parameters: {'n_estimators': 695, 'learning_rate': 0.08132108638730476, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 696}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:35,025]\u001b[0m Trial 306 finished with value: 0.6831824356089263 and parameters: {'n_estimators': 692, 'learning_rate': 0.074821405549771, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 695}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:38,504]\u001b[0m Trial 307 finished with value: 0.6882011018634444 and parameters: {'n_estimators': 709, 'learning_rate': 0.08746674279213715, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 701}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:42,047]\u001b[0m Trial 308 finished with value: 0.6897293726250529 and parameters: {'n_estimators': 708, 'learning_rate': 0.07973203812299819, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 708}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:45,540]\u001b[0m Trial 309 finished with value: 0.6872924312142283 and parameters: {'n_estimators': 711, 'learning_rate': 0.07773018839342107, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 711}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:49,055]\u001b[0m Trial 310 finished with value: 0.684375220310148 and parameters: {'n_estimators': 708, 'learning_rate': 0.07684821263999854, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 712}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:52,352]\u001b[0m Trial 311 finished with value: 0.6842093598889394 and parameters: {'n_estimators': 722, 'learning_rate': 0.08360508632291627, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 708}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:55,724]\u001b[0m Trial 312 finished with value: 0.6856496550807911 and parameters: {'n_estimators': 707, 'learning_rate': 0.08133333181606908, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 698}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 02:59:59,439]\u001b[0m Trial 313 finished with value: 0.6893500680085525 and parameters: {'n_estimators': 746, 'learning_rate': 0.07659584386413644, 'max_depth': 12, 'max_bin': 169, 'num_leaves': 734}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:03,245]\u001b[0m Trial 314 finished with value: 0.6874502883366276 and parameters: {'n_estimators': 756, 'learning_rate': 0.0788152537786309, 'max_depth': 12, 'max_bin': 165, 'num_leaves': 750}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:06,551]\u001b[0m Trial 315 finished with value: 0.6854536820125093 and parameters: {'n_estimators': 780, 'learning_rate': 0.07771997860284395, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 739}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:10,051]\u001b[0m Trial 316 finished with value: 0.6878930044889711 and parameters: {'n_estimators': 758, 'learning_rate': 0.07927220411569524, 'max_depth': 12, 'max_bin': 171, 'num_leaves': 728}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:13,374]\u001b[0m Trial 317 finished with value: 0.6841439481927504 and parameters: {'n_estimators': 764, 'learning_rate': 0.08080527157015276, 'max_depth': 12, 'max_bin': 167, 'num_leaves': 750}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:16,084]\u001b[0m Trial 318 finished with value: 0.677384052238055 and parameters: {'n_estimators': 795, 'learning_rate': 0.07741212406817194, 'max_depth': 8, 'max_bin': 168, 'num_leaves': 727}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:19,574]\u001b[0m Trial 319 finished with value: 0.6874572510712706 and parameters: {'n_estimators': 752, 'learning_rate': 0.08675873561715691, 'max_depth': 12, 'max_bin': 151, 'num_leaves': 736}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:23,364]\u001b[0m Trial 320 finished with value: 0.688831287579969 and parameters: {'n_estimators': 806, 'learning_rate': 0.08542982641231983, 'max_depth': 12, 'max_bin': 167, 'num_leaves': 734}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:27,350]\u001b[0m Trial 321 finished with value: 0.6867884501304475 and parameters: {'n_estimators': 801, 'learning_rate': 0.08706503624450765, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 738}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:30,989]\u001b[0m Trial 322 finished with value: 0.689183084122962 and parameters: {'n_estimators': 769, 'learning_rate': 0.0837975722534441, 'max_depth': 12, 'max_bin': 172, 'num_leaves': 729}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:34,595]\u001b[0m Trial 323 finished with value: 0.6895564298158572 and parameters: {'n_estimators': 806, 'learning_rate': 0.08384294243560789, 'max_depth': 12, 'max_bin': 173, 'num_leaves': 729}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:38,418]\u001b[0m Trial 324 finished with value: 0.685714658538328 and parameters: {'n_estimators': 807, 'learning_rate': 0.08306852467124508, 'max_depth': 12, 'max_bin': 172, 'num_leaves': 727}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:40,758]\u001b[0m Trial 325 finished with value: 0.6513645788047144 and parameters: {'n_estimators': 814, 'learning_rate': 0.0839298738024079, 'max_depth': 3, 'max_bin': 165, 'num_leaves': 718}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:44,474]\u001b[0m Trial 326 finished with value: 0.6865995283408436 and parameters: {'n_estimators': 765, 'learning_rate': 0.08741742785544301, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 750}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:48,462]\u001b[0m Trial 327 finished with value: 0.6870862133294884 and parameters: {'n_estimators': 779, 'learning_rate': 0.08068999130673356, 'max_depth': 12, 'max_bin': 175, 'num_leaves': 733}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:52,079]\u001b[0m Trial 328 finished with value: 0.6867888819712606 and parameters: {'n_estimators': 784, 'learning_rate': 0.078725937676397, 'max_depth': 12, 'max_bin': 171, 'num_leaves': 733}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:56,169]\u001b[0m Trial 329 finished with value: 0.6903671812220293 and parameters: {'n_estimators': 770, 'learning_rate': 0.08460699956415783, 'max_depth': 12, 'max_bin': 175, 'num_leaves': 730}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:00:59,956]\u001b[0m Trial 330 finished with value: 0.6822764967879122 and parameters: {'n_estimators': 779, 'learning_rate': 0.08509074437293211, 'max_depth': 12, 'max_bin': 174, 'num_leaves': 736}. Best is trial 302 with value: 0.6907864632713362.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:01:03,947]\u001b[0m Trial 331 finished with value: 0.6909973218309033 and parameters: {'n_estimators': 826, 'learning_rate': 0.0784023810566808, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 720}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:07,003]\u001b[0m Trial 332 finished with value: 0.6829427445877927 and parameters: {'n_estimators': 835, 'learning_rate': 0.09114124562015191, 'max_depth': 12, 'max_bin': 169, 'num_leaves': 716}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:10,868]\u001b[0m Trial 333 finished with value: 0.6839449827623705 and parameters: {'n_estimators': 808, 'learning_rate': 0.0852746294439996, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 721}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:14,296]\u001b[0m Trial 334 finished with value: 0.6843251433253945 and parameters: {'n_estimators': 752, 'learning_rate': 0.07435722807700856, 'max_depth': 12, 'max_bin': 178, 'num_leaves': 749}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:18,047]\u001b[0m Trial 335 finished with value: 0.6836473259224378 and parameters: {'n_estimators': 800, 'learning_rate': 0.0786628115389844, 'max_depth': 12, 'max_bin': 166, 'num_leaves': 717}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:21,955]\u001b[0m Trial 336 finished with value: 0.6871210925440192 and parameters: {'n_estimators': 748, 'learning_rate': 0.07531673541342553, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 735}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:25,735]\u001b[0m Trial 337 finished with value: 0.6840426539080158 and parameters: {'n_estimators': 768, 'learning_rate': 0.08511773550282763, 'max_depth': 12, 'max_bin': 170, 'num_leaves': 707}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:29,162]\u001b[0m Trial 338 finished with value: 0.6867630421082536 and parameters: {'n_estimators': 788, 'learning_rate': 0.09018188259211303, 'max_depth': 12, 'max_bin': 154, 'num_leaves': 727}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:32,599]\u001b[0m Trial 339 finished with value: 0.6858772999540179 and parameters: {'n_estimators': 738, 'learning_rate': 0.07897447629416192, 'max_depth': 12, 'max_bin': 164, 'num_leaves': 744}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:36,187]\u001b[0m Trial 340 finished with value: 0.6837368409528605 and parameters: {'n_estimators': 827, 'learning_rate': 0.08313271840381688, 'max_depth': 12, 'max_bin': 163, 'num_leaves': 708}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:39,466]\u001b[0m Trial 341 finished with value: 0.6857271687416715 and parameters: {'n_estimators': 758, 'learning_rate': 0.0930358514088623, 'max_depth': 12, 'max_bin': 169, 'num_leaves': 727}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:43,013]\u001b[0m Trial 342 finished with value: 0.6899621014569123 and parameters: {'n_estimators': 820, 'learning_rate': 0.08744834192439851, 'max_depth': 12, 'max_bin': 172, 'num_leaves': 716}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:46,831]\u001b[0m Trial 343 finished with value: 0.689053515822784 and parameters: {'n_estimators': 819, 'learning_rate': 0.08743457785935517, 'max_depth': 12, 'max_bin': 155, 'num_leaves': 716}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:49,044]\u001b[0m Trial 344 finished with value: 0.6594123480477536 and parameters: {'n_estimators': 835, 'learning_rate': 0.08762976636740342, 'max_depth': 4, 'max_bin': 155, 'num_leaves': 750}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:52,700]\u001b[0m Trial 345 finished with value: 0.686554836203018 and parameters: {'n_estimators': 820, 'learning_rate': 0.08788395090127299, 'max_depth': 12, 'max_bin': 172, 'num_leaves': 725}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:56,029]\u001b[0m Trial 346 finished with value: 0.6818628495332296 and parameters: {'n_estimators': 814, 'learning_rate': 0.09362657778645311, 'max_depth': 12, 'max_bin': 157, 'num_leaves': 715}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:01:59,766]\u001b[0m Trial 347 finished with value: 0.6883857456713021 and parameters: {'n_estimators': 857, 'learning_rate': 0.08343636312077998, 'max_depth': 12, 'max_bin': 163, 'num_leaves': 736}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:03,484]\u001b[0m Trial 348 finished with value: 0.684181561299868 and parameters: {'n_estimators': 852, 'learning_rate': 0.08210442067344449, 'max_depth': 12, 'max_bin': 152, 'num_leaves': 737}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:07,327]\u001b[0m Trial 349 finished with value: 0.6836358911053769 and parameters: {'n_estimators': 844, 'learning_rate': 0.08519658267740086, 'max_depth': 12, 'max_bin': 162, 'num_leaves': 735}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.690997\n",
      "\tBest params:\n",
      "\t\tn_estimators: 826\n",
      "\t\tlearning_rate: 0.0784023810566808\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 161\n",
      "\t\tnum_leaves: 720\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.665689    0.729387    0.729845    0.683805   \n",
      "1                    TP  314.000000  332.000000  324.000000  323.000000   \n",
      "2                    TN  180.000000  168.000000  195.000000  186.000000   \n",
      "3                    FP   57.000000   50.000000   42.000000   45.000000   \n",
      "4                    FN   44.000000   45.000000   34.000000   41.000000   \n",
      "5              Accuracy    0.830252    0.840336    0.872269    0.855462   \n",
      "6             Precision    0.846361    0.869110    0.885246    0.877717   \n",
      "7           Sensitivity    0.877095    0.880637    0.905028    0.887363   \n",
      "8           Specificity    0.759500    0.770600    0.822800    0.805200   \n",
      "9              F1 score    0.861454    0.874835    0.895028    0.882514   \n",
      "10  F1 score (weighted)    0.829372    0.839936    0.871878    0.855226   \n",
      "11     F1 score (macro)    0.821183    0.827209    0.865969    0.847370   \n",
      "12    Balanced Accuracy    0.818294    0.825639    0.863906    0.846279   \n",
      "13                  MCC    0.643226    0.654552    0.732280    0.694825   \n",
      "14                  NPV    0.803600    0.788700    0.851500    0.819400   \n",
      "15              ROC_AUC    0.818294    0.825639    0.863906    0.846279   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.714233    0.733553    0.700294  \n",
      "1   329.000000  329.000000  323.000000  \n",
      "2   185.000000  187.000000  169.000000  \n",
      "3    45.000000   35.000000   63.000000  \n",
      "4    36.000000   44.000000   40.000000  \n",
      "5     0.863866    0.867227    0.826891  \n",
      "6     0.879679    0.903846    0.836788  \n",
      "7     0.901370    0.882038    0.889807  \n",
      "8     0.804300    0.842300    0.728400  \n",
      "9     0.890392    0.892809    0.862483  \n",
      "10    0.863336    0.867735    0.825034  \n",
      "11    0.855396    0.859208    0.814462  \n",
      "12    0.852859    0.862190    0.809128  \n",
      "13    0.711229    0.718854    0.631682  \n",
      "14    0.837100    0.809500    0.808600  \n",
      "15    0.852859    0.862190    0.809128  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_lgbm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_6_cat = np.where((y_pred_lgbm_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:02:11,097]\u001b[0m Trial 350 finished with value: 0.6883697664295152 and parameters: {'n_estimators': 868, 'learning_rate': 0.09062608106390874, 'max_depth': 12, 'max_bin': 164, 'num_leaves': 720}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:14,582]\u001b[0m Trial 351 finished with value: 0.690955116978368 and parameters: {'n_estimators': 869, 'learning_rate': 0.08974798452464128, 'max_depth': 12, 'max_bin': 166, 'num_leaves': 723}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:17,659]\u001b[0m Trial 352 finished with value: 0.6882128565339253 and parameters: {'n_estimators': 873, 'learning_rate': 0.09055833651224889, 'max_depth': 12, 'max_bin': 164, 'num_leaves': 726}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:21,033]\u001b[0m Trial 353 finished with value: 0.687404423667386 and parameters: {'n_estimators': 882, 'learning_rate': 0.09135735435305033, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 721}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:24,535]\u001b[0m Trial 354 finished with value: 0.6897590775758261 and parameters: {'n_estimators': 860, 'learning_rate': 0.08917798155429701, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 691}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:27,598]\u001b[0m Trial 355 finished with value: 0.6873386361673268 and parameters: {'n_estimators': 868, 'learning_rate': 0.09389717396332317, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 700}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:30,885]\u001b[0m Trial 356 finished with value: 0.6881275556807098 and parameters: {'n_estimators': 863, 'learning_rate': 0.08928087446865886, 'max_depth': 12, 'max_bin': 172, 'num_leaves': 689}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:34,263]\u001b[0m Trial 357 finished with value: 0.6885259600259417 and parameters: {'n_estimators': 875, 'learning_rate': 0.08980422757654813, 'max_depth': 11, 'max_bin': 171, 'num_leaves': 688}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:37,751]\u001b[0m Trial 358 finished with value: 0.6909865008994363 and parameters: {'n_estimators': 863, 'learning_rate': 0.08997889042050751, 'max_depth': 11, 'max_bin': 172, 'num_leaves': 689}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:40,975]\u001b[0m Trial 359 finished with value: 0.6844611139755005 and parameters: {'n_estimators': 873, 'learning_rate': 0.09136022167074341, 'max_depth': 11, 'max_bin': 176, 'num_leaves': 689}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:43,767]\u001b[0m Trial 360 finished with value: 0.6837399591767657 and parameters: {'n_estimators': 853, 'learning_rate': 0.0892219313882758, 'max_depth': 9, 'max_bin': 174, 'num_leaves': 684}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:47,035]\u001b[0m Trial 361 finished with value: 0.6875908833003221 and parameters: {'n_estimators': 883, 'learning_rate': 0.09453551895214754, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 693}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:50,377]\u001b[0m Trial 362 finished with value: 0.6894829720865133 and parameters: {'n_estimators': 861, 'learning_rate': 0.08854830478013122, 'max_depth': 11, 'max_bin': 164, 'num_leaves': 707}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:53,464]\u001b[0m Trial 363 finished with value: 0.6884886219501812 and parameters: {'n_estimators': 848, 'learning_rate': 0.08932249699749965, 'max_depth': 11, 'max_bin': 163, 'num_leaves': 707}. Best is trial 331 with value: 0.6909973218309033.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:02:57,208]\u001b[0m Trial 364 finished with value: 0.6925263373026846 and parameters: {'n_estimators': 898, 'learning_rate': 0.09427485699656701, 'max_depth': 11, 'max_bin': 164, 'num_leaves': 711}. Best is trial 364 with value: 0.6925263373026846.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:00,538]\u001b[0m Trial 365 finished with value: 0.6900057176408161 and parameters: {'n_estimators': 893, 'learning_rate': 0.09976418596374406, 'max_depth': 11, 'max_bin': 163, 'num_leaves': 715}. Best is trial 364 with value: 0.6925263373026846.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:03,514]\u001b[0m Trial 366 finished with value: 0.6875782126039676 and parameters: {'n_estimators': 887, 'learning_rate': 0.09607496165187279, 'max_depth': 11, 'max_bin': 162, 'num_leaves': 712}. Best is trial 364 with value: 0.6925263373026846.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:06,394]\u001b[0m Trial 367 finished with value: 0.6848978811592966 and parameters: {'n_estimators': 895, 'learning_rate': 0.09959154315139962, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 713}. Best is trial 364 with value: 0.6925263373026846.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:09,614]\u001b[0m Trial 368 finished with value: 0.6883383376764894 and parameters: {'n_estimators': 851, 'learning_rate': 0.09749096771719695, 'max_depth': 11, 'max_bin': 165, 'num_leaves': 705}. Best is trial 364 with value: 0.6925263373026846.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:12,837]\u001b[0m Trial 369 finished with value: 0.6884886852339518 and parameters: {'n_estimators': 900, 'learning_rate': 0.09481740799251426, 'max_depth': 11, 'max_bin': 168, 'num_leaves': 718}. Best is trial 364 with value: 0.6925263373026846.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:15,948]\u001b[0m Trial 370 finished with value: 0.687629394807196 and parameters: {'n_estimators': 896, 'learning_rate': 0.09359741051124165, 'max_depth': 11, 'max_bin': 169, 'num_leaves': 702}. Best is trial 364 with value: 0.6925263373026846.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:19,391]\u001b[0m Trial 371 finished with value: 0.693441747718406 and parameters: {'n_estimators': 900, 'learning_rate': 0.08495534562049006, 'max_depth': 11, 'max_bin': 169, 'num_leaves': 716}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:22,261]\u001b[0m Trial 372 finished with value: 0.6884164190525357 and parameters: {'n_estimators': 839, 'learning_rate': 0.10311146066085751, 'max_depth': 11, 'max_bin': 179, 'num_leaves': 685}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:25,659]\u001b[0m Trial 373 finished with value: 0.6865574228993914 and parameters: {'n_estimators': 894, 'learning_rate': 0.09486394283514536, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 714}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:29,113]\u001b[0m Trial 374 finished with value: 0.6870842207390724 and parameters: {'n_estimators': 883, 'learning_rate': 0.08579000377820353, 'max_depth': 11, 'max_bin': 172, 'num_leaves': 696}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:32,483]\u001b[0m Trial 375 finished with value: 0.686247441167742 and parameters: {'n_estimators': 897, 'learning_rate': 0.08587573862327, 'max_depth': 11, 'max_bin': 169, 'num_leaves': 716}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:35,707]\u001b[0m Trial 376 finished with value: 0.6871239993075589 and parameters: {'n_estimators': 830, 'learning_rate': 0.09966420855207377, 'max_depth': 11, 'max_bin': 174, 'num_leaves': 681}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:39,114]\u001b[0m Trial 377 finished with value: 0.6899644426478386 and parameters: {'n_estimators': 864, 'learning_rate': 0.09589881914671103, 'max_depth': 11, 'max_bin': 161, 'num_leaves': 702}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:42,160]\u001b[0m Trial 378 finished with value: 0.6889080461519266 and parameters: {'n_estimators': 864, 'learning_rate': 0.09626785002034385, 'max_depth': 11, 'max_bin': 161, 'num_leaves': 699}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:45,151]\u001b[0m Trial 379 finished with value: 0.6864778945503685 and parameters: {'n_estimators': 870, 'learning_rate': 0.09679111462727386, 'max_depth': 11, 'max_bin': 157, 'num_leaves': 700}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:48,022]\u001b[0m Trial 380 finished with value: 0.6869136833282705 and parameters: {'n_estimators': 900, 'learning_rate': 0.1088038658161085, 'max_depth': 10, 'max_bin': 161, 'num_leaves': 688}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:03:51,181]\u001b[0m Trial 381 finished with value: 0.6899262393760218 and parameters: {'n_estimators': 876, 'learning_rate': 0.1033354249494803, 'max_depth': 11, 'max_bin': 169, 'num_leaves': 719}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:54,393]\u001b[0m Trial 382 finished with value: 0.6921698645855614 and parameters: {'n_estimators': 863, 'learning_rate': 0.10222335038704605, 'max_depth': 11, 'max_bin': 176, 'num_leaves': 675}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:03:57,410]\u001b[0m Trial 383 finished with value: 0.6870959835940136 and parameters: {'n_estimators': 867, 'learning_rate': 0.10018963389626696, 'max_depth': 11, 'max_bin': 175, 'num_leaves': 677}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:00,203]\u001b[0m Trial 384 finished with value: 0.6812278116713344 and parameters: {'n_estimators': 854, 'learning_rate': 0.10813475023009239, 'max_depth': 10, 'max_bin': 177, 'num_leaves': 695}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:03,373]\u001b[0m Trial 385 finished with value: 0.6898855473705987 and parameters: {'n_estimators': 826, 'learning_rate': 0.10388538070507465, 'max_depth': 11, 'max_bin': 182, 'num_leaves': 725}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:06,441]\u001b[0m Trial 386 finished with value: 0.6865180353938168 and parameters: {'n_estimators': 826, 'learning_rate': 0.10296492710159379, 'max_depth': 11, 'max_bin': 180, 'num_leaves': 722}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:09,486]\u001b[0m Trial 387 finished with value: 0.689655864057536 and parameters: {'n_estimators': 828, 'learning_rate': 0.10314869771182891, 'max_depth': 11, 'max_bin': 160, 'num_leaves': 706}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:12,671]\u001b[0m Trial 388 finished with value: 0.6894759117618724 and parameters: {'n_estimators': 827, 'learning_rate': 0.10778865176133444, 'max_depth': 11, 'max_bin': 160, 'num_leaves': 730}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:15,619]\u001b[0m Trial 389 finished with value: 0.6855833726877587 and parameters: {'n_estimators': 838, 'learning_rate': 0.10587337696615681, 'max_depth': 11, 'max_bin': 155, 'num_leaves': 730}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:18,561]\u001b[0m Trial 390 finished with value: 0.688916626315814 and parameters: {'n_estimators': 819, 'learning_rate': 0.11312094607568994, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 722}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:21,182]\u001b[0m Trial 391 finished with value: 0.6854484968063879 and parameters: {'n_estimators': 835, 'learning_rate': 0.11519918407115944, 'max_depth': 11, 'max_bin': 159, 'num_leaves': 712}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:23,953]\u001b[0m Trial 392 finished with value: 0.6902813395487224 and parameters: {'n_estimators': 819, 'learning_rate': 0.11070815654673968, 'max_depth': 11, 'max_bin': 160, 'num_leaves': 719}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:26,643]\u001b[0m Trial 393 finished with value: 0.6870854389070177 and parameters: {'n_estimators': 824, 'learning_rate': 0.11100193718171028, 'max_depth': 11, 'max_bin': 156, 'num_leaves': 724}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:29,568]\u001b[0m Trial 394 finished with value: 0.6877164001632399 and parameters: {'n_estimators': 818, 'learning_rate': 0.112790188462045, 'max_depth': 11, 'max_bin': 160, 'num_leaves': 740}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:32,619]\u001b[0m Trial 395 finished with value: 0.689634691447474 and parameters: {'n_estimators': 840, 'learning_rate': 0.10650965936371287, 'max_depth': 11, 'max_bin': 154, 'num_leaves': 714}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:35,173]\u001b[0m Trial 396 finished with value: 0.6839518152699545 and parameters: {'n_estimators': 846, 'learning_rate': 0.11831231927065983, 'max_depth': 11, 'max_bin': 153, 'num_leaves': 712}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:38,099]\u001b[0m Trial 397 finished with value: 0.6914780710613269 and parameters: {'n_estimators': 853, 'learning_rate': 0.10679094786115009, 'max_depth': 11, 'max_bin': 165, 'num_leaves': 727}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:40,980]\u001b[0m Trial 398 finished with value: 0.6872002507795658 and parameters: {'n_estimators': 854, 'learning_rate': 0.10599033083589557, 'max_depth': 11, 'max_bin': 165, 'num_leaves': 747}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:43,536]\u001b[0m Trial 399 finished with value: 0.6840928203125174 and parameters: {'n_estimators': 837, 'learning_rate': 0.10924340809630965, 'max_depth': 11, 'max_bin': 183, 'num_leaves': 727}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6934417\n",
      "\tBest params:\n",
      "\t\tn_estimators: 900\n",
      "\t\tlearning_rate: 0.08495534562049006\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 169\n",
      "\t\tnum_leaves: 716\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.665689    0.729387    0.729845    0.683805   \n",
      "1                    TP  314.000000  332.000000  324.000000  323.000000   \n",
      "2                    TN  180.000000  168.000000  195.000000  186.000000   \n",
      "3                    FP   57.000000   50.000000   42.000000   45.000000   \n",
      "4                    FN   44.000000   45.000000   34.000000   41.000000   \n",
      "5              Accuracy    0.830252    0.840336    0.872269    0.855462   \n",
      "6             Precision    0.846361    0.869110    0.885246    0.877717   \n",
      "7           Sensitivity    0.877095    0.880637    0.905028    0.887363   \n",
      "8           Specificity    0.759500    0.770600    0.822800    0.805200   \n",
      "9              F1 score    0.861454    0.874835    0.895028    0.882514   \n",
      "10  F1 score (weighted)    0.829372    0.839936    0.871878    0.855226   \n",
      "11     F1 score (macro)    0.821183    0.827209    0.865969    0.847370   \n",
      "12    Balanced Accuracy    0.818294    0.825639    0.863906    0.846279   \n",
      "13                  MCC    0.643226    0.654552    0.732280    0.694825   \n",
      "14                  NPV    0.803600    0.788700    0.851500    0.819400   \n",
      "15              ROC_AUC    0.818294    0.825639    0.863906    0.846279   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.714233    0.733553    0.700294    0.678610  \n",
      "1   329.000000  329.000000  323.000000  315.000000  \n",
      "2   185.000000  187.000000  169.000000  184.000000  \n",
      "3    45.000000   35.000000   63.000000   44.000000  \n",
      "4    36.000000   44.000000   40.000000   52.000000  \n",
      "5     0.863866    0.867227    0.826891    0.838655  \n",
      "6     0.879679    0.903846    0.836788    0.877437  \n",
      "7     0.901370    0.882038    0.889807    0.858311  \n",
      "8     0.804300    0.842300    0.728400    0.807000  \n",
      "9     0.890392    0.892809    0.862483    0.867769  \n",
      "10    0.863336    0.867735    0.825034    0.839157  \n",
      "11    0.855396    0.859208    0.814462    0.830436  \n",
      "12    0.852859    0.862190    0.809128    0.832664  \n",
      "13    0.711229    0.718854    0.631682    0.661200  \n",
      "14    0.837100    0.809500    0.808600    0.779700  \n",
      "15    0.852859    0.862190    0.809128    0.832664  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_lgbm_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_7_cat = np.where((y_pred_lgbm_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:04:46,732]\u001b[0m Trial 400 finished with value: 0.685419083636934 and parameters: {'n_estimators': 882, 'learning_rate': 0.10358598065939557, 'max_depth': 10, 'max_bin': 170, 'num_leaves': 711}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:49,513]\u001b[0m Trial 401 finished with value: 0.683520074660577 and parameters: {'n_estimators': 857, 'learning_rate': 0.106762926731263, 'max_depth': 11, 'max_bin': 166, 'num_leaves': 731}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:52,471]\u001b[0m Trial 402 finished with value: 0.6893185554887626 and parameters: {'n_estimators': 841, 'learning_rate': 0.10200157240422342, 'max_depth': 11, 'max_bin': 155, 'num_leaves': 707}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:55,472]\u001b[0m Trial 403 finished with value: 0.6869706793053133 and parameters: {'n_estimators': 877, 'learning_rate': 0.10335988648353667, 'max_depth': 11, 'max_bin': 162, 'num_leaves': 707}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:04:58,322]\u001b[0m Trial 404 finished with value: 0.6866111215446287 and parameters: {'n_estimators': 845, 'learning_rate': 0.1011301951270819, 'max_depth': 11, 'max_bin': 174, 'num_leaves': 748}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:01,435]\u001b[0m Trial 405 finished with value: 0.6852246788844366 and parameters: {'n_estimators': 862, 'learning_rate': 0.11011724619190418, 'max_depth': 10, 'max_bin': 170, 'num_leaves': 727}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:04,455]\u001b[0m Trial 406 finished with value: 0.6866446466696224 and parameters: {'n_estimators': 879, 'learning_rate': 0.10583567905155758, 'max_depth': 11, 'max_bin': 164, 'num_leaves': 705}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:07,097]\u001b[0m Trial 407 finished with value: 0.6842364091213566 and parameters: {'n_estimators': 802, 'learning_rate': 0.12388196688292075, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 733}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:10,000]\u001b[0m Trial 408 finished with value: 0.6843371665528972 and parameters: {'n_estimators': 830, 'learning_rate': 0.10158814869863371, 'max_depth': 11, 'max_bin': 177, 'num_leaves': 718}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:12,934]\u001b[0m Trial 409 finished with value: 0.6878450700359493 and parameters: {'n_estimators': 853, 'learning_rate': 0.1158682146092655, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 696}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:15,917]\u001b[0m Trial 410 finished with value: 0.6832139425046515 and parameters: {'n_estimators': 839, 'learning_rate': 0.10952110560202911, 'max_depth': 11, 'max_bin': 161, 'num_leaves': 679}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:19,079]\u001b[0m Trial 411 finished with value: 0.6874892738978742 and parameters: {'n_estimators': 863, 'learning_rate': 0.09967091663006301, 'max_depth': 11, 'max_bin': 172, 'num_leaves': 737}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:22,062]\u001b[0m Trial 412 finished with value: 0.6850244514987539 and parameters: {'n_estimators': 819, 'learning_rate': 0.1044762721730465, 'max_depth': 11, 'max_bin': 156, 'num_leaves': 718}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:24,977]\u001b[0m Trial 413 finished with value: 0.6859684843963755 and parameters: {'n_estimators': 880, 'learning_rate': 0.11239190595532109, 'max_depth': 11, 'max_bin': 165, 'num_leaves': 704}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:27,945]\u001b[0m Trial 414 finished with value: 0.6852738958439365 and parameters: {'n_estimators': 842, 'learning_rate': 0.09860106417695186, 'max_depth': 11, 'max_bin': 169, 'num_leaves': 740}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:30,889]\u001b[0m Trial 415 finished with value: 0.685355479984033 and parameters: {'n_estimators': 801, 'learning_rate': 0.10702253556918767, 'max_depth': 11, 'max_bin': 160, 'num_leaves': 723}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:33,766]\u001b[0m Trial 416 finished with value: 0.688788313696908 and parameters: {'n_estimators': 864, 'learning_rate': 0.10072959310680815, 'max_depth': 11, 'max_bin': 153, 'num_leaves': 750}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:36,781]\u001b[0m Trial 417 finished with value: 0.6850035607465422 and parameters: {'n_estimators': 884, 'learning_rate': 0.09372737864374285, 'max_depth': 10, 'max_bin': 163, 'num_leaves': 686}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:39,950]\u001b[0m Trial 418 finished with value: 0.6848708770017806 and parameters: {'n_estimators': 830, 'learning_rate': 0.10380733535361879, 'max_depth': 11, 'max_bin': 173, 'num_leaves': 706}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:42,699]\u001b[0m Trial 419 finished with value: 0.6850745564982954 and parameters: {'n_estimators': 809, 'learning_rate': 0.1120405051989056, 'max_depth': 11, 'max_bin': 177, 'num_leaves': 726}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:45,728]\u001b[0m Trial 420 finished with value: 0.6841463198815905 and parameters: {'n_estimators': 847, 'learning_rate': 0.09645042223858338, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 704}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:48,777]\u001b[0m Trial 421 finished with value: 0.6876256612956223 and parameters: {'n_estimators': 868, 'learning_rate': 0.10832040798381483, 'max_depth': 11, 'max_bin': 170, 'num_leaves': 675}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:51,823]\u001b[0m Trial 422 finished with value: 0.6828828944255422 and parameters: {'n_estimators': 825, 'learning_rate': 0.08235193805342657, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 716}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:54,539]\u001b[0m Trial 423 finished with value: 0.6804121586030435 and parameters: {'n_estimators': 854, 'learning_rate': 0.09819772621551054, 'max_depth': 11, 'max_bin': 163, 'num_leaves': 691}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:05:57,367]\u001b[0m Trial 424 finished with value: 0.6862568910332636 and parameters: {'n_estimators': 892, 'learning_rate': 0.10307725375402292, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 733}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:00,127]\u001b[0m Trial 425 finished with value: 0.6855172561492412 and parameters: {'n_estimators': 801, 'learning_rate': 0.1176573122669197, 'max_depth': 11, 'max_bin': 174, 'num_leaves': 716}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:03,625]\u001b[0m Trial 426 finished with value: 0.6863503630993248 and parameters: {'n_estimators': 900, 'learning_rate': 0.09217032095933206, 'max_depth': 11, 'max_bin': 150, 'num_leaves': 750}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:07,529]\u001b[0m Trial 427 finished with value: 0.6894254711195995 and parameters: {'n_estimators': 874, 'learning_rate': 0.0736644916570372, 'max_depth': 10, 'max_bin': 167, 'num_leaves': 693}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:11,297]\u001b[0m Trial 428 finished with value: 0.6876765227329776 and parameters: {'n_estimators': 874, 'learning_rate': 0.07338049614212712, 'max_depth': 10, 'max_bin': 166, 'num_leaves': 680}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:14,910]\u001b[0m Trial 429 finished with value: 0.6840157639296447 and parameters: {'n_estimators': 860, 'learning_rate': 0.07519255946697626, 'max_depth': 11, 'max_bin': 161, 'num_leaves': 692}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:17,386]\u001b[0m Trial 430 finished with value: 0.6826543075741137 and parameters: {'n_estimators': 183, 'learning_rate': 0.1065875837910497, 'max_depth': 11, 'max_bin': 156, 'num_leaves': 701}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:06:20,511]\u001b[0m Trial 431 finished with value: 0.6843207030843732 and parameters: {'n_estimators': 836, 'learning_rate': 0.09728033621181233, 'max_depth': 10, 'max_bin': 165, 'num_leaves': 706}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:23,714]\u001b[0m Trial 432 finished with value: 0.6869796865957793 and parameters: {'n_estimators': 881, 'learning_rate': 0.10181609126547418, 'max_depth': 11, 'max_bin': 169, 'num_leaves': 674}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:27,096]\u001b[0m Trial 433 finished with value: 0.6875893128058104 and parameters: {'n_estimators': 848, 'learning_rate': 0.08093461941076811, 'max_depth': 11, 'max_bin': 160, 'num_leaves': 691}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:28,558]\u001b[0m Trial 434 finished with value: 0.6630405295983263 and parameters: {'n_estimators': 80, 'learning_rate': 0.09307930331574615, 'max_depth': 10, 'max_bin': 181, 'num_leaves': 714}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:31,549]\u001b[0m Trial 435 finished with value: 0.6874855928672271 and parameters: {'n_estimators': 868, 'learning_rate': 0.10865716008279061, 'max_depth': 11, 'max_bin': 163, 'num_leaves': 726}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:34,556]\u001b[0m Trial 436 finished with value: 0.6833235769815849 and parameters: {'n_estimators': 822, 'learning_rate': 0.12169581008320507, 'max_depth': 11, 'max_bin': 168, 'num_leaves': 707}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:37,932]\u001b[0m Trial 437 finished with value: 0.6861101093765967 and parameters: {'n_estimators': 882, 'learning_rate': 0.07456084756580131, 'max_depth': 11, 'max_bin': 153, 'num_leaves': 737}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:40,808]\u001b[0m Trial 438 finished with value: 0.6837524616886415 and parameters: {'n_estimators': 839, 'learning_rate': 0.09945949250144533, 'max_depth': 11, 'max_bin': 159, 'num_leaves': 693}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:43,622]\u001b[0m Trial 439 finished with value: 0.6833845271278679 and parameters: {'n_estimators': 857, 'learning_rate': 0.10434785428875684, 'max_depth': 11, 'max_bin': 165, 'num_leaves': 719}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:46,044]\u001b[0m Trial 440 finished with value: 0.6818195046760636 and parameters: {'n_estimators': 815, 'learning_rate': 0.17177970141105844, 'max_depth': 11, 'max_bin': 176, 'num_leaves': 675}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:48,521]\u001b[0m Trial 441 finished with value: 0.6832049308149899 and parameters: {'n_estimators': 900, 'learning_rate': 0.11265426022979161, 'max_depth': 10, 'max_bin': 171, 'num_leaves': 705}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:51,711]\u001b[0m Trial 442 finished with value: 0.6854414260630188 and parameters: {'n_estimators': 881, 'learning_rate': 0.09311247921598964, 'max_depth': 11, 'max_bin': 162, 'num_leaves': 733}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:55,281]\u001b[0m Trial 443 finished with value: 0.6893355120027975 and parameters: {'n_estimators': 842, 'learning_rate': 0.08239977735916973, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 690}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:06:58,381]\u001b[0m Trial 444 finished with value: 0.684058003971012 and parameters: {'n_estimators': 864, 'learning_rate': 0.08129490369662622, 'max_depth': 11, 'max_bin': 166, 'num_leaves': 690}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:01,927]\u001b[0m Trial 445 finished with value: 0.6845357202254304 and parameters: {'n_estimators': 855, 'learning_rate': 0.07676578077305285, 'max_depth': 11, 'max_bin': 169, 'num_leaves': 679}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:05,154]\u001b[0m Trial 446 finished with value: 0.6865142314518553 and parameters: {'n_estimators': 833, 'learning_rate': 0.08551175095725583, 'max_depth': 10, 'max_bin': 173, 'num_leaves': 719}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:08,426]\u001b[0m Trial 447 finished with value: 0.6900244837652866 and parameters: {'n_estimators': 796, 'learning_rate': 0.07998150862488992, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 739}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:11,369]\u001b[0m Trial 448 finished with value: 0.6834037106236479 and parameters: {'n_estimators': 801, 'learning_rate': 0.08928761962548461, 'max_depth': 11, 'max_bin': 163, 'num_leaves': 744}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:15,180]\u001b[0m Trial 449 finished with value: 0.6870752522910449 and parameters: {'n_estimators': 784, 'learning_rate': 0.07244330162629663, 'max_depth': 11, 'max_bin': 171, 'num_leaves': 738}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.69344175\n",
      "\tBest params:\n",
      "\t\tn_estimators: 900\n",
      "\t\tlearning_rate: 0.08495534562049006\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 169\n",
      "\t\tnum_leaves: 716\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.665689    0.729387    0.729845    0.683805   \n",
      "1                    TP  314.000000  332.000000  324.000000  323.000000   \n",
      "2                    TN  180.000000  168.000000  195.000000  186.000000   \n",
      "3                    FP   57.000000   50.000000   42.000000   45.000000   \n",
      "4                    FN   44.000000   45.000000   34.000000   41.000000   \n",
      "5              Accuracy    0.830252    0.840336    0.872269    0.855462   \n",
      "6             Precision    0.846361    0.869110    0.885246    0.877717   \n",
      "7           Sensitivity    0.877095    0.880637    0.905028    0.887363   \n",
      "8           Specificity    0.759500    0.770600    0.822800    0.805200   \n",
      "9              F1 score    0.861454    0.874835    0.895028    0.882514   \n",
      "10  F1 score (weighted)    0.829372    0.839936    0.871878    0.855226   \n",
      "11     F1 score (macro)    0.821183    0.827209    0.865969    0.847370   \n",
      "12    Balanced Accuracy    0.818294    0.825639    0.863906    0.846279   \n",
      "13                  MCC    0.643226    0.654552    0.732280    0.694825   \n",
      "14                  NPV    0.803600    0.788700    0.851500    0.819400   \n",
      "15              ROC_AUC    0.818294    0.825639    0.863906    0.846279   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.714233    0.733553    0.700294    0.678610    0.705371  \n",
      "1   329.000000  329.000000  323.000000  315.000000  309.000000  \n",
      "2   185.000000  187.000000  169.000000  184.000000  191.000000  \n",
      "3    45.000000   35.000000   63.000000   44.000000   53.000000  \n",
      "4    36.000000   44.000000   40.000000   52.000000   42.000000  \n",
      "5     0.863866    0.867227    0.826891    0.838655    0.840336  \n",
      "6     0.879679    0.903846    0.836788    0.877437    0.853591  \n",
      "7     0.901370    0.882038    0.889807    0.858311    0.880342  \n",
      "8     0.804300    0.842300    0.728400    0.807000    0.782800  \n",
      "9     0.890392    0.892809    0.862483    0.867769    0.866760  \n",
      "10    0.863336    0.867735    0.825034    0.839157    0.839727  \n",
      "11    0.855396    0.859208    0.814462    0.830436    0.833799  \n",
      "12    0.852859    0.862190    0.809128    0.832664    0.831564  \n",
      "13    0.711229    0.718854    0.631682    0.661200    0.668212  \n",
      "14    0.837100    0.809500    0.808600    0.779700    0.819700  \n",
      "15    0.852859    0.862190    0.809128    0.832664    0.831564  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_lgbm_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_8_cat = np.where((y_pred_lgbm_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:07:18,559]\u001b[0m Trial 450 finished with value: 0.6823925543789385 and parameters: {'n_estimators': 791, 'learning_rate': 0.07812289670761263, 'max_depth': 9, 'max_bin': 177, 'num_leaves': 723}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:21,537]\u001b[0m Trial 451 finished with value: 0.6848797826605673 and parameters: {'n_estimators': 281, 'learning_rate': 0.08740048284687742, 'max_depth': 11, 'max_bin': 166, 'num_leaves': 737}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:25,110]\u001b[0m Trial 452 finished with value: 0.6865591321880139 and parameters: {'n_estimators': 811, 'learning_rate': 0.07858096737787286, 'max_depth': 11, 'max_bin': 169, 'num_leaves': 723}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:27,549]\u001b[0m Trial 453 finished with value: 0.6875880597956172 and parameters: {'n_estimators': 878, 'learning_rate': 0.1397264816297259, 'max_depth': 11, 'max_bin': 174, 'num_leaves': 712}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:30,368]\u001b[0m Trial 454 finished with value: 0.6805582708835513 and parameters: {'n_estimators': 900, 'learning_rate': 0.08352714610772805, 'max_depth': 8, 'max_bin': 161, 'num_leaves': 736}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:33,504]\u001b[0m Trial 455 finished with value: 0.6850967434963382 and parameters: {'n_estimators': 817, 'learning_rate': 0.09199948729725688, 'max_depth': 11, 'max_bin': 164, 'num_leaves': 727}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:36,545]\u001b[0m Trial 456 finished with value: 0.6839991022914113 and parameters: {'n_estimators': 868, 'learning_rate': 0.09691922495051813, 'max_depth': 11, 'max_bin': 179, 'num_leaves': 746}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:40,109]\u001b[0m Trial 457 finished with value: 0.6831784449939342 and parameters: {'n_estimators': 791, 'learning_rate': 0.07381335920048647, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 699}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:43,191]\u001b[0m Trial 458 finished with value: 0.68193935954832 and parameters: {'n_estimators': 829, 'learning_rate': 0.08765658981781373, 'max_depth': 11, 'max_bin': 171, 'num_leaves': 712}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:46,232]\u001b[0m Trial 459 finished with value: 0.6858374196682251 and parameters: {'n_estimators': 882, 'learning_rate': 0.08229782907441986, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 726}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:49,022]\u001b[0m Trial 460 finished with value: 0.6846901487298703 and parameters: {'n_estimators': 856, 'learning_rate': 0.11589277500353909, 'max_depth': 11, 'max_bin': 185, 'num_leaves': 702}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:52,214]\u001b[0m Trial 461 finished with value: 0.6826596190243992 and parameters: {'n_estimators': 846, 'learning_rate': 0.09448617162328438, 'max_depth': 10, 'max_bin': 163, 'num_leaves': 747}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:55,046]\u001b[0m Trial 462 finished with value: 0.6879201224288302 and parameters: {'n_estimators': 811, 'learning_rate': 0.10729158368003915, 'max_depth': 11, 'max_bin': 169, 'num_leaves': 750}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:07:58,611]\u001b[0m Trial 463 finished with value: 0.6870255415826219 and parameters: {'n_estimators': 886, 'learning_rate': 0.09035903966257432, 'max_depth': 11, 'max_bin': 160, 'num_leaves': 719}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:02,113]\u001b[0m Trial 464 finished with value: 0.6853526035751644 and parameters: {'n_estimators': 869, 'learning_rate': 0.07858774726368581, 'max_depth': 11, 'max_bin': 175, 'num_leaves': 708}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:05,276]\u001b[0m Trial 465 finished with value: 0.6876813987887885 and parameters: {'n_estimators': 827, 'learning_rate': 0.09809200912633742, 'max_depth': 11, 'max_bin': 165, 'num_leaves': 732}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:08,880]\u001b[0m Trial 466 finished with value: 0.6865220031969557 and parameters: {'n_estimators': 849, 'learning_rate': 0.07203693808564174, 'max_depth': 11, 'max_bin': 171, 'num_leaves': 668}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:12,189]\u001b[0m Trial 467 finished with value: 0.6840222795411105 and parameters: {'n_estimators': 792, 'learning_rate': 0.08660999831134558, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 694}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:15,199]\u001b[0m Trial 468 finished with value: 0.6852255047172353 and parameters: {'n_estimators': 862, 'learning_rate': 0.10965588979374755, 'max_depth': 11, 'max_bin': 157, 'num_leaves': 718}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:18,504]\u001b[0m Trial 469 finished with value: 0.67971999316099 and parameters: {'n_estimators': 900, 'learning_rate': 0.08375444677117944, 'max_depth': 11, 'max_bin': 162, 'num_leaves': 685}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:21,280]\u001b[0m Trial 470 finished with value: 0.6827159544786287 and parameters: {'n_estimators': 832, 'learning_rate': 0.10282982988073203, 'max_depth': 9, 'max_bin': 173, 'num_leaves': 727}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:23,417]\u001b[0m Trial 471 finished with value: 0.6735238433240375 and parameters: {'n_estimators': 881, 'learning_rate': 0.09259173877645975, 'max_depth': 6, 'max_bin': 164, 'num_leaves': 703}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:26,752]\u001b[0m Trial 472 finished with value: 0.6812258667804902 and parameters: {'n_estimators': 807, 'learning_rate': 0.07636877123652792, 'max_depth': 10, 'max_bin': 182, 'num_leaves': 736}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:29,883]\u001b[0m Trial 473 finished with value: 0.6830032345151604 and parameters: {'n_estimators': 868, 'learning_rate': 0.10548086321172209, 'max_depth': 11, 'max_bin': 168, 'num_leaves': 713}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:33,122]\u001b[0m Trial 474 finished with value: 0.6852548529382283 and parameters: {'n_estimators': 845, 'learning_rate': 0.09901775290742919, 'max_depth': 11, 'max_bin': 160, 'num_leaves': 671}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:36,007]\u001b[0m Trial 475 finished with value: 0.6840661534547882 and parameters: {'n_estimators': 819, 'learning_rate': 0.11234455356172013, 'max_depth': 11, 'max_bin': 171, 'num_leaves': 700}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:39,143]\u001b[0m Trial 476 finished with value: 0.6818965122513996 and parameters: {'n_estimators': 854, 'learning_rate': 0.07949881343506873, 'max_depth': 11, 'max_bin': 178, 'num_leaves': 724}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:42,412]\u001b[0m Trial 477 finished with value: 0.6836924573838662 and parameters: {'n_estimators': 886, 'learning_rate': 0.09557521149888533, 'max_depth': 11, 'max_bin': 165, 'num_leaves': 750}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:44,550]\u001b[0m Trial 478 finished with value: 0.6768292612291171 and parameters: {'n_estimators': 828, 'learning_rate': 0.18063140902181685, 'max_depth': 10, 'max_bin': 175, 'num_leaves': 690}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:47,592]\u001b[0m Trial 479 finished with value: 0.6824780158191306 and parameters: {'n_estimators': 795, 'learning_rate': 0.08860302266944352, 'max_depth': 11, 'max_bin': 157, 'num_leaves': 714}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:51,048]\u001b[0m Trial 480 finished with value: 0.6804725140913612 and parameters: {'n_estimators': 869, 'learning_rate': 0.08501597377464383, 'max_depth': 11, 'max_bin': 161, 'num_leaves': 734}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:08:53,894]\u001b[0m Trial 481 finished with value: 0.680377745321045 and parameters: {'n_estimators': 845, 'learning_rate': 0.10099992258450384, 'max_depth': 11, 'max_bin': 168, 'num_leaves': 711}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:08:57,015]\u001b[0m Trial 482 finished with value: 0.6815236171350991 and parameters: {'n_estimators': 887, 'learning_rate': 0.08074496115484947, 'max_depth': 10, 'max_bin': 165, 'num_leaves': 731}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:00,292]\u001b[0m Trial 483 finished with value: 0.6822569525338199 and parameters: {'n_estimators': 809, 'learning_rate': 0.09136225545819426, 'max_depth': 11, 'max_bin': 172, 'num_leaves': 683}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:03,167]\u001b[0m Trial 484 finished with value: 0.6849869848193656 and parameters: {'n_estimators': 863, 'learning_rate': 0.10620554261770596, 'max_depth': 11, 'max_bin': 154, 'num_leaves': 701}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:06,525]\u001b[0m Trial 485 finished with value: 0.6877457712109993 and parameters: {'n_estimators': 832, 'learning_rate': 0.09577991133816346, 'max_depth': 11, 'max_bin': 169, 'num_leaves': 722}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:10,016]\u001b[0m Trial 486 finished with value: 0.685038996514327 and parameters: {'n_estimators': 786, 'learning_rate': 0.07203077646936214, 'max_depth': 11, 'max_bin': 159, 'num_leaves': 735}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:13,289]\u001b[0m Trial 487 finished with value: 0.682319154650728 and parameters: {'n_estimators': 876, 'learning_rate': 0.0845908855152196, 'max_depth': 11, 'max_bin': 162, 'num_leaves': 713}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:16,846]\u001b[0m Trial 488 finished with value: 0.6847692531902123 and parameters: {'n_estimators': 850, 'learning_rate': 0.07560223934835447, 'max_depth': 10, 'max_bin': 176, 'num_leaves': 683}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:19,862]\u001b[0m Trial 489 finished with value: 0.6823053223318822 and parameters: {'n_estimators': 899, 'learning_rate': 0.08860011396046817, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 700}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:22,714]\u001b[0m Trial 490 finished with value: 0.6835177678769565 and parameters: {'n_estimators': 823, 'learning_rate': 0.10968797557518076, 'max_depth': 11, 'max_bin': 164, 'num_leaves': 723}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:25,755]\u001b[0m Trial 491 finished with value: 0.681898091534934 and parameters: {'n_estimators': 862, 'learning_rate': 0.101893082352432, 'max_depth': 11, 'max_bin': 173, 'num_leaves': 737}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:28,500]\u001b[0m Trial 492 finished with value: 0.6852800395774199 and parameters: {'n_estimators': 841, 'learning_rate': 0.11580685637557363, 'max_depth': 11, 'max_bin': 157, 'num_leaves': 697}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:31,006]\u001b[0m Trial 493 finished with value: 0.6773338518396654 and parameters: {'n_estimators': 809, 'learning_rate': 0.08129193687659689, 'max_depth': 7, 'max_bin': 170, 'num_leaves': 749}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:34,346]\u001b[0m Trial 494 finished with value: 0.6859786323727544 and parameters: {'n_estimators': 880, 'learning_rate': 0.0932920055266693, 'max_depth': 11, 'max_bin': 162, 'num_leaves': 714}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:37,286]\u001b[0m Trial 495 finished with value: 0.6866611199311674 and parameters: {'n_estimators': 834, 'learning_rate': 0.0988710809371157, 'max_depth': 11, 'max_bin': 179, 'num_leaves': 671}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:40,210]\u001b[0m Trial 496 finished with value: 0.682909962935778 and parameters: {'n_estimators': 778, 'learning_rate': 0.10519005249035, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 728}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:43,662]\u001b[0m Trial 497 finished with value: 0.6851642791067015 and parameters: {'n_estimators': 856, 'learning_rate': 0.08642986547435082, 'max_depth': 11, 'max_bin': 165, 'num_leaves': 707}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:46,976]\u001b[0m Trial 498 finished with value: 0.6857399057441389 and parameters: {'n_estimators': 885, 'learning_rate': 0.08103565863962092, 'max_depth': 11, 'max_bin': 159, 'num_leaves': 693}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:09:49,582]\u001b[0m Trial 499 finished with value: 0.6848708428749786 and parameters: {'n_estimators': 900, 'learning_rate': 0.1269927750909554, 'max_depth': 10, 'max_bin': 152, 'num_leaves': 739}. Best is trial 371 with value: 0.693441747718406.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.693441748\n",
      "\tBest params:\n",
      "\t\tn_estimators: 900\n",
      "\t\tlearning_rate: 0.08495534562049006\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 169\n",
      "\t\tnum_leaves: 716\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.665689    0.729387    0.729845    0.683805   \n",
      "1                    TP  314.000000  332.000000  324.000000  323.000000   \n",
      "2                    TN  180.000000  168.000000  195.000000  186.000000   \n",
      "3                    FP   57.000000   50.000000   42.000000   45.000000   \n",
      "4                    FN   44.000000   45.000000   34.000000   41.000000   \n",
      "5              Accuracy    0.830252    0.840336    0.872269    0.855462   \n",
      "6             Precision    0.846361    0.869110    0.885246    0.877717   \n",
      "7           Sensitivity    0.877095    0.880637    0.905028    0.887363   \n",
      "8           Specificity    0.759500    0.770600    0.822800    0.805200   \n",
      "9              F1 score    0.861454    0.874835    0.895028    0.882514   \n",
      "10  F1 score (weighted)    0.829372    0.839936    0.871878    0.855226   \n",
      "11     F1 score (macro)    0.821183    0.827209    0.865969    0.847370   \n",
      "12    Balanced Accuracy    0.818294    0.825639    0.863906    0.846279   \n",
      "13                  MCC    0.643226    0.654552    0.732280    0.694825   \n",
      "14                  NPV    0.803600    0.788700    0.851500    0.819400   \n",
      "15              ROC_AUC    0.818294    0.825639    0.863906    0.846279   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.714233    0.733553    0.700294    0.678610    0.705371    0.721373  \n",
      "1   329.000000  329.000000  323.000000  315.000000  309.000000  332.000000  \n",
      "2   185.000000  187.000000  169.000000  184.000000  191.000000  170.000000  \n",
      "3    45.000000   35.000000   63.000000   44.000000   53.000000   50.000000  \n",
      "4    36.000000   44.000000   40.000000   52.000000   42.000000   43.000000  \n",
      "5     0.863866    0.867227    0.826891    0.838655    0.840336    0.843697  \n",
      "6     0.879679    0.903846    0.836788    0.877437    0.853591    0.869110  \n",
      "7     0.901370    0.882038    0.889807    0.858311    0.880342    0.885333  \n",
      "8     0.804300    0.842300    0.728400    0.807000    0.782800    0.772700  \n",
      "9     0.890392    0.892809    0.862483    0.867769    0.866760    0.877147  \n",
      "10    0.863336    0.867735    0.825034    0.839157    0.839727    0.843157  \n",
      "11    0.855396    0.859208    0.814462    0.830436    0.833799    0.831183  \n",
      "12    0.852859    0.862190    0.809128    0.832664    0.831564    0.829030  \n",
      "13    0.711229    0.718854    0.631682    0.661200    0.668212    0.662630  \n",
      "14    0.837100    0.809500    0.808600    0.779700    0.819700    0.798100  \n",
      "15    0.852859    0.862190    0.809128    0.832664    0.831564    0.829030  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_lgbm_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_9_cat = np.where((y_pred_lgbm_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.705194     0.041882\n",
      "1                    TP       163.300000     6.650814\n",
      "2                    TN        90.700000     4.001389\n",
      "3                    FP        22.700000     2.750757\n",
      "4                    FN        20.400000     6.186006\n",
      "5              Accuracy         0.854928     0.017799\n",
      "6             Precision         0.878083     0.012896\n",
      "7           Sensitivity         0.889071     0.032831\n",
      "8           Specificity         0.799910     0.022451\n",
      "9              F1 score         0.883169     0.015977\n",
      "10  F1 score (weighted)         0.854648     0.017163\n",
      "11     F1 score (macro)         0.845677     0.017831\n",
      "12    Balanced Accuracy         0.844490     0.014635\n",
      "13                  MCC         0.692704     0.035889\n",
      "14                  NPV         0.818480     0.045044\n",
      "15              ROC_AUC         0.844490     0.014635\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.665689</td>\n",
       "      <td>0.729387</td>\n",
       "      <td>0.729845</td>\n",
       "      <td>0.683805</td>\n",
       "      <td>0.714233</td>\n",
       "      <td>0.733553</td>\n",
       "      <td>0.700294</td>\n",
       "      <td>0.678610</td>\n",
       "      <td>0.705371</td>\n",
       "      <td>0.721373</td>\n",
       "      <td>0.706216</td>\n",
       "      <td>0.023756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>323.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>323.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>323.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>181.500000</td>\n",
       "      <td>9.513149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>48.400000</td>\n",
       "      <td>8.002777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>42.100000</td>\n",
       "      <td>4.976612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.830252</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.872269</td>\n",
       "      <td>0.855462</td>\n",
       "      <td>0.863866</td>\n",
       "      <td>0.867227</td>\n",
       "      <td>0.826891</td>\n",
       "      <td>0.838655</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.847899</td>\n",
       "      <td>0.015811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.846361</td>\n",
       "      <td>0.869110</td>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.877717</td>\n",
       "      <td>0.879679</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.836788</td>\n",
       "      <td>0.877437</td>\n",
       "      <td>0.853591</td>\n",
       "      <td>0.869110</td>\n",
       "      <td>0.869889</td>\n",
       "      <td>0.019774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.877095</td>\n",
       "      <td>0.880637</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.887363</td>\n",
       "      <td>0.901370</td>\n",
       "      <td>0.882038</td>\n",
       "      <td>0.889807</td>\n",
       "      <td>0.858311</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.885333</td>\n",
       "      <td>0.884732</td>\n",
       "      <td>0.013002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.759500</td>\n",
       "      <td>0.770600</td>\n",
       "      <td>0.822800</td>\n",
       "      <td>0.805200</td>\n",
       "      <td>0.804300</td>\n",
       "      <td>0.842300</td>\n",
       "      <td>0.728400</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.782800</td>\n",
       "      <td>0.772700</td>\n",
       "      <td>0.789560</td>\n",
       "      <td>0.033310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.861454</td>\n",
       "      <td>0.874835</td>\n",
       "      <td>0.895028</td>\n",
       "      <td>0.882514</td>\n",
       "      <td>0.890392</td>\n",
       "      <td>0.892809</td>\n",
       "      <td>0.862483</td>\n",
       "      <td>0.867769</td>\n",
       "      <td>0.866760</td>\n",
       "      <td>0.877147</td>\n",
       "      <td>0.877119</td>\n",
       "      <td>0.012613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.829372</td>\n",
       "      <td>0.839936</td>\n",
       "      <td>0.871878</td>\n",
       "      <td>0.855226</td>\n",
       "      <td>0.863336</td>\n",
       "      <td>0.867735</td>\n",
       "      <td>0.825034</td>\n",
       "      <td>0.839157</td>\n",
       "      <td>0.839727</td>\n",
       "      <td>0.843157</td>\n",
       "      <td>0.847456</td>\n",
       "      <td>0.016171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.821183</td>\n",
       "      <td>0.827209</td>\n",
       "      <td>0.865969</td>\n",
       "      <td>0.847370</td>\n",
       "      <td>0.855396</td>\n",
       "      <td>0.859208</td>\n",
       "      <td>0.814462</td>\n",
       "      <td>0.830436</td>\n",
       "      <td>0.833799</td>\n",
       "      <td>0.831183</td>\n",
       "      <td>0.838621</td>\n",
       "      <td>0.017299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.818294</td>\n",
       "      <td>0.825639</td>\n",
       "      <td>0.863906</td>\n",
       "      <td>0.846279</td>\n",
       "      <td>0.852859</td>\n",
       "      <td>0.862190</td>\n",
       "      <td>0.809128</td>\n",
       "      <td>0.832664</td>\n",
       "      <td>0.831564</td>\n",
       "      <td>0.829030</td>\n",
       "      <td>0.837155</td>\n",
       "      <td>0.018450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.643226</td>\n",
       "      <td>0.654552</td>\n",
       "      <td>0.732280</td>\n",
       "      <td>0.694825</td>\n",
       "      <td>0.711229</td>\n",
       "      <td>0.718854</td>\n",
       "      <td>0.631682</td>\n",
       "      <td>0.661200</td>\n",
       "      <td>0.668212</td>\n",
       "      <td>0.662630</td>\n",
       "      <td>0.677869</td>\n",
       "      <td>0.034191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.803600</td>\n",
       "      <td>0.788700</td>\n",
       "      <td>0.851500</td>\n",
       "      <td>0.819400</td>\n",
       "      <td>0.837100</td>\n",
       "      <td>0.809500</td>\n",
       "      <td>0.808600</td>\n",
       "      <td>0.779700</td>\n",
       "      <td>0.819700</td>\n",
       "      <td>0.798100</td>\n",
       "      <td>0.811590</td>\n",
       "      <td>0.021526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.818294</td>\n",
       "      <td>0.825639</td>\n",
       "      <td>0.863906</td>\n",
       "      <td>0.846279</td>\n",
       "      <td>0.852859</td>\n",
       "      <td>0.862190</td>\n",
       "      <td>0.809128</td>\n",
       "      <td>0.832664</td>\n",
       "      <td>0.831564</td>\n",
       "      <td>0.829030</td>\n",
       "      <td>0.837155</td>\n",
       "      <td>0.018450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.665689    0.729387    0.729845    0.683805   \n",
       "1                    TP  314.000000  332.000000  324.000000  323.000000   \n",
       "2                    TN  180.000000  168.000000  195.000000  186.000000   \n",
       "3                    FP   57.000000   50.000000   42.000000   45.000000   \n",
       "4                    FN   44.000000   45.000000   34.000000   41.000000   \n",
       "5              Accuracy    0.830252    0.840336    0.872269    0.855462   \n",
       "6             Precision    0.846361    0.869110    0.885246    0.877717   \n",
       "7           Sensitivity    0.877095    0.880637    0.905028    0.887363   \n",
       "8           Specificity    0.759500    0.770600    0.822800    0.805200   \n",
       "9              F1 score    0.861454    0.874835    0.895028    0.882514   \n",
       "10  F1 score (weighted)    0.829372    0.839936    0.871878    0.855226   \n",
       "11     F1 score (macro)    0.821183    0.827209    0.865969    0.847370   \n",
       "12    Balanced Accuracy    0.818294    0.825639    0.863906    0.846279   \n",
       "13                  MCC    0.643226    0.654552    0.732280    0.694825   \n",
       "14                  NPV    0.803600    0.788700    0.851500    0.819400   \n",
       "15              ROC_AUC    0.818294    0.825639    0.863906    0.846279   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.714233    0.733553    0.700294    0.678610    0.705371    0.721373   \n",
       "1   329.000000  329.000000  323.000000  315.000000  309.000000  332.000000   \n",
       "2   185.000000  187.000000  169.000000  184.000000  191.000000  170.000000   \n",
       "3    45.000000   35.000000   63.000000   44.000000   53.000000   50.000000   \n",
       "4    36.000000   44.000000   40.000000   52.000000   42.000000   43.000000   \n",
       "5     0.863866    0.867227    0.826891    0.838655    0.840336    0.843697   \n",
       "6     0.879679    0.903846    0.836788    0.877437    0.853591    0.869110   \n",
       "7     0.901370    0.882038    0.889807    0.858311    0.880342    0.885333   \n",
       "8     0.804300    0.842300    0.728400    0.807000    0.782800    0.772700   \n",
       "9     0.890392    0.892809    0.862483    0.867769    0.866760    0.877147   \n",
       "10    0.863336    0.867735    0.825034    0.839157    0.839727    0.843157   \n",
       "11    0.855396    0.859208    0.814462    0.830436    0.833799    0.831183   \n",
       "12    0.852859    0.862190    0.809128    0.832664    0.831564    0.829030   \n",
       "13    0.711229    0.718854    0.631682    0.661200    0.668212    0.662630   \n",
       "14    0.837100    0.809500    0.808600    0.779700    0.819700    0.798100   \n",
       "15    0.852859    0.862190    0.809128    0.832664    0.831564    0.829030   \n",
       "\n",
       "           ave       std  \n",
       "0     0.706216  0.023756  \n",
       "1   323.000000  8.000000  \n",
       "2   181.500000  9.513149  \n",
       "3    48.400000  8.002777  \n",
       "4    42.100000  4.976612  \n",
       "5     0.847899  0.015811  \n",
       "6     0.869889  0.019774  \n",
       "7     0.884732  0.013002  \n",
       "8     0.789560  0.033310  \n",
       "9     0.877119  0.012613  \n",
       "10    0.847456  0.016171  \n",
       "11    0.838621  0.017299  \n",
       "12    0.837155  0.018450  \n",
       "13    0.677869  0.034191  \n",
       "14    0.811590  0.021526  \n",
       "15    0.837155  0.018450  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test['std'] = mat_met_lgbm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_lgbm0</th>\n",
       "      <th>y_pred_lgbm1</th>\n",
       "      <th>y_pred_lgbm2</th>\n",
       "      <th>y_pred_lgbm3</th>\n",
       "      <th>y_pred_lgbm4</th>\n",
       "      <th>y_pred_lgbm_ave</th>\n",
       "      <th>y_pred_lgbm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL356066</td>\n",
       "      <td>0</td>\n",
       "      <td>8.02</td>\n",
       "      <td>7.027552</td>\n",
       "      <td>6.994031</td>\n",
       "      <td>7.679035</td>\n",
       "      <td>7.172198</td>\n",
       "      <td>7.084122</td>\n",
       "      <td>7.329490</td>\n",
       "      <td>0.384628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3652228</td>\n",
       "      <td>1</td>\n",
       "      <td>8.05</td>\n",
       "      <td>8.039660</td>\n",
       "      <td>7.974594</td>\n",
       "      <td>7.963269</td>\n",
       "      <td>7.993696</td>\n",
       "      <td>7.979001</td>\n",
       "      <td>8.000037</td>\n",
       "      <td>0.033036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3939518</td>\n",
       "      <td>2</td>\n",
       "      <td>6.87</td>\n",
       "      <td>7.045209</td>\n",
       "      <td>7.017585</td>\n",
       "      <td>7.267522</td>\n",
       "      <td>7.224131</td>\n",
       "      <td>7.060251</td>\n",
       "      <td>7.080783</td>\n",
       "      <td>0.132621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4281792</td>\n",
       "      <td>3</td>\n",
       "      <td>7.22</td>\n",
       "      <td>8.295245</td>\n",
       "      <td>7.772152</td>\n",
       "      <td>7.460480</td>\n",
       "      <td>7.994341</td>\n",
       "      <td>7.864734</td>\n",
       "      <td>7.767825</td>\n",
       "      <td>0.349634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4070232</td>\n",
       "      <td>4</td>\n",
       "      <td>7.15</td>\n",
       "      <td>6.248655</td>\n",
       "      <td>6.406398</td>\n",
       "      <td>6.466830</td>\n",
       "      <td>6.340835</td>\n",
       "      <td>6.302526</td>\n",
       "      <td>6.485874</td>\n",
       "      <td>0.305142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL4202521</td>\n",
       "      <td>2966</td>\n",
       "      <td>7.43</td>\n",
       "      <td>6.972672</td>\n",
       "      <td>6.848559</td>\n",
       "      <td>7.107165</td>\n",
       "      <td>7.115588</td>\n",
       "      <td>7.112335</td>\n",
       "      <td>7.097720</td>\n",
       "      <td>0.177365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL216641</td>\n",
       "      <td>2967</td>\n",
       "      <td>7.35</td>\n",
       "      <td>7.330716</td>\n",
       "      <td>6.786085</td>\n",
       "      <td>7.129787</td>\n",
       "      <td>7.115661</td>\n",
       "      <td>7.422361</td>\n",
       "      <td>7.189102</td>\n",
       "      <td>0.212794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL3693750</td>\n",
       "      <td>2968</td>\n",
       "      <td>7.43</td>\n",
       "      <td>7.561387</td>\n",
       "      <td>7.460591</td>\n",
       "      <td>7.490798</td>\n",
       "      <td>7.648749</td>\n",
       "      <td>7.615294</td>\n",
       "      <td>7.534470</td>\n",
       "      <td>0.080204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL152665</td>\n",
       "      <td>2969</td>\n",
       "      <td>5.96</td>\n",
       "      <td>6.489407</td>\n",
       "      <td>6.276246</td>\n",
       "      <td>6.501565</td>\n",
       "      <td>6.342075</td>\n",
       "      <td>6.105291</td>\n",
       "      <td>6.279097</td>\n",
       "      <td>0.195673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL3693789</td>\n",
       "      <td>2970</td>\n",
       "      <td>8.52</td>\n",
       "      <td>7.948753</td>\n",
       "      <td>7.901957</td>\n",
       "      <td>7.553439</td>\n",
       "      <td>7.811749</td>\n",
       "      <td>8.020894</td>\n",
       "      <td>7.959465</td>\n",
       "      <td>0.290996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_lgbm0  y_pred_lgbm1  \\\n",
       "0          CHEMBL356066            0     8.02      7.027552      6.994031   \n",
       "1         CHEMBL3652228            1     8.05      8.039660      7.974594   \n",
       "2         CHEMBL3939518            2     6.87      7.045209      7.017585   \n",
       "3         CHEMBL4281792            3     7.22      8.295245      7.772152   \n",
       "4         CHEMBL4070232            4     7.15      6.248655      6.406398   \n",
       "...                 ...          ...      ...           ...           ...   \n",
       "2966      CHEMBL4202521         2966     7.43      6.972672      6.848559   \n",
       "2967       CHEMBL216641         2967     7.35      7.330716      6.786085   \n",
       "2968      CHEMBL3693750         2968     7.43      7.561387      7.460591   \n",
       "2969       CHEMBL152665         2969     5.96      6.489407      6.276246   \n",
       "2970      CHEMBL3693789         2970     8.52      7.948753      7.901957   \n",
       "\n",
       "      y_pred_lgbm2  y_pred_lgbm3  y_pred_lgbm4  y_pred_lgbm_ave  \\\n",
       "0         7.679035      7.172198      7.084122         7.329490   \n",
       "1         7.963269      7.993696      7.979001         8.000037   \n",
       "2         7.267522      7.224131      7.060251         7.080783   \n",
       "3         7.460480      7.994341      7.864734         7.767825   \n",
       "4         6.466830      6.340835      6.302526         6.485874   \n",
       "...            ...           ...           ...              ...   \n",
       "2966      7.107165      7.115588      7.112335         7.097720   \n",
       "2967      7.129787      7.115661      7.422361         7.189102   \n",
       "2968      7.490798      7.648749      7.615294         7.534470   \n",
       "2969      6.501565      6.342075      6.105291         6.279097   \n",
       "2970      7.553439      7.811749      8.020894         7.959465   \n",
       "\n",
       "      y_pred_lgbm_std  \n",
       "0            0.384628  \n",
       "1            0.033036  \n",
       "2            0.132621  \n",
       "3            0.349634  \n",
       "4            0.305142  \n",
       "...               ...  \n",
       "2966         0.177365  \n",
       "2967         0.212794  \n",
       "2968         0.080204  \n",
       "2969         0.195673  \n",
       "2970         0.290996  \n",
       "\n",
       "[2971 rows x 10 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_lgbm_cat = np.where((y_pred_optimized_lgbm >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_lgbm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_lgbm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "lgbm_run0 = data_lgbm[['y_test_idx0', 'y_test0', 'y_pred_lgbm0']]\n",
    "lgbm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "lgbm_run0.reset_index(inplace=True, drop=True)\n",
    "lgbm_run1 = data_lgbm[['y_test_idx1', 'y_test1', 'y_pred_lgbm1']]\n",
    "lgbm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "lgbm_run1.reset_index(inplace=True, drop=True)\n",
    "lgbm_run2 = data_lgbm[['y_test_idx2', 'y_test2', 'y_pred_lgbm2']]\n",
    "lgbm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "lgbm_run2.reset_index(inplace=True, drop=True)\n",
    "lgbm_run3 = data_lgbm[['y_test_idx3', 'y_test3', 'y_pred_lgbm3']]\n",
    "lgbm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "lgbm_run3.reset_index(inplace=True, drop=True)\n",
    "lgbm_run4 = data_lgbm[['y_test_idx4', 'y_test4', 'y_pred_lgbm4']]\n",
    "lgbm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "lgbm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "lgbm_5preds = pd.concat([chembl_id, lgbm_run0, lgbm_run1, lgbm_run2, lgbm_run3, lgbm_run4], axis=1)\n",
    "lgbm_5preds = lgbm_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_lgbm0', 'y_pred_lgbm1', 'y_pred_lgbm2', 'y_pred_lgbm3', 'y_pred_lgbm4']]\n",
    "lgbm_5preds['y_pred_lgbm_ave'] = lgbm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "lgbm_5preds['y_pred_lgbm_std'] = lgbm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "lgbm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c16510fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_lgbm.to_csv('mat_met_lgbm_opt.csv')\n",
    "lgbm_5preds.to_csv('lgbm_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "db4ac315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model r2_score 0.6870 with a standard deviation of 0.0402\n",
      "LightGBM optimized model r2_score 0.6978 with a standard deviation of 0.0398\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#r2_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"r2\")\n",
    "r2_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"r2\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)r2_score %0.4f with a standard deviation of %0.4f\" % (r2_cv_lgbm_opt_testSet.mean(), r2_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(r2_cv_lgbm_opt), np.std(r2_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_lgbm.joblib']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm_reg, \"./lgbm_reg.joblib\")\n",
    "#joblib.dump(optimized_lgbm, \"./optimized_lgbm.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"./optimizedCV_lgbm.joblib\") \n",
    "#loaded_rf = joblib.load(\"./optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.662865     0.044659\n",
      "1                    TP       162.200000     5.808040\n",
      "2                    TN        87.700000     5.538752\n",
      "3                    FP        25.700000     3.433495\n",
      "4                    FN        21.500000     5.700877\n",
      "5              Accuracy         0.841132     0.022316\n",
      "6             Precision         0.863380     0.016355\n",
      "7           Sensitivity         0.883143     0.029800\n",
      "8           Specificity         0.773040     0.032204\n",
      "9              F1 score         0.872897     0.018306\n",
      "10  F1 score (weighted)         0.840527     0.021972\n",
      "11     F1 score (macro)         0.830275     0.023931\n",
      "12    Balanced Accuracy         0.828093     0.022616\n",
      "13                  MCC         0.661775     0.047727\n",
      "14                  NPV         0.804110     0.043988\n",
      "15              ROC_AUC         0.828093     0.022616\n",
      "CPU times: user 57.8 s, sys: 144 ms, total: 57.9 s\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_reg.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores),np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:10:49,485]\u001b[0m A new study created in memory with name: XGBRegressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:10:57,085]\u001b[0m Trial 0 finished with value: 0.6887606553467972 and parameters: {'n_estimators': 556, 'eta': 0.06090927343192556, 'max_depth': 7, 'alpha': 0.9495, 'lambda': 4.150146102913757, 'max_bin': 367}. Best is trial 0 with value: 0.6887606553467972.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:11:01,589]\u001b[0m Trial 1 finished with value: 0.674115205876621 and parameters: {'n_estimators': 157, 'eta': 0.06874298517611252, 'max_depth': 10, 'alpha': 0.525, 'lambda': 24.11514418522569, 'max_bin': 254}. Best is trial 0 with value: 0.6887606553467972.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:11:16,362]\u001b[0m Trial 2 finished with value: 0.671304516793783 and parameters: {'n_estimators': 698, 'eta': 0.009944054838148492, 'max_depth': 8, 'alpha': 0.4535, 'lambda': 2.1661046817828793, 'max_bin': 355}. Best is trial 0 with value: 0.6887606553467972.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:11:28,294]\u001b[0m Trial 3 finished with value: 0.6854489472980386 and parameters: {'n_estimators': 427, 'eta': 0.03205620401526085, 'max_depth': 10, 'alpha': 0.1539, 'lambda': 19.45455145472958, 'max_bin': 363}. Best is trial 0 with value: 0.6887606553467972.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:11:40,065]\u001b[0m Trial 4 finished with value: 0.6887792152159008 and parameters: {'n_estimators': 889, 'eta': 0.07317620972897343, 'max_depth': 9, 'alpha': 0.0741, 'lambda': 38.12517757109213, 'max_bin': 427}. Best is trial 4 with value: 0.6887792152159008.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:11:42,792]\u001b[0m Trial 5 finished with value: -13.494845451517074 and parameters: {'n_estimators': 354, 'eta': 0.0012001154786591257, 'max_depth': 10, 'alpha': 0.027700000000000002, 'lambda': 14.520856093579415, 'max_bin': 282}. Best is trial 4 with value: 0.6887792152159008.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:11:53,185]\u001b[0m Trial 6 finished with value: 0.6733120919461705 and parameters: {'n_estimators': 650, 'eta': 0.02158369296918237, 'max_depth': 6, 'alpha': 0.2295, 'lambda': 14.904159947998268, 'max_bin': 456}. Best is trial 4 with value: 0.6887792152159008.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:11:59,387]\u001b[0m Trial 7 finished with value: 0.6849540679998888 and parameters: {'n_estimators': 696, 'eta': 0.09116361947315094, 'max_depth': 8, 'alpha': 0.3461, 'lambda': 3.7312455373165347, 'max_bin': 361}. Best is trial 4 with value: 0.6887792152159008.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:12:10,508]\u001b[0m Trial 8 finished with value: 0.687438137616137 and parameters: {'n_estimators': 616, 'eta': 0.027509063034857794, 'max_depth': 7, 'alpha': 0.5984, 'lambda': 4.856169597654649, 'max_bin': 380}. Best is trial 4 with value: 0.6887792152159008.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:12:22,855]\u001b[0m Trial 9 finished with value: 0.6813205036858074 and parameters: {'n_estimators': 584, 'eta': 0.08086103650207924, 'max_depth': 12, 'alpha': 0.1448, 'lambda': 37.48195026153979, 'max_bin': 303}. Best is trial 4 with value: 0.6887792152159008.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:12:34,729]\u001b[0m Trial 10 finished with value: 0.6845061809520969 and parameters: {'n_estimators': 898, 'eta': 0.04651438573375853, 'max_depth': 5, 'alpha': 0.8411000000000001, 'lambda': 39.65633718520153, 'max_bin': 487}. Best is trial 4 with value: 0.6887792152159008.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:12:47,723]\u001b[0m Trial 11 finished with value: 0.6917094784011449 and parameters: {'n_estimators': 797, 'eta': 0.0636518365186749, 'max_depth': 9, 'alpha': 0.9841000000000001, 'lambda': 29.679770008032424, 'max_bin': 419}. Best is trial 11 with value: 0.6917094784011449.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:13:02,504]\u001b[0m Trial 12 finished with value: 0.69145079441943 and parameters: {'n_estimators': 892, 'eta': 0.05194730429132159, 'max_depth': 9, 'alpha': 0.7200000000000001, 'lambda': 30.922680567539892, 'max_bin': 423}. Best is trial 11 with value: 0.6917094784011449.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:13:19,725]\u001b[0m Trial 13 finished with value: 0.6858983775639085 and parameters: {'n_estimators': 817, 'eta': 0.04693707117308322, 'max_depth': 12, 'alpha': 0.7355, 'lambda': 30.149222453803162, 'max_bin': 417}. Best is trial 11 with value: 0.6917094784011449.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:13:32,881]\u001b[0m Trial 14 finished with value: 0.690639057592872 and parameters: {'n_estimators': 774, 'eta': 0.056202167888923696, 'max_depth': 9, 'alpha': 0.9968, 'lambda': 30.168484258258204, 'max_bin': 422}. Best is trial 11 with value: 0.6917094784011449.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:13:41,623]\u001b[0m Trial 15 finished with value: 0.6796428183517933 and parameters: {'n_estimators': 284, 'eta': 0.04317788790743354, 'max_depth': 11, 'alpha': 0.7385, 'lambda': 30.572727016517632, 'max_bin': 489}. Best is trial 11 with value: 0.6917094784011449.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:13:52,303]\u001b[0m Trial 16 finished with value: 0.691193739104682 and parameters: {'n_estimators': 780, 'eta': 0.0934065047988727, 'max_depth': 9, 'alpha': 0.8433, 'lambda': 33.69736316589026, 'max_bin': 399}. Best is trial 11 with value: 0.6917094784011449.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:13:53,262]\u001b[0m Trial 17 finished with value: 0.47578128547561366 and parameters: {'n_estimators': 51, 'eta': 0.06242414797917953, 'max_depth': 7, 'alpha': 0.6673, 'lambda': 24.96116140034038, 'max_bin': 450}. Best is trial 11 with value: 0.6917094784011449.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:14:02,865]\u001b[0m Trial 18 finished with value: 0.6859818084874647 and parameters: {'n_estimators': 497, 'eta': 0.08001380498609029, 'max_depth': 11, 'alpha': 0.8548, 'lambda': 23.56514482869666, 'max_bin': 324}. Best is trial 11 with value: 0.6917094784011449.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:14:19,675]\u001b[0m Trial 19 finished with value: 0.6935245480924468 and parameters: {'n_estimators': 823, 'eta': 0.03609266086019035, 'max_depth': 8, 'alpha': 0.8882, 'lambda': 34.0192867736503, 'max_bin': 462}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:14:31,641]\u001b[0m Trial 20 finished with value: 0.6845301490368376 and parameters: {'n_estimators': 740, 'eta': 0.036312026718213423, 'max_depth': 6, 'alpha': 0.9246000000000001, 'lambda': 35.29619234559177, 'max_bin': 467}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:14:44,470]\u001b[0m Trial 21 finished with value: 0.6915913048110938 and parameters: {'n_estimators': 831, 'eta': 0.05428674853334764, 'max_depth': 8, 'alpha': 0.7196, 'lambda': 27.736075360197425, 'max_bin': 443}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:15:00,581]\u001b[0m Trial 22 finished with value: 0.6921441186291755 and parameters: {'n_estimators': 820, 'eta': 0.037613051587349636, 'max_depth': 8, 'alpha': 0.8791, 'lambda': 26.891590323933023, 'max_bin': 443}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:15:15,639]\u001b[0m Trial 23 finished with value: 0.6821497309724525 and parameters: {'n_estimators': 701, 'eta': 0.019976432654037375, 'max_depth': 8, 'alpha': 0.8891, 'lambda': 19.63505955699698, 'max_bin': 471}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:15:30,231]\u001b[0m Trial 24 finished with value: 0.6913541066035653 and parameters: {'n_estimators': 817, 'eta': 0.038972036822270725, 'max_depth': 7, 'alpha': 0.999, 'lambda': 33.4471674006098, 'max_bin': 392}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:15:41,412]\u001b[0m Trial 25 finished with value: 0.6815706029879858 and parameters: {'n_estimators': 507, 'eta': 0.028325525155935376, 'max_depth': 8, 'alpha': 0.8177000000000001, 'lambda': 26.57549520146635, 'max_bin': 499}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:15:52,261]\u001b[0m Trial 26 finished with value: 0.6526795444251178 and parameters: {'n_estimators': 659, 'eta': 0.015588552313810278, 'max_depth': 6, 'alpha': 0.7889, 'lambda': 35.39699946387386, 'max_bin': 438}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:16:07,184]\u001b[0m Trial 27 finished with value: 0.6902707063588315 and parameters: {'n_estimators': 829, 'eta': 0.03989993329585782, 'max_depth': 10, 'alpha': 0.6415000000000001, 'lambda': 14.680680941870746, 'max_bin': 397}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:16:19,133]\u001b[0m Trial 28 finished with value: 0.6896886166060237 and parameters: {'n_estimators': 747, 'eta': 0.06686175857686744, 'max_depth': 9, 'alpha': 0.9347000000000001, 'lambda': 22.03466782565409, 'max_bin': 466}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:16:28,910]\u001b[0m Trial 29 finished with value: 0.6880330775781932 and parameters: {'n_estimators': 560, 'eta': 0.05860116552518734, 'max_depth': 7, 'alpha': 0.9313, 'lambda': 27.593319353043785, 'max_bin': 343}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:16:41,960]\u001b[0m Trial 30 finished with value: 0.6872716847397677 and parameters: {'n_estimators': 420, 'eta': 0.04969994409280595, 'max_depth': 11, 'alpha': 0.49360000000000004, 'lambda': 32.53003789974839, 'max_bin': 407}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:16:54,743]\u001b[0m Trial 31 finished with value: 0.6902844083645432 and parameters: {'n_estimators': 820, 'eta': 0.05617221999712053, 'max_depth': 8, 'alpha': 0.8931, 'lambda': 28.642365307472236, 'max_bin': 442}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:17:11,319]\u001b[0m Trial 32 finished with value: 0.6899766787524524 and parameters: {'n_estimators': 840, 'eta': 0.03343235237921502, 'max_depth': 8, 'alpha': 0.7731, 'lambda': 25.957733575365054, 'max_bin': 444}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:17:19,935]\u001b[0m Trial 33 finished with value: 0.6848229026952609 and parameters: {'n_estimators': 717, 'eta': 0.06574809798029263, 'max_depth': 8, 'alpha': 0.9961000000000001, 'lambda': 21.838735804499663, 'max_bin': 480}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:17:34,118]\u001b[0m Trial 34 finished with value: 0.6930740692471506 and parameters: {'n_estimators': 855, 'eta': 0.051675888109148235, 'max_depth': 9, 'alpha': 0.5605, 'lambda': 27.853782608243566, 'max_bin': 435}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:17:43,053]\u001b[0m Trial 35 finished with value: 0.6846067430465237 and parameters: {'n_estimators': 772, 'eta': 0.0713999136498988, 'max_depth': 9, 'alpha': 0.371, 'lambda': 18.134045020626587, 'max_bin': 458}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:17:59,699]\u001b[0m Trial 36 finished with value: 0.6873961426894193 and parameters: {'n_estimators': 863, 'eta': 0.04397882661899994, 'max_depth': 10, 'alpha': 0.5483, 'lambda': 36.04307644017992, 'max_bin': 381}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:18:16,187]\u001b[0m Trial 37 finished with value: 0.6890585123148352 and parameters: {'n_estimators': 657, 'eta': 0.02689238986857422, 'max_depth': 9, 'alpha': 0.4514, 'lambda': 23.874514874018477, 'max_bin': 428}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:18:24,688]\u001b[0m Trial 38 finished with value: 0.6848463088905894 and parameters: {'n_estimators': 769, 'eta': 0.07833988498258325, 'max_depth': 10, 'alpha': 0.2903, 'lambda': 10.276197131083455, 'max_bin': 408}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:18:32,257]\u001b[0m Trial 39 finished with value: 0.6828077733364715 and parameters: {'n_estimators': 264, 'eta': 0.0872607820473569, 'max_depth': 10, 'alpha': 0.6583, 'lambda': 32.61180571311799, 'max_bin': 434}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:18:46,049]\u001b[0m Trial 40 finished with value: 0.5155236387472245 and parameters: {'n_estimators': 861, 'eta': 0.004101299128572079, 'max_depth': 7, 'alpha': 0.5603, 'lambda': 28.594760188587365, 'max_bin': 460}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:18:58,578]\u001b[0m Trial 41 finished with value: 0.6918979765750087 and parameters: {'n_estimators': 806, 'eta': 0.05308148082148711, 'max_depth': 8, 'alpha': 0.6983, 'lambda': 27.522382534557615, 'max_bin': 452}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:19:09,615]\u001b[0m Trial 42 finished with value: 0.6897863851821431 and parameters: {'n_estimators': 688, 'eta': 0.06186623292629155, 'max_depth': 9, 'alpha': 0.6285000000000001, 'lambda': 25.885377349112893, 'max_bin': 476}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:19:20,874]\u001b[0m Trial 43 finished with value: 0.6914572308600959 and parameters: {'n_estimators': 622, 'eta': 0.05163841919926834, 'max_depth': 8, 'alpha': 0.8816, 'lambda': 18.04115624835982, 'max_bin': 253}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:19:40,428]\u001b[0m Trial 44 finished with value: 0.6914087726267562 and parameters: {'n_estimators': 891, 'eta': 0.032896230898349935, 'max_depth': 9, 'alpha': 0.446, 'lambda': 31.572523422681087, 'max_bin': 413}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:19:55,493]\u001b[0m Trial 45 finished with value: 0.690811852236682 and parameters: {'n_estimators': 800, 'eta': 0.041670185328378906, 'max_depth': 8, 'alpha': 0.8009000000000001, 'lambda': 29.17147080296525, 'max_bin': 455}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:20:08,574]\u001b[0m Trial 46 finished with value: 0.6875724303360868 and parameters: {'n_estimators': 727, 'eta': 0.04750367402988726, 'max_depth': 7, 'alpha': 0.5921000000000001, 'lambda': 37.92937438074169, 'max_bin': 429}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:20:19,567]\u001b[0m Trial 47 finished with value: 0.6899395629117413 and parameters: {'n_estimators': 861, 'eta': 0.06002347284037795, 'max_depth': 8, 'alpha': 0.9575, 'lambda': 21.584381498786794, 'max_bin': 490}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:20:27,797]\u001b[0m Trial 48 finished with value: 0.6892181272070984 and parameters: {'n_estimators': 792, 'eta': 0.0988408839709213, 'max_depth': 9, 'alpha': 0.7030000000000001, 'lambda': 23.847077258206987, 'max_bin': 450}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:20:45,223]\u001b[0m Trial 49 finished with value: 0.6831092116718119 and parameters: {'n_estimators': 897, 'eta': 0.02345643566833755, 'max_depth': 7, 'alpha': 0.5003000000000001, 'lambda': 39.543262075807995, 'max_bin': 387}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6935\n",
      "\tBest params:\n",
      "\t\tn_estimators: 823\n",
      "\t\teta: 0.03609266086019035\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.8882\n",
      "\t\tlambda: 34.0192867736503\n",
      "\t\tmax_bin: 462\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBRegressor\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.684712\n",
      "1                    TP  314.000000\n",
      "2                    TN  184.000000\n",
      "3                    FP   53.000000\n",
      "4                    FN   44.000000\n",
      "5              Accuracy    0.836975\n",
      "6             Precision    0.855586\n",
      "7           Sensitivity    0.877095\n",
      "8           Specificity    0.776400\n",
      "9              F1 score    0.866207\n",
      "10  F1 score (weighted)    0.836409\n",
      "11     F1 score (macro)    0.828802\n",
      "12    Balanced Accuracy    0.826733\n",
      "13                  MCC    0.658019\n",
      "14                  NPV    0.807000\n",
      "15              ROC_AUC    0.826733\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_xgb_0)\n",
    "y_pred_xgb_0_cat = np.where((y_pred_xgb_0 >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:20:59,620]\u001b[0m Trial 50 finished with value: 0.6821483230457794 and parameters: {'n_estimators': 750, 'eta': 0.0751575649766919, 'max_depth': 9, 'alpha': 0.7603000000000001, 'lambda': 34.49400410663728, 'max_bin': 277}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:21:13,530]\u001b[0m Trial 51 finished with value: 0.683271281527716 and parameters: {'n_estimators': 843, 'eta': 0.054199632705991756, 'max_depth': 8, 'alpha': 0.6944, 'lambda': 27.34477664148404, 'max_bin': 442}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:21:29,283]\u001b[0m Trial 52 finished with value: 0.6792667223808357 and parameters: {'n_estimators': 799, 'eta': 0.03708038642065492, 'max_depth': 8, 'alpha': 0.8278000000000001, 'lambda': 29.45987537631089, 'max_bin': 415}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:21:44,599]\u001b[0m Trial 53 finished with value: 0.6836177075856209 and parameters: {'n_estimators': 866, 'eta': 0.044977337237953975, 'max_depth': 8, 'alpha': 0.8951, 'lambda': 31.102693618078685, 'max_bin': 369}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:21:54,577]\u001b[0m Trial 54 finished with value: 0.6754312498938269 and parameters: {'n_estimators': 684, 'eta': 0.06424482667488167, 'max_depth': 6, 'alpha': 0.9661000000000001, 'lambda': 27.54882441332992, 'max_bin': 432}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:22:06,679]\u001b[0m Trial 55 finished with value: 0.6804321301943037 and parameters: {'n_estimators': 757, 'eta': 0.054421233385160514, 'max_depth': 10, 'alpha': 0.5988, 'lambda': 25.529207689347288, 'max_bin': 464}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:22:23,683]\u001b[0m Trial 56 finished with value: 0.6848418021456525 and parameters: {'n_estimators': 809, 'eta': 0.04858850238797063, 'max_depth': 9, 'alpha': 0.8622000000000001, 'lambda': 32.13706805611792, 'max_bin': 451}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:22:35,321]\u001b[0m Trial 57 finished with value: 0.6829476428737209 and parameters: {'n_estimators': 608, 'eta': 0.07054273306878545, 'max_depth': 8, 'alpha': 0.739, 'lambda': 36.614573525834935, 'max_bin': 420}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:22:46,078]\u001b[0m Trial 58 finished with value: 0.6727879569894396 and parameters: {'n_estimators': 837, 'eta': 0.0585079421413619, 'max_depth': 5, 'alpha': 0.6834, 'lambda': 33.73743409026966, 'max_bin': 480}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:22:58,360]\u001b[0m Trial 59 finished with value: 0.6810693644373064 and parameters: {'n_estimators': 730, 'eta': 0.05138583873685394, 'max_depth': 7, 'alpha': 0.38930000000000003, 'lambda': 30.554138793743086, 'max_bin': 499}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:23:07,360]\u001b[0m Trial 60 finished with value: 0.6682900000092357 and parameters: {'n_estimators': 357, 'eta': 0.03537966162429854, 'max_depth': 9, 'alpha': 0.81, 'lambda': 24.834799459422406, 'max_bin': 445}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:23:20,148]\u001b[0m Trial 61 finished with value: 0.6800355257792283 and parameters: {'n_estimators': 779, 'eta': 0.05222372968230506, 'max_depth': 8, 'alpha': 0.8905000000000001, 'lambda': 18.41690888626541, 'max_bin': 323}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:23:35,442]\u001b[0m Trial 62 finished with value: 0.6821673028364383 and parameters: {'n_estimators': 866, 'eta': 0.04087679726571098, 'max_depth': 8, 'alpha': 0.8545, 'lambda': 16.963449970477342, 'max_bin': 403}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:23:50,458]\u001b[0m Trial 63 finished with value: 0.6753890682559548 and parameters: {'n_estimators': 625, 'eta': 0.029317769201245743, 'max_depth': 9, 'alpha': 0.9165000000000001, 'lambda': 16.27616113518373, 'max_bin': 470}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:24:00,698]\u001b[0m Trial 64 finished with value: 0.6824020620931461 and parameters: {'n_estimators': 524, 'eta': 0.05721963046657227, 'max_depth': 8, 'alpha': 0.9656, 'lambda': 11.237983954303784, 'max_bin': 342}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:24:13,260]\u001b[0m Trial 65 finished with value: 0.6823844710606055 and parameters: {'n_estimators': 834, 'eta': 0.051404422834185136, 'max_depth': 7, 'alpha': 0.8661000000000001, 'lambda': 20.321764321422712, 'max_bin': 272}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:24:23,413]\u001b[0m Trial 66 finished with value: 0.6770688567946024 and parameters: {'n_estimators': 447, 'eta': 0.04548933093640688, 'max_depth': 8, 'alpha': 0.7619, 'lambda': 28.238601319320995, 'max_bin': 436}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:24:34,176]\u001b[0m Trial 67 finished with value: 0.6756363629317895 and parameters: {'n_estimators': 716, 'eta': 0.06885761917519077, 'max_depth': 9, 'alpha': 0.1155, 'lambda': 22.296318577058017, 'max_bin': 260}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:24:44,150]\u001b[0m Trial 68 finished with value: 0.6745569130212876 and parameters: {'n_estimators': 786, 'eta': 0.06284972218217888, 'max_depth': 8, 'alpha': 0.7168, 'lambda': 12.450431912988966, 'max_bin': 423}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:24:54,678]\u001b[0m Trial 69 finished with value: 0.6799073044657973 and parameters: {'n_estimators': 570, 'eta': 0.05432525326038708, 'max_depth': 7, 'alpha': 0.9143, 'lambda': 26.672157233276835, 'max_bin': 316}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:25:08,387]\u001b[0m Trial 70 finished with value: 0.6809286099956467 and parameters: {'n_estimators': 669, 'eta': 0.048396076575642, 'max_depth': 8, 'alpha': 0.9498000000000001, 'lambda': 29.403622614732438, 'max_bin': 250}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:25:28,659]\u001b[0m Trial 71 finished with value: 0.6823008795083576 and parameters: {'n_estimators': 868, 'eta': 0.03843426607846022, 'max_depth': 9, 'alpha': 0.7866000000000001, 'lambda': 30.494768885510055, 'max_bin': 301}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:25:44,923]\u001b[0m Trial 72 finished with value: 0.6789669866496453 and parameters: {'n_estimators': 887, 'eta': 0.044157769288389805, 'max_depth': 9, 'alpha': 0.7239, 'lambda': 33.40017715823811, 'max_bin': 448}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:25:58,356]\u001b[0m Trial 73 finished with value: 0.6795444086617898 and parameters: {'n_estimators': 819, 'eta': 0.06028186586622891, 'max_depth': 10, 'alpha': 0.6111, 'lambda': 23.14003714328546, 'max_bin': 459}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:26:09,557]\u001b[0m Trial 74 finished with value: 0.6799350857750293 and parameters: {'n_estimators': 837, 'eta': 0.0667068111381014, 'max_depth': 8, 'alpha': 0.0034000000000000002, 'lambda': 26.560056977944996, 'max_bin': 423}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:26:22,988]\u001b[0m Trial 75 finished with value: 0.6810429573491141 and parameters: {'n_estimators': 756, 'eta': 0.05267679462594688, 'max_depth': 9, 'alpha': 0.649, 'lambda': 24.718145433227612, 'max_bin': 439}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:26:34,763]\u001b[0m Trial 76 finished with value: 0.6745890025644672 and parameters: {'n_estimators': 899, 'eta': 0.05000972271095138, 'max_depth': 10, 'alpha': 0.8378, 'lambda': 7.59145897335863, 'max_bin': 398}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:26:49,194]\u001b[0m Trial 77 finished with value: 0.6813485654555843 and parameters: {'n_estimators': 805, 'eta': 0.05693419507027511, 'max_depth': 9, 'alpha': 0.5534, 'lambda': 31.637254319657536, 'max_bin': 414}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:26:51,321]\u001b[0m Trial 78 finished with value: 0.5773618615590506 and parameters: {'n_estimators': 104, 'eta': 0.0419256600544822, 'max_depth': 8, 'alpha': 0.9794, 'lambda': 34.65115357571941, 'max_bin': 474}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:27:05,735]\u001b[0m Trial 79 finished with value: 0.6807117742269896 and parameters: {'n_estimators': 850, 'eta': 0.04666580676586686, 'max_depth': 9, 'alpha': 0.2159, 'lambda': 28.383651669831522, 'max_bin': 351}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:27:28,052]\u001b[0m Trial 80 finished with value: 0.6825180938074769 and parameters: {'n_estimators': 875, 'eta': 0.0297903950010681, 'max_depth': 11, 'alpha': 0.8148000000000001, 'lambda': 29.602130958451724, 'max_bin': 455}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:27:40,866]\u001b[0m Trial 81 finished with value: 0.6788727336321808 and parameters: {'n_estimators': 877, 'eta': 0.035576096414033295, 'max_depth': 9, 'alpha': 0.44020000000000004, 'lambda': 1.2390782417734023, 'max_bin': 410}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:28:00,683]\u001b[0m Trial 82 finished with value: 0.6805042274850763 and parameters: {'n_estimators': 899, 'eta': 0.031857530811584156, 'max_depth': 9, 'alpha': 0.4179, 'lambda': 30.809598344157596, 'max_bin': 433}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:28:18,747]\u001b[0m Trial 83 finished with value: 0.6761129050001086 and parameters: {'n_estimators': 819, 'eta': 0.024576194996844516, 'max_depth': 8, 'alpha': 0.5012, 'lambda': 32.63530295206793, 'max_bin': 427}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:28:35,553]\u001b[0m Trial 84 finished with value: 0.6804076325327888 and parameters: {'n_estimators': 848, 'eta': 0.033002067125637836, 'max_depth': 8, 'alpha': 0.4771, 'lambda': 31.306656679198397, 'max_bin': 464}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:28:55,224]\u001b[0m Trial 85 finished with value: 0.6733197255231301 and parameters: {'n_estimators': 772, 'eta': 0.01807600311408419, 'max_depth': 9, 'alpha': 0.5835, 'lambda': 28.105588069072024, 'max_bin': 440}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:29:09,670]\u001b[0m Trial 86 finished with value: 0.6841454829254108 and parameters: {'n_estimators': 795, 'eta': 0.05626068331371719, 'max_depth': 10, 'alpha': 0.5258, 'lambda': 27.044875575129815, 'max_bin': 417}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:29:25,407]\u001b[0m Trial 87 finished with value: 0.6815907385409244 and parameters: {'n_estimators': 708, 'eta': 0.040094247461279965, 'max_depth': 8, 'alpha': 0.667, 'lambda': 36.783038421923386, 'max_bin': 389}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:29:38,031]\u001b[0m Trial 88 finished with value: 0.6844722897663157 and parameters: {'n_estimators': 737, 'eta': 0.06074549996111703, 'max_depth': 9, 'alpha': 0.8784000000000001, 'lambda': 33.02127195808609, 'max_bin': 454}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:29:56,371]\u001b[0m Trial 89 finished with value: 0.6575153019415232 and parameters: {'n_estimators': 823, 'eta': 0.011456062055172762, 'max_depth': 8, 'alpha': 0.753, 'lambda': 34.340612060118865, 'max_bin': 448}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:30:07,604]\u001b[0m Trial 90 finished with value: 0.6803230185391355 and parameters: {'n_estimators': 851, 'eta': 0.0641546731234674, 'max_depth': 7, 'alpha': 0.9356000000000001, 'lambda': 35.46296612392716, 'max_bin': 378}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:30:23,656]\u001b[0m Trial 91 finished with value: 0.6800076496724571 and parameters: {'n_estimators': 876, 'eta': 0.031698117497727635, 'max_depth': 7, 'alpha': 0.3316, 'lambda': 32.22798084119938, 'max_bin': 405}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:30:38,990]\u001b[0m Trial 92 finished with value: 0.6786445352969667 and parameters: {'n_estimators': 811, 'eta': 0.03889200570569618, 'max_depth': 7, 'alpha': 0.9992000000000001, 'lambda': 30.116432190797173, 'max_bin': 393}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:30:56,041]\u001b[0m Trial 93 finished with value: 0.6790980183670156 and parameters: {'n_estimators': 761, 'eta': 0.03725909184588906, 'max_depth': 9, 'alpha': 0.7849, 'lambda': 31.790908574311455, 'max_bin': 410}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:31:11,294]\u001b[0m Trial 94 finished with value: 0.6754183886616278 and parameters: {'n_estimators': 789, 'eta': 0.026145091294413482, 'max_depth': 7, 'alpha': 0.9767, 'lambda': 28.94656806504632, 'max_bin': 426}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:31:27,970]\u001b[0m Trial 95 finished with value: 0.6834226018621322 and parameters: {'n_estimators': 833, 'eta': 0.042492951232126194, 'max_depth': 8, 'alpha': 0.9091, 'lambda': 38.71221101257487, 'max_bin': 383}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:31:41,258]\u001b[0m Trial 96 finished with value: 0.6781140837650462 and parameters: {'n_estimators': 637, 'eta': 0.050618857204532276, 'max_depth': 10, 'alpha': 0.6292, 'lambda': 25.72330709609311, 'max_bin': 402}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:31:54,255]\u001b[0m Trial 97 finished with value: 0.6809740580556126 and parameters: {'n_estimators': 881, 'eta': 0.05353966583651512, 'max_depth': 6, 'alpha': 0.9336000000000001, 'lambda': 33.640662215280386, 'max_bin': 370}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:32:08,273]\u001b[0m Trial 98 finished with value: 0.6831447062597287 and parameters: {'n_estimators': 599, 'eta': 0.047200162756860446, 'max_depth': 9, 'alpha': 0.6921, 'lambda': 29.92483073516673, 'max_bin': 420}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:32:12,307]\u001b[0m Trial 99 finished with value: 0.6358131311252263 and parameters: {'n_estimators': 239, 'eta': 0.03530818816980168, 'max_depth': 6, 'alpha': 0.8681000000000001, 'lambda': 21.09021774314971, 'max_bin': 432}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6935\n",
      "\tBest params:\n",
      "\t\tn_estimators: 823\n",
      "\t\teta: 0.03609266086019035\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.8882\n",
      "\t\tlambda: 34.0192867736503\n",
      "\t\tmax_bin: 462\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.684712    0.733307\n",
      "1                    TP  314.000000  332.000000\n",
      "2                    TN  184.000000  177.000000\n",
      "3                    FP   53.000000   41.000000\n",
      "4                    FN   44.000000   45.000000\n",
      "5              Accuracy    0.836975    0.855462\n",
      "6             Precision    0.855586    0.890080\n",
      "7           Sensitivity    0.877095    0.880637\n",
      "8           Specificity    0.776400    0.811900\n",
      "9              F1 score    0.866207    0.885333\n",
      "10  F1 score (weighted)    0.836409    0.855734\n",
      "11     F1 score (macro)    0.828802    0.844939\n",
      "12    Balanced Accuracy    0.826733    0.846282\n",
      "13                  MCC    0.658019    0.689966\n",
      "14                  NPV    0.807000    0.797300\n",
      "15              ROC_AUC    0.826733    0.846282\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_xgb_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_1_cat = np.where((y_pred_xgb_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:32:30,325]\u001b[0m Trial 100 finished with value: 0.6933756200301825 and parameters: {'n_estimators': 863, 'eta': 0.04939059069821651, 'max_depth': 8, 'alpha': 0.9466, 'lambda': 22.962584944253337, 'max_bin': 394}. Best is trial 19 with value: 0.6935245480924468.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:32:47,082]\u001b[0m Trial 101 finished with value: 0.6955126623367652 and parameters: {'n_estimators': 854, 'eta': 0.04308318865959443, 'max_depth': 8, 'alpha': 0.9506, 'lambda': 22.691672811930044, 'max_bin': 395}. Best is trial 101 with value: 0.6955126623367652.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:33:03,188]\u001b[0m Trial 102 finished with value: 0.692536170895372 and parameters: {'n_estimators': 854, 'eta': 0.04531239430052874, 'max_depth': 8, 'alpha': 0.8944000000000001, 'lambda': 18.522682921392033, 'max_bin': 410}. Best is trial 101 with value: 0.6955126623367652.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:33:19,237]\u001b[0m Trial 103 finished with value: 0.693432697784413 and parameters: {'n_estimators': 855, 'eta': 0.04538448327436697, 'max_depth': 8, 'alpha': 0.898, 'lambda': 19.144100796470344, 'max_bin': 442}. Best is trial 101 with value: 0.6955126623367652.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:33:36,528]\u001b[0m Trial 104 finished with value: 0.6955762270520764 and parameters: {'n_estimators': 859, 'eta': 0.044953444137325514, 'max_depth': 8, 'alpha': 0.8972, 'lambda': 19.35126588820438, 'max_bin': 444}. Best is trial 104 with value: 0.6955762270520764.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:33:52,481]\u001b[0m Trial 105 finished with value: 0.6910494908074203 and parameters: {'n_estimators': 858, 'eta': 0.04423714832006999, 'max_depth': 8, 'alpha': 0.9071, 'lambda': 19.466344041769915, 'max_bin': 443}. Best is trial 104 with value: 0.6955762270520764.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:34:08,244]\u001b[0m Trial 106 finished with value: 0.6935049935205899 and parameters: {'n_estimators': 827, 'eta': 0.0487622418936893, 'max_depth': 8, 'alpha': 0.9487000000000001, 'lambda': 22.956526512789242, 'max_bin': 436}. Best is trial 104 with value: 0.6955762270520764.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:34:23,596]\u001b[0m Trial 107 finished with value: 0.6946729048684862 and parameters: {'n_estimators': 804, 'eta': 0.04893128862385038, 'max_depth': 8, 'alpha': 0.9542, 'lambda': 20.459387120147277, 'max_bin': 463}. Best is trial 104 with value: 0.6955762270520764.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:34:39,030]\u001b[0m Trial 108 finished with value: 0.6912872030972028 and parameters: {'n_estimators': 847, 'eta': 0.04534108902114212, 'max_depth': 8, 'alpha': 0.9530000000000001, 'lambda': 20.854996734122928, 'max_bin': 461}. Best is trial 104 with value: 0.6955762270520764.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:34:54,848]\u001b[0m Trial 109 finished with value: 0.6927722145130892 and parameters: {'n_estimators': 808, 'eta': 0.04803869758702052, 'max_depth': 8, 'alpha': 0.8467, 'lambda': 22.83074434054253, 'max_bin': 471}. Best is trial 104 with value: 0.6955762270520764.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:35:12,383]\u001b[0m Trial 110 finished with value: 0.6942700925260359 and parameters: {'n_estimators': 825, 'eta': 0.04278045278687989, 'max_depth': 8, 'alpha': 0.8474, 'lambda': 22.819984620862673, 'max_bin': 493}. Best is trial 104 with value: 0.6955762270520764.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:35:28,418]\u001b[0m Trial 111 finished with value: 0.6918614450412605 and parameters: {'n_estimators': 822, 'eta': 0.04876405517078453, 'max_depth': 8, 'alpha': 0.8339000000000001, 'lambda': 22.81618986270621, 'max_bin': 484}. Best is trial 104 with value: 0.6955762270520764.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:35:44,535]\u001b[0m Trial 112 finished with value: 0.6917730315824878 and parameters: {'n_estimators': 781, 'eta': 0.04277126853654976, 'max_depth': 8, 'alpha': 0.9334, 'lambda': 18.72298736353274, 'max_bin': 469}. Best is trial 104 with value: 0.6955762270520764.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:36:00,663]\u001b[0m Trial 113 finished with value: 0.6907782409434309 and parameters: {'n_estimators': 853, 'eta': 0.040637649699369324, 'max_depth': 8, 'alpha': 0.8979, 'lambda': 16.84720131603773, 'max_bin': 464}. Best is trial 104 with value: 0.6955762270520764.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:36:16,885]\u001b[0m Trial 114 finished with value: 0.6928718485847124 and parameters: {'n_estimators': 803, 'eta': 0.04715315090842237, 'max_depth': 8, 'alpha': 0.8504, 'lambda': 20.051849690312594, 'max_bin': 495}. Best is trial 104 with value: 0.6955762270520764.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:36:31,866]\u001b[0m Trial 115 finished with value: 0.6930762075520308 and parameters: {'n_estimators': 800, 'eta': 0.04678058807796257, 'max_depth': 8, 'alpha': 0.8580000000000001, 'lambda': 20.198428641899852, 'max_bin': 496}. Best is trial 104 with value: 0.6955762270520764.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:36:47,696]\u001b[0m Trial 116 finished with value: 0.6924694099965737 and parameters: {'n_estimators': 803, 'eta': 0.04745661740347447, 'max_depth': 8, 'alpha': 0.8557, 'lambda': 22.434806258316645, 'max_bin': 492}. Best is trial 104 with value: 0.6955762270520764.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:37:03,368]\u001b[0m Trial 117 finished with value: 0.6959078630808192 and parameters: {'n_estimators': 835, 'eta': 0.04941804308703609, 'max_depth': 8, 'alpha': 0.9226000000000001, 'lambda': 20.099673507838197, 'max_bin': 500}. Best is trial 117 with value: 0.6959078630808192.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:37:17,942]\u001b[0m Trial 118 finished with value: 0.6924697973134537 and parameters: {'n_estimators': 769, 'eta': 0.050070003652101396, 'max_depth': 8, 'alpha': 0.9472, 'lambda': 20.109133555685943, 'max_bin': 497}. Best is trial 117 with value: 0.6959078630808192.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:37:34,278]\u001b[0m Trial 119 finished with value: 0.6924488968574092 and parameters: {'n_estimators': 828, 'eta': 0.04152885453683876, 'max_depth': 8, 'alpha': 0.9641000000000001, 'lambda': 15.710505647450464, 'max_bin': 494}. Best is trial 117 with value: 0.6959078630808192.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:37:52,224]\u001b[0m Trial 120 finished with value: 0.6935031572331415 and parameters: {'n_estimators': 885, 'eta': 0.04372016211398188, 'max_depth': 8, 'alpha': 0.9216000000000001, 'lambda': 21.439139182166414, 'max_bin': 483}. Best is trial 117 with value: 0.6959078630808192.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:38:08,699]\u001b[0m Trial 121 finished with value: 0.6927341355964562 and parameters: {'n_estimators': 882, 'eta': 0.04368740632970082, 'max_depth': 8, 'alpha': 0.9234, 'lambda': 21.48398227889596, 'max_bin': 485}. Best is trial 117 with value: 0.6959078630808192.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:38:25,159]\u001b[0m Trial 122 finished with value: 0.6933011916658209 and parameters: {'n_estimators': 868, 'eta': 0.04587417692637543, 'max_depth': 8, 'alpha': 0.8772000000000001, 'lambda': 17.554689521124917, 'max_bin': 480}. Best is trial 117 with value: 0.6959078630808192.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:38:40,551]\u001b[0m Trial 123 finished with value: 0.6944653964167602 and parameters: {'n_estimators': 865, 'eta': 0.049960481877382654, 'max_depth': 8, 'alpha': 0.8733000000000001, 'lambda': 17.352984299912748, 'max_bin': 478}. Best is trial 117 with value: 0.6959078630808192.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:38:56,151]\u001b[0m Trial 124 finished with value: 0.6972549107298986 and parameters: {'n_estimators': 869, 'eta': 0.04617477429928047, 'max_depth': 8, 'alpha': 0.8785000000000001, 'lambda': 17.514476657462126, 'max_bin': 479}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:39:11,266]\u001b[0m Trial 125 finished with value: 0.692441931525942 and parameters: {'n_estimators': 871, 'eta': 0.04928353641267387, 'max_depth': 8, 'alpha': 0.9847, 'lambda': 14.204392809487565, 'max_bin': 478}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:39:28,488]\u001b[0m Trial 126 finished with value: 0.6905489768778001 and parameters: {'n_estimators': 888, 'eta': 0.038402927760509756, 'max_depth': 8, 'alpha': 0.8771, 'lambda': 17.683798951084835, 'max_bin': 487}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:39:43,159]\u001b[0m Trial 127 finished with value: 0.6936732170209732 and parameters: {'n_estimators': 897, 'eta': 0.043214428624830215, 'max_depth': 7, 'alpha': 0.9162, 'lambda': 15.084785059247652, 'max_bin': 482}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:39:59,220]\u001b[0m Trial 128 finished with value: 0.6920917516769975 and parameters: {'n_estimators': 895, 'eta': 0.043004072386509266, 'max_depth': 7, 'alpha': 0.9178000000000001, 'lambda': 19.023589454056236, 'max_bin': 475}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:40:13,705]\u001b[0m Trial 129 finished with value: 0.6925970746865386 and parameters: {'n_estimators': 839, 'eta': 0.04042232953187772, 'max_depth': 7, 'alpha': 0.9460000000000001, 'lambda': 13.907718799114857, 'max_bin': 482}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:40:27,665]\u001b[0m Trial 130 finished with value: 0.6932380711095119 and parameters: {'n_estimators': 899, 'eta': 0.05499011732053737, 'max_depth': 7, 'alpha': 0.9715, 'lambda': 24.173279597718114, 'max_bin': 489}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:40:43,882]\u001b[0m Trial 131 finished with value: 0.6927030763610282 and parameters: {'n_estimators': 866, 'eta': 0.045370177442478676, 'max_depth': 8, 'alpha': 0.8825000000000001, 'lambda': 17.52549254860841, 'max_bin': 479}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:40:59,493]\u001b[0m Trial 132 finished with value: 0.6933313397062191 and parameters: {'n_estimators': 873, 'eta': 0.05039223682416485, 'max_depth': 8, 'alpha': 0.9101, 'lambda': 15.570233703014946, 'max_bin': 472}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:41:16,191]\u001b[0m Trial 133 finished with value: 0.6943002911328272 and parameters: {'n_estimators': 844, 'eta': 0.04317594944763685, 'max_depth': 8, 'alpha': 0.9064000000000001, 'lambda': 16.02760929680445, 'max_bin': 472}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:41:31,964]\u001b[0m Trial 134 finished with value: 0.6926740038764339 and parameters: {'n_estimators': 837, 'eta': 0.04354617822296078, 'max_depth': 8, 'alpha': 0.935, 'lambda': 13.404810133858142, 'max_bin': 490}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:41:48,892]\u001b[0m Trial 135 finished with value: 0.6928529807910296 and parameters: {'n_estimators': 827, 'eta': 0.0393955618993902, 'max_depth': 8, 'alpha': 0.9590000000000001, 'lambda': 19.287392779744575, 'max_bin': 467}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:42:06,811]\u001b[0m Trial 136 finished with value: 0.6902109613757897 and parameters: {'n_estimators': 844, 'eta': 0.034440231026603464, 'max_depth': 8, 'alpha': 0.8201, 'lambda': 16.4922543784106, 'max_bin': 500}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:42:22,915]\u001b[0m Trial 137 finished with value: 0.692187226821024 and parameters: {'n_estimators': 856, 'eta': 0.03708407527771669, 'max_depth': 7, 'alpha': 0.8948, 'lambda': 21.848349327449984, 'max_bin': 485}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:42:39,935]\u001b[0m Trial 138 finished with value: 0.695216557221851 and parameters: {'n_estimators': 883, 'eta': 0.042233292020596974, 'max_depth': 8, 'alpha': 0.9131, 'lambda': 15.19534463104495, 'max_bin': 459}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:42:56,245]\u001b[0m Trial 139 finished with value: 0.6899966913676927 and parameters: {'n_estimators': 887, 'eta': 0.04132058132941864, 'max_depth': 8, 'alpha': 0.9196000000000001, 'lambda': 15.170348730172096, 'max_bin': 458}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:43:12,177]\u001b[0m Trial 140 finished with value: 0.692115013438614 and parameters: {'n_estimators': 900, 'eta': 0.04265728376051965, 'max_depth': 8, 'alpha': 0.9861000000000001, 'lambda': 11.716189382198225, 'max_bin': 474}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:43:28,510]\u001b[0m Trial 141 finished with value: 0.6941690493802524 and parameters: {'n_estimators': 880, 'eta': 0.048804487398277085, 'max_depth': 8, 'alpha': 0.9402, 'lambda': 20.95360680124569, 'max_bin': 462}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:43:42,940]\u001b[0m Trial 142 finished with value: 0.6924511992766578 and parameters: {'n_estimators': 878, 'eta': 0.05238799429963236, 'max_depth': 8, 'alpha': 0.8993, 'lambda': 16.376358881952, 'max_bin': 461}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:43:58,723]\u001b[0m Trial 143 finished with value: 0.6894007663371664 and parameters: {'n_estimators': 838, 'eta': 0.044808608532988875, 'max_depth': 8, 'alpha': 0.9242, 'lambda': 12.823708215464466, 'max_bin': 467}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:44:14,825]\u001b[0m Trial 144 finished with value: 0.6935889431246849 and parameters: {'n_estimators': 822, 'eta': 0.047755648905255424, 'max_depth': 8, 'alpha': 0.8817, 'lambda': 20.962691570541626, 'max_bin': 452}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:44:30,038]\u001b[0m Trial 145 finished with value: 0.696066014292464 and parameters: {'n_estimators': 820, 'eta': 0.0517395103039755, 'max_depth': 8, 'alpha': 0.8337, 'lambda': 21.43128848816871, 'max_bin': 453}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:44:45,259]\u001b[0m Trial 146 finished with value: 0.6963560746871122 and parameters: {'n_estimators': 823, 'eta': 0.0519416828621793, 'max_depth': 8, 'alpha': 0.8301000000000001, 'lambda': 20.797410327251125, 'max_bin': 450}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:44:58,585]\u001b[0m Trial 147 finished with value: 0.6934989108400538 and parameters: {'n_estimators': 818, 'eta': 0.05490976307391508, 'max_depth': 7, 'alpha': 0.7944, 'lambda': 17.878785554739782, 'max_bin': 450}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:45:12,526]\u001b[0m Trial 148 finished with value: 0.6958934150007321 and parameters: {'n_estimators': 786, 'eta': 0.058596084339352456, 'max_depth': 8, 'alpha': 0.8315, 'lambda': 20.758835475025077, 'max_bin': 454}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:45:26,924]\u001b[0m Trial 149 finished with value: 0.6962625054755216 and parameters: {'n_estimators': 785, 'eta': 0.052317969406482145, 'max_depth': 8, 'alpha': 0.8404, 'lambda': 19.646344924755525, 'max_bin': 451}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6973\n",
      "\tBest params:\n",
      "\t\tn_estimators: 869\n",
      "\t\teta: 0.04617477429928047\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.8785000000000001\n",
      "\t\tlambda: 17.514476657462126\n",
      "\t\tmax_bin: 479\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.684712    0.733307    0.708160\n",
      "1                    TP  314.000000  332.000000  320.000000\n",
      "2                    TN  184.000000  177.000000  187.000000\n",
      "3                    FP   53.000000   41.000000   50.000000\n",
      "4                    FN   44.000000   45.000000   38.000000\n",
      "5              Accuracy    0.836975    0.855462    0.852101\n",
      "6             Precision    0.855586    0.890080    0.864865\n",
      "7           Sensitivity    0.877095    0.880637    0.893855\n",
      "8           Specificity    0.776400    0.811900    0.789000\n",
      "9              F1 score    0.866207    0.885333    0.879121\n",
      "10  F1 score (weighted)    0.836409    0.855734    0.851399\n",
      "11     F1 score (macro)    0.828802    0.844939    0.844322\n",
      "12    Balanced Accuracy    0.826733    0.846282    0.841442\n",
      "13                  MCC    0.658019    0.689966    0.689399\n",
      "14                  NPV    0.807000    0.797300    0.831100\n",
      "15              ROC_AUC    0.826733    0.846282    0.841442\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_xgb_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_2_cat = np.where((y_pred_xgb_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:45:40,360]\u001b[0m Trial 150 finished with value: 0.6828814671389879 and parameters: {'n_estimators': 786, 'eta': 0.05854950493132243, 'max_depth': 8, 'alpha': 0.8345, 'lambda': 20.552176470906026, 'max_bin': 459}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:45:53,966]\u001b[0m Trial 151 finished with value: 0.6845209460747055 and parameters: {'n_estimators': 783, 'eta': 0.05228606092060687, 'max_depth': 8, 'alpha': 0.8666, 'lambda': 20.88448050496823, 'max_bin': 453}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:46:05,776]\u001b[0m Trial 152 finished with value: 0.6786949605162145 and parameters: {'n_estimators': 749, 'eta': 0.051789366154700686, 'max_depth': 8, 'alpha': 0.8424, 'lambda': 19.606783773596582, 'max_bin': 447}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:46:18,144]\u001b[0m Trial 153 finished with value: 0.6828167280597635 and parameters: {'n_estimators': 817, 'eta': 0.05669258151991975, 'max_depth': 8, 'alpha': 0.8167000000000001, 'lambda': 21.85850819451695, 'max_bin': 456}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:46:26,482]\u001b[0m Trial 154 finished with value: 0.6757419441600914 and parameters: {'n_estimators': 390, 'eta': 0.04747146696315088, 'max_depth': 8, 'alpha': 0.8679, 'lambda': 14.754917321234053, 'max_bin': 465}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:46:40,052]\u001b[0m Trial 155 finished with value: 0.6747681636970649 and parameters: {'n_estimators': 841, 'eta': 0.05031505741563286, 'max_depth': 12, 'alpha': 0.8193, 'lambda': 18.67085777353305, 'max_bin': 453}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:46:53,116]\u001b[0m Trial 156 finished with value: 0.6814884940407887 and parameters: {'n_estimators': 797, 'eta': 0.05351909967692804, 'max_depth': 8, 'alpha': 0.7784000000000001, 'lambda': 23.739766800102483, 'max_bin': 476}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:47:05,957]\u001b[0m Trial 157 finished with value: 0.6827733790476656 and parameters: {'n_estimators': 861, 'eta': 0.05534300111787884, 'max_depth': 8, 'alpha': 0.8828, 'lambda': 20.69882885901921, 'max_bin': 469}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:47:16,657]\u001b[0m Trial 158 finished with value: 0.6776028508501628 and parameters: {'n_estimators': 768, 'eta': 0.0488108080550146, 'max_depth': 7, 'alpha': 0.8055, 'lambda': 16.959797235834298, 'max_bin': 463}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:47:26,527]\u001b[0m Trial 159 finished with value: 0.6781564784973123 and parameters: {'n_estimators': 826, 'eta': 0.050942946418967155, 'max_depth': 5, 'alpha': 0.8421000000000001, 'lambda': 19.54550564429431, 'max_bin': 458}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:47:40,085]\u001b[0m Trial 160 finished with value: 0.6821675176958558 and parameters: {'n_estimators': 846, 'eta': 0.04635637206399505, 'max_depth': 8, 'alpha': 0.8636, 'lambda': 22.206636723652444, 'max_bin': 447}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:47:50,447]\u001b[0m Trial 161 finished with value: 0.6818873061934879 and parameters: {'n_estimators': 811, 'eta': 0.05902880878163328, 'max_depth': 8, 'alpha': 0.9022, 'lambda': 15.605309785703753, 'max_bin': 472}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:48:05,008]\u001b[0m Trial 162 finished with value: 0.6833230617981315 and parameters: {'n_estimators': 872, 'eta': 0.040283615480544537, 'max_depth': 8, 'alpha': 0.8869, 'lambda': 18.381896530480144, 'max_bin': 452}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:48:17,347]\u001b[0m Trial 163 finished with value: 0.6832668043067032 and parameters: {'n_estimators': 814, 'eta': 0.05301530496017494, 'max_depth': 8, 'alpha': 0.8450000000000001, 'lambda': 21.170933272033103, 'max_bin': 464}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:48:30,587]\u001b[0m Trial 164 finished with value: 0.6828706836540042 and parameters: {'n_estimators': 852, 'eta': 0.04732993185506008, 'max_depth': 8, 'alpha': 0.9099, 'lambda': 20.39603484909498, 'max_bin': 478}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:48:43,402]\u001b[0m Trial 165 finished with value: 0.6823741558258614 and parameters: {'n_estimators': 794, 'eta': 0.04262161495394412, 'max_depth': 8, 'alpha': 0.9315, 'lambda': 17.23488574533071, 'max_bin': 456}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:48:56,682]\u001b[0m Trial 166 finished with value: 0.6832719646780638 and parameters: {'n_estimators': 838, 'eta': 0.04874769916150366, 'max_depth': 8, 'alpha': 0.8266, 'lambda': 19.63896948736687, 'max_bin': 469}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:49:09,197]\u001b[0m Trial 167 finished with value: 0.683483676565395 and parameters: {'n_estimators': 876, 'eta': 0.04503396244308887, 'max_depth': 8, 'alpha': 0.8708, 'lambda': 16.206363817981586, 'max_bin': 445}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:49:23,509]\u001b[0m Trial 168 finished with value: 0.68355546747127 and parameters: {'n_estimators': 827, 'eta': 0.03915570691937011, 'max_depth': 8, 'alpha': 0.9500000000000001, 'lambda': 18.445100604849337, 'max_bin': 460}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:49:33,285]\u001b[0m Trial 169 finished with value: 0.6766645319268878 and parameters: {'n_estimators': 859, 'eta': 0.050644316165861805, 'max_depth': 7, 'alpha': 0.9722000000000001, 'lambda': 5.242790513471046, 'max_bin': 451}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:49:48,359]\u001b[0m Trial 170 finished with value: 0.684052539042137 and parameters: {'n_estimators': 777, 'eta': 0.03697508342121649, 'max_depth': 8, 'alpha': 0.8904000000000001, 'lambda': 21.24940538715969, 'max_bin': 486}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:50:02,585]\u001b[0m Trial 171 finished with value: 0.6834139959714619 and parameters: {'n_estimators': 830, 'eta': 0.04815966064212561, 'max_depth': 8, 'alpha': 0.9416, 'lambda': 23.4629596740965, 'max_bin': 439}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:50:16,069]\u001b[0m Trial 172 finished with value: 0.6817316973669664 and parameters: {'n_estimators': 809, 'eta': 0.04661962309241278, 'max_depth': 8, 'alpha': 0.9113, 'lambda': 22.398724899425417, 'max_bin': 358}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:50:26,581]\u001b[0m Trial 173 finished with value: 0.6817786125345567 and parameters: {'n_estimators': 540, 'eta': 0.04935266109603249, 'max_depth': 8, 'alpha': 0.9608000000000001, 'lambda': 20.01212957348911, 'max_bin': 490}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:50:40,400]\u001b[0m Trial 174 finished with value: 0.6814853021796554 and parameters: {'n_estimators': 846, 'eta': 0.04393817104406056, 'max_depth': 8, 'alpha': 0.9973000000000001, 'lambda': 24.700145527914987, 'max_bin': 443}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:50:52,921]\u001b[0m Trial 175 finished with value: 0.6799319783688754 and parameters: {'n_estimators': 829, 'eta': 0.05415525593590757, 'max_depth': 8, 'alpha': 0.8557, 'lambda': 21.729206184029582, 'max_bin': 462}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:51:07,031]\u001b[0m Trial 176 finished with value: 0.683398232090536 and parameters: {'n_estimators': 883, 'eta': 0.04166314665309476, 'max_depth': 8, 'alpha': 0.9281, 'lambda': 19.24867626611067, 'max_bin': 448}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:51:17,237]\u001b[0m Trial 177 finished with value: 0.6828637687454393 and parameters: {'n_estimators': 793, 'eta': 0.05764126007994464, 'max_depth': 8, 'alpha': 0.7964, 'lambda': 14.53977051847974, 'max_bin': 500}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:51:28,892]\u001b[0m Trial 178 finished with value: 0.6821957576996716 and parameters: {'n_estimators': 862, 'eta': 0.05140211108438092, 'max_depth': 8, 'alpha': 0.8837, 'lambda': 23.237994451863358, 'max_bin': 437}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:51:42,597]\u001b[0m Trial 179 finished with value: 0.6826804004581949 and parameters: {'n_estimators': 816, 'eta': 0.04577156515176119, 'max_depth': 8, 'alpha': 0.9091, 'lambda': 20.679685314238377, 'max_bin': 476}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:51:55,969]\u001b[0m Trial 180 finished with value: 0.6857161755441853 and parameters: {'n_estimators': 757, 'eta': 0.04872110045531806, 'max_depth': 8, 'alpha': 0.9502, 'lambda': 17.968310144731433, 'max_bin': 493}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:52:05,858]\u001b[0m Trial 181 finished with value: 0.6795272810535382 and parameters: {'n_estimators': 475, 'eta': 0.04353647461901762, 'max_depth': 8, 'alpha': 0.9249, 'lambda': 21.497890458736144, 'max_bin': 481}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:52:18,827]\u001b[0m Trial 182 finished with value: 0.6829846189338087 and parameters: {'n_estimators': 898, 'eta': 0.044455243972767934, 'max_depth': 8, 'alpha': 0.8733000000000001, 'lambda': 22.459309704018093, 'max_bin': 483}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:52:33,225]\u001b[0m Trial 183 finished with value: 0.6837514844349326 and parameters: {'n_estimators': 888, 'eta': 0.04156011223262397, 'max_depth': 8, 'alpha': 0.9184, 'lambda': 20.128567649588817, 'max_bin': 472}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:52:46,344]\u001b[0m Trial 184 finished with value: 0.6803680175497415 and parameters: {'n_estimators': 873, 'eta': 0.04766227735809416, 'max_depth': 8, 'alpha': 0.9735, 'lambda': 21.547211053273294, 'max_bin': 456}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:53:00,627]\u001b[0m Trial 185 finished with value: 0.6814914782747665 and parameters: {'n_estimators': 845, 'eta': 0.04627703965219037, 'max_depth': 8, 'alpha': 0.8941, 'lambda': 24.293640614103804, 'max_bin': 465}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:53:12,874]\u001b[0m Trial 186 finished with value: 0.681266638214522 and parameters: {'n_estimators': 863, 'eta': 0.050187828633059255, 'max_depth': 8, 'alpha': 0.8316, 'lambda': 20.86041729266829, 'max_bin': 487}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:53:25,410]\u001b[0m Trial 187 finished with value: 0.6857381120815849 and parameters: {'n_estimators': 880, 'eta': 0.053018772967840896, 'max_depth': 8, 'alpha': 0.9406, 'lambda': 19.04624226802609, 'max_bin': 469}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:53:39,320]\u001b[0m Trial 188 finished with value: 0.6831690360183275 and parameters: {'n_estimators': 900, 'eta': 0.04375464152288251, 'max_depth': 8, 'alpha': 0.8569, 'lambda': 16.114872506140387, 'max_bin': 452}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:53:45,867]\u001b[0m Trial 189 finished with value: 0.6677480618919154 and parameters: {'n_estimators': 300, 'eta': 0.039682709671803915, 'max_depth': 8, 'alpha': 0.9044000000000001, 'lambda': 22.922774062679952, 'max_bin': 480}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:53:58,012]\u001b[0m Trial 190 finished with value: 0.6831783937403174 and parameters: {'n_estimators': 829, 'eta': 0.04941455638302523, 'max_depth': 8, 'alpha': 0.9594, 'lambda': 13.747138443605067, 'max_bin': 460}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:54:10,074]\u001b[0m Trial 191 finished with value: 0.6846386155987441 and parameters: {'n_estimators': 814, 'eta': 0.05409474883536451, 'max_depth': 7, 'alpha': 0.8109000000000001, 'lambda': 18.110930104381925, 'max_bin': 450}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:54:21,685]\u001b[0m Trial 192 finished with value: 0.6817185958789732 and parameters: {'n_estimators': 849, 'eta': 0.051665939477847554, 'max_depth': 7, 'alpha': 0.8019000000000001, 'lambda': 17.409859704463727, 'max_bin': 445}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:54:30,522]\u001b[0m Trial 193 finished with value: 0.6864955965682125 and parameters: {'n_estimators': 803, 'eta': 0.08532421043113192, 'max_depth': 7, 'alpha': 0.8473, 'lambda': 15.100089800153118, 'max_bin': 455}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:54:41,517]\u001b[0m Trial 194 finished with value: 0.6798536010260249 and parameters: {'n_estimators': 823, 'eta': 0.05616008252197946, 'max_depth': 8, 'alpha': 0.7753, 'lambda': 19.9942217412472, 'max_bin': 435}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:54:52,088]\u001b[0m Trial 195 finished with value: 0.6795827072829235 and parameters: {'n_estimators': 860, 'eta': 0.05498871928814511, 'max_depth': 7, 'alpha': 0.8751, 'lambda': 16.73424586062883, 'max_bin': 449}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:55:02,685]\u001b[0m Trial 196 finished with value: 0.6847711962214029 and parameters: {'n_estimators': 782, 'eta': 0.06053969614393651, 'max_depth': 8, 'alpha': 0.9285, 'lambda': 18.911132149307004, 'max_bin': 492}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:55:14,521]\u001b[0m Trial 197 finished with value: 0.6807725065362449 and parameters: {'n_estimators': 836, 'eta': 0.04618013652584138, 'max_depth': 6, 'alpha': 0.8356, 'lambda': 21.244306729731992, 'max_bin': 474}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:55:29,078]\u001b[0m Trial 198 finished with value: 0.6831102312297925 and parameters: {'n_estimators': 878, 'eta': 0.04224910976860316, 'max_depth': 8, 'alpha': 0.8862, 'lambda': 17.857146142227887, 'max_bin': 465}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:55:40,823]\u001b[0m Trial 199 finished with value: 0.6822203489991382 and parameters: {'n_estimators': 813, 'eta': 0.05565023861969491, 'max_depth': 7, 'alpha': 0.91, 'lambda': 20.642969163455927, 'max_bin': 457}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6973\n",
      "\tBest params:\n",
      "\t\tn_estimators: 869\n",
      "\t\teta: 0.04617477429928047\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.8785000000000001\n",
      "\t\tlambda: 17.514476657462126\n",
      "\t\tmax_bin: 479\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.684712    0.733307    0.708160    0.688529\n",
      "1                    TP  314.000000  332.000000  320.000000  321.000000\n",
      "2                    TN  184.000000  177.000000  187.000000  180.000000\n",
      "3                    FP   53.000000   41.000000   50.000000   51.000000\n",
      "4                    FN   44.000000   45.000000   38.000000   43.000000\n",
      "5              Accuracy    0.836975    0.855462    0.852101    0.842017\n",
      "6             Precision    0.855586    0.890080    0.864865    0.862903\n",
      "7           Sensitivity    0.877095    0.880637    0.893855    0.881868\n",
      "8           Specificity    0.776400    0.811900    0.789000    0.779200\n",
      "9              F1 score    0.866207    0.885333    0.879121    0.872283\n",
      "10  F1 score (weighted)    0.836409    0.855734    0.851399    0.841483\n",
      "11     F1 score (macro)    0.828802    0.844939    0.844322    0.832617\n",
      "12    Balanced Accuracy    0.826733    0.846282    0.841442    0.830544\n",
      "13                  MCC    0.658019    0.689966    0.689399    0.665568\n",
      "14                  NPV    0.807000    0.797300    0.831100    0.807200\n",
      "15              ROC_AUC    0.826733    0.846282    0.841442    0.830544\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_xgb_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_3_cat = np.where((y_pred_xgb_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 03:55:54,921]\u001b[0m Trial 200 finished with value: 0.6884615541293856 and parameters: {'n_estimators': 848, 'eta': 0.0481598781792154, 'max_depth': 8, 'alpha': 0.8706, 'lambda': 21.825943136612125, 'max_bin': 441}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:56:08,112]\u001b[0m Trial 201 finished with value: 0.6900726826512786 and parameters: {'n_estimators': 860, 'eta': 0.044854802209268005, 'max_depth': 8, 'alpha': 0.9363, 'lambda': 19.069574627595596, 'max_bin': 449}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:56:20,444]\u001b[0m Trial 202 finished with value: 0.6902012268298068 and parameters: {'n_estimators': 834, 'eta': 0.05196272888132909, 'max_depth': 8, 'alpha': 0.8996000000000001, 'lambda': 19.60754467531184, 'max_bin': 443}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:56:32,705]\u001b[0m Trial 203 finished with value: 0.6899369211704147 and parameters: {'n_estimators': 884, 'eta': 0.04608189007216606, 'max_depth': 8, 'alpha': 0.8488, 'lambda': 17.214833574748923, 'max_bin': 439}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:56:44,276]\u001b[0m Trial 204 finished with value: 0.6913147091285243 and parameters: {'n_estimators': 856, 'eta': 0.04925535308326641, 'max_depth': 8, 'alpha': 0.8972, 'lambda': 18.418141285863527, 'max_bin': 430}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:56:57,679]\u001b[0m Trial 205 finished with value: 0.689713061319287 and parameters: {'n_estimators': 795, 'eta': 0.042872505156392016, 'max_depth': 8, 'alpha': 0.9201, 'lambda': 20.078736159641537, 'max_bin': 454}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:57:12,381]\u001b[0m Trial 206 finished with value: 0.6899263644494276 and parameters: {'n_estimators': 822, 'eta': 0.03850918228916856, 'max_depth': 8, 'alpha': 0.8822000000000001, 'lambda': 22.18869710788868, 'max_bin': 463}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:57:25,531]\u001b[0m Trial 207 finished with value: 0.6911608648450457 and parameters: {'n_estimators': 870, 'eta': 0.04463446766730894, 'max_depth': 8, 'alpha': 0.8228000000000001, 'lambda': 15.771904502064967, 'max_bin': 446}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:57:37,213]\u001b[0m Trial 208 finished with value: 0.6887056475234297 and parameters: {'n_estimators': 843, 'eta': 0.05057512647950048, 'max_depth': 8, 'alpha': 0.8604, 'lambda': 19.078492097238623, 'max_bin': 485}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:57:51,712]\u001b[0m Trial 209 finished with value: 0.690212285945469 and parameters: {'n_estimators': 805, 'eta': 0.04681519212479772, 'max_depth': 8, 'alpha': 0.9409000000000001, 'lambda': 21.1027033643701, 'max_bin': 460}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:58:06,018]\u001b[0m Trial 210 finished with value: 0.6882657324126804 and parameters: {'n_estimators': 882, 'eta': 0.04088271673715484, 'max_depth': 8, 'alpha': 0.9780000000000001, 'lambda': 20.300235706971677, 'max_bin': 468}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:58:20,713]\u001b[0m Trial 211 finished with value: 0.6879723011582837 and parameters: {'n_estimators': 855, 'eta': 0.0484215686348186, 'max_depth': 8, 'alpha': 0.9517, 'lambda': 23.380336789720566, 'max_bin': 386}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:58:33,191]\u001b[0m Trial 212 finished with value: 0.6887026186974398 and parameters: {'n_estimators': 865, 'eta': 0.053244417156488415, 'max_depth': 8, 'alpha': 0.9186000000000001, 'lambda': 23.20278009846561, 'max_bin': 349}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:58:37,026]\u001b[0m Trial 213 finished with value: 0.6656649025997433 and parameters: {'n_estimators': 171, 'eta': 0.04997757125536283, 'max_depth': 8, 'alpha': 0.9509000000000001, 'lambda': 22.53900145131735, 'max_bin': 453}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:58:49,149]\u001b[0m Trial 214 finished with value: 0.689510036041533 and parameters: {'n_estimators': 837, 'eta': 0.047860485648516804, 'max_depth': 8, 'alpha': 0.8994000000000001, 'lambda': 21.828031002206114, 'max_bin': 398}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:59:03,631]\u001b[0m Trial 215 finished with value: 0.6898122353712022 and parameters: {'n_estimators': 900, 'eta': 0.044768480741717694, 'max_depth': 8, 'alpha': 0.9284, 'lambda': 25.17532510852997, 'max_bin': 494}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:59:16,005]\u001b[0m Trial 216 finished with value: 0.6942891752828183 and parameters: {'n_estimators': 865, 'eta': 0.051581760269131825, 'max_depth': 8, 'alpha': 0.9642000000000001, 'lambda': 20.91658433146498, 'max_bin': 474}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:59:28,479]\u001b[0m Trial 217 finished with value: 0.6917542807634152 and parameters: {'n_estimators': 823, 'eta': 0.05281993945789789, 'max_depth': 8, 'alpha': 0.9687, 'lambda': 20.798822728105435, 'max_bin': 477}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:59:39,829]\u001b[0m Trial 218 finished with value: 0.6899398836757666 and parameters: {'n_estimators': 870, 'eta': 0.057388942073231336, 'max_depth': 8, 'alpha': 0.868, 'lambda': 19.401897252253203, 'max_bin': 472}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 03:59:52,485]\u001b[0m Trial 219 finished with value: 0.6902275359197367 and parameters: {'n_estimators': 848, 'eta': 0.0512566779171785, 'max_depth': 8, 'alpha': 0.9952000000000001, 'lambda': 18.265497545061184, 'max_bin': 479}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:00:04,147]\u001b[0m Trial 220 finished with value: 0.6918548596231673 and parameters: {'n_estimators': 789, 'eta': 0.054490130647077, 'max_depth': 7, 'alpha': 0.8373, 'lambda': 16.76072373849803, 'max_bin': 458}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:00:16,416]\u001b[0m Trial 221 finished with value: 0.6878229181950102 and parameters: {'n_estimators': 867, 'eta': 0.04954492308553218, 'max_depth': 8, 'alpha': 0.9563, 'lambda': 21.201545187738375, 'max_bin': 378}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:00:28,459]\u001b[0m Trial 222 finished with value: 0.6860966787582916 and parameters: {'n_estimators': 886, 'eta': 0.04664112355785255, 'max_depth': 8, 'alpha': 0.9366000000000001, 'lambda': 8.878521462048532, 'max_bin': 448}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:00:43,288]\u001b[0m Trial 223 finished with value: 0.6903256450897166 and parameters: {'n_estimators': 835, 'eta': 0.043014467377353, 'max_depth': 8, 'alpha': 0.9133, 'lambda': 23.964435934666938, 'max_bin': 372}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:00:56,414]\u001b[0m Trial 224 finished with value: 0.6916385505238661 and parameters: {'n_estimators': 852, 'eta': 0.05099813376890699, 'max_depth': 8, 'alpha': 0.891, 'lambda': 22.38625122094414, 'max_bin': 364}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:01:09,306]\u001b[0m Trial 225 finished with value: 0.6897451090149656 and parameters: {'n_estimators': 817, 'eta': 0.047870423343248585, 'max_depth': 8, 'alpha': 0.9750000000000001, 'lambda': 19.87387737038876, 'max_bin': 467}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:01:21,836]\u001b[0m Trial 226 finished with value: 0.6891210296767444 and parameters: {'n_estimators': 876, 'eta': 0.04533192742147915, 'max_depth': 8, 'alpha': 0.7968000000000001, 'lambda': 15.149871112928013, 'max_bin': 484}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:01:33,831]\u001b[0m Trial 227 finished with value: 0.6902094394876286 and parameters: {'n_estimators': 857, 'eta': 0.05293822206478395, 'max_depth': 8, 'alpha': 0.9325, 'lambda': 21.88215202050906, 'max_bin': 500}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:01:45,645]\u001b[0m Trial 228 finished with value: 0.6877781825082453 and parameters: {'n_estimators': 837, 'eta': 0.04924020758782199, 'max_depth': 8, 'alpha': 0.8589, 'lambda': 20.939646081387178, 'max_bin': 488}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:02:00,175]\u001b[0m Trial 229 finished with value: 0.6896649582637908 and parameters: {'n_estimators': 804, 'eta': 0.04189002174699714, 'max_depth': 8, 'alpha': 0.9071, 'lambda': 20.318154329030936, 'max_bin': 453}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:02:11,270]\u001b[0m Trial 230 finished with value: 0.6911342124663684 and parameters: {'n_estimators': 886, 'eta': 0.055756959087967684, 'max_depth': 8, 'alpha': 0.883, 'lambda': 22.577147851726764, 'max_bin': 460}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:02:23,189]\u001b[0m Trial 231 finished with value: 0.6889457110020485 and parameters: {'n_estimators': 869, 'eta': 0.051240497343082586, 'max_depth': 8, 'alpha': 0.9133, 'lambda': 15.437396159017014, 'max_bin': 473}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:02:34,555]\u001b[0m Trial 232 finished with value: 0.6905158570439678 and parameters: {'n_estimators': 867, 'eta': 0.04970810259010618, 'max_depth': 8, 'alpha': 0.9494, 'lambda': 16.30178601306497, 'max_bin': 470}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:02:46,442]\u001b[0m Trial 233 finished with value: 0.6830213971420315 and parameters: {'n_estimators': 846, 'eta': 0.047085630478651254, 'max_depth': 8, 'alpha': 0.2381, 'lambda': 14.136571986886212, 'max_bin': 475}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:02:59,128]\u001b[0m Trial 234 finished with value: 0.6879255584582693 and parameters: {'n_estimators': 885, 'eta': 0.04364645465838857, 'max_depth': 8, 'alpha': 0.8893000000000001, 'lambda': 17.817310551562684, 'max_bin': 482}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:03:11,290]\u001b[0m Trial 235 finished with value: 0.6902865050369373 and parameters: {'n_estimators': 900, 'eta': 0.05177133990555801, 'max_depth': 8, 'alpha': 0.92, 'lambda': 15.603109291509769, 'max_bin': 335}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:03:24,334]\u001b[0m Trial 236 finished with value: 0.692154582890584 and parameters: {'n_estimators': 828, 'eta': 0.04664618387852817, 'max_depth': 8, 'alpha': 0.9686, 'lambda': 18.940984602603343, 'max_bin': 442}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:03:34,863]\u001b[0m Trial 237 finished with value: 0.6866813538679725 and parameters: {'n_estimators': 862, 'eta': 0.049633823208955555, 'max_depth': 7, 'alpha': 0.0519, 'lambda': 17.1667983867679, 'max_bin': 464}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:03:45,831]\u001b[0m Trial 238 finished with value: 0.6916492764078087 and parameters: {'n_estimators': 818, 'eta': 0.053663056833981614, 'max_depth': 8, 'alpha': 0.9415, 'lambda': 21.488102271897297, 'max_bin': 450}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:03:57,792]\u001b[0m Trial 239 finished with value: 0.6889823531139676 and parameters: {'n_estimators': 846, 'eta': 0.04501122166107063, 'max_depth': 8, 'alpha': 0.9041, 'lambda': 13.15621645085097, 'max_bin': 457}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:04:10,899]\u001b[0m Trial 240 finished with value: 0.6884632251248746 and parameters: {'n_estimators': 877, 'eta': 0.04045772407626436, 'max_depth': 8, 'alpha': 0.867, 'lambda': 14.454328530820398, 'max_bin': 471}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:04:23,091]\u001b[0m Trial 241 finished with value: 0.6891636292987011 and parameters: {'n_estimators': 866, 'eta': 0.0453376641406279, 'max_depth': 8, 'alpha': 0.879, 'lambda': 17.7545409698802, 'max_bin': 478}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:04:35,226]\u001b[0m Trial 242 finished with value: 0.6892038171971471 and parameters: {'n_estimators': 854, 'eta': 0.04759922895441776, 'max_depth': 8, 'alpha': 0.8343, 'lambda': 16.155261338124827, 'max_bin': 480}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:04:47,628]\u001b[0m Trial 243 finished with value: 0.6900227559874654 and parameters: {'n_estimators': 833, 'eta': 0.04890793723357047, 'max_depth': 8, 'alpha': 0.8589, 'lambda': 19.69900548273146, 'max_bin': 492}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:05:00,342]\u001b[0m Trial 244 finished with value: 0.6918208225989045 and parameters: {'n_estimators': 880, 'eta': 0.05096845738163929, 'max_depth': 8, 'alpha': 0.8862, 'lambda': 17.588353585244235, 'max_bin': 394}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:05:14,261]\u001b[0m Trial 245 finished with value: 0.6901812236268203 and parameters: {'n_estimators': 813, 'eta': 0.04327093781371448, 'max_depth': 8, 'alpha': 0.9109, 'lambda': 20.710988707803065, 'max_bin': 483}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:05:26,195]\u001b[0m Trial 246 finished with value: 0.6903919955108935 and parameters: {'n_estimators': 850, 'eta': 0.04586728743396803, 'max_depth': 8, 'alpha': 0.9308000000000001, 'lambda': 16.753170896961887, 'max_bin': 474}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:05:37,995]\u001b[0m Trial 247 finished with value: 0.689930169201145 and parameters: {'n_estimators': 869, 'eta': 0.048172359158291204, 'max_depth': 8, 'alpha': 0.8539, 'lambda': 18.663378032274668, 'max_bin': 468}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:05:51,734]\u001b[0m Trial 248 finished with value: 0.6907218927124518 and parameters: {'n_estimators': 831, 'eta': 0.04177254964408781, 'max_depth': 8, 'alpha': 0.8190000000000001, 'lambda': 19.929939743356076, 'max_bin': 444}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:06:03,163]\u001b[0m Trial 249 finished with value: 0.6868144428873956 and parameters: {'n_estimators': 888, 'eta': 0.05252024486318019, 'max_depth': 8, 'alpha': 0.9567, 'lambda': 14.96726139446316, 'max_bin': 462}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6973\n",
      "\tBest params:\n",
      "\t\tn_estimators: 869\n",
      "\t\teta: 0.04617477429928047\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.8785000000000001\n",
      "\t\tlambda: 17.514476657462126\n",
      "\t\tmax_bin: 479\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.684712    0.733307    0.708160    0.688529   \n",
      "1                    TP  314.000000  332.000000  320.000000  321.000000   \n",
      "2                    TN  184.000000  177.000000  187.000000  180.000000   \n",
      "3                    FP   53.000000   41.000000   50.000000   51.000000   \n",
      "4                    FN   44.000000   45.000000   38.000000   43.000000   \n",
      "5              Accuracy    0.836975    0.855462    0.852101    0.842017   \n",
      "6             Precision    0.855586    0.890080    0.864865    0.862903   \n",
      "7           Sensitivity    0.877095    0.880637    0.893855    0.881868   \n",
      "8           Specificity    0.776400    0.811900    0.789000    0.779200   \n",
      "9              F1 score    0.866207    0.885333    0.879121    0.872283   \n",
      "10  F1 score (weighted)    0.836409    0.855734    0.851399    0.841483   \n",
      "11     F1 score (macro)    0.828802    0.844939    0.844322    0.832617   \n",
      "12    Balanced Accuracy    0.826733    0.846282    0.841442    0.830544   \n",
      "13                  MCC    0.658019    0.689966    0.689399    0.665568   \n",
      "14                  NPV    0.807000    0.797300    0.831100    0.807200   \n",
      "15              ROC_AUC    0.826733    0.846282    0.841442    0.830544   \n",
      "\n",
      "          Set4  \n",
      "0     0.708776  \n",
      "1   326.000000  \n",
      "2   182.000000  \n",
      "3    48.000000  \n",
      "4    39.000000  \n",
      "5     0.853782  \n",
      "6     0.871658  \n",
      "7     0.893151  \n",
      "8     0.791300  \n",
      "9     0.882273  \n",
      "10    0.853213  \n",
      "11    0.844684  \n",
      "12    0.842228  \n",
      "13    0.689800  \n",
      "14    0.823500  \n",
      "15    0.842228  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_xgb_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_4_cat = np.where((y_pred_xgb_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c6c1fb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_xgb_4_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:06:17,717]\u001b[0m Trial 250 finished with value: 0.6761223140913089 and parameters: {'n_estimators': 789, 'eta': 0.05840916367374552, 'max_depth': 8, 'alpha': 0.8934000000000001, 'lambda': 21.315734101397926, 'max_bin': 487}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:06:32,770]\u001b[0m Trial 251 finished with value: 0.6744999439515248 and parameters: {'n_estimators': 857, 'eta': 0.04433218656038, 'max_depth': 8, 'alpha': 0.8716, 'lambda': 19.411763082839954, 'max_bin': 454}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:06:40,497]\u001b[0m Trial 252 finished with value: -2.337052722352459 and parameters: {'n_estimators': 838, 'eta': 0.0015473160827854174, 'max_depth': 8, 'alpha': 0.9168000000000001, 'lambda': 22.95503508609185, 'max_bin': 479}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:06:54,175]\u001b[0m Trial 253 finished with value: 0.6743388992566163 and parameters: {'n_estimators': 802, 'eta': 0.04658519442172134, 'max_depth': 8, 'alpha': 0.9441, 'lambda': 18.319810460921456, 'max_bin': 438}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:07:07,886]\u001b[0m Trial 254 finished with value: 0.6764983535079742 and parameters: {'n_estimators': 873, 'eta': 0.0493075146987232, 'max_depth': 8, 'alpha': 0.8452000000000001, 'lambda': 21.881331292970458, 'max_bin': 448}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:07:20,953]\u001b[0m Trial 255 finished with value: 0.675366387809242 and parameters: {'n_estimators': 899, 'eta': 0.05013620419492748, 'max_depth': 8, 'alpha': 0.8995000000000001, 'lambda': 20.444341803260777, 'max_bin': 495}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:07:34,148]\u001b[0m Trial 256 finished with value: 0.6751404020547203 and parameters: {'n_estimators': 768, 'eta': 0.05427494632731594, 'max_depth': 8, 'alpha': 0.9809, 'lambda': 15.973123062813444, 'max_bin': 466}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:07:50,806]\u001b[0m Trial 257 finished with value: 0.6786646938736874 and parameters: {'n_estimators': 821, 'eta': 0.03990370008756007, 'max_depth': 8, 'alpha': 0.9285, 'lambda': 23.618335456025815, 'max_bin': 457}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:08:01,793]\u001b[0m Trial 258 finished with value: 0.6734803983649333 and parameters: {'n_estimators': 857, 'eta': 0.061685175394900385, 'max_depth': 8, 'alpha': 0.8774000000000001, 'lambda': 17.424208309557827, 'max_bin': 473}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:08:17,588]\u001b[0m Trial 259 finished with value: 0.6747097393464545 and parameters: {'n_estimators': 882, 'eta': 0.03756621756713812, 'max_depth': 8, 'alpha': 0.8103, 'lambda': 19.354759915269778, 'max_bin': 451}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:08:32,335]\u001b[0m Trial 260 finished with value: 0.677924019478351 and parameters: {'n_estimators': 838, 'eta': 0.04549446495036139, 'max_depth': 7, 'alpha': 0.8417, 'lambda': 20.79501805623977, 'max_bin': 476}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:08:46,717]\u001b[0m Trial 261 finished with value: 0.6760352128344167 and parameters: {'n_estimators': 812, 'eta': 0.0429317280829207, 'max_depth': 8, 'alpha': 0.9536, 'lambda': 21.9041858473992, 'max_bin': 489}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:09:00,957]\u001b[0m Trial 262 finished with value: 0.6778827609212834 and parameters: {'n_estimators': 868, 'eta': 0.05608254824663374, 'max_depth': 9, 'alpha': 0.9158000000000001, 'lambda': 18.61633329846047, 'max_bin': 461}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:09:15,268]\u001b[0m Trial 263 finished with value: 0.6745869697892954 and parameters: {'n_estimators': 845, 'eta': 0.04781012179594998, 'max_depth': 7, 'alpha': 0.8938, 'lambda': 22.771545176465683, 'max_bin': 434}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:09:27,375]\u001b[0m Trial 264 finished with value: 0.6775155095483795 and parameters: {'n_estimators': 888, 'eta': 0.051714947862862314, 'max_depth': 8, 'alpha': 0.8677, 'lambda': 16.707884976959896, 'max_bin': 482}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:09:41,339]\u001b[0m Trial 265 finished with value: 0.677265850930947 and parameters: {'n_estimators': 829, 'eta': 0.050286489057044616, 'max_depth': 8, 'alpha': 0.8257, 'lambda': 20.537323905988284, 'max_bin': 468}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:09:55,853]\u001b[0m Trial 266 finished with value: 0.6771066174109391 and parameters: {'n_estimators': 799, 'eta': 0.046176177185566464, 'max_depth': 8, 'alpha': 0.9372, 'lambda': 21.2711564719769, 'max_bin': 443}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:10:08,433]\u001b[0m Trial 267 finished with value: 0.6751252372415548 and parameters: {'n_estimators': 857, 'eta': 0.04371791182307698, 'max_depth': 6, 'alpha': 0.9047000000000001, 'lambda': 20.042429151085646, 'max_bin': 453}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:10:21,335]\u001b[0m Trial 268 finished with value: 0.6773165791920872 and parameters: {'n_estimators': 820, 'eta': 0.05312608320605416, 'max_depth': 8, 'alpha': 0.9623, 'lambda': 19.14889420248407, 'max_bin': 462}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:10:34,537]\u001b[0m Trial 269 finished with value: 0.6771473711615363 and parameters: {'n_estimators': 871, 'eta': 0.048590851599182944, 'max_depth': 8, 'alpha': 0.9876, 'lambda': 15.355873999590768, 'max_bin': 446}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:10:49,158]\u001b[0m Trial 270 finished with value: 0.6752580796492882 and parameters: {'n_estimators': 845, 'eta': 0.041248952042147, 'max_depth': 8, 'alpha': 0.8793000000000001, 'lambda': 18.076876099876323, 'max_bin': 456}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:11:02,301]\u001b[0m Trial 271 finished with value: 0.6768292539497163 and parameters: {'n_estimators': 900, 'eta': 0.04713881849668851, 'max_depth': 8, 'alpha': 0.9233, 'lambda': 22.14367634218086, 'max_bin': 497}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:11:15,614]\u001b[0m Trial 272 finished with value: 0.6754697123091652 and parameters: {'n_estimators': 775, 'eta': 0.05071234376754562, 'max_depth': 8, 'alpha': 0.8432000000000001, 'lambda': 16.935433411459023, 'max_bin': 470}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:11:28,522]\u001b[0m Trial 273 finished with value: 0.6790806974002231 and parameters: {'n_estimators': 861, 'eta': 0.05490479315747933, 'max_depth': 7, 'alpha': 0.9367000000000001, 'lambda': 24.460119311442746, 'max_bin': 477}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:11:44,771]\u001b[0m Trial 274 finished with value: 0.6766072424513652 and parameters: {'n_estimators': 827, 'eta': 0.03389022179631385, 'max_depth': 8, 'alpha': 0.8968, 'lambda': 14.42996755814739, 'max_bin': 488}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:12:01,106]\u001b[0m Trial 275 finished with value: 0.6739153533408596 and parameters: {'n_estimators': 805, 'eta': 0.044547574989764305, 'max_depth': 9, 'alpha': 0.8585, 'lambda': 19.773696365668226, 'max_bin': 482}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:12:14,011]\u001b[0m Trial 276 finished with value: 0.6745961764727216 and parameters: {'n_estimators': 880, 'eta': 0.052073932529044624, 'max_depth': 8, 'alpha': 0.7971, 'lambda': 21.44862744758089, 'max_bin': 450}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:12:26,956]\u001b[0m Trial 277 finished with value: 0.6758537596173072 and parameters: {'n_estimators': 848, 'eta': 0.04834502147169086, 'max_depth': 8, 'alpha': 0.9133, 'lambda': 16.19969357626511, 'max_bin': 463}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:12:42,758]\u001b[0m Trial 278 finished with value: 0.6778698340509279 and parameters: {'n_estimators': 873, 'eta': 0.04617409685744521, 'max_depth': 8, 'alpha': 0.9633, 'lambda': 38.797085164691104, 'max_bin': 457}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:12:58,624]\u001b[0m Trial 279 finished with value: 0.6785395317466997 and parameters: {'n_estimators': 788, 'eta': 0.04202893516383133, 'max_depth': 8, 'alpha': 0.8804000000000001, 'lambda': 20.61468656613419, 'max_bin': 439}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:13:05,853]\u001b[0m Trial 280 finished with value: 0.6710019006685595 and parameters: {'n_estimators': 840, 'eta': 0.09926527611776612, 'max_depth': 7, 'alpha': 0.9343, 'lambda': 22.8902246601114, 'max_bin': 405}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:13:21,058]\u001b[0m Trial 281 finished with value: 0.6773835962319523 and parameters: {'n_estimators': 818, 'eta': 0.039727209896350764, 'max_depth': 8, 'alpha': 0.8273, 'lambda': 17.6747119266396, 'max_bin': 473}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:13:34,871]\u001b[0m Trial 282 finished with value: 0.6741118894922876 and parameters: {'n_estimators': 866, 'eta': 0.04378352048097352, 'max_depth': 8, 'alpha': 0.8998, 'lambda': 12.066803730181281, 'max_bin': 467}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:13:49,991]\u001b[0m Trial 283 finished with value: 0.6756426169276477 and parameters: {'n_estimators': 886, 'eta': 0.05021925665933307, 'max_depth': 8, 'alpha': 0.8675, 'lambda': 18.746993529637148, 'max_bin': 484}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:14:07,985]\u001b[0m Trial 284 finished with value: 0.6769239432885337 and parameters: {'n_estimators': 852, 'eta': 0.03591397289570714, 'max_depth': 8, 'alpha': 0.9233, 'lambda': 23.736983971471403, 'max_bin': 491}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:14:17,518]\u001b[0m Trial 285 finished with value: 0.6722031663617145 and parameters: {'n_estimators': 833, 'eta': 0.057384487495751736, 'max_depth': 8, 'alpha': 0.7529, 'lambda': 2.9701739764475796, 'max_bin': 459}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:14:31,308]\u001b[0m Trial 286 finished with value: 0.6764646877495915 and parameters: {'n_estimators': 806, 'eta': 0.05291061528160814, 'max_depth': 8, 'alpha': 0.9623, 'lambda': 19.97817112360666, 'max_bin': 286}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:14:45,764]\u001b[0m Trial 287 finished with value: 0.676772220427376 and parameters: {'n_estimators': 858, 'eta': 0.0474591524402709, 'max_depth': 8, 'alpha': 0.8521000000000001, 'lambda': 21.25044996198893, 'max_bin': 446}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:14:59,312]\u001b[0m Trial 288 finished with value: 0.6752543411839899 and parameters: {'n_estimators': 887, 'eta': 0.045556657259975875, 'max_depth': 8, 'alpha': 0.8855000000000001, 'lambda': 22.465208825190714, 'max_bin': 477}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:15:15,133]\u001b[0m Trial 289 finished with value: 0.6737001028307671 and parameters: {'n_estimators': 832, 'eta': 0.029991108353722765, 'max_depth': 8, 'alpha': 0.9409000000000001, 'lambda': 15.152558993565236, 'max_bin': 451}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:15:27,439]\u001b[0m Trial 290 finished with value: 0.6769068037374023 and parameters: {'n_estimators': 900, 'eta': 0.05432027548050643, 'max_depth': 8, 'alpha': 0.9089, 'lambda': 19.33970227795184, 'max_bin': 425}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:15:41,039]\u001b[0m Trial 291 finished with value: 0.6746346733278477 and parameters: {'n_estimators': 873, 'eta': 0.049402862697880454, 'max_depth': 9, 'alpha': 0.9966, 'lambda': 18.112916788141135, 'max_bin': 465}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:15:42,500]\u001b[0m Trial 292 finished with value: 0.4896026131995327 and parameters: {'n_estimators': 74, 'eta': 0.0435574821973055, 'max_depth': 8, 'alpha': 0.8194, 'lambda': 13.728105156609542, 'max_bin': 472}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:15:55,616]\u001b[0m Trial 293 finished with value: 0.675672866824776 and parameters: {'n_estimators': 743, 'eta': 0.0506964104931513, 'max_depth': 8, 'alpha': 0.9491, 'lambda': 21.87206901261302, 'max_bin': 384}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:16:11,619]\u001b[0m Trial 294 finished with value: 0.6792321414557545 and parameters: {'n_estimators': 786, 'eta': 0.03891424936254038, 'max_depth': 8, 'alpha': 0.8713000000000001, 'lambda': 20.53220197617481, 'max_bin': 391}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:16:21,110]\u001b[0m Trial 295 finished with value: 0.672353958432459 and parameters: {'n_estimators': 439, 'eta': 0.0589953432717176, 'max_depth': 8, 'alpha': 0.9032, 'lambda': 16.00169273638219, 'max_bin': 496}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:16:33,377]\u001b[0m Trial 296 finished with value: 0.674174263860457 and parameters: {'n_estimators': 848, 'eta': 0.04778366101488786, 'max_depth': 6, 'alpha': 0.7879, 'lambda': 17.320098175384352, 'max_bin': 453}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:16:47,376]\u001b[0m Trial 297 finished with value: 0.6786570445913702 and parameters: {'n_estimators': 819, 'eta': 0.04574909924158269, 'max_depth': 7, 'alpha': 0.9721000000000001, 'lambda': 21.006155063545883, 'max_bin': 479}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:17:03,156]\u001b[0m Trial 298 finished with value: 0.6776248324918075 and parameters: {'n_estimators': 864, 'eta': 0.041561672408899436, 'max_depth': 8, 'alpha': 0.8486, 'lambda': 18.86316310086131, 'max_bin': 442}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:17:18,244]\u001b[0m Trial 299 finished with value: 0.6725195358146911 and parameters: {'n_estimators': 835, 'eta': 0.05550789512693882, 'max_depth': 11, 'alpha': 0.9224, 'lambda': 20.028872619695655, 'max_bin': 460}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6973\n",
      "\tBest params:\n",
      "\t\tn_estimators: 869\n",
      "\t\teta: 0.04617477429928047\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.8785000000000001\n",
      "\t\tlambda: 17.514476657462126\n",
      "\t\tmax_bin: 479\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.684712    0.733307    0.708160    0.688529   \n",
      "1                    TP  314.000000  332.000000  320.000000  321.000000   \n",
      "2                    TN  184.000000  177.000000  187.000000  180.000000   \n",
      "3                    FP   53.000000   41.000000   50.000000   51.000000   \n",
      "4                    FN   44.000000   45.000000   38.000000   43.000000   \n",
      "5              Accuracy    0.836975    0.855462    0.852101    0.842017   \n",
      "6             Precision    0.855586    0.890080    0.864865    0.862903   \n",
      "7           Sensitivity    0.877095    0.880637    0.893855    0.881868   \n",
      "8           Specificity    0.776400    0.811900    0.789000    0.779200   \n",
      "9              F1 score    0.866207    0.885333    0.879121    0.872283   \n",
      "10  F1 score (weighted)    0.836409    0.855734    0.851399    0.841483   \n",
      "11     F1 score (macro)    0.828802    0.844939    0.844322    0.832617   \n",
      "12    Balanced Accuracy    0.826733    0.846282    0.841442    0.830544   \n",
      "13                  MCC    0.658019    0.689966    0.689399    0.665568   \n",
      "14                  NPV    0.807000    0.797300    0.831100    0.807200   \n",
      "15              ROC_AUC    0.826733    0.846282    0.841442    0.830544   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.708776    0.730538  \n",
      "1   326.000000  341.000000  \n",
      "2   182.000000  173.000000  \n",
      "3    48.000000   49.000000  \n",
      "4    39.000000   32.000000  \n",
      "5     0.853782    0.863866  \n",
      "6     0.871658    0.874359  \n",
      "7     0.893151    0.914209  \n",
      "8     0.791300    0.779300  \n",
      "9     0.882273    0.893840  \n",
      "10    0.853213    0.862672  \n",
      "11    0.844684    0.852072  \n",
      "12    0.842228    0.846744  \n",
      "13    0.689800    0.705766  \n",
      "14    0.823500    0.843900  \n",
      "15    0.842228    0.846744  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_xgb_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "\n",
    "y_pred_xgb_5_cat = np.where((y_pred_xgb_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:17:33,509]\u001b[0m Trial 300 finished with value: 0.6964480494524439 and parameters: {'n_estimators': 801, 'eta': 0.052056845083922804, 'max_depth': 7, 'alpha': 0.1213, 'lambda': 23.14309738324572, 'max_bin': 431}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:17:47,185]\u001b[0m Trial 301 finished with value: 0.6948495367922202 and parameters: {'n_estimators': 798, 'eta': 0.05275569809452932, 'max_depth': 7, 'alpha': 0.3356, 'lambda': 24.192918764322016, 'max_bin': 436}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:17:59,750]\u001b[0m Trial 302 finished with value: 0.6956275577684201 and parameters: {'n_estimators': 776, 'eta': 0.05279109907807509, 'max_depth': 7, 'alpha': 0.2265, 'lambda': 23.40602450058991, 'max_bin': 431}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:18:11,053]\u001b[0m Trial 303 finished with value: 0.6908256078278546 and parameters: {'n_estimators': 794, 'eta': 0.05690349495846777, 'max_depth': 7, 'alpha': 0.1087, 'lambda': 24.89946482137729, 'max_bin': 429}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:18:23,464]\u001b[0m Trial 304 finished with value: 0.6922430414848512 and parameters: {'n_estimators': 759, 'eta': 0.05360293117078617, 'max_depth': 7, 'alpha': 0.1986, 'lambda': 25.611713020083798, 'max_bin': 433}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:18:37,072]\u001b[0m Trial 305 finished with value: 0.6944543362909232 and parameters: {'n_estimators': 772, 'eta': 0.05256436207049006, 'max_depth': 7, 'alpha': 0.36100000000000004, 'lambda': 23.574167562533123, 'max_bin': 437}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:18:49,488]\u001b[0m Trial 306 finished with value: 0.6918124126667321 and parameters: {'n_estimators': 770, 'eta': 0.054570983445074724, 'max_depth': 7, 'alpha': 0.3201, 'lambda': 23.78667015009676, 'max_bin': 437}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:19:01,515]\u001b[0m Trial 307 finished with value: 0.6953025856609941 and parameters: {'n_estimators': 727, 'eta': 0.05219905085263917, 'max_depth': 7, 'alpha': 0.16790000000000002, 'lambda': 22.809378255557228, 'max_bin': 429}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:19:13,568]\u001b[0m Trial 308 finished with value: 0.6929894384627857 and parameters: {'n_estimators': 760, 'eta': 0.05206817762709623, 'max_depth': 7, 'alpha': 0.1519, 'lambda': 23.553854254649366, 'max_bin': 428}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:19:24,515]\u001b[0m Trial 309 finished with value: 0.6910325434978579 and parameters: {'n_estimators': 775, 'eta': 0.06429185749152488, 'max_depth': 7, 'alpha': 0.1121, 'lambda': 23.14413609338225, 'max_bin': 430}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:19:36,550]\u001b[0m Trial 310 finished with value: 0.6937094721286405 and parameters: {'n_estimators': 732, 'eta': 0.05262841912371752, 'max_depth': 7, 'alpha': 0.1976, 'lambda': 24.038489811387677, 'max_bin': 421}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:19:50,123]\u001b[0m Trial 311 finished with value: 0.6955968194192261 and parameters: {'n_estimators': 722, 'eta': 0.052493135402044415, 'max_depth': 7, 'alpha': 0.1963, 'lambda': 26.156343195541282, 'max_bin': 420}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:20:02,710]\u001b[0m Trial 312 finished with value: 0.693909498117148 and parameters: {'n_estimators': 690, 'eta': 0.05248739650928884, 'max_depth': 7, 'alpha': 0.2272, 'lambda': 24.392123905803963, 'max_bin': 417}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:20:15,468]\u001b[0m Trial 313 finished with value: 0.697035500117958 and parameters: {'n_estimators': 712, 'eta': 0.05306311272857246, 'max_depth': 7, 'alpha': 0.1778, 'lambda': 26.32687730341202, 'max_bin': 420}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:20:27,643]\u001b[0m Trial 314 finished with value: 0.6946482872308578 and parameters: {'n_estimators': 714, 'eta': 0.05302753277373547, 'max_depth': 7, 'alpha': 0.2466, 'lambda': 26.375597833420734, 'max_bin': 416}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:20:39,845]\u001b[0m Trial 315 finished with value: 0.6939156565500527 and parameters: {'n_estimators': 702, 'eta': 0.055745178070390485, 'max_depth': 7, 'alpha': 0.1951, 'lambda': 26.586215731608686, 'max_bin': 415}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:20:51,607]\u001b[0m Trial 316 finished with value: 0.6935816565846514 and parameters: {'n_estimators': 689, 'eta': 0.05679882964205835, 'max_depth': 7, 'alpha': 0.2474, 'lambda': 27.031756793395417, 'max_bin': 417}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:21:04,127]\u001b[0m Trial 317 finished with value: 0.694263412154071 and parameters: {'n_estimators': 741, 'eta': 0.05581541056218209, 'max_depth': 7, 'alpha': 0.18130000000000002, 'lambda': 25.904312553320825, 'max_bin': 414}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:21:17,352]\u001b[0m Trial 318 finished with value: 0.6943112188995058 and parameters: {'n_estimators': 733, 'eta': 0.055846163727646475, 'max_depth': 7, 'alpha': 0.1741, 'lambda': 26.478193610411836, 'max_bin': 412}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:21:28,926]\u001b[0m Trial 319 finished with value: 0.6942933109906599 and parameters: {'n_estimators': 707, 'eta': 0.05841881062569709, 'max_depth': 7, 'alpha': 0.18530000000000002, 'lambda': 26.54502229082238, 'max_bin': 422}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:21:41,687]\u001b[0m Trial 320 finished with value: 0.6940103752479669 and parameters: {'n_estimators': 714, 'eta': 0.06066227340306817, 'max_depth': 7, 'alpha': 0.1709, 'lambda': 25.942094740045583, 'max_bin': 424}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:21:54,394]\u001b[0m Trial 321 finished with value: 0.6938246494084568 and parameters: {'n_estimators': 749, 'eta': 0.059518925941785136, 'max_depth': 7, 'alpha': 0.17270000000000002, 'lambda': 27.729898972011075, 'max_bin': 408}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:22:07,239]\u001b[0m Trial 322 finished with value: 0.6944724094267787 and parameters: {'n_estimators': 723, 'eta': 0.05854405177269084, 'max_depth': 7, 'alpha': 0.128, 'lambda': 26.18272799971176, 'max_bin': 421}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:22:18,171]\u001b[0m Trial 323 finished with value: 0.6911146800980423 and parameters: {'n_estimators': 664, 'eta': 0.06274661183568779, 'max_depth': 7, 'alpha': 0.1358, 'lambda': 26.43686823016373, 'max_bin': 420}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:22:30,282]\u001b[0m Trial 324 finished with value: 0.694885917336886 and parameters: {'n_estimators': 720, 'eta': 0.05801840598010058, 'max_depth': 7, 'alpha': 0.138, 'lambda': 25.495026289001345, 'max_bin': 422}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:22:42,381]\u001b[0m Trial 325 finished with value: 0.6940896038093961 and parameters: {'n_estimators': 720, 'eta': 0.05823672734964888, 'max_depth': 7, 'alpha': 0.37310000000000004, 'lambda': 25.35151309694288, 'max_bin': 425}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:22:55,584]\u001b[0m Trial 326 finished with value: 0.6967051448870489 and parameters: {'n_estimators': 728, 'eta': 0.05678268502135424, 'max_depth': 7, 'alpha': 0.273, 'lambda': 26.456168113031698, 'max_bin': 423}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:23:07,921]\u001b[0m Trial 327 finished with value: 0.6963194975609694 and parameters: {'n_estimators': 713, 'eta': 0.060151289572808006, 'max_depth': 7, 'alpha': 0.1386, 'lambda': 27.141845782059516, 'max_bin': 419}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:23:20,340]\u001b[0m Trial 328 finished with value: 0.6965370245564813 and parameters: {'n_estimators': 733, 'eta': 0.060484386333963254, 'max_depth': 7, 'alpha': 0.1297, 'lambda': 27.40341944759458, 'max_bin': 413}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:23:32,799]\u001b[0m Trial 329 finished with value: 0.6945773580119723 and parameters: {'n_estimators': 727, 'eta': 0.06137791519251653, 'max_depth': 7, 'alpha': 0.28090000000000004, 'lambda': 28.137617954591057, 'max_bin': 412}. Best is trial 124 with value: 0.6972549107298986.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:23:44,908]\u001b[0m Trial 330 finished with value: 0.6973610577578825 and parameters: {'n_estimators': 676, 'eta': 0.0664486518754298, 'max_depth': 7, 'alpha': 0.26890000000000003, 'lambda': 28.387879494762064, 'max_bin': 426}. Best is trial 330 with value: 0.6973610577578825.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:23:55,739]\u001b[0m Trial 331 finished with value: 0.6932182279502431 and parameters: {'n_estimators': 680, 'eta': 0.06636454251132855, 'max_depth': 7, 'alpha': 0.2778, 'lambda': 27.447084953271734, 'max_bin': 422}. Best is trial 330 with value: 0.6973610577578825.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:24:08,404]\u001b[0m Trial 332 finished with value: 0.6947732699080666 and parameters: {'n_estimators': 724, 'eta': 0.06132831345990185, 'max_depth': 7, 'alpha': 0.2602, 'lambda': 28.59371188971536, 'max_bin': 418}. Best is trial 330 with value: 0.6973610577578825.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:24:19,251]\u001b[0m Trial 333 finished with value: 0.6953135410066109 and parameters: {'n_estimators': 724, 'eta': 0.06804594844464028, 'max_depth': 7, 'alpha': 0.2933, 'lambda': 28.901252391299384, 'max_bin': 411}. Best is trial 330 with value: 0.6973610577578825.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:24:30,497]\u001b[0m Trial 334 finished with value: 0.6937089439567271 and parameters: {'n_estimators': 714, 'eta': 0.07112313027612199, 'max_depth': 7, 'alpha': 0.267, 'lambda': 27.866901500140703, 'max_bin': 410}. Best is trial 330 with value: 0.6973610577578825.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:24:41,749]\u001b[0m Trial 335 finished with value: 0.6960777492509911 and parameters: {'n_estimators': 699, 'eta': 0.06912771491165919, 'max_depth': 7, 'alpha': 0.3002, 'lambda': 29.274833340418443, 'max_bin': 414}. Best is trial 330 with value: 0.6973610577578825.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:24:53,634]\u001b[0m Trial 336 finished with value: 0.6964880991637941 and parameters: {'n_estimators': 703, 'eta': 0.06926502632963441, 'max_depth': 7, 'alpha': 0.3181, 'lambda': 29.150147363630882, 'max_bin': 403}. Best is trial 330 with value: 0.6973610577578825.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:25:04,909]\u001b[0m Trial 337 finished with value: 0.6973151399330969 and parameters: {'n_estimators': 698, 'eta': 0.07633775906601588, 'max_depth': 7, 'alpha': 0.3116, 'lambda': 28.753891229272455, 'max_bin': 400}. Best is trial 330 with value: 0.6973610577578825.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:25:16,363]\u001b[0m Trial 338 finished with value: 0.69464121365832 and parameters: {'n_estimators': 701, 'eta': 0.06867996665813733, 'max_depth': 7, 'alpha': 0.3052, 'lambda': 29.206493053391863, 'max_bin': 406}. Best is trial 330 with value: 0.6973610577578825.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:25:27,042]\u001b[0m Trial 339 finished with value: 0.6921911765130929 and parameters: {'n_estimators': 682, 'eta': 0.0687819225609188, 'max_depth': 7, 'alpha': 0.0879, 'lambda': 28.613159023822835, 'max_bin': 405}. Best is trial 330 with value: 0.6973610577578825.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:25:38,593]\u001b[0m Trial 340 finished with value: 0.6984448906190349 and parameters: {'n_estimators': 696, 'eta': 0.07443151480610584, 'max_depth': 7, 'alpha': 0.3135, 'lambda': 29.355874927964816, 'max_bin': 426}. Best is trial 340 with value: 0.6984448906190349.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:25:49,409]\u001b[0m Trial 341 finished with value: 0.6987044649927325 and parameters: {'n_estimators': 664, 'eta': 0.07661568123004531, 'max_depth': 7, 'alpha': 0.33590000000000003, 'lambda': 30.08089368243512, 'max_bin': 398}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:26:00,711]\u001b[0m Trial 342 finished with value: 0.6985935066855736 and parameters: {'n_estimators': 651, 'eta': 0.07397250014547484, 'max_depth': 7, 'alpha': 0.2917, 'lambda': 30.304136905453294, 'max_bin': 398}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:26:11,276]\u001b[0m Trial 343 finished with value: 0.6966043058255524 and parameters: {'n_estimators': 649, 'eta': 0.07334143353170902, 'max_depth': 7, 'alpha': 0.2828, 'lambda': 30.22016810760313, 'max_bin': 398}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:26:22,048]\u001b[0m Trial 344 finished with value: 0.6953814154276202 and parameters: {'n_estimators': 638, 'eta': 0.07460041465126128, 'max_depth': 7, 'alpha': 0.30670000000000003, 'lambda': 30.165139631979287, 'max_bin': 402}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:26:32,794]\u001b[0m Trial 345 finished with value: 0.6956874235795926 and parameters: {'n_estimators': 657, 'eta': 0.07521365564549935, 'max_depth': 7, 'alpha': 0.3168, 'lambda': 30.64519886127999, 'max_bin': 400}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:26:43,153]\u001b[0m Trial 346 finished with value: 0.696592460244917 and parameters: {'n_estimators': 636, 'eta': 0.077736438737285, 'max_depth': 7, 'alpha': 0.3088, 'lambda': 30.05287184909136, 'max_bin': 401}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:26:52,905]\u001b[0m Trial 347 finished with value: 0.6948677008980841 and parameters: {'n_estimators': 652, 'eta': 0.07783509921278815, 'max_depth': 6, 'alpha': 0.3183, 'lambda': 30.556233285964623, 'max_bin': 399}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:27:03,417]\u001b[0m Trial 348 finished with value: 0.6948482448963296 and parameters: {'n_estimators': 629, 'eta': 0.07650487696231878, 'max_depth': 7, 'alpha': 0.3507, 'lambda': 31.159216541463742, 'max_bin': 396}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:27:13,624]\u001b[0m Trial 349 finished with value: 0.695907440069561 and parameters: {'n_estimators': 684, 'eta': 0.08052675240742624, 'max_depth': 7, 'alpha': 0.29710000000000003, 'lambda': 29.88488663233992, 'max_bin': 399}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6987\n",
      "\tBest params:\n",
      "\t\tn_estimators: 664\n",
      "\t\teta: 0.07661568123004531\n",
      "\t\tmax_depth: 7\n",
      "\t\talpha: 0.33590000000000003\n",
      "\t\tlambda: 30.08089368243512\n",
      "\t\tmax_bin: 398\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.684712    0.733307    0.708160    0.688529   \n",
      "1                    TP  314.000000  332.000000  320.000000  321.000000   \n",
      "2                    TN  184.000000  177.000000  187.000000  180.000000   \n",
      "3                    FP   53.000000   41.000000   50.000000   51.000000   \n",
      "4                    FN   44.000000   45.000000   38.000000   43.000000   \n",
      "5              Accuracy    0.836975    0.855462    0.852101    0.842017   \n",
      "6             Precision    0.855586    0.890080    0.864865    0.862903   \n",
      "7           Sensitivity    0.877095    0.880637    0.893855    0.881868   \n",
      "8           Specificity    0.776400    0.811900    0.789000    0.779200   \n",
      "9              F1 score    0.866207    0.885333    0.879121    0.872283   \n",
      "10  F1 score (weighted)    0.836409    0.855734    0.851399    0.841483   \n",
      "11     F1 score (macro)    0.828802    0.844939    0.844322    0.832617   \n",
      "12    Balanced Accuracy    0.826733    0.846282    0.841442    0.830544   \n",
      "13                  MCC    0.658019    0.689966    0.689399    0.665568   \n",
      "14                  NPV    0.807000    0.797300    0.831100    0.807200   \n",
      "15              ROC_AUC    0.826733    0.846282    0.841442    0.830544   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.708776    0.730538    0.699183  \n",
      "1   326.000000  341.000000  325.000000  \n",
      "2   182.000000  173.000000  176.000000  \n",
      "3    48.000000   49.000000   56.000000  \n",
      "4    39.000000   32.000000   38.000000  \n",
      "5     0.853782    0.863866    0.842017  \n",
      "6     0.871658    0.874359    0.853018  \n",
      "7     0.893151    0.914209    0.895317  \n",
      "8     0.791300    0.779300    0.758600  \n",
      "9     0.882273    0.893840    0.873656  \n",
      "10    0.853213    0.862672    0.840740  \n",
      "11    0.844684    0.852072    0.831447  \n",
      "12    0.842228    0.846744    0.826969  \n",
      "13    0.689800    0.705766    0.664606  \n",
      "14    0.823500    0.843900    0.822400  \n",
      "15    0.842228    0.846744    0.826969  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_xgb_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_6_cat = np.where((y_pred_xgb_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:27:24,162]\u001b[0m Trial 350 finished with value: 0.6903695909291098 and parameters: {'n_estimators': 667, 'eta': 0.08208362508524658, 'max_depth': 7, 'alpha': 0.2913, 'lambda': 29.60648877869309, 'max_bin': 400}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:27:34,297]\u001b[0m Trial 351 finished with value: 0.6915233015173616 and parameters: {'n_estimators': 644, 'eta': 0.07383249134660519, 'max_depth': 7, 'alpha': 0.3316, 'lambda': 29.8540758359935, 'max_bin': 392}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:27:44,536]\u001b[0m Trial 352 finished with value: 0.6941546278913797 and parameters: {'n_estimators': 670, 'eta': 0.07919523684775641, 'max_depth': 7, 'alpha': 0.2146, 'lambda': 31.021973230270554, 'max_bin': 402}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:27:54,984]\u001b[0m Trial 353 finished with value: 0.6932967834325426 and parameters: {'n_estimators': 653, 'eta': 0.07186351303057588, 'max_depth': 7, 'alpha': 0.3054, 'lambda': 30.140751978720626, 'max_bin': 397}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:28:05,084]\u001b[0m Trial 354 finished with value: 0.6917168974628074 and parameters: {'n_estimators': 614, 'eta': 0.0770170601771558, 'max_depth': 7, 'alpha': 0.26680000000000004, 'lambda': 29.61232001247743, 'max_bin': 406}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:28:14,759]\u001b[0m Trial 355 finished with value: 0.6928888568973077 and parameters: {'n_estimators': 682, 'eta': 0.0824961627352642, 'max_depth': 7, 'alpha': 0.32280000000000003, 'lambda': 28.63661955813632, 'max_bin': 403}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:28:24,404]\u001b[0m Trial 356 finished with value: 0.6926486844479354 and parameters: {'n_estimators': 694, 'eta': 0.07609763705696534, 'max_depth': 7, 'alpha': 0.2962, 'lambda': 31.8260255274885, 'max_bin': 392}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:28:34,402]\u001b[0m Trial 357 finished with value: 0.6918968333425473 and parameters: {'n_estimators': 668, 'eta': 0.07240938355859097, 'max_depth': 7, 'alpha': 0.28500000000000003, 'lambda': 29.061106367591236, 'max_bin': 388}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:28:43,853]\u001b[0m Trial 358 finished with value: 0.6928190187116767 and parameters: {'n_estimators': 697, 'eta': 0.07803731017599945, 'max_depth': 7, 'alpha': 0.3482, 'lambda': 29.48058749667762, 'max_bin': 408}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:28:52,373]\u001b[0m Trial 359 finished with value: 0.6871441512400305 and parameters: {'n_estimators': 675, 'eta': 0.07481609246845179, 'max_depth': 6, 'alpha': 0.2636, 'lambda': 31.20216570236499, 'max_bin': 399}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:29:01,831]\u001b[0m Trial 360 finished with value: 0.6886339610502048 and parameters: {'n_estimators': 587, 'eta': 0.07280195353903152, 'max_depth': 7, 'alpha': 0.2187, 'lambda': 30.572483314587963, 'max_bin': 410}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:29:10,540]\u001b[0m Trial 361 finished with value: 0.6906605510913824 and parameters: {'n_estimators': 647, 'eta': 0.08038776902449961, 'max_depth': 7, 'alpha': 0.31880000000000003, 'lambda': 27.48190384137036, 'max_bin': 427}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:29:20,631]\u001b[0m Trial 362 finished with value: 0.6921423334564513 and parameters: {'n_estimators': 685, 'eta': 0.07430150091842712, 'max_depth': 7, 'alpha': 0.0916, 'lambda': 28.281469141781674, 'max_bin': 416}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:29:31,655]\u001b[0m Trial 363 finished with value: 0.6910635070175266 and parameters: {'n_estimators': 698, 'eta': 0.0707108620997808, 'max_depth': 7, 'alpha': 0.28390000000000004, 'lambda': 32.50852750882278, 'max_bin': 402}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:29:41,652]\u001b[0m Trial 364 finished with value: 0.6933443899586815 and parameters: {'n_estimators': 659, 'eta': 0.07937972920931007, 'max_depth': 7, 'alpha': 0.3945, 'lambda': 30.36002520046202, 'max_bin': 395}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:29:51,689]\u001b[0m Trial 365 finished with value: 0.6924278222353599 and parameters: {'n_estimators': 631, 'eta': 0.07621844738730117, 'max_depth': 7, 'alpha': 0.34040000000000004, 'lambda': 29.184445633815283, 'max_bin': 431}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:30:03,172]\u001b[0m Trial 366 finished with value: 0.6920454045788681 and parameters: {'n_estimators': 700, 'eta': 0.06439594178262752, 'max_depth': 7, 'alpha': 0.308, 'lambda': 27.54244521129723, 'max_bin': 414}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:30:13,655]\u001b[0m Trial 367 finished with value: 0.6922990976958152 and parameters: {'n_estimators': 671, 'eta': 0.07291932566620095, 'max_depth': 7, 'alpha': 0.2516, 'lambda': 29.981792243220468, 'max_bin': 424}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:30:22,599]\u001b[0m Trial 368 finished with value: 0.6935321157913962 and parameters: {'n_estimators': 603, 'eta': 0.07498482192149128, 'max_depth': 7, 'alpha': 0.2735, 'lambda': 28.08585369833644, 'max_bin': 398}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:30:33,356]\u001b[0m Trial 369 finished with value: 0.6923803867824836 and parameters: {'n_estimators': 745, 'eta': 0.07004100869522878, 'max_depth': 7, 'alpha': 0.1452, 'lambda': 31.786327071389575, 'max_bin': 407}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:30:42,439]\u001b[0m Trial 370 finished with value: 0.6901987531878466 and parameters: {'n_estimators': 654, 'eta': 0.08436886026733961, 'max_depth': 7, 'alpha': 0.23090000000000002, 'lambda': 29.198401667396958, 'max_bin': 418}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:30:53,145]\u001b[0m Trial 371 finished with value: 0.6915553495850852 and parameters: {'n_estimators': 704, 'eta': 0.06629663114166558, 'max_depth': 7, 'alpha': 0.3044, 'lambda': 27.223651111364788, 'max_bin': 388}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:31:02,940]\u001b[0m Trial 372 finished with value: 0.6924686494860539 and parameters: {'n_estimators': 681, 'eta': 0.07782414997832504, 'max_depth': 7, 'alpha': 0.24880000000000002, 'lambda': 30.652608999263546, 'max_bin': 428}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:31:13,742]\u001b[0m Trial 373 finished with value: 0.6939152774888555 and parameters: {'n_estimators': 690, 'eta': 0.07065199227633394, 'max_depth': 7, 'alpha': 0.3309, 'lambda': 28.34352625268126, 'max_bin': 404}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:31:22,404]\u001b[0m Trial 374 finished with value: 0.6889923284309607 and parameters: {'n_estimators': 660, 'eta': 0.08157723900003112, 'max_depth': 7, 'alpha': 0.2858, 'lambda': 29.488782108887538, 'max_bin': 411}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:31:32,454]\u001b[0m Trial 375 finished with value: 0.6901584926559361 and parameters: {'n_estimators': 627, 'eta': 0.0733035051077049, 'max_depth': 7, 'alpha': 0.1544, 'lambda': 30.958505169154282, 'max_bin': 419}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:31:42,166]\u001b[0m Trial 376 finished with value: 0.6937682167997616 and parameters: {'n_estimators': 708, 'eta': 0.07644217628591062, 'max_depth': 7, 'alpha': 0.31570000000000004, 'lambda': 29.945568503562978, 'max_bin': 381}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:31:52,085]\u001b[0m Trial 377 finished with value: 0.6890528736044764 and parameters: {'n_estimators': 742, 'eta': 0.08038056318778877, 'max_depth': 7, 'alpha': 0.2083, 'lambda': 28.69520150842289, 'max_bin': 393}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:32:02,040]\u001b[0m Trial 378 finished with value: 0.6916487403990284 and parameters: {'n_estimators': 680, 'eta': 0.06781250601066198, 'max_depth': 6, 'alpha': 0.0671, 'lambda': 27.899049560034072, 'max_bin': 425}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:32:11,773]\u001b[0m Trial 379 finished with value: 0.689481200157547 and parameters: {'n_estimators': 642, 'eta': 0.07521569855776752, 'max_depth': 7, 'alpha': 0.1153, 'lambda': 27.045371155915063, 'max_bin': 432}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:32:22,287]\u001b[0m Trial 380 finished with value: 0.69419995109283 and parameters: {'n_estimators': 708, 'eta': 0.0788300648484258, 'max_depth': 7, 'alpha': 0.3675, 'lambda': 31.58980327524169, 'max_bin': 399}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:32:33,358]\u001b[0m Trial 381 finished with value: 0.6929038321796217 and parameters: {'n_estimators': 737, 'eta': 0.06498675784593108, 'max_depth': 7, 'alpha': 0.2912, 'lambda': 28.88237947699082, 'max_bin': 415}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:32:43,970]\u001b[0m Trial 382 finished with value: 0.6928704897513047 and parameters: {'n_estimators': 670, 'eta': 0.07019598185559744, 'max_depth': 7, 'alpha': 0.27140000000000003, 'lambda': 30.26084144760806, 'max_bin': 407}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:32:54,358]\u001b[0m Trial 383 finished with value: 0.6957254266732549 and parameters: {'n_estimators': 692, 'eta': 0.07499622218401132, 'max_depth': 7, 'alpha': 0.33640000000000003, 'lambda': 27.068373166009028, 'max_bin': 401}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:33:04,445]\u001b[0m Trial 384 finished with value: 0.6917089335720448 and parameters: {'n_estimators': 699, 'eta': 0.07273443833902614, 'max_depth': 7, 'alpha': 0.34, 'lambda': 27.512744705063245, 'max_bin': 397}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:33:14,151]\u001b[0m Trial 385 finished with value: 0.6908163412044577 and parameters: {'n_estimators': 692, 'eta': 0.07712491806703058, 'max_depth': 7, 'alpha': 0.38620000000000004, 'lambda': 29.52417076150095, 'max_bin': 403}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:33:24,170]\u001b[0m Trial 386 finished with value: 0.6887997825278344 and parameters: {'n_estimators': 754, 'eta': 0.07425449852327781, 'max_depth': 7, 'alpha': 0.3225, 'lambda': 28.15790080386454, 'max_bin': 391}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:33:34,299]\u001b[0m Trial 387 finished with value: 0.6904047677394202 and parameters: {'n_estimators': 715, 'eta': 0.0697424836429447, 'max_depth': 7, 'alpha': 0.3005, 'lambda': 26.959875565646193, 'max_bin': 410}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:33:44,247]\u001b[0m Trial 388 finished with value: 0.6955888522254491 and parameters: {'n_estimators': 661, 'eta': 0.07557032766161907, 'max_depth': 7, 'alpha': 0.2366, 'lambda': 28.953985037590126, 'max_bin': 402}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:33:53,959]\u001b[0m Trial 389 finished with value: 0.6901894506702176 and parameters: {'n_estimators': 683, 'eta': 0.08390423236487532, 'max_depth': 7, 'alpha': 0.2662, 'lambda': 30.48377482896507, 'max_bin': 419}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:34:02,325]\u001b[0m Trial 390 finished with value: 0.689360285613097 and parameters: {'n_estimators': 503, 'eta': 0.07932429112492417, 'max_depth': 7, 'alpha': 0.35750000000000004, 'lambda': 27.209753716089562, 'max_bin': 412}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:34:12,851]\u001b[0m Trial 391 finished with value: 0.6910687357408228 and parameters: {'n_estimators': 736, 'eta': 0.07313823084459738, 'max_depth': 7, 'alpha': 0.1593, 'lambda': 28.570817419137654, 'max_bin': 425}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:34:21,951]\u001b[0m Trial 392 finished with value: 0.6939665274870181 and parameters: {'n_estimators': 644, 'eta': 0.07708805998127286, 'max_depth': 6, 'alpha': 0.32830000000000004, 'lambda': 29.56098439061711, 'max_bin': 397}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:34:32,295]\u001b[0m Trial 393 finished with value: 0.6919687039957385 and parameters: {'n_estimators': 618, 'eta': 0.06702427810110828, 'max_depth': 7, 'alpha': 0.1268, 'lambda': 25.805523847610353, 'max_bin': 387}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:34:43,334]\u001b[0m Trial 394 finished with value: 0.6913565910873087 and parameters: {'n_estimators': 711, 'eta': 0.06275729844933627, 'max_depth': 7, 'alpha': 0.2963, 'lambda': 31.499514646833305, 'max_bin': 404}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:34:52,625]\u001b[0m Trial 395 finished with value: 0.6885049342483183 and parameters: {'n_estimators': 677, 'eta': 0.0881787890766241, 'max_depth': 7, 'alpha': 0.3123, 'lambda': 30.22227469788168, 'max_bin': 415}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:35:02,784]\u001b[0m Trial 396 finished with value: 0.6913181124878719 and parameters: {'n_estimators': 691, 'eta': 0.0722405780159774, 'max_depth': 7, 'alpha': 0.3362, 'lambda': 26.86897743899945, 'max_bin': 408}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:35:12,313]\u001b[0m Trial 397 finished with value: 0.6912103175482797 and parameters: {'n_estimators': 659, 'eta': 0.07434753029689185, 'max_depth': 7, 'alpha': 0.09870000000000001, 'lambda': 32.65874589105634, 'max_bin': 395}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:35:22,686]\u001b[0m Trial 398 finished with value: 0.6944024161311345 and parameters: {'n_estimators': 728, 'eta': 0.07854748598322688, 'max_depth': 7, 'alpha': 0.4748, 'lambda': 27.93754651598317, 'max_bin': 421}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:35:32,547]\u001b[0m Trial 399 finished with value: 0.6955221204175704 and parameters: {'n_estimators': 709, 'eta': 0.07580189274008617, 'max_depth': 7, 'alpha': 0.42600000000000005, 'lambda': 29.426792966687916, 'max_bin': 431}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6987\n",
      "\tBest params:\n",
      "\t\tn_estimators: 664\n",
      "\t\teta: 0.07661568123004531\n",
      "\t\tmax_depth: 7\n",
      "\t\talpha: 0.33590000000000003\n",
      "\t\tlambda: 30.08089368243512\n",
      "\t\tmax_bin: 398\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.684712    0.733307    0.708160    0.688529   \n",
      "1                    TP  314.000000  332.000000  320.000000  321.000000   \n",
      "2                    TN  184.000000  177.000000  187.000000  180.000000   \n",
      "3                    FP   53.000000   41.000000   50.000000   51.000000   \n",
      "4                    FN   44.000000   45.000000   38.000000   43.000000   \n",
      "5              Accuracy    0.836975    0.855462    0.852101    0.842017   \n",
      "6             Precision    0.855586    0.890080    0.864865    0.862903   \n",
      "7           Sensitivity    0.877095    0.880637    0.893855    0.881868   \n",
      "8           Specificity    0.776400    0.811900    0.789000    0.779200   \n",
      "9              F1 score    0.866207    0.885333    0.879121    0.872283   \n",
      "10  F1 score (weighted)    0.836409    0.855734    0.851399    0.841483   \n",
      "11     F1 score (macro)    0.828802    0.844939    0.844322    0.832617   \n",
      "12    Balanced Accuracy    0.826733    0.846282    0.841442    0.830544   \n",
      "13                  MCC    0.658019    0.689966    0.689399    0.665568   \n",
      "14                  NPV    0.807000    0.797300    0.831100    0.807200   \n",
      "15              ROC_AUC    0.826733    0.846282    0.841442    0.830544   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.708776    0.730538    0.699183    0.691330  \n",
      "1   326.000000  341.000000  325.000000  316.000000  \n",
      "2   182.000000  173.000000  176.000000  185.000000  \n",
      "3    48.000000   49.000000   56.000000   43.000000  \n",
      "4    39.000000   32.000000   38.000000   51.000000  \n",
      "5     0.853782    0.863866    0.842017    0.842017  \n",
      "6     0.871658    0.874359    0.853018    0.880223  \n",
      "7     0.893151    0.914209    0.895317    0.861035  \n",
      "8     0.791300    0.779300    0.758600    0.811400  \n",
      "9     0.882273    0.893840    0.873656    0.870523  \n",
      "10    0.853213    0.862672    0.840740    0.842508  \n",
      "11    0.844684    0.852072    0.831447    0.833969  \n",
      "12    0.842228    0.846744    0.826969    0.836219  \n",
      "13    0.689800    0.705766    0.664606    0.668267  \n",
      "14    0.823500    0.843900    0.822400    0.783900  \n",
      "15    0.842228    0.846744    0.826969    0.836219  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_xgb_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "y_pred_xgb_7_cat = np.where((y_pred_xgb_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:35:44,762]\u001b[0m Trial 400 finished with value: 0.6885007633285096 and parameters: {'n_estimators': 676, 'eta': 0.060155672648433106, 'max_depth': 7, 'alpha': 0.2777, 'lambda': 28.65541007425871, 'max_bin': 426}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:35:54,699]\u001b[0m Trial 401 finished with value: 0.687483566184936 and parameters: {'n_estimators': 698, 'eta': 0.07062753414878953, 'max_depth': 7, 'alpha': 0.24760000000000001, 'lambda': 30.92961608368772, 'max_bin': 401}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:36:03,423]\u001b[0m Trial 402 finished with value: 0.6861676965842913 and parameters: {'n_estimators': 755, 'eta': 0.08070195615762384, 'max_depth': 7, 'alpha': 0.2258, 'lambda': 25.504238322443623, 'max_bin': 412}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:36:14,660]\u001b[0m Trial 403 finished with value: 0.6909309167571094 and parameters: {'n_estimators': 660, 'eta': 0.06834570985530732, 'max_depth': 7, 'alpha': 0.3028, 'lambda': 29.74026716005633, 'max_bin': 420}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:36:24,835]\u001b[0m Trial 404 finished with value: 0.6917178805442876 and parameters: {'n_estimators': 732, 'eta': 0.07197442642092663, 'max_depth': 7, 'alpha': 0.3507, 'lambda': 28.026037335007988, 'max_bin': 390}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:36:34,063]\u001b[0m Trial 405 finished with value: 0.6863765577594899 and parameters: {'n_estimators': 629, 'eta': 0.07742606606135953, 'max_depth': 7, 'alpha': 0.2632, 'lambda': 26.29399622710485, 'max_bin': 407}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:36:44,401]\u001b[0m Trial 406 finished with value: 0.686894535147998 and parameters: {'n_estimators': 719, 'eta': 0.06293643266641755, 'max_depth': 7, 'alpha': 0.1893, 'lambda': 25.01450662631509, 'max_bin': 399}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:36:54,591]\u001b[0m Trial 407 finished with value: 0.689716471527163 and parameters: {'n_estimators': 688, 'eta': 0.07584506175981329, 'max_depth': 7, 'alpha': 0.2896, 'lambda': 28.911427415955245, 'max_bin': 433}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:37:02,730]\u001b[0m Trial 408 finished with value: 0.6818695468206528 and parameters: {'n_estimators': 649, 'eta': 0.06540877704548227, 'max_depth': 5, 'alpha': 0.31880000000000003, 'lambda': 30.773739722295563, 'max_bin': 426}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:37:12,225]\u001b[0m Trial 409 finished with value: 0.6878865920475588 and parameters: {'n_estimators': 706, 'eta': 0.07321258272323493, 'max_depth': 7, 'alpha': 0.1295, 'lambda': 27.128270382523613, 'max_bin': 415}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:37:22,640]\u001b[0m Trial 410 finished with value: 0.6879179659157676 and parameters: {'n_estimators': 759, 'eta': 0.0717977117947353, 'max_depth': 7, 'alpha': 0.15410000000000001, 'lambda': 30.019712545349194, 'max_bin': 394}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:37:31,618]\u001b[0m Trial 411 finished with value: 0.6892062948837625 and parameters: {'n_estimators': 673, 'eta': 0.0689417974657561, 'max_depth': 6, 'alpha': 0.2122, 'lambda': 27.643109520859273, 'max_bin': 406}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:37:42,253]\u001b[0m Trial 412 finished with value: 0.6902394644645751 and parameters: {'n_estimators': 749, 'eta': 0.07514652826407217, 'max_depth': 7, 'alpha': 0.3775, 'lambda': 31.266000261070786, 'max_bin': 419}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:37:51,880]\u001b[0m Trial 413 finished with value: 0.687644134143137 and parameters: {'n_estimators': 693, 'eta': 0.08251214466515702, 'max_depth': 7, 'alpha': 0.2848, 'lambda': 29.058900417725802, 'max_bin': 400}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:38:05,331]\u001b[0m Trial 414 finished with value: 0.639130360679942 and parameters: {'n_estimators': 719, 'eta': 0.009316368201931324, 'max_depth': 7, 'alpha': 0.34190000000000004, 'lambda': 26.54574592628877, 'max_bin': 423}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:38:14,716]\u001b[0m Trial 415 finished with value: 0.689870412817083 and parameters: {'n_estimators': 667, 'eta': 0.0798752591561616, 'max_depth': 7, 'alpha': 0.2655, 'lambda': 28.387674836881963, 'max_bin': 413}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:38:25,705]\u001b[0m Trial 416 finished with value: 0.6894192858162107 and parameters: {'n_estimators': 638, 'eta': 0.059908291158123905, 'max_depth': 7, 'alpha': 0.31120000000000003, 'lambda': 30.33546797206405, 'max_bin': 382}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:38:34,772]\u001b[0m Trial 417 finished with value: 0.688534930729739 and parameters: {'n_estimators': 551, 'eta': 0.07753562618944908, 'max_depth': 7, 'alpha': 0.2444, 'lambda': 31.942225410028076, 'max_bin': 405}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:38:45,222]\u001b[0m Trial 418 finished with value: 0.688499441138654 and parameters: {'n_estimators': 770, 'eta': 0.07413167665969678, 'max_depth': 7, 'alpha': 0.2931, 'lambda': 29.312322981027986, 'max_bin': 429}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:38:54,167]\u001b[0m Trial 419 finished with value: 0.6853480196941871 and parameters: {'n_estimators': 478, 'eta': 0.05716082847750664, 'max_depth': 7, 'alpha': 0.3226, 'lambda': 27.86276655824679, 'max_bin': 410}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:39:00,250]\u001b[0m Trial 420 finished with value: 0.6771069514750516 and parameters: {'n_estimators': 375, 'eta': 0.06724061437180351, 'max_depth': 6, 'alpha': 0.3637, 'lambda': 29.713779886797735, 'max_bin': 394}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:39:10,374]\u001b[0m Trial 421 finished with value: 0.6891257553258872 and parameters: {'n_estimators': 735, 'eta': 0.0640142765467884, 'max_depth': 7, 'alpha': 0.0721, 'lambda': 25.177228502162972, 'max_bin': 418}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:39:20,241]\u001b[0m Trial 422 finished with value: 0.6880442649937526 and parameters: {'n_estimators': 705, 'eta': 0.07188529297006443, 'max_depth': 7, 'alpha': 0.5324, 'lambda': 26.769475274366606, 'max_bin': 400}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:39:32,102]\u001b[0m Trial 423 finished with value: 0.6885044039075748 and parameters: {'n_estimators': 687, 'eta': 0.054790489286186385, 'max_depth': 7, 'alpha': 0.2763, 'lambda': 31.12520102511839, 'max_bin': 423}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:39:40,638]\u001b[0m Trial 424 finished with value: 0.6879988628868835 and parameters: {'n_estimators': 589, 'eta': 0.09673718503825937, 'max_depth': 7, 'alpha': 0.5832, 'lambda': 28.69225079725849, 'max_bin': 435}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:39:49,456]\u001b[0m Trial 425 finished with value: 0.6869828900498643 and parameters: {'n_estimators': 651, 'eta': 0.07803118686195559, 'max_depth': 7, 'alpha': 0.2315, 'lambda': 27.59120134845811, 'max_bin': 386}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:40:00,636]\u001b[0m Trial 426 finished with value: 0.6910633725601765 and parameters: {'n_estimators': 674, 'eta': 0.06993829665848511, 'max_depth': 7, 'alpha': 0.4047, 'lambda': 25.94680618604419, 'max_bin': 408}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:40:10,400]\u001b[0m Trial 427 finished with value: 0.6861345505785014 and parameters: {'n_estimators': 717, 'eta': 0.07402117180534414, 'max_depth': 7, 'alpha': 0.3069, 'lambda': 30.35727337686152, 'max_bin': 403}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:40:20,283]\u001b[0m Trial 428 finished with value: 0.6888814206046523 and parameters: {'n_estimators': 741, 'eta': 0.07618585177839657, 'max_depth': 7, 'alpha': 0.3441, 'lambda': 28.429917364177165, 'max_bin': 416}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:40:35,652]\u001b[0m Trial 429 finished with value: 0.684661861622572 and parameters: {'n_estimators': 696, 'eta': 0.05554751023763539, 'max_depth': 12, 'alpha': 0.14500000000000002, 'lambda': 29.307345742988243, 'max_bin': 428}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:40:46,332]\u001b[0m Trial 430 finished with value: 0.6896085599053581 and parameters: {'n_estimators': 609, 'eta': 0.06197005887394587, 'max_depth': 7, 'alpha': 0.1163, 'lambda': 30.030465449960012, 'max_bin': 376}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:40:56,280]\u001b[0m Trial 431 finished with value: 0.69028380192158 and parameters: {'n_estimators': 780, 'eta': 0.07974010673831894, 'max_depth': 7, 'alpha': 0.1794, 'lambda': 27.174965394819594, 'max_bin': 396}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:41:07,020]\u001b[0m Trial 432 finished with value: 0.6908360940602283 and parameters: {'n_estimators': 663, 'eta': 0.06616106726428442, 'max_depth': 7, 'alpha': 0.2031, 'lambda': 31.989072300700215, 'max_bin': 412}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:41:11,030]\u001b[0m Trial 433 finished with value: 0.6710383264153836 and parameters: {'n_estimators': 204, 'eta': 0.08663527591294459, 'max_depth': 7, 'alpha': 0.2958, 'lambda': 27.855850011457832, 'max_bin': 422}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:41:24,071]\u001b[0m Trial 434 finished with value: 0.6908910599406747 and parameters: {'n_estimators': 760, 'eta': 0.057212725856332226, 'max_depth': 7, 'alpha': 0.25320000000000004, 'lambda': 33.12648420204077, 'max_bin': 402}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:41:32,877]\u001b[0m Trial 435 finished with value: 0.6789070418783233 and parameters: {'n_estimators': 688, 'eta': 0.06027304190532821, 'max_depth': 7, 'alpha': 0.33190000000000003, 'lambda': 5.402069521097619, 'max_bin': 432}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:41:43,591]\u001b[0m Trial 436 finished with value: 0.6905861112244601 and parameters: {'n_estimators': 725, 'eta': 0.07084858436625154, 'max_depth': 7, 'alpha': 0.2765, 'lambda': 30.66187237841845, 'max_bin': 391}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:41:53,290]\u001b[0m Trial 437 finished with value: 0.6898038450230843 and parameters: {'n_estimators': 641, 'eta': 0.07382614782306555, 'max_depth': 7, 'alpha': 0.3125, 'lambda': 26.094104063730676, 'max_bin': 416}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:42:02,225]\u001b[0m Trial 438 finished with value: 0.6882196197196262 and parameters: {'n_estimators': 523, 'eta': 0.08110385707146794, 'max_depth': 7, 'alpha': 0.3532, 'lambda': 29.372587703024294, 'max_bin': 406}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:42:12,734]\u001b[0m Trial 439 finished with value: 0.68984416044397 and parameters: {'n_estimators': 700, 'eta': 0.07638146502933865, 'max_depth': 7, 'alpha': 0.25620000000000004, 'lambda': 28.4266622978618, 'max_bin': 304}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:42:23,165]\u001b[0m Trial 440 finished with value: 0.6872894597780042 and parameters: {'n_estimators': 676, 'eta': 0.07777632848485191, 'max_depth': 7, 'alpha': 0.3292, 'lambda': 26.762405126827886, 'max_bin': 397}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:42:35,091]\u001b[0m Trial 441 finished with value: 0.6877391145357366 and parameters: {'n_estimators': 711, 'eta': 0.05501517023540189, 'max_depth': 7, 'alpha': 0.2891, 'lambda': 29.725439928762587, 'max_bin': 409}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:42:46,324]\u001b[0m Trial 442 finished with value: 0.6901583972931302 and parameters: {'n_estimators': 743, 'eta': 0.06891647760805666, 'max_depth': 7, 'alpha': 0.1573, 'lambda': 31.08986238534824, 'max_bin': 428}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:42:57,645]\u001b[0m Trial 443 finished with value: 0.6895400322900509 and parameters: {'n_estimators': 661, 'eta': 0.058870364026474505, 'max_depth': 7, 'alpha': 0.2278, 'lambda': 24.774216650462744, 'max_bin': 438}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:43:06,338]\u001b[0m Trial 444 finished with value: 0.6863307747054386 and parameters: {'n_estimators': 779, 'eta': 0.07220140313077646, 'max_depth': 6, 'alpha': 0.30110000000000003, 'lambda': 28.595569118564953, 'max_bin': 421}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:43:15,961]\u001b[0m Trial 445 finished with value: 0.6866674269572226 and parameters: {'n_estimators': 624, 'eta': 0.07543078523343812, 'max_depth': 7, 'alpha': 0.1307, 'lambda': 27.310050467965237, 'max_bin': 390}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:43:28,440]\u001b[0m Trial 446 finished with value: 0.6894609694154208 and parameters: {'n_estimators': 730, 'eta': 0.05409548569981693, 'max_depth': 7, 'alpha': 0.1937, 'lambda': 30.39938996690263, 'max_bin': 413}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:43:34,108]\u001b[0m Trial 447 finished with value: 0.6795111078090109 and parameters: {'n_estimators': 300, 'eta': 0.08291642760257076, 'max_depth': 7, 'alpha': 0.272, 'lambda': 25.463011338104607, 'max_bin': 401}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:43:45,835]\u001b[0m Trial 448 finished with value: 0.6903121766019392 and parameters: {'n_estimators': 709, 'eta': 0.05683956020172976, 'max_depth': 7, 'alpha': 0.3194, 'lambda': 29.062864845009575, 'max_bin': 264}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:43:55,747]\u001b[0m Trial 449 finished with value: 0.6879897598201079 and parameters: {'n_estimators': 681, 'eta': 0.07302499624182722, 'max_depth': 7, 'alpha': 0.1009, 'lambda': 27.58120758773088, 'max_bin': 425}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6987\n",
      "\tBest params:\n",
      "\t\tn_estimators: 664\n",
      "\t\teta: 0.07661568123004531\n",
      "\t\tmax_depth: 7\n",
      "\t\talpha: 0.33590000000000003\n",
      "\t\tlambda: 30.08089368243512\n",
      "\t\tmax_bin: 398\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.684712    0.733307    0.708160    0.688529   \n",
      "1                    TP  314.000000  332.000000  320.000000  321.000000   \n",
      "2                    TN  184.000000  177.000000  187.000000  180.000000   \n",
      "3                    FP   53.000000   41.000000   50.000000   51.000000   \n",
      "4                    FN   44.000000   45.000000   38.000000   43.000000   \n",
      "5              Accuracy    0.836975    0.855462    0.852101    0.842017   \n",
      "6             Precision    0.855586    0.890080    0.864865    0.862903   \n",
      "7           Sensitivity    0.877095    0.880637    0.893855    0.881868   \n",
      "8           Specificity    0.776400    0.811900    0.789000    0.779200   \n",
      "9              F1 score    0.866207    0.885333    0.879121    0.872283   \n",
      "10  F1 score (weighted)    0.836409    0.855734    0.851399    0.841483   \n",
      "11     F1 score (macro)    0.828802    0.844939    0.844322    0.832617   \n",
      "12    Balanced Accuracy    0.826733    0.846282    0.841442    0.830544   \n",
      "13                  MCC    0.658019    0.689966    0.689399    0.665568   \n",
      "14                  NPV    0.807000    0.797300    0.831100    0.807200   \n",
      "15              ROC_AUC    0.826733    0.846282    0.841442    0.830544   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.708776    0.730538    0.699183    0.691330    0.720596  \n",
      "1   326.000000  341.000000  325.000000  316.000000  313.000000  \n",
      "2   182.000000  173.000000  176.000000  185.000000  189.000000  \n",
      "3    48.000000   49.000000   56.000000   43.000000   55.000000  \n",
      "4    39.000000   32.000000   38.000000   51.000000   38.000000  \n",
      "5     0.853782    0.863866    0.842017    0.842017    0.843697  \n",
      "6     0.871658    0.874359    0.853018    0.880223    0.850543  \n",
      "7     0.893151    0.914209    0.895317    0.861035    0.891738  \n",
      "8     0.791300    0.779300    0.758600    0.811400    0.774600  \n",
      "9     0.882273    0.893840    0.873656    0.870523    0.870654  \n",
      "10    0.853213    0.862672    0.840740    0.842508    0.842725  \n",
      "11    0.844684    0.852072    0.831447    0.833969    0.836601  \n",
      "12    0.842228    0.846744    0.826969    0.836219    0.833164  \n",
      "13    0.689800    0.705766    0.664606    0.668267    0.674683  \n",
      "14    0.823500    0.843900    0.822400    0.783900    0.832600  \n",
      "15    0.842228    0.846744    0.826969    0.836219    0.833164  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_xgb_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "y_pred_xgb_8_cat = np.where((y_pred_xgb_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:44:06,284]\u001b[0m Trial 450 finished with value: 0.6865637968815093 and parameters: {'n_estimators': 650, 'eta': 0.06498183090520163, 'max_depth': 6, 'alpha': 0.2833, 'lambda': 26.35228725697042, 'max_bin': 418}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:44:15,586]\u001b[0m Trial 451 finished with value: 0.6871515476298781 and parameters: {'n_estimators': 570, 'eta': 0.07935553092520287, 'max_depth': 7, 'alpha': 0.33990000000000004, 'lambda': 29.879669089550234, 'max_bin': 433}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:44:28,237]\u001b[0m Trial 452 finished with value: 0.6849839010935544 and parameters: {'n_estimators': 764, 'eta': 0.07006164437085743, 'max_depth': 11, 'alpha': 0.0379, 'lambda': 28.218790017485215, 'max_bin': 395}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:44:39,020]\u001b[0m Trial 453 finished with value: 0.6840783661484713 and parameters: {'n_estimators': 697, 'eta': 0.06257755604716281, 'max_depth': 7, 'alpha': 0.3048, 'lambda': 32.35524243843802, 'max_bin': 405}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:44:48,568]\u001b[0m Trial 454 finished with value: 0.6870523286221206 and parameters: {'n_estimators': 788, 'eta': 0.0751598084619333, 'max_depth': 7, 'alpha': 0.21350000000000002, 'lambda': 31.460882362320042, 'max_bin': 411}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:45:00,418]\u001b[0m Trial 455 finished with value: 0.6848062904243692 and parameters: {'n_estimators': 719, 'eta': 0.05410770876870185, 'max_depth': 7, 'alpha': 0.24830000000000002, 'lambda': 28.963157990702623, 'max_bin': 425}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:45:10,151]\u001b[0m Trial 456 finished with value: 0.6856646145308753 and parameters: {'n_estimators': 673, 'eta': 0.06728821640730322, 'max_depth': 7, 'alpha': 0.1734, 'lambda': 30.668778342733024, 'max_bin': 401}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:45:20,682]\u001b[0m Trial 457 finished with value: 0.685301903984632 and parameters: {'n_estimators': 739, 'eta': 0.059909530785779645, 'max_depth': 7, 'alpha': 0.3564, 'lambda': 24.600682458763746, 'max_bin': 386}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:45:29,874]\u001b[0m Trial 458 finished with value: 0.6849369676611411 and parameters: {'n_estimators': 688, 'eta': 0.07688793401781935, 'max_depth': 7, 'alpha': 0.3234, 'lambda': 29.816323854733415, 'max_bin': 418}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:45:40,898]\u001b[0m Trial 459 finished with value: 0.6857378730448827 and parameters: {'n_estimators': 642, 'eta': 0.05725503931661486, 'max_depth': 7, 'alpha': 0.0886, 'lambda': 27.003805950932282, 'max_bin': 407}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:45:50,776]\u001b[0m Trial 460 finished with value: 0.6877494284941469 and parameters: {'n_estimators': 711, 'eta': 0.07121406901028739, 'max_depth': 7, 'alpha': 0.28290000000000004, 'lambda': 27.845307797348074, 'max_bin': 440}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:46:01,446]\u001b[0m Trial 461 finished with value: 0.6854667616805168 and parameters: {'n_estimators': 754, 'eta': 0.05354636415007099, 'max_depth': 7, 'alpha': 0.1462, 'lambda': 29.110424807021598, 'max_bin': 397}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:46:09,934]\u001b[0m Trial 462 finished with value: 0.6835132667932452 and parameters: {'n_estimators': 662, 'eta': 0.07861984127965885, 'max_depth': 7, 'alpha': 0.2641, 'lambda': 28.186263856826162, 'max_bin': 430}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:46:18,975]\u001b[0m Trial 463 finished with value: 0.6889517737700053 and parameters: {'n_estimators': 695, 'eta': 0.07413977950422532, 'max_depth': 7, 'alpha': 0.3089, 'lambda': 25.84443912758205, 'max_bin': 414}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:46:30,046]\u001b[0m Trial 464 finished with value: 0.684921928930824 and parameters: {'n_estimators': 729, 'eta': 0.05193604010488686, 'max_depth': 7, 'alpha': 0.23800000000000002, 'lambda': 30.0841208458596, 'max_bin': 421}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:46:41,447]\u001b[0m Trial 465 finished with value: 0.6836967435963325 and parameters: {'n_estimators': 781, 'eta': 0.056040272750755654, 'max_depth': 7, 'alpha': 0.3738, 'lambda': 30.962024618273198, 'max_bin': 393}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:46:50,805]\u001b[0m Trial 466 finished with value: 0.6883622856431492 and parameters: {'n_estimators': 675, 'eta': 0.07284983155942502, 'max_depth': 7, 'alpha': 0.29600000000000004, 'lambda': 26.75939680948198, 'max_bin': 402}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:46:59,643]\u001b[0m Trial 467 finished with value: 0.685280625521669 and parameters: {'n_estimators': 617, 'eta': 0.07604379448078101, 'max_depth': 7, 'alpha': 0.1706, 'lambda': 28.830228665250864, 'max_bin': 433}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:47:08,353]\u001b[0m Trial 468 finished with value: 0.6864049034746937 and parameters: {'n_estimators': 705, 'eta': 0.08140182610836617, 'max_depth': 7, 'alpha': 0.6186, 'lambda': 25.04055212585791, 'max_bin': 409}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:47:16,231]\u001b[0m Trial 469 finished with value: 0.6787812080173754 and parameters: {'n_estimators': 406, 'eta': 0.05139733833794225, 'max_depth': 7, 'alpha': 0.3405, 'lambda': 29.6360015769264, 'max_bin': 415}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:47:25,306]\u001b[0m Trial 470 finished with value: 0.6820871147223398 and parameters: {'n_estimators': 686, 'eta': 0.060831653391606094, 'max_depth': 6, 'alpha': 0.3226, 'lambda': 27.286151957733964, 'max_bin': 425}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:47:37,811]\u001b[0m Trial 471 finished with value: 0.6863079000060248 and parameters: {'n_estimators': 744, 'eta': 0.06961528326141372, 'max_depth': 10, 'alpha': 0.2132, 'lambda': 28.210634840569504, 'max_bin': 398}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:47:51,600]\u001b[0m Trial 472 finished with value: 0.6862019236069079 and parameters: {'n_estimators': 722, 'eta': 0.05819374785648865, 'max_depth': 9, 'alpha': 0.11130000000000001, 'lambda': 30.587089643920894, 'max_bin': 389}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:48:02,404]\u001b[0m Trial 473 finished with value: 0.6842546505453831 and parameters: {'n_estimators': 658, 'eta': 0.05424230481813378, 'max_depth': 7, 'alpha': 0.2666, 'lambda': 26.14698813473867, 'max_bin': 404}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:48:10,708]\u001b[0m Trial 474 finished with value: 0.6848582822109484 and parameters: {'n_estimators': 791, 'eta': 0.07884887493048306, 'max_depth': 7, 'alpha': 0.1915, 'lambda': 29.228278494449697, 'max_bin': 437}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:48:21,263]\u001b[0m Trial 475 finished with value: 0.6879577231267232 and parameters: {'n_estimators': 762, 'eta': 0.06407423712921442, 'max_depth': 7, 'alpha': 0.7665000000000001, 'lambda': 27.715520315597665, 'max_bin': 419}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:48:30,913]\u001b[0m Trial 476 finished with value: 0.6877031266378024 and parameters: {'n_estimators': 704, 'eta': 0.07455857853010069, 'max_depth': 7, 'alpha': 0.2972, 'lambda': 31.288991088160593, 'max_bin': 429}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:48:40,301]\u001b[0m Trial 477 finished with value: 0.6854262257888101 and parameters: {'n_estimators': 678, 'eta': 0.06682383554850804, 'max_depth': 7, 'alpha': 0.274, 'lambda': 32.15306223522976, 'max_bin': 410}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:48:50,634]\u001b[0m Trial 478 finished with value: 0.6828681385340494 and parameters: {'n_estimators': 632, 'eta': 0.05068552636147675, 'max_depth': 7, 'alpha': 0.3185, 'lambda': 28.673892144625995, 'max_bin': 423}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:49:01,125]\u001b[0m Trial 479 finished with value: 0.6879052413668136 and parameters: {'n_estimators': 656, 'eta': 0.07109553383947634, 'max_depth': 7, 'alpha': 0.1301, 'lambda': 30.54021048843263, 'max_bin': 393}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:49:12,137]\u001b[0m Trial 480 finished with value: 0.6829664050268777 and parameters: {'n_estimators': 724, 'eta': 0.05578903956798558, 'max_depth': 7, 'alpha': 0.25320000000000004, 'lambda': 29.897068633148216, 'max_bin': 405}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:49:24,947]\u001b[0m Trial 481 finished with value: 0.684236745892593 and parameters: {'n_estimators': 801, 'eta': 0.053435655324065676, 'max_depth': 7, 'alpha': 0.6586000000000001, 'lambda': 26.880280885076722, 'max_bin': 399}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:49:34,919]\u001b[0m Trial 482 finished with value: 0.6876675212713668 and parameters: {'n_estimators': 771, 'eta': 0.07714674491953603, 'max_depth': 7, 'alpha': 0.7259, 'lambda': 22.301151551109754, 'max_bin': 411}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:49:44,818]\u001b[0m Trial 483 finished with value: 0.688133717272281 and parameters: {'n_estimators': 690, 'eta': 0.07379875720236806, 'max_depth': 7, 'alpha': 0.47200000000000003, 'lambda': 29.1768169924613, 'max_bin': 442}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:49:54,140]\u001b[0m Trial 484 finished with value: 0.6834414173126399 and parameters: {'n_estimators': 668, 'eta': 0.08040819454745457, 'max_depth': 7, 'alpha': 0.16060000000000002, 'lambda': 25.218111143597906, 'max_bin': 416}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:50:03,969]\u001b[0m Trial 485 finished with value: 0.685813066665825 and parameters: {'n_estimators': 752, 'eta': 0.06819109004678128, 'max_depth': 7, 'alpha': 0.3569, 'lambda': 28.134507740161865, 'max_bin': 428}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:50:13,180]\u001b[0m Trial 486 finished with value: 0.6839446590010676 and parameters: {'n_estimators': 715, 'eta': 0.061561211830238, 'max_depth': 6, 'alpha': 0.2861, 'lambda': 26.293957372114566, 'max_bin': 435}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:50:25,182]\u001b[0m Trial 487 finished with value: 0.6863271932284566 and parameters: {'n_estimators': 700, 'eta': 0.058895117674528534, 'max_depth': 7, 'alpha': 0.3065, 'lambda': 31.645671815561688, 'max_bin': 421}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:50:34,960]\u001b[0m Trial 488 finished with value: 0.6879600781044782 and parameters: {'n_estimators': 652, 'eta': 0.07596068052133255, 'max_depth': 7, 'alpha': 0.8133, 'lambda': 29.835429640627375, 'max_bin': 406}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:50:37,611]\u001b[0m Trial 489 finished with value: 0.6556234263926102 and parameters: {'n_estimators': 132, 'eta': 0.07168205273618272, 'max_depth': 7, 'alpha': 0.2369, 'lambda': 27.482535014406206, 'max_bin': 385}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:50:48,934]\u001b[0m Trial 490 finished with value: 0.6842075008408237 and parameters: {'n_estimators': 737, 'eta': 0.050294385194523526, 'max_depth': 7, 'alpha': 0.3265, 'lambda': 21.584594265482323, 'max_bin': 396}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:51:00,012]\u001b[0m Trial 491 finished with value: 0.68378492493014 and parameters: {'n_estimators': 686, 'eta': 0.055067664160989606, 'max_depth': 7, 'alpha': 0.3393, 'lambda': 28.507966052591048, 'max_bin': 402}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:51:12,119]\u001b[0m Trial 492 finished with value: 0.6700367857729755 and parameters: {'n_estimators': 641, 'eta': 0.021793013548088505, 'max_depth': 7, 'alpha': 0.1439, 'lambda': 30.383224164278474, 'max_bin': 414}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:51:21,896]\u001b[0m Trial 493 finished with value: 0.6864124733061234 and parameters: {'n_estimators': 792, 'eta': 0.0778579124388118, 'max_depth': 7, 'alpha': 0.2872, 'lambda': 25.87710308343912, 'max_bin': 426}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:51:33,503]\u001b[0m Trial 494 finished with value: 0.6857537671024635 and parameters: {'n_estimators': 712, 'eta': 0.05181737694344364, 'max_depth': 7, 'alpha': 0.2599, 'lambda': 29.40010563068695, 'max_bin': 446}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:51:42,617]\u001b[0m Trial 495 finished with value: 0.6896656650246162 and parameters: {'n_estimators': 603, 'eta': 0.0751939166879314, 'max_depth': 7, 'alpha': 0.2242, 'lambda': 24.219848673948785, 'max_bin': 391}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:51:53,071]\u001b[0m Trial 496 finished with value: 0.6857544182963005 and parameters: {'n_estimators': 673, 'eta': 0.07300763939039419, 'max_depth': 9, 'alpha': 0.196, 'lambda': 26.91779775020989, 'max_bin': 420}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:52:04,561]\u001b[0m Trial 497 finished with value: 0.6873187355615872 and parameters: {'n_estimators': 730, 'eta': 0.05731566355608917, 'max_depth': 7, 'alpha': 0.3073, 'lambda': 31.18253646774193, 'max_bin': 410}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:52:14,240]\u001b[0m Trial 498 finished with value: 0.6843923782657476 and parameters: {'n_estimators': 693, 'eta': 0.06548442177031158, 'max_depth': 7, 'alpha': 0.1173, 'lambda': 19.875598961256724, 'max_bin': 400}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:52:24,299]\u001b[0m Trial 499 finished with value: 0.6847725033470671 and parameters: {'n_estimators': 777, 'eta': 0.05404389650147026, 'max_depth': 7, 'alpha': 0.0777, 'lambda': 10.697172367494298, 'max_bin': 433}. Best is trial 341 with value: 0.6987044649927325.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6987\n",
      "\tBest params:\n",
      "\t\tn_estimators: 664\n",
      "\t\teta: 0.07661568123004531\n",
      "\t\tmax_depth: 7\n",
      "\t\talpha: 0.33590000000000003\n",
      "\t\tlambda: 30.08089368243512\n",
      "\t\tmax_bin: 398\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.684712    0.733307    0.708160    0.688529   \n",
      "1                    TP  314.000000  332.000000  320.000000  321.000000   \n",
      "2                    TN  184.000000  177.000000  187.000000  180.000000   \n",
      "3                    FP   53.000000   41.000000   50.000000   51.000000   \n",
      "4                    FN   44.000000   45.000000   38.000000   43.000000   \n",
      "5              Accuracy    0.836975    0.855462    0.852101    0.842017   \n",
      "6             Precision    0.855586    0.890080    0.864865    0.862903   \n",
      "7           Sensitivity    0.877095    0.880637    0.893855    0.881868   \n",
      "8           Specificity    0.776400    0.811900    0.789000    0.779200   \n",
      "9              F1 score    0.866207    0.885333    0.879121    0.872283   \n",
      "10  F1 score (weighted)    0.836409    0.855734    0.851399    0.841483   \n",
      "11     F1 score (macro)    0.828802    0.844939    0.844322    0.832617   \n",
      "12    Balanced Accuracy    0.826733    0.846282    0.841442    0.830544   \n",
      "13                  MCC    0.658019    0.689966    0.689399    0.665568   \n",
      "14                  NPV    0.807000    0.797300    0.831100    0.807200   \n",
      "15              ROC_AUC    0.826733    0.846282    0.841442    0.830544   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.708776    0.730538    0.699183    0.691330    0.720596    0.723794  \n",
      "1   326.000000  341.000000  325.000000  316.000000  313.000000  334.000000  \n",
      "2   182.000000  173.000000  176.000000  185.000000  189.000000  171.000000  \n",
      "3    48.000000   49.000000   56.000000   43.000000   55.000000   49.000000  \n",
      "4    39.000000   32.000000   38.000000   51.000000   38.000000   41.000000  \n",
      "5     0.853782    0.863866    0.842017    0.842017    0.843697    0.848739  \n",
      "6     0.871658    0.874359    0.853018    0.880223    0.850543    0.872063  \n",
      "7     0.893151    0.914209    0.895317    0.861035    0.891738    0.890667  \n",
      "8     0.791300    0.779300    0.758600    0.811400    0.774600    0.777300  \n",
      "9     0.882273    0.893840    0.873656    0.870523    0.870654    0.881266  \n",
      "10    0.853213    0.862672    0.840740    0.842508    0.842725    0.848137  \n",
      "11    0.844684    0.852072    0.831447    0.833969    0.836601    0.836467  \n",
      "12    0.842228    0.846744    0.826969    0.836219    0.833164    0.833970  \n",
      "13    0.689800    0.705766    0.664606    0.668267    0.674683    0.673282  \n",
      "14    0.823500    0.843900    0.822400    0.783900    0.832600    0.806600  \n",
      "15    0.842228    0.846744    0.826969    0.836219    0.833164    0.833970  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_xgb_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_9_cat = np.where((y_pred_xgb_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.710370</td>\n",
       "      <td>0.042931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>164.600000</td>\n",
       "      <td>6.203941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>4.396969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>4.005552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>5.506562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.853582</td>\n",
       "      <td>0.023143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.870995</td>\n",
       "      <td>0.020376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.896123</td>\n",
       "      <td>0.029191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.785090</td>\n",
       "      <td>0.032566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.883115</td>\n",
       "      <td>0.019413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.852876</td>\n",
       "      <td>0.023028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.843403</td>\n",
       "      <td>0.024116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.840599</td>\n",
       "      <td>0.022870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.688290</td>\n",
       "      <td>0.048193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.824550</td>\n",
       "      <td>0.042779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.840599</td>\n",
       "      <td>0.022870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.710370     0.042931\n",
       "1                    TP       164.600000     6.203941\n",
       "2                    TN        89.000000     4.396969\n",
       "3                    FP        24.400000     4.005552\n",
       "4                    FN        19.100000     5.506562\n",
       "5              Accuracy         0.853582     0.023143\n",
       "6             Precision         0.870995     0.020376\n",
       "7           Sensitivity         0.896123     0.029191\n",
       "8           Specificity         0.785090     0.032566\n",
       "9              F1 score         0.883115     0.019413\n",
       "10  F1 score (weighted)         0.852876     0.023028\n",
       "11     F1 score (macro)         0.843403     0.024116\n",
       "12    Balanced Accuracy         0.840599     0.022870\n",
       "13                  MCC         0.688290     0.048193\n",
       "14                  NPV         0.824550     0.042779\n",
       "15              ROC_AUC         0.840599     0.022870"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.684712</td>\n",
       "      <td>0.733307</td>\n",
       "      <td>0.708160</td>\n",
       "      <td>0.688529</td>\n",
       "      <td>0.708776</td>\n",
       "      <td>0.730538</td>\n",
       "      <td>0.699183</td>\n",
       "      <td>0.691330</td>\n",
       "      <td>0.720596</td>\n",
       "      <td>0.723794</td>\n",
       "      <td>0.708892</td>\n",
       "      <td>0.017721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>316.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>324.200000</td>\n",
       "      <td>9.235198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>180.400000</td>\n",
       "      <td>6.040603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>4.766783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>40.900000</td>\n",
       "      <td>5.173651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.836975</td>\n",
       "      <td>0.855462</td>\n",
       "      <td>0.852101</td>\n",
       "      <td>0.842017</td>\n",
       "      <td>0.853782</td>\n",
       "      <td>0.863866</td>\n",
       "      <td>0.842017</td>\n",
       "      <td>0.842017</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>0.848067</td>\n",
       "      <td>0.008203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.855586</td>\n",
       "      <td>0.890080</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.862903</td>\n",
       "      <td>0.871658</td>\n",
       "      <td>0.874359</td>\n",
       "      <td>0.853018</td>\n",
       "      <td>0.880223</td>\n",
       "      <td>0.850543</td>\n",
       "      <td>0.872063</td>\n",
       "      <td>0.867530</td>\n",
       "      <td>0.012579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.877095</td>\n",
       "      <td>0.880637</td>\n",
       "      <td>0.893855</td>\n",
       "      <td>0.881868</td>\n",
       "      <td>0.893151</td>\n",
       "      <td>0.914209</td>\n",
       "      <td>0.895317</td>\n",
       "      <td>0.861035</td>\n",
       "      <td>0.891738</td>\n",
       "      <td>0.890667</td>\n",
       "      <td>0.887957</td>\n",
       "      <td>0.013985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.776400</td>\n",
       "      <td>0.811900</td>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.779200</td>\n",
       "      <td>0.791300</td>\n",
       "      <td>0.779300</td>\n",
       "      <td>0.758600</td>\n",
       "      <td>0.811400</td>\n",
       "      <td>0.774600</td>\n",
       "      <td>0.777300</td>\n",
       "      <td>0.784900</td>\n",
       "      <td>0.016602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.866207</td>\n",
       "      <td>0.885333</td>\n",
       "      <td>0.879121</td>\n",
       "      <td>0.872283</td>\n",
       "      <td>0.882273</td>\n",
       "      <td>0.893840</td>\n",
       "      <td>0.873656</td>\n",
       "      <td>0.870523</td>\n",
       "      <td>0.870654</td>\n",
       "      <td>0.881266</td>\n",
       "      <td>0.877516</td>\n",
       "      <td>0.008387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.836409</td>\n",
       "      <td>0.855734</td>\n",
       "      <td>0.851399</td>\n",
       "      <td>0.841483</td>\n",
       "      <td>0.853213</td>\n",
       "      <td>0.862672</td>\n",
       "      <td>0.840740</td>\n",
       "      <td>0.842508</td>\n",
       "      <td>0.842725</td>\n",
       "      <td>0.848137</td>\n",
       "      <td>0.847502</td>\n",
       "      <td>0.008157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.828802</td>\n",
       "      <td>0.844939</td>\n",
       "      <td>0.844322</td>\n",
       "      <td>0.832617</td>\n",
       "      <td>0.844684</td>\n",
       "      <td>0.852072</td>\n",
       "      <td>0.831447</td>\n",
       "      <td>0.833969</td>\n",
       "      <td>0.836601</td>\n",
       "      <td>0.836467</td>\n",
       "      <td>0.838592</td>\n",
       "      <td>0.007486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.826733</td>\n",
       "      <td>0.846282</td>\n",
       "      <td>0.841442</td>\n",
       "      <td>0.830544</td>\n",
       "      <td>0.842228</td>\n",
       "      <td>0.846744</td>\n",
       "      <td>0.826969</td>\n",
       "      <td>0.836219</td>\n",
       "      <td>0.833164</td>\n",
       "      <td>0.833970</td>\n",
       "      <td>0.836429</td>\n",
       "      <td>0.007432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.658019</td>\n",
       "      <td>0.689966</td>\n",
       "      <td>0.689399</td>\n",
       "      <td>0.665568</td>\n",
       "      <td>0.689800</td>\n",
       "      <td>0.705766</td>\n",
       "      <td>0.664606</td>\n",
       "      <td>0.668267</td>\n",
       "      <td>0.674683</td>\n",
       "      <td>0.673282</td>\n",
       "      <td>0.677936</td>\n",
       "      <td>0.015072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.797300</td>\n",
       "      <td>0.831100</td>\n",
       "      <td>0.807200</td>\n",
       "      <td>0.823500</td>\n",
       "      <td>0.843900</td>\n",
       "      <td>0.822400</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>0.832600</td>\n",
       "      <td>0.806600</td>\n",
       "      <td>0.815550</td>\n",
       "      <td>0.018270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.826733</td>\n",
       "      <td>0.846282</td>\n",
       "      <td>0.841442</td>\n",
       "      <td>0.830544</td>\n",
       "      <td>0.842228</td>\n",
       "      <td>0.846744</td>\n",
       "      <td>0.826969</td>\n",
       "      <td>0.836219</td>\n",
       "      <td>0.833164</td>\n",
       "      <td>0.833970</td>\n",
       "      <td>0.836429</td>\n",
       "      <td>0.007432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.684712    0.733307    0.708160    0.688529   \n",
       "1                    TP  314.000000  332.000000  320.000000  321.000000   \n",
       "2                    TN  184.000000  177.000000  187.000000  180.000000   \n",
       "3                    FP   53.000000   41.000000   50.000000   51.000000   \n",
       "4                    FN   44.000000   45.000000   38.000000   43.000000   \n",
       "5              Accuracy    0.836975    0.855462    0.852101    0.842017   \n",
       "6             Precision    0.855586    0.890080    0.864865    0.862903   \n",
       "7           Sensitivity    0.877095    0.880637    0.893855    0.881868   \n",
       "8           Specificity    0.776400    0.811900    0.789000    0.779200   \n",
       "9              F1 score    0.866207    0.885333    0.879121    0.872283   \n",
       "10  F1 score (weighted)    0.836409    0.855734    0.851399    0.841483   \n",
       "11     F1 score (macro)    0.828802    0.844939    0.844322    0.832617   \n",
       "12    Balanced Accuracy    0.826733    0.846282    0.841442    0.830544   \n",
       "13                  MCC    0.658019    0.689966    0.689399    0.665568   \n",
       "14                  NPV    0.807000    0.797300    0.831100    0.807200   \n",
       "15              ROC_AUC    0.826733    0.846282    0.841442    0.830544   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.708776    0.730538    0.699183    0.691330    0.720596    0.723794   \n",
       "1   326.000000  341.000000  325.000000  316.000000  313.000000  334.000000   \n",
       "2   182.000000  173.000000  176.000000  185.000000  189.000000  171.000000   \n",
       "3    48.000000   49.000000   56.000000   43.000000   55.000000   49.000000   \n",
       "4    39.000000   32.000000   38.000000   51.000000   38.000000   41.000000   \n",
       "5     0.853782    0.863866    0.842017    0.842017    0.843697    0.848739   \n",
       "6     0.871658    0.874359    0.853018    0.880223    0.850543    0.872063   \n",
       "7     0.893151    0.914209    0.895317    0.861035    0.891738    0.890667   \n",
       "8     0.791300    0.779300    0.758600    0.811400    0.774600    0.777300   \n",
       "9     0.882273    0.893840    0.873656    0.870523    0.870654    0.881266   \n",
       "10    0.853213    0.862672    0.840740    0.842508    0.842725    0.848137   \n",
       "11    0.844684    0.852072    0.831447    0.833969    0.836601    0.836467   \n",
       "12    0.842228    0.846744    0.826969    0.836219    0.833164    0.833970   \n",
       "13    0.689800    0.705766    0.664606    0.668267    0.674683    0.673282   \n",
       "14    0.823500    0.843900    0.822400    0.783900    0.832600    0.806600   \n",
       "15    0.842228    0.846744    0.826969    0.836219    0.833164    0.833970   \n",
       "\n",
       "           ave       std  \n",
       "0     0.708892  0.017721  \n",
       "1   324.200000  9.235198  \n",
       "2   180.400000  6.040603  \n",
       "3    49.500000  4.766783  \n",
       "4    40.900000  5.173651  \n",
       "5     0.848067  0.008203  \n",
       "6     0.867530  0.012579  \n",
       "7     0.887957  0.013985  \n",
       "8     0.784900  0.016602  \n",
       "9     0.877516  0.008387  \n",
       "10    0.847502  0.008157  \n",
       "11    0.838592  0.007486  \n",
       "12    0.836429  0.007432  \n",
       "13    0.677936  0.015072  \n",
       "14    0.815550  0.018270  \n",
       "15    0.836429  0.007432  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test['std'] = mat_met_xgb_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_xgb0</th>\n",
       "      <th>y_pred_xgb1</th>\n",
       "      <th>y_pred_xgb2</th>\n",
       "      <th>y_pred_xgb3</th>\n",
       "      <th>y_pred_xgb4</th>\n",
       "      <th>y_pred_xgb_ave</th>\n",
       "      <th>y_pred_xgb_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL356066</td>\n",
       "      <td>0</td>\n",
       "      <td>8.02</td>\n",
       "      <td>6.989207</td>\n",
       "      <td>7.113615</td>\n",
       "      <td>7.074097</td>\n",
       "      <td>7.089230</td>\n",
       "      <td>7.059748</td>\n",
       "      <td>7.224316</td>\n",
       "      <td>0.357897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3652228</td>\n",
       "      <td>1</td>\n",
       "      <td>8.05</td>\n",
       "      <td>8.047163</td>\n",
       "      <td>8.086877</td>\n",
       "      <td>8.001071</td>\n",
       "      <td>8.181213</td>\n",
       "      <td>7.995982</td>\n",
       "      <td>8.060384</td>\n",
       "      <td>0.062249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3939518</td>\n",
       "      <td>2</td>\n",
       "      <td>6.87</td>\n",
       "      <td>7.316361</td>\n",
       "      <td>7.274952</td>\n",
       "      <td>7.437621</td>\n",
       "      <td>7.388706</td>\n",
       "      <td>7.586619</td>\n",
       "      <td>7.312377</td>\n",
       "      <td>0.221297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4281792</td>\n",
       "      <td>3</td>\n",
       "      <td>7.22</td>\n",
       "      <td>8.302556</td>\n",
       "      <td>7.889215</td>\n",
       "      <td>7.464929</td>\n",
       "      <td>8.024971</td>\n",
       "      <td>7.858667</td>\n",
       "      <td>7.793390</td>\n",
       "      <td>0.356601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4070232</td>\n",
       "      <td>4</td>\n",
       "      <td>7.15</td>\n",
       "      <td>6.653418</td>\n",
       "      <td>6.527023</td>\n",
       "      <td>6.503296</td>\n",
       "      <td>6.711822</td>\n",
       "      <td>6.748535</td>\n",
       "      <td>6.715682</td>\n",
       "      <td>0.213816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL4202521</td>\n",
       "      <td>2966</td>\n",
       "      <td>7.43</td>\n",
       "      <td>6.988164</td>\n",
       "      <td>7.099664</td>\n",
       "      <td>6.795776</td>\n",
       "      <td>6.892704</td>\n",
       "      <td>7.030558</td>\n",
       "      <td>7.039478</td>\n",
       "      <td>0.199906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL216641</td>\n",
       "      <td>2967</td>\n",
       "      <td>7.35</td>\n",
       "      <td>7.318423</td>\n",
       "      <td>7.414459</td>\n",
       "      <td>7.453772</td>\n",
       "      <td>7.209116</td>\n",
       "      <td>7.442297</td>\n",
       "      <td>7.364678</td>\n",
       "      <td>0.084651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL3693750</td>\n",
       "      <td>2968</td>\n",
       "      <td>7.43</td>\n",
       "      <td>7.539712</td>\n",
       "      <td>7.512382</td>\n",
       "      <td>7.543885</td>\n",
       "      <td>7.694427</td>\n",
       "      <td>7.559728</td>\n",
       "      <td>7.546689</td>\n",
       "      <td>0.078365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL152665</td>\n",
       "      <td>2969</td>\n",
       "      <td>5.96</td>\n",
       "      <td>6.506430</td>\n",
       "      <td>6.347733</td>\n",
       "      <td>6.382062</td>\n",
       "      <td>6.371231</td>\n",
       "      <td>6.269535</td>\n",
       "      <td>6.306165</td>\n",
       "      <td>0.169804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL3693789</td>\n",
       "      <td>2970</td>\n",
       "      <td>8.52</td>\n",
       "      <td>8.060612</td>\n",
       "      <td>8.083906</td>\n",
       "      <td>8.293570</td>\n",
       "      <td>8.214449</td>\n",
       "      <td>8.158390</td>\n",
       "      <td>8.221821</td>\n",
       "      <td>0.154459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_xgb0  y_pred_xgb1  \\\n",
       "0          CHEMBL356066            0     8.02     6.989207     7.113615   \n",
       "1         CHEMBL3652228            1     8.05     8.047163     8.086877   \n",
       "2         CHEMBL3939518            2     6.87     7.316361     7.274952   \n",
       "3         CHEMBL4281792            3     7.22     8.302556     7.889215   \n",
       "4         CHEMBL4070232            4     7.15     6.653418     6.527023   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "2966      CHEMBL4202521         2966     7.43     6.988164     7.099664   \n",
       "2967       CHEMBL216641         2967     7.35     7.318423     7.414459   \n",
       "2968      CHEMBL3693750         2968     7.43     7.539712     7.512382   \n",
       "2969       CHEMBL152665         2969     5.96     6.506430     6.347733   \n",
       "2970      CHEMBL3693789         2970     8.52     8.060612     8.083906   \n",
       "\n",
       "      y_pred_xgb2  y_pred_xgb3  y_pred_xgb4  y_pred_xgb_ave  y_pred_xgb_std  \n",
       "0        7.074097     7.089230     7.059748        7.224316        0.357897  \n",
       "1        8.001071     8.181213     7.995982        8.060384        0.062249  \n",
       "2        7.437621     7.388706     7.586619        7.312377        0.221297  \n",
       "3        7.464929     8.024971     7.858667        7.793390        0.356601  \n",
       "4        6.503296     6.711822     6.748535        6.715682        0.213816  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "2966     6.795776     6.892704     7.030558        7.039478        0.199906  \n",
       "2967     7.453772     7.209116     7.442297        7.364678        0.084651  \n",
       "2968     7.543885     7.694427     7.559728        7.546689        0.078365  \n",
       "2969     6.382062     6.371231     6.269535        6.306165        0.169804  \n",
       "2970     8.293570     8.214449     8.158390        8.221821        0.154459  \n",
       "\n",
       "[2971 rows x 10 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"rmse\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_xgb_cat = np.where((y_pred_optimized_xgb >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_xgb_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_xgb))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "xgb_run0 = data_xgb[['y_test_idx0', 'y_test0', 'y_pred_xgb0']]\n",
    "xgb_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "xgb_run0.reset_index(inplace=True, drop=True)\n",
    "xgb_run1 = data_xgb[['y_test_idx1', 'y_test1', 'y_pred_xgb1']]\n",
    "xgb_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "xgb_run1.reset_index(inplace=True, drop=True)\n",
    "xgb_run2 = data_xgb[['y_test_idx2', 'y_test2', 'y_pred_xgb2']]\n",
    "xgb_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "xgb_run2.reset_index(inplace=True, drop=True)\n",
    "xgb_run3 = data_xgb[['y_test_idx3', 'y_test3', 'y_pred_xgb3']]\n",
    "xgb_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "xgb_run3.reset_index(inplace=True, drop=True)\n",
    "xgb_run4 = data_xgb[['y_test_idx4', 'y_test4', 'y_pred_xgb4']]\n",
    "xgb_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "xgb_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "xgb_5preds = pd.concat([chembl_id, xgb_run0, xgb_run1, xgb_run2, xgb_run3, xgb_run4], axis=1)\n",
    "xgb_5preds = xgb_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_xgb0', 'y_pred_xgb1', 'y_pred_xgb2', 'y_pred_xgb3', 'y_pred_xgb4']]\n",
    "xgb_5preds['y_pred_xgb_ave'] = xgb_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "xgb_5preds['y_pred_xgb_std'] = xgb_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "xgb_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c2883ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_xgb.to_csv('mat_met_xgb_opt.csv')\n",
    "xgb_5preds.to_csv('xgb_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "02aaad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b19aca7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt  \n",
    "# retrieve performance metrics\n",
    "results = optimized_xgb_0.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "    \n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "pyplot.show()\n",
    "\n",
    " # plot classification error\n",
    "#fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "#ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "#ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "#ax.legend()\n",
    "    \n",
    "#pyplot.ylabel('Classification Error')\n",
    "#pyplot.title('XGBoost Classification Error')\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost baseline model r2_score 0.6689 with a standard deviation of 0.0535\n",
      "XGBoost optimized model r2_score 0.7125 with a standard deviation of 0.0457\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_reg, X, Y, cv=10, scoring=\"r2\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_xgb.joblib']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_reg, \"./xgb_reg.joblib\")\n",
    "#joblib.dump(optimized_xgb, \"./optimized_xgb.joblib\")\n",
    "joblib.dump(optimizedCV_xgb, \"./optimizedCV_xgb.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.640294     0.021893\n",
      "1                    TP       164.200000     6.596295\n",
      "2                    TN        81.700000     4.372896\n",
      "3                    FP        31.700000     5.292552\n",
      "4                    FN        19.500000     3.894440\n",
      "5              Accuracy         0.827670     0.022963\n",
      "6             Precision         0.838339     0.025792\n",
      "7           Sensitivity         0.893677     0.022003\n",
      "8           Specificity         0.721060     0.042036\n",
      "9              F1 score         0.864913     0.019432\n",
      "10  F1 score (weighted)         0.825460     0.023476\n",
      "11     F1 score (macro)         0.813203     0.023870\n",
      "12    Balanced Accuracy         0.807363     0.024384\n",
      "13                  MCC         0.630362     0.047065\n",
      "14                  NPV         0.808180     0.030880\n",
      "15              ROC_AUC         0.807363     0.024384\n",
      "CPU times: user 8.39 s, sys: 0 ns, total: 8.39 s\n",
      "Wall time: 8.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    \n",
    "    knn_reg.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:54:20,676]\u001b[0m A new study created in memory with name: KNNregressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:22,015]\u001b[0m Trial 0 finished with value: 0.5869906984038908 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 85}. Best is trial 0 with value: 0.5869906984038908.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:23,351]\u001b[0m Trial 1 finished with value: 0.6179908483450154 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 81}. Best is trial 1 with value: 0.6179908483450154.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:24,890]\u001b[0m Trial 2 finished with value: 0.5389824541910032 and parameters: {'n_neighbors': 22, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 1 with value: 0.6179908483450154.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:26,734]\u001b[0m Trial 3 finished with value: 0.562188045195297 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 31}. Best is trial 1 with value: 0.6179908483450154.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:28,560]\u001b[0m Trial 4 finished with value: 0.5991519667157357 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 1 with value: 0.6179908483450154.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:29,917]\u001b[0m Trial 5 finished with value: 0.6308733122800059 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 5 with value: 0.6308733122800059.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:31,259]\u001b[0m Trial 6 finished with value: 0.5215687595491295 and parameters: {'n_neighbors': 27, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 69}. Best is trial 5 with value: 0.6308733122800059.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:32,787]\u001b[0m Trial 7 finished with value: 0.5653111690899051 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 36}. Best is trial 5 with value: 0.6308733122800059.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:34,123]\u001b[0m Trial 8 finished with value: 0.6112195958212913 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 5 with value: 0.6308733122800059.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:35,899]\u001b[0m Trial 9 finished with value: 0.5389598740361661 and parameters: {'n_neighbors': 22, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 5 with value: 0.6308733122800059.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:37,440]\u001b[0m Trial 10 finished with value: 0.6439983937389043 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 10 with value: 0.6439983937389043.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:38,976]\u001b[0m Trial 11 finished with value: 0.6439983937389043 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 10 with value: 0.6439983937389043.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:40,508]\u001b[0m Trial 12 finished with value: 0.6439983937389043 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 10 with value: 0.6439983937389043.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:42,050]\u001b[0m Trial 13 finished with value: 0.6214546966136295 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 10 with value: 0.6439983937389043.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:43,585]\u001b[0m Trial 14 finished with value: 0.6113254558774882 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 10 with value: 0.6439983937389043.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:45,119]\u001b[0m Trial 15 finished with value: 0.6394842334610509 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 10 with value: 0.6439983937389043.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:46,457]\u001b[0m Trial 16 finished with value: 0.6446132537477182 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:47,814]\u001b[0m Trial 17 finished with value: 0.6342482869807855 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:49,158]\u001b[0m Trial 18 finished with value: 0.5479765066367545 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:50,509]\u001b[0m Trial 19 finished with value: 0.611460464206063 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:52,381]\u001b[0m Trial 20 finished with value: 0.6213254879347929 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:53,981]\u001b[0m Trial 21 finished with value: 0.6439983937389043 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:55,563]\u001b[0m Trial 22 finished with value: 0.6439983937389043 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:57,140]\u001b[0m Trial 23 finished with value: 0.6429317392641223 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:54:58,776]\u001b[0m Trial 24 finished with value: 0.6329810473810674 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:00,340]\u001b[0m Trial 25 finished with value: 0.6329591628139108 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 63}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:01,893]\u001b[0m Trial 26 finished with value: 0.571903113152381 and parameters: {'n_neighbors': 14, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:03,255]\u001b[0m Trial 27 finished with value: 0.6399910889711313 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:04,858]\u001b[0m Trial 28 finished with value: 0.6439983937389043 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:06,466]\u001b[0m Trial 29 finished with value: 0.5876167591425459 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:08,048]\u001b[0m Trial 30 finished with value: 0.6068862934921548 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:09,641]\u001b[0m Trial 31 finished with value: 0.6439983937389043 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:11,222]\u001b[0m Trial 32 finished with value: 0.6429317392641223 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:12,617]\u001b[0m Trial 33 finished with value: 0.6437152999384443 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:14,195]\u001b[0m Trial 34 finished with value: 0.6083517098403362 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 58}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:55:15,755]\u001b[0m Trial 35 finished with value: 0.6429317392641223 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:17,115]\u001b[0m Trial 36 finished with value: 0.642493805297369 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:18,474]\u001b[0m Trial 37 finished with value: 0.6220756319961891 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 67}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:20,028]\u001b[0m Trial 38 finished with value: 0.6329810473810674 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:21,600]\u001b[0m Trial 39 finished with value: 0.6315247593567788 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 50}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:22,984]\u001b[0m Trial 40 finished with value: 0.6352440681361061 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 80}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:24,355]\u001b[0m Trial 41 finished with value: 0.6446132537477182 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:25,718]\u001b[0m Trial 42 finished with value: 0.6446132537477182 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:27,094]\u001b[0m Trial 43 finished with value: 0.642493805297369 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:28,486]\u001b[0m Trial 44 finished with value: 0.6437152999384443 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:29,878]\u001b[0m Trial 45 finished with value: 0.6342482869807855 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:31,459]\u001b[0m Trial 46 finished with value: 0.6223617093149476 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 65}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:32,816]\u001b[0m Trial 47 finished with value: 0.6437152999384443 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:34,391]\u001b[0m Trial 48 finished with value: 0.6068862934921548 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:35,772]\u001b[0m Trial 49 finished with value: 0.6342482869807855 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6446\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 70\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNregressor\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.615816\n",
      "1                    TP  320.000000\n",
      "2                    TN  169.000000\n",
      "3                    FP   68.000000\n",
      "4                    FN   38.000000\n",
      "5              Accuracy    0.821849\n",
      "6             Precision    0.824742\n",
      "7           Sensitivity    0.893855\n",
      "8           Specificity    0.713100\n",
      "9              F1 score    0.857909\n",
      "10  F1 score (weighted)    0.819412\n",
      "11     F1 score (macro)    0.809585\n",
      "12    Balanced Accuracy    0.803467\n",
      "13                  MCC    0.623816\n",
      "14                  NPV    0.816400\n",
      "15              ROC_AUC    0.803467\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_knn_0)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_0_cat = np.where((y_pred_knn_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_knn_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:55:37,661]\u001b[0m Trial 50 finished with value: 0.5759301947256056 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:39,241]\u001b[0m Trial 51 finished with value: 0.6237878054026407 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:40,806]\u001b[0m Trial 52 finished with value: 0.6209547381491618 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:42,200]\u001b[0m Trial 53 finished with value: 0.6215299968009292 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:43,780]\u001b[0m Trial 54 finished with value: 0.6237878054026407 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:45,592]\u001b[0m Trial 55 finished with value: 0.6234013449445734 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:47,148]\u001b[0m Trial 56 finished with value: 0.5387697887564412 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 54}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:48,497]\u001b[0m Trial 57 finished with value: 0.6240017293380253 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:49,890]\u001b[0m Trial 58 finished with value: 0.6011268851883745 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:51,748]\u001b[0m Trial 59 finished with value: 0.617859088022949 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:53,128]\u001b[0m Trial 60 finished with value: 0.618304092052379 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 79}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:54,701]\u001b[0m Trial 61 finished with value: 0.6237878054026407 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:56,304]\u001b[0m Trial 62 finished with value: 0.6237878054026407 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:57,889]\u001b[0m Trial 63 finished with value: 0.6206289654537734 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:55:59,466]\u001b[0m Trial 64 finished with value: 0.6209547381491618 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:01,031]\u001b[0m Trial 65 finished with value: 0.6241565860418825 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:02,593]\u001b[0m Trial 66 finished with value: 0.6030173414059166 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:04,209]\u001b[0m Trial 67 finished with value: 0.6237878054026407 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:05,590]\u001b[0m Trial 68 finished with value: 0.6215299968009292 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:07,194]\u001b[0m Trial 69 finished with value: 0.5828027734499853 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:08,565]\u001b[0m Trial 70 finished with value: 0.6091376111200917 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 68}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:10,170]\u001b[0m Trial 71 finished with value: 0.6237878054026407 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:11,753]\u001b[0m Trial 72 finished with value: 0.6241565860418825 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:13,355]\u001b[0m Trial 73 finished with value: 0.6241565860418825 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:14,916]\u001b[0m Trial 74 finished with value: 0.6237878054026407 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:16,468]\u001b[0m Trial 75 finished with value: 0.6047170055724038 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 55}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:17,852]\u001b[0m Trial 76 finished with value: 0.6215299968009292 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:19,457]\u001b[0m Trial 77 finished with value: 0.6241565860418825 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:21,061]\u001b[0m Trial 78 finished with value: 0.5871919011377881 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:22,674]\u001b[0m Trial 79 finished with value: 0.614368762818845 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:24,030]\u001b[0m Trial 80 finished with value: 0.6194830286600157 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:25,575]\u001b[0m Trial 81 finished with value: 0.6241565860418825 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:27,137]\u001b[0m Trial 82 finished with value: 0.6206289654537734 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:28,710]\u001b[0m Trial 83 finished with value: 0.6237878054026407 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:30,088]\u001b[0m Trial 84 finished with value: 0.6240017293380253 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:56:31,645]\u001b[0m Trial 85 finished with value: 0.6175723900457786 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 65}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:33,216]\u001b[0m Trial 86 finished with value: 0.6209547381491618 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:34,602]\u001b[0m Trial 87 finished with value: 0.6127408630798923 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 69}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:36,164]\u001b[0m Trial 88 finished with value: 0.6241565860418825 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:37,720]\u001b[0m Trial 89 finished with value: 0.6237878054026407 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:39,320]\u001b[0m Trial 90 finished with value: 0.6206289654537734 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:40,914]\u001b[0m Trial 91 finished with value: 0.6237878054026407 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:42,467]\u001b[0m Trial 92 finished with value: 0.6241565860418825 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:44,035]\u001b[0m Trial 93 finished with value: 0.6237878054026407 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:45,611]\u001b[0m Trial 94 finished with value: 0.6241565860418825 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:47,205]\u001b[0m Trial 95 finished with value: 0.5597435150712157 and parameters: {'n_neighbors': 16, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:48,774]\u001b[0m Trial 96 finished with value: 0.6168610298380939 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:50,178]\u001b[0m Trial 97 finished with value: 0.6215299968009292 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:51,743]\u001b[0m Trial 98 finished with value: 0.6237878054026407 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:53,121]\u001b[0m Trial 99 finished with value: 0.6127408630798923 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6446\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 70\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.615816    0.672816\n",
      "1                    TP  320.000000  335.000000\n",
      "2                    TN  169.000000  170.000000\n",
      "3                    FP   68.000000   48.000000\n",
      "4                    FN   38.000000   42.000000\n",
      "5              Accuracy    0.821849    0.848739\n",
      "6             Precision    0.824742    0.874674\n",
      "7           Sensitivity    0.893855    0.888594\n",
      "8           Specificity    0.713100    0.779800\n",
      "9              F1 score    0.857909    0.881579\n",
      "10  F1 score (weighted)    0.819412    0.848281\n",
      "11     F1 score (macro)    0.809585    0.836138\n",
      "12    Balanced Accuracy    0.803467    0.834205\n",
      "13                  MCC    0.623816    0.672473\n",
      "14                  NPV    0.816400    0.801900\n",
      "15              ROC_AUC    0.803467    0.834205\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_knn_1)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_1_cat = np.where((y_pred_knn_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_knn_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:56:55,049]\u001b[0m Trial 100 finished with value: 0.6101141020133378 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:56,452]\u001b[0m Trial 101 finished with value: 0.6371165818503824 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:58,071]\u001b[0m Trial 102 finished with value: 0.6417335768807011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:56:59,464]\u001b[0m Trial 103 finished with value: 0.6429123920293375 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:01,092]\u001b[0m Trial 104 finished with value: 0.6376222563598113 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:02,492]\u001b[0m Trial 105 finished with value: 0.6279387005584595 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 92}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:04,098]\u001b[0m Trial 106 finished with value: 0.6417335768807011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:05,752]\u001b[0m Trial 107 finished with value: 0.6263234946047137 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:07,145]\u001b[0m Trial 108 finished with value: 0.6171286965686871 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:08,511]\u001b[0m Trial 109 finished with value: 0.5671812389869264 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:09,900]\u001b[0m Trial 110 finished with value: 0.6231884187389899 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:11,303]\u001b[0m Trial 111 finished with value: 0.6429123920293375 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:12,752]\u001b[0m Trial 112 finished with value: 0.6371165818503824 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:14,371]\u001b[0m Trial 113 finished with value: 0.6305141037356369 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:16,006]\u001b[0m Trial 114 finished with value: 0.6417335768807011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:17,594]\u001b[0m Trial 115 finished with value: 0.6376222563598113 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:19,183]\u001b[0m Trial 116 finished with value: 0.5811549116439545 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:20,575]\u001b[0m Trial 117 finished with value: 0.6312591818138305 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:22,181]\u001b[0m Trial 118 finished with value: 0.6358301854348055 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 50}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:23,622]\u001b[0m Trial 119 finished with value: 0.6371165818503824 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:25,000]\u001b[0m Trial 120 finished with value: 0.6371444571070956 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 88}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:26,365]\u001b[0m Trial 121 finished with value: 0.6371165818503824 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:27,718]\u001b[0m Trial 122 finished with value: 0.6429123920293375 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:29,337]\u001b[0m Trial 123 finished with value: 0.6305141037356369 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:30,714]\u001b[0m Trial 124 finished with value: 0.6371165818503824 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:32,315]\u001b[0m Trial 125 finished with value: 0.6263234946047137 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:33,742]\u001b[0m Trial 126 finished with value: 0.6429123920293375 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:35,151]\u001b[0m Trial 127 finished with value: 0.605977709811125 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:36,778]\u001b[0m Trial 128 finished with value: 0.6376222563598113 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:38,220]\u001b[0m Trial 129 finished with value: 0.6429123920293375 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:39,821]\u001b[0m Trial 130 finished with value: 0.6376222563598113 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:41,463]\u001b[0m Trial 131 finished with value: 0.6417335768807011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:43,098]\u001b[0m Trial 132 finished with value: 0.574664496748618 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:44,778]\u001b[0m Trial 133 finished with value: 0.6305141037356369 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:46,421]\u001b[0m Trial 134 finished with value: 0.6417335768807011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:57:48,020]\u001b[0m Trial 135 finished with value: 0.6305141037356369 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:49,622]\u001b[0m Trial 136 finished with value: 0.6376222563598113 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:50,971]\u001b[0m Trial 137 finished with value: 0.6371165818503824 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:52,613]\u001b[0m Trial 138 finished with value: 0.586907298101206 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:54,198]\u001b[0m Trial 139 finished with value: 0.6358301854348055 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:55,789]\u001b[0m Trial 140 finished with value: 0.6417335768807011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:57,400]\u001b[0m Trial 141 finished with value: 0.6305141037356369 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:57:58,997]\u001b[0m Trial 142 finished with value: 0.6263234946047137 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:00,614]\u001b[0m Trial 143 finished with value: 0.6376222563598113 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:02,267]\u001b[0m Trial 144 finished with value: 0.6376222563598113 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:03,872]\u001b[0m Trial 145 finished with value: 0.6417335768807011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:05,232]\u001b[0m Trial 146 finished with value: 0.5070271885710298 and parameters: {'n_neighbors': 29, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 67}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:06,854]\u001b[0m Trial 147 finished with value: 0.5965027511511185 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:08,237]\u001b[0m Trial 148 finished with value: 0.6312591818138305 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:09,847]\u001b[0m Trial 149 finished with value: 0.6417335768807011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6446\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 70\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.615816    0.672816    0.669154\n",
      "1                    TP  320.000000  335.000000  329.000000\n",
      "2                    TN  169.000000  170.000000  175.000000\n",
      "3                    FP   68.000000   48.000000   62.000000\n",
      "4                    FN   38.000000   42.000000   29.000000\n",
      "5              Accuracy    0.821849    0.848739    0.847059\n",
      "6             Precision    0.824742    0.874674    0.841432\n",
      "7           Sensitivity    0.893855    0.888594    0.918994\n",
      "8           Specificity    0.713100    0.779800    0.738400\n",
      "9              F1 score    0.857909    0.881579    0.878505\n",
      "10  F1 score (weighted)    0.819412    0.848281    0.844706\n",
      "11     F1 score (macro)    0.809585    0.836138    0.836078\n",
      "12    Balanced Accuracy    0.803467    0.834205    0.828696\n",
      "13                  MCC    0.623816    0.672473    0.678010\n",
      "14                  NPV    0.816400    0.801900    0.857800\n",
      "15              ROC_AUC    0.803467    0.834205    0.828696\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_knn_2)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_2_cat = np.where((y_pred_knn_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_knn_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:58:11,792]\u001b[0m Trial 150 finished with value: 0.6279865774112976 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:13,443]\u001b[0m Trial 151 finished with value: 0.6279865774112976 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:15,041]\u001b[0m Trial 152 finished with value: 0.6250579214019312 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:16,665]\u001b[0m Trial 153 finished with value: 0.6239541272763166 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:18,277]\u001b[0m Trial 154 finished with value: 0.6255758619799033 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:19,669]\u001b[0m Trial 155 finished with value: 0.6253861096575535 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:21,046]\u001b[0m Trial 156 finished with value: 0.6237021990310142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:22,638]\u001b[0m Trial 157 finished with value: 0.6279865774112976 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:24,056]\u001b[0m Trial 158 finished with value: 0.6244967819166509 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:25,650]\u001b[0m Trial 159 finished with value: 0.6239541272763166 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:27,274]\u001b[0m Trial 160 finished with value: 0.6279865774112976 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:28,680]\u001b[0m Trial 161 finished with value: 0.6237021990310142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:30,129]\u001b[0m Trial 162 finished with value: 0.6237021990310142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:31,548]\u001b[0m Trial 163 finished with value: 0.6281445198185436 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:32,925]\u001b[0m Trial 164 finished with value: 0.6237021990310142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:34,322]\u001b[0m Trial 165 finished with value: 0.6281445198185436 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:35,710]\u001b[0m Trial 166 finished with value: 0.6178108870843856 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 84}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:37,321]\u001b[0m Trial 167 finished with value: 0.6250579214019312 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:38,693]\u001b[0m Trial 168 finished with value: 0.6135682166688554 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:40,076]\u001b[0m Trial 169 finished with value: 0.6178108870843856 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 72}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:41,448]\u001b[0m Trial 170 finished with value: 0.6281445198185436 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:42,838]\u001b[0m Trial 171 finished with value: 0.6237021990310142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:44,255]\u001b[0m Trial 172 finished with value: 0.6237021990310142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:45,878]\u001b[0m Trial 173 finished with value: 0.6239541272763166 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:47,276]\u001b[0m Trial 174 finished with value: 0.6281445198185436 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:48,897]\u001b[0m Trial 175 finished with value: 0.6239541272763166 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:50,272]\u001b[0m Trial 176 finished with value: 0.6244967819166509 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:51,640]\u001b[0m Trial 177 finished with value: 0.6281445198185436 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:53,226]\u001b[0m Trial 178 finished with value: 0.6239541272763166 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:54,594]\u001b[0m Trial 179 finished with value: 0.6244967819166509 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:56,008]\u001b[0m Trial 180 finished with value: 0.6281445198185436 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:57,591]\u001b[0m Trial 181 finished with value: 0.6239541272763166 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:58:59,013]\u001b[0m Trial 182 finished with value: 0.6237021990310142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:00,403]\u001b[0m Trial 183 finished with value: 0.6281445198185436 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:01,763]\u001b[0m Trial 184 finished with value: 0.6237021990310142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:59:03,119]\u001b[0m Trial 185 finished with value: 0.6237021990310142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:04,750]\u001b[0m Trial 186 finished with value: 0.6279865774112976 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:06,385]\u001b[0m Trial 187 finished with value: 0.6239541272763166 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:08,002]\u001b[0m Trial 188 finished with value: 0.6279865774112976 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:09,591]\u001b[0m Trial 189 finished with value: 0.6123871960144609 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:10,982]\u001b[0m Trial 190 finished with value: 0.6244967819166509 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:12,357]\u001b[0m Trial 191 finished with value: 0.6237021990310142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:13,743]\u001b[0m Trial 192 finished with value: 0.6237021990310142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:15,126]\u001b[0m Trial 193 finished with value: 0.6281445198185436 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:16,508]\u001b[0m Trial 194 finished with value: 0.6237021990310142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:17,904]\u001b[0m Trial 195 finished with value: 0.6237021990310142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:19,514]\u001b[0m Trial 196 finished with value: 0.6279865774112976 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:21,451]\u001b[0m Trial 197 finished with value: 0.611917286069725 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:22,878]\u001b[0m Trial 198 finished with value: 0.6157880428321757 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:24,263]\u001b[0m Trial 199 finished with value: 0.6237021990310142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6446\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 70\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.615816    0.672816    0.669154    0.659064\n",
      "1                    TP  320.000000  335.000000  329.000000  328.000000\n",
      "2                    TN  169.000000  170.000000  175.000000  175.000000\n",
      "3                    FP   68.000000   48.000000   62.000000   56.000000\n",
      "4                    FN   38.000000   42.000000   29.000000   36.000000\n",
      "5              Accuracy    0.821849    0.848739    0.847059    0.845378\n",
      "6             Precision    0.824742    0.874674    0.841432    0.854167\n",
      "7           Sensitivity    0.893855    0.888594    0.918994    0.901099\n",
      "8           Specificity    0.713100    0.779800    0.738400    0.757600\n",
      "9              F1 score    0.857909    0.881579    0.878505    0.877005\n",
      "10  F1 score (weighted)    0.819412    0.848281    0.844706    0.843947\n",
      "11     F1 score (macro)    0.809585    0.836138    0.836078    0.834430\n",
      "12    Balanced Accuracy    0.803467    0.834205    0.828696    0.829337\n",
      "13                  MCC    0.623816    0.672473    0.678010    0.670997\n",
      "14                  NPV    0.816400    0.801900    0.857800    0.829400\n",
      "15              ROC_AUC    0.803467    0.834205    0.828696    0.829337\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_knn_3)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_3_cat = np.where((y_pred_knn_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_knn_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 04:59:26,213]\u001b[0m Trial 200 finished with value: 0.6322254279477072 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:27,570]\u001b[0m Trial 201 finished with value: 0.6330424831675278 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:28,925]\u001b[0m Trial 202 finished with value: 0.6330424831675278 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:30,587]\u001b[0m Trial 203 finished with value: 0.6322254279477072 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:32,193]\u001b[0m Trial 204 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:33,590]\u001b[0m Trial 205 finished with value: 0.6281493268831124 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 84}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:35,207]\u001b[0m Trial 206 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:36,557]\u001b[0m Trial 207 finished with value: 0.6346302242828952 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:37,970]\u001b[0m Trial 208 finished with value: 0.635701512678896 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:39,598]\u001b[0m Trial 209 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:40,988]\u001b[0m Trial 210 finished with value: 0.6346302242828952 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:42,409]\u001b[0m Trial 211 finished with value: 0.635701512678896 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:44,351]\u001b[0m Trial 212 finished with value: 0.6329670739262505 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:45,723]\u001b[0m Trial 213 finished with value: 0.6346302242828952 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:47,098]\u001b[0m Trial 214 finished with value: 0.6330424831675278 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:48,482]\u001b[0m Trial 215 finished with value: 0.6330424831675278 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:49,886]\u001b[0m Trial 216 finished with value: 0.635701512678896 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:51,483]\u001b[0m Trial 217 finished with value: 0.6343872333859615 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:52,907]\u001b[0m Trial 218 finished with value: 0.6346302242828952 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:54,513]\u001b[0m Trial 219 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:56,120]\u001b[0m Trial 220 finished with value: 0.6322254279477072 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:57,561]\u001b[0m Trial 221 finished with value: 0.6342021029175122 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 04:59:58,920]\u001b[0m Trial 222 finished with value: 0.6342021029175122 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:00,300]\u001b[0m Trial 223 finished with value: 0.635701512678896 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:01,689]\u001b[0m Trial 224 finished with value: 0.6330424831675278 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:03,060]\u001b[0m Trial 225 finished with value: 0.6297498350886923 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:04,693]\u001b[0m Trial 226 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:06,307]\u001b[0m Trial 227 finished with value: 0.6158599542785129 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:07,923]\u001b[0m Trial 228 finished with value: 0.6129408207324272 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:09,591]\u001b[0m Trial 229 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:10,988]\u001b[0m Trial 230 finished with value: 0.6270359053808437 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 70}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:12,582]\u001b[0m Trial 231 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:14,203]\u001b[0m Trial 232 finished with value: 0.6322254279477072 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:15,770]\u001b[0m Trial 233 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:17,410]\u001b[0m Trial 234 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:00:19,054]\u001b[0m Trial 235 finished with value: 0.6322254279477072 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:20,697]\u001b[0m Trial 236 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:22,090]\u001b[0m Trial 237 finished with value: 0.6346302242828952 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:23,463]\u001b[0m Trial 238 finished with value: 0.6330424831675278 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:25,066]\u001b[0m Trial 239 finished with value: 0.625530691910748 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 62}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:26,695]\u001b[0m Trial 240 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:28,330]\u001b[0m Trial 241 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:29,952]\u001b[0m Trial 242 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:31,582]\u001b[0m Trial 243 finished with value: 0.6322254279477072 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:33,182]\u001b[0m Trial 244 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:34,778]\u001b[0m Trial 245 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:36,428]\u001b[0m Trial 246 finished with value: 0.6302781990087154 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:38,041]\u001b[0m Trial 247 finished with value: 0.6341974135578111 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:39,657]\u001b[0m Trial 248 finished with value: 0.6322254279477072 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:41,284]\u001b[0m Trial 249 finished with value: 0.6336596789331118 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6446\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 70\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.615816    0.672816    0.669154    0.659064   \n",
      "1                    TP  320.000000  335.000000  329.000000  328.000000   \n",
      "2                    TN  169.000000  170.000000  175.000000  175.000000   \n",
      "3                    FP   68.000000   48.000000   62.000000   56.000000   \n",
      "4                    FN   38.000000   42.000000   29.000000   36.000000   \n",
      "5              Accuracy    0.821849    0.848739    0.847059    0.845378   \n",
      "6             Precision    0.824742    0.874674    0.841432    0.854167   \n",
      "7           Sensitivity    0.893855    0.888594    0.918994    0.901099   \n",
      "8           Specificity    0.713100    0.779800    0.738400    0.757600   \n",
      "9              F1 score    0.857909    0.881579    0.878505    0.877005   \n",
      "10  F1 score (weighted)    0.819412    0.848281    0.844706    0.843947   \n",
      "11     F1 score (macro)    0.809585    0.836138    0.836078    0.834430   \n",
      "12    Balanced Accuracy    0.803467    0.834205    0.828696    0.829337   \n",
      "13                  MCC    0.623816    0.672473    0.678010    0.670997   \n",
      "14                  NPV    0.816400    0.801900    0.857800    0.829400   \n",
      "15              ROC_AUC    0.803467    0.834205    0.828696    0.829337   \n",
      "\n",
      "          Set4  \n",
      "0     0.689168  \n",
      "1   334.000000  \n",
      "2   178.000000  \n",
      "3    52.000000  \n",
      "4    31.000000  \n",
      "5     0.860504  \n",
      "6     0.865285  \n",
      "7     0.915068  \n",
      "8     0.773900  \n",
      "9     0.889481  \n",
      "10    0.859118  \n",
      "11    0.850207  \n",
      "12    0.844491  \n",
      "13    0.702831  \n",
      "14    0.851700  \n",
      "15    0.844491  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_knn_4)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_4_cat = np.where((y_pred_knn_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_knn_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:00:43,198]\u001b[0m Trial 250 finished with value: 0.6167672984103204 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:44,778]\u001b[0m Trial 251 finished with value: 0.6164411932380309 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:46,159]\u001b[0m Trial 252 finished with value: 0.6184994673635387 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:47,516]\u001b[0m Trial 253 finished with value: 0.6174163479752133 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:49,144]\u001b[0m Trial 254 finished with value: 0.6149674106919938 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:50,502]\u001b[0m Trial 255 finished with value: 0.6184994673635387 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:52,096]\u001b[0m Trial 256 finished with value: 0.6059934611195694 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:53,725]\u001b[0m Trial 257 finished with value: 0.6167672984103204 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:55,308]\u001b[0m Trial 258 finished with value: 0.6164411932380309 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:56,935]\u001b[0m Trial 259 finished with value: 0.6178005580230208 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:58,546]\u001b[0m Trial 260 finished with value: 0.6091091791216667 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 66}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:00:59,929]\u001b[0m Trial 261 finished with value: 0.6174163479752133 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:01,571]\u001b[0m Trial 262 finished with value: 0.6164411932380309 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:02,976]\u001b[0m Trial 263 finished with value: 0.6162228947992856 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:04,362]\u001b[0m Trial 264 finished with value: 0.6184994673635387 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:05,959]\u001b[0m Trial 265 finished with value: 0.6091091791216667 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 56}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:07,545]\u001b[0m Trial 266 finished with value: 0.6164411932380309 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:08,964]\u001b[0m Trial 267 finished with value: 0.6171589971772194 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:10,568]\u001b[0m Trial 268 finished with value: 0.5736129095747285 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:12,187]\u001b[0m Trial 269 finished with value: 0.6176173124560151 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:13,814]\u001b[0m Trial 270 finished with value: 0.6167672984103204 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:15,428]\u001b[0m Trial 271 finished with value: 0.6164411932380309 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:17,021]\u001b[0m Trial 272 finished with value: 0.6131120882571596 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:18,434]\u001b[0m Trial 273 finished with value: 0.6184994673635387 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:19,833]\u001b[0m Trial 274 finished with value: 0.5977174179010176 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:21,429]\u001b[0m Trial 275 finished with value: 0.6167672984103204 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:23,050]\u001b[0m Trial 276 finished with value: 0.6164411932380309 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:24,655]\u001b[0m Trial 277 finished with value: 0.6167672984103204 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:26,092]\u001b[0m Trial 278 finished with value: 0.6162228947992856 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:27,465]\u001b[0m Trial 279 finished with value: 0.6184994673635387 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:28,828]\u001b[0m Trial 280 finished with value: 0.6174163479752133 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:30,440]\u001b[0m Trial 281 finished with value: 0.5869362873023725 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:31,815]\u001b[0m Trial 282 finished with value: 0.6139045336961017 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 67}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:33,451]\u001b[0m Trial 283 finished with value: 0.6164411932380309 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:35,108]\u001b[0m Trial 284 finished with value: 0.6167672984103204 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:01:36,701]\u001b[0m Trial 285 finished with value: 0.6164411932380309 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:38,315]\u001b[0m Trial 286 finished with value: 0.6149674106919938 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:39,675]\u001b[0m Trial 287 finished with value: 0.6096995196639606 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 71}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:41,273]\u001b[0m Trial 288 finished with value: 0.6164411932380309 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:42,644]\u001b[0m Trial 289 finished with value: 0.6174163479752133 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:44,229]\u001b[0m Trial 290 finished with value: 0.6164411932380309 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:45,590]\u001b[0m Trial 291 finished with value: 0.6184994673635387 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:47,182]\u001b[0m Trial 292 finished with value: 0.6149674106919938 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:48,583]\u001b[0m Trial 293 finished with value: 0.6174163479752133 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:50,204]\u001b[0m Trial 294 finished with value: 0.601021420658155 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:51,575]\u001b[0m Trial 295 finished with value: 0.6171589971772194 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:52,964]\u001b[0m Trial 296 finished with value: 0.6184994673635387 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:54,383]\u001b[0m Trial 297 finished with value: 0.6184994673635387 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:55,788]\u001b[0m Trial 298 finished with value: 0.6174163479752133 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:01:57,162]\u001b[0m Trial 299 finished with value: 0.6162228947992856 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6446\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 70\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.615816    0.672816    0.669154    0.659064   \n",
      "1                    TP  320.000000  335.000000  329.000000  328.000000   \n",
      "2                    TN  169.000000  170.000000  175.000000  175.000000   \n",
      "3                    FP   68.000000   48.000000   62.000000   56.000000   \n",
      "4                    FN   38.000000   42.000000   29.000000   36.000000   \n",
      "5              Accuracy    0.821849    0.848739    0.847059    0.845378   \n",
      "6             Precision    0.824742    0.874674    0.841432    0.854167   \n",
      "7           Sensitivity    0.893855    0.888594    0.918994    0.901099   \n",
      "8           Specificity    0.713100    0.779800    0.738400    0.757600   \n",
      "9              F1 score    0.857909    0.881579    0.878505    0.877005   \n",
      "10  F1 score (weighted)    0.819412    0.848281    0.844706    0.843947   \n",
      "11     F1 score (macro)    0.809585    0.836138    0.836078    0.834430   \n",
      "12    Balanced Accuracy    0.803467    0.834205    0.828696    0.829337   \n",
      "13                  MCC    0.623816    0.672473    0.678010    0.670997   \n",
      "14                  NPV    0.816400    0.801900    0.857800    0.829400   \n",
      "15              ROC_AUC    0.803467    0.834205    0.828696    0.829337   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.689168    0.651150  \n",
      "1   334.000000  342.000000  \n",
      "2   178.000000  170.000000  \n",
      "3    52.000000   52.000000  \n",
      "4    31.000000   31.000000  \n",
      "5     0.860504    0.860504  \n",
      "6     0.865285    0.868020  \n",
      "7     0.915068    0.916890  \n",
      "8     0.773900    0.765800  \n",
      "9     0.889481    0.891786  \n",
      "10    0.859118    0.858951  \n",
      "11    0.850207    0.847784  \n",
      "12    0.844491    0.841328  \n",
      "13    0.702831    0.698050  \n",
      "14    0.851700    0.845800  \n",
      "15    0.844491    0.841328  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_knn_5)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_5_cat = np.where((y_pred_knn_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_knn_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:01:59,105]\u001b[0m Trial 300 finished with value: 0.6099400473007227 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:00,708]\u001b[0m Trial 301 finished with value: 0.638408971741637 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:02,363]\u001b[0m Trial 302 finished with value: 0.638408971741637 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:03,989]\u001b[0m Trial 303 finished with value: 0.6362385289477439 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:05,376]\u001b[0m Trial 304 finished with value: 0.6331729157671993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 88}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:07,008]\u001b[0m Trial 305 finished with value: 0.6362385289477439 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:08,577]\u001b[0m Trial 306 finished with value: 0.638408971741637 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:09,956]\u001b[0m Trial 307 finished with value: 0.6362231459196036 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:11,550]\u001b[0m Trial 308 finished with value: 0.6331336889437795 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:13,134]\u001b[0m Trial 309 finished with value: 0.6214517231976849 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 62}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:14,492]\u001b[0m Trial 310 finished with value: 0.62909321846613 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:15,872]\u001b[0m Trial 311 finished with value: 0.6386458005664057 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:17,244]\u001b[0m Trial 312 finished with value: 0.6386458005664057 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:19,176]\u001b[0m Trial 313 finished with value: 0.6151722699216602 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:20,826]\u001b[0m Trial 314 finished with value: 0.6362385289477439 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:22,201]\u001b[0m Trial 315 finished with value: 0.6386458005664057 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:23,808]\u001b[0m Trial 316 finished with value: 0.6166708723807384 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:25,401]\u001b[0m Trial 317 finished with value: 0.6325061040440622 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:26,809]\u001b[0m Trial 318 finished with value: 0.6386458005664057 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:28,187]\u001b[0m Trial 319 finished with value: 0.6386458005664057 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:29,577]\u001b[0m Trial 320 finished with value: 0.6362231459196036 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:30,953]\u001b[0m Trial 321 finished with value: 0.5996077657866461 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:32,563]\u001b[0m Trial 322 finished with value: 0.6362385289477439 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:34,159]\u001b[0m Trial 323 finished with value: 0.623190008032794 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:35,794]\u001b[0m Trial 324 finished with value: 0.638408971741637 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:37,162]\u001b[0m Trial 325 finished with value: 0.6331520684770648 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:38,552]\u001b[0m Trial 326 finished with value: 0.6341267737790036 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:40,130]\u001b[0m Trial 327 finished with value: 0.6274605940253093 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 66}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:41,717]\u001b[0m Trial 328 finished with value: 0.638408971741637 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:43,346]\u001b[0m Trial 329 finished with value: 0.5905934276979 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:44,937]\u001b[0m Trial 330 finished with value: 0.638408971741637 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:46,321]\u001b[0m Trial 331 finished with value: 0.6362231459196036 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:47,676]\u001b[0m Trial 332 finished with value: 0.6331729157671993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 76}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:49,286]\u001b[0m Trial 333 finished with value: 0.6076776464322341 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:50,925]\u001b[0m Trial 334 finished with value: 0.6362385289477439 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:02:52,325]\u001b[0m Trial 335 finished with value: 0.577847566322659 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:53,682]\u001b[0m Trial 336 finished with value: 0.6386458005664057 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:55,283]\u001b[0m Trial 337 finished with value: 0.638408971741637 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:56,920]\u001b[0m Trial 338 finished with value: 0.6362385289477439 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:58,276]\u001b[0m Trial 339 finished with value: 0.6386458005664057 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:02:59,912]\u001b[0m Trial 340 finished with value: 0.6325061040440622 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:01,523]\u001b[0m Trial 341 finished with value: 0.6362385289477439 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:02,901]\u001b[0m Trial 342 finished with value: 0.6386458005664057 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:04,316]\u001b[0m Trial 343 finished with value: 0.6362231459196036 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:05,723]\u001b[0m Trial 344 finished with value: 0.62909321846613 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:07,326]\u001b[0m Trial 345 finished with value: 0.638408971741637 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:08,738]\u001b[0m Trial 346 finished with value: 0.6386458005664057 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:10,149]\u001b[0m Trial 347 finished with value: 0.6362231459196036 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:11,785]\u001b[0m Trial 348 finished with value: 0.6325061040440622 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:13,167]\u001b[0m Trial 349 finished with value: 0.5634655463610566 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 67}. Best is trial 16 with value: 0.6446132537477182.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6446\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 70\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.615816    0.672816    0.669154    0.659064   \n",
      "1                    TP  320.000000  335.000000  329.000000  328.000000   \n",
      "2                    TN  169.000000  170.000000  175.000000  175.000000   \n",
      "3                    FP   68.000000   48.000000   62.000000   56.000000   \n",
      "4                    FN   38.000000   42.000000   29.000000   36.000000   \n",
      "5              Accuracy    0.821849    0.848739    0.847059    0.845378   \n",
      "6             Precision    0.824742    0.874674    0.841432    0.854167   \n",
      "7           Sensitivity    0.893855    0.888594    0.918994    0.901099   \n",
      "8           Specificity    0.713100    0.779800    0.738400    0.757600   \n",
      "9              F1 score    0.857909    0.881579    0.878505    0.877005   \n",
      "10  F1 score (weighted)    0.819412    0.848281    0.844706    0.843947   \n",
      "11     F1 score (macro)    0.809585    0.836138    0.836078    0.834430   \n",
      "12    Balanced Accuracy    0.803467    0.834205    0.828696    0.829337   \n",
      "13                  MCC    0.623816    0.672473    0.678010    0.670997   \n",
      "14                  NPV    0.816400    0.801900    0.857800    0.829400   \n",
      "15              ROC_AUC    0.803467    0.834205    0.828696    0.829337   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.689168    0.651150    0.650946  \n",
      "1   334.000000  342.000000  329.000000  \n",
      "2   178.000000  170.000000  159.000000  \n",
      "3    52.000000   52.000000   73.000000  \n",
      "4    31.000000   31.000000   34.000000  \n",
      "5     0.860504    0.860504    0.820168  \n",
      "6     0.865285    0.868020    0.818408  \n",
      "7     0.915068    0.916890    0.906336  \n",
      "8     0.773900    0.765800    0.685300  \n",
      "9     0.889481    0.891786    0.860131  \n",
      "10    0.859118    0.858951    0.816501  \n",
      "11    0.850207    0.847784    0.804183  \n",
      "12    0.844491    0.841328    0.795840  \n",
      "13    0.702831    0.698050    0.616443  \n",
      "14    0.851700    0.845800    0.823800  \n",
      "15    0.844491    0.841328    0.795840  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_knn_6)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_6_cat = np.where((y_pred_knn_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_knn_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:03:15,135]\u001b[0m Trial 350 finished with value: 0.6464883939383655 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:16,783]\u001b[0m Trial 351 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:18,419]\u001b[0m Trial 352 finished with value: 0.6400041966081982 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:20,067]\u001b[0m Trial 353 finished with value: 0.609660380468139 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:21,649]\u001b[0m Trial 354 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:23,301]\u001b[0m Trial 355 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:24,897]\u001b[0m Trial 356 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:26,535]\u001b[0m Trial 357 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:28,132]\u001b[0m Trial 358 finished with value: 0.6354095915239961 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:29,776]\u001b[0m Trial 359 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:31,408]\u001b[0m Trial 360 finished with value: 0.6426590402814019 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:33,074]\u001b[0m Trial 361 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:34,714]\u001b[0m Trial 362 finished with value: 0.6426590402814019 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:36,324]\u001b[0m Trial 363 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:37,912]\u001b[0m Trial 364 finished with value: 0.6426590402814019 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:39,603]\u001b[0m Trial 365 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:41,247]\u001b[0m Trial 366 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:42,887]\u001b[0m Trial 367 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:44,544]\u001b[0m Trial 368 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:46,178]\u001b[0m Trial 369 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:47,857]\u001b[0m Trial 370 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:49,484]\u001b[0m Trial 371 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:51,072]\u001b[0m Trial 372 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:52,647]\u001b[0m Trial 373 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:54,243]\u001b[0m Trial 374 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:55,863]\u001b[0m Trial 375 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:57,493]\u001b[0m Trial 376 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:03:59,065]\u001b[0m Trial 377 finished with value: 0.6426590402814019 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:00,658]\u001b[0m Trial 378 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:02,257]\u001b[0m Trial 379 finished with value: 0.6426590402814019 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:03,855]\u001b[0m Trial 380 finished with value: 0.6426590402814019 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:05,475]\u001b[0m Trial 381 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:07,097]\u001b[0m Trial 382 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:08,729]\u001b[0m Trial 383 finished with value: 0.6426590402814019 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:10,357]\u001b[0m Trial 384 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:04:11,987]\u001b[0m Trial 385 finished with value: 0.6426590402814019 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:13,606]\u001b[0m Trial 386 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:15,201]\u001b[0m Trial 387 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:16,821]\u001b[0m Trial 388 finished with value: 0.6426590402814019 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:18,443]\u001b[0m Trial 389 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:20,051]\u001b[0m Trial 390 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:21,700]\u001b[0m Trial 391 finished with value: 0.6426590402814019 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:23,321]\u001b[0m Trial 392 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:24,913]\u001b[0m Trial 393 finished with value: 0.61893896622732 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:26,565]\u001b[0m Trial 394 finished with value: 0.6299305166046812 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:28,210]\u001b[0m Trial 395 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:29,833]\u001b[0m Trial 396 finished with value: 0.6426590402814019 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:31,423]\u001b[0m Trial 397 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:33,047]\u001b[0m Trial 398 finished with value: 0.6452520782344424 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:34,704]\u001b[0m Trial 399 finished with value: 0.6299305166046812 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6465\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 55\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.615816    0.672816    0.669154    0.659064   \n",
      "1                    TP  320.000000  335.000000  329.000000  328.000000   \n",
      "2                    TN  169.000000  170.000000  175.000000  175.000000   \n",
      "3                    FP   68.000000   48.000000   62.000000   56.000000   \n",
      "4                    FN   38.000000   42.000000   29.000000   36.000000   \n",
      "5              Accuracy    0.821849    0.848739    0.847059    0.845378   \n",
      "6             Precision    0.824742    0.874674    0.841432    0.854167   \n",
      "7           Sensitivity    0.893855    0.888594    0.918994    0.901099   \n",
      "8           Specificity    0.713100    0.779800    0.738400    0.757600   \n",
      "9              F1 score    0.857909    0.881579    0.878505    0.877005   \n",
      "10  F1 score (weighted)    0.819412    0.848281    0.844706    0.843947   \n",
      "11     F1 score (macro)    0.809585    0.836138    0.836078    0.834430   \n",
      "12    Balanced Accuracy    0.803467    0.834205    0.828696    0.829337   \n",
      "13                  MCC    0.623816    0.672473    0.678010    0.670997   \n",
      "14                  NPV    0.816400    0.801900    0.857800    0.829400   \n",
      "15              ROC_AUC    0.803467    0.834205    0.828696    0.829337   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.689168    0.651150    0.650946    0.623832  \n",
      "1   334.000000  342.000000  329.000000  324.000000  \n",
      "2   178.000000  170.000000  159.000000  171.000000  \n",
      "3    52.000000   52.000000   73.000000   57.000000  \n",
      "4    31.000000   31.000000   34.000000   43.000000  \n",
      "5     0.860504    0.860504    0.820168    0.831933  \n",
      "6     0.865285    0.868020    0.818408    0.850394  \n",
      "7     0.915068    0.916890    0.906336    0.882834  \n",
      "8     0.773900    0.765800    0.685300    0.750000  \n",
      "9     0.889481    0.891786    0.860131    0.866310  \n",
      "10    0.859118    0.858951    0.816501    0.830844  \n",
      "11    0.850207    0.847784    0.804183    0.820033  \n",
      "12    0.844491    0.841328    0.795840    0.816417  \n",
      "13    0.702831    0.698050    0.616443    0.641093  \n",
      "14    0.851700    0.845800    0.823800    0.799100  \n",
      "15    0.844491    0.841328    0.795840    0.816417  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_knn_7)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_7_cat = np.where((y_pred_knn_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_knn_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:04:36,654]\u001b[0m Trial 400 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:38,237]\u001b[0m Trial 401 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:39,822]\u001b[0m Trial 402 finished with value: 0.6294742684570054 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:41,445]\u001b[0m Trial 403 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:43,076]\u001b[0m Trial 404 finished with value: 0.6294742684570054 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:44,738]\u001b[0m Trial 405 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:46,321]\u001b[0m Trial 406 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:47,912]\u001b[0m Trial 407 finished with value: 0.6294742684570054 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:49,533]\u001b[0m Trial 408 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:51,164]\u001b[0m Trial 409 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:52,796]\u001b[0m Trial 410 finished with value: 0.6294742684570054 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:54,403]\u001b[0m Trial 411 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:55,978]\u001b[0m Trial 412 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:57,583]\u001b[0m Trial 413 finished with value: 0.6119733606552741 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:04:59,200]\u001b[0m Trial 414 finished with value: 0.6294742684570054 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:00,793]\u001b[0m Trial 415 finished with value: 0.6294742684570054 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:02,409]\u001b[0m Trial 416 finished with value: 0.6197959164071006 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:04,041]\u001b[0m Trial 417 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:05,676]\u001b[0m Trial 418 finished with value: 0.6294742684570054 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:07,294]\u001b[0m Trial 419 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:08,919]\u001b[0m Trial 420 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:10,509]\u001b[0m Trial 421 finished with value: 0.6227494219771368 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 48}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:12,152]\u001b[0m Trial 422 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:13,813]\u001b[0m Trial 423 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:15,459]\u001b[0m Trial 424 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:17,061]\u001b[0m Trial 425 finished with value: 0.6294742684570054 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:18,658]\u001b[0m Trial 426 finished with value: 0.6294742684570054 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:20,278]\u001b[0m Trial 427 finished with value: 0.6253392318459479 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:21,907]\u001b[0m Trial 428 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:23,517]\u001b[0m Trial 429 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:25,132]\u001b[0m Trial 430 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:26,791]\u001b[0m Trial 431 finished with value: 0.6294742684570054 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:28,435]\u001b[0m Trial 432 finished with value: 0.6119733606552741 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:30,010]\u001b[0m Trial 433 finished with value: 0.6294742684570054 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:31,603]\u001b[0m Trial 434 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:05:33,259]\u001b[0m Trial 435 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:34,914]\u001b[0m Trial 436 finished with value: 0.6294742684570054 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:36,514]\u001b[0m Trial 437 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:38,129]\u001b[0m Trial 438 finished with value: 0.6161348445259944 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:39,718]\u001b[0m Trial 439 finished with value: 0.6066630624453085 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:41,344]\u001b[0m Trial 440 finished with value: 0.6294742684570054 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:42,957]\u001b[0m Trial 441 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:44,573]\u001b[0m Trial 442 finished with value: 0.5624726310762512 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:46,228]\u001b[0m Trial 443 finished with value: 0.6197959164071006 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 55}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:47,889]\u001b[0m Trial 444 finished with value: 0.6294742684570054 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:49,547]\u001b[0m Trial 445 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:51,161]\u001b[0m Trial 446 finished with value: 0.6294742684570054 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:52,810]\u001b[0m Trial 447 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:54,467]\u001b[0m Trial 448 finished with value: 0.5890946629592866 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:56,116]\u001b[0m Trial 449 finished with value: 0.6255374012580631 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6465\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 55\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.615816    0.672816    0.669154    0.659064   \n",
      "1                    TP  320.000000  335.000000  329.000000  328.000000   \n",
      "2                    TN  169.000000  170.000000  175.000000  175.000000   \n",
      "3                    FP   68.000000   48.000000   62.000000   56.000000   \n",
      "4                    FN   38.000000   42.000000   29.000000   36.000000   \n",
      "5              Accuracy    0.821849    0.848739    0.847059    0.845378   \n",
      "6             Precision    0.824742    0.874674    0.841432    0.854167   \n",
      "7           Sensitivity    0.893855    0.888594    0.918994    0.901099   \n",
      "8           Specificity    0.713100    0.779800    0.738400    0.757600   \n",
      "9              F1 score    0.857909    0.881579    0.878505    0.877005   \n",
      "10  F1 score (weighted)    0.819412    0.848281    0.844706    0.843947   \n",
      "11     F1 score (macro)    0.809585    0.836138    0.836078    0.834430   \n",
      "12    Balanced Accuracy    0.803467    0.834205    0.828696    0.829337   \n",
      "13                  MCC    0.623816    0.672473    0.678010    0.670997   \n",
      "14                  NPV    0.816400    0.801900    0.857800    0.829400   \n",
      "15              ROC_AUC    0.803467    0.834205    0.828696    0.829337   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.689168    0.651150    0.650946    0.623832    0.648132  \n",
      "1   334.000000  342.000000  329.000000  324.000000  322.000000  \n",
      "2   178.000000  170.000000  159.000000  171.000000  175.000000  \n",
      "3    52.000000   52.000000   73.000000   57.000000   69.000000  \n",
      "4    31.000000   31.000000   34.000000   43.000000   29.000000  \n",
      "5     0.860504    0.860504    0.820168    0.831933    0.835294  \n",
      "6     0.865285    0.868020    0.818408    0.850394    0.823529  \n",
      "7     0.915068    0.916890    0.906336    0.882834    0.917379  \n",
      "8     0.773900    0.765800    0.685300    0.750000    0.717200  \n",
      "9     0.889481    0.891786    0.860131    0.866310    0.867925  \n",
      "10    0.859118    0.858951    0.816501    0.830844    0.832381  \n",
      "11    0.850207    0.847784    0.804183    0.820033    0.824587  \n",
      "12    0.844491    0.841328    0.795840    0.816417    0.817296  \n",
      "13    0.702831    0.698050    0.616443    0.641093    0.657566  \n",
      "14    0.851700    0.845800    0.823800    0.799100    0.857800  \n",
      "15    0.844491    0.841328    0.795840    0.816417    0.817296  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_knn_8)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_8_cat = np.where((y_pred_knn_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_knn_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:05:58,094]\u001b[0m Trial 450 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:05:59,730]\u001b[0m Trial 451 finished with value: 0.6316233645458864 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:01,371]\u001b[0m Trial 452 finished with value: 0.6238047233556737 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:03,042]\u001b[0m Trial 453 finished with value: 0.6316233645458864 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:04,660]\u001b[0m Trial 454 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:06,242]\u001b[0m Trial 455 finished with value: 0.6316233645458864 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:07,834]\u001b[0m Trial 456 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:09,450]\u001b[0m Trial 457 finished with value: 0.6102821479811139 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:11,071]\u001b[0m Trial 458 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:12,694]\u001b[0m Trial 459 finished with value: 0.6291188505432823 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:14,310]\u001b[0m Trial 460 finished with value: 0.6281152890108558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:15,910]\u001b[0m Trial 461 finished with value: 0.6316233645458864 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:17,511]\u001b[0m Trial 462 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:19,089]\u001b[0m Trial 463 finished with value: 0.6316233645458864 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:20,727]\u001b[0m Trial 464 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:22,350]\u001b[0m Trial 465 finished with value: 0.6183357011091053 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:23,949]\u001b[0m Trial 466 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:25,535]\u001b[0m Trial 467 finished with value: 0.6236752179858863 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:27,156]\u001b[0m Trial 468 finished with value: 0.6316233645458864 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:28,781]\u001b[0m Trial 469 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:30,395]\u001b[0m Trial 470 finished with value: 0.6238047233556737 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:31,984]\u001b[0m Trial 471 finished with value: 0.6291188505432823 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:33,605]\u001b[0m Trial 472 finished with value: 0.6316233645458864 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:35,226]\u001b[0m Trial 473 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:36,894]\u001b[0m Trial 474 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:38,502]\u001b[0m Trial 475 finished with value: 0.6026618356028851 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:40,099]\u001b[0m Trial 476 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:41,681]\u001b[0m Trial 477 finished with value: 0.6316233645458864 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:43,300]\u001b[0m Trial 478 finished with value: 0.5945502830218239 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:44,875]\u001b[0m Trial 479 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:46,477]\u001b[0m Trial 480 finished with value: 0.6308260264487194 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:48,110]\u001b[0m Trial 481 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:49,683]\u001b[0m Trial 482 finished with value: 0.6236752179858863 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:51,322]\u001b[0m Trial 483 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:52,977]\u001b[0m Trial 484 finished with value: 0.5914421941252048 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:06:54,616]\u001b[0m Trial 485 finished with value: 0.6316233645458864 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:56,231]\u001b[0m Trial 486 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:57,851]\u001b[0m Trial 487 finished with value: 0.6251784069562346 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 51}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:06:59,467]\u001b[0m Trial 488 finished with value: 0.6308260264487194 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:07:01,135]\u001b[0m Trial 489 finished with value: 0.6238047233556737 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:07:02,769]\u001b[0m Trial 490 finished with value: 0.6291188505432823 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:07:04,374]\u001b[0m Trial 491 finished with value: 0.6316233645458864 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:07:06,022]\u001b[0m Trial 492 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:07:07,657]\u001b[0m Trial 493 finished with value: 0.6308260264487194 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:07:09,288]\u001b[0m Trial 494 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:07:10,933]\u001b[0m Trial 495 finished with value: 0.6316233645458864 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:07:12,611]\u001b[0m Trial 496 finished with value: 0.6308260264487194 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:07:14,211]\u001b[0m Trial 497 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:07:15,824]\u001b[0m Trial 498 finished with value: 0.6344341698965692 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:07:17,425]\u001b[0m Trial 499 finished with value: 0.6316233645458864 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 350 with value: 0.6464883939383655.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6465\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 55\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.615816    0.672816    0.669154    0.659064   \n",
      "1                    TP  320.000000  335.000000  329.000000  328.000000   \n",
      "2                    TN  169.000000  170.000000  175.000000  175.000000   \n",
      "3                    FP   68.000000   48.000000   62.000000   56.000000   \n",
      "4                    FN   38.000000   42.000000   29.000000   36.000000   \n",
      "5              Accuracy    0.821849    0.848739    0.847059    0.845378   \n",
      "6             Precision    0.824742    0.874674    0.841432    0.854167   \n",
      "7           Sensitivity    0.893855    0.888594    0.918994    0.901099   \n",
      "8           Specificity    0.713100    0.779800    0.738400    0.757600   \n",
      "9              F1 score    0.857909    0.881579    0.878505    0.877005   \n",
      "10  F1 score (weighted)    0.819412    0.848281    0.844706    0.843947   \n",
      "11     F1 score (macro)    0.809585    0.836138    0.836078    0.834430   \n",
      "12    Balanced Accuracy    0.803467    0.834205    0.828696    0.829337   \n",
      "13                  MCC    0.623816    0.672473    0.678010    0.670997   \n",
      "14                  NPV    0.816400    0.801900    0.857800    0.829400   \n",
      "15              ROC_AUC    0.803467    0.834205    0.828696    0.829337   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.689168    0.651150    0.650946    0.623832    0.648132    0.665763  \n",
      "1   334.000000  342.000000  329.000000  324.000000  322.000000  344.000000  \n",
      "2   178.000000  170.000000  159.000000  171.000000  175.000000  167.000000  \n",
      "3    52.000000   52.000000   73.000000   57.000000   69.000000   53.000000  \n",
      "4    31.000000   31.000000   34.000000   43.000000   29.000000   31.000000  \n",
      "5     0.860504    0.860504    0.820168    0.831933    0.835294    0.858824  \n",
      "6     0.865285    0.868020    0.818408    0.850394    0.823529    0.866499  \n",
      "7     0.915068    0.916890    0.906336    0.882834    0.917379    0.917333  \n",
      "8     0.773900    0.765800    0.685300    0.750000    0.717200    0.759100  \n",
      "9     0.889481    0.891786    0.860131    0.866310    0.867925    0.891192  \n",
      "10    0.859118    0.858951    0.816501    0.830844    0.832381    0.857120  \n",
      "11    0.850207    0.847784    0.804183    0.820033    0.824587    0.845117  \n",
      "12    0.844491    0.841328    0.795840    0.816417    0.817296    0.838212  \n",
      "13    0.702831    0.698050    0.616443    0.641093    0.657566    0.692976  \n",
      "14    0.851700    0.845800    0.823800    0.799100    0.857800    0.843400  \n",
      "15    0.844491    0.841328    0.795840    0.816417    0.817296    0.838212  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_knn_9)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_9_cat = np.where((y_pred_knn_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_knn_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.655326</td>\n",
       "      <td>0.026284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>166.200000</td>\n",
       "      <td>6.696599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>82.800000</td>\n",
       "      <td>3.765339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>30.600000</td>\n",
       "      <td>5.660781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>4.648775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.838098</td>\n",
       "      <td>0.025392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.844672</td>\n",
       "      <td>0.027525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.904638</td>\n",
       "      <td>0.025431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.731080</td>\n",
       "      <td>0.042736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.873356</td>\n",
       "      <td>0.021205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.835867</td>\n",
       "      <td>0.025882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.824300</td>\n",
       "      <td>0.026217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.817859</td>\n",
       "      <td>0.025945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.653329</td>\n",
       "      <td>0.051721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.826910</td>\n",
       "      <td>0.038450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.817859</td>\n",
       "      <td>0.025945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.655326     0.026284\n",
       "1                    TP       166.200000     6.696599\n",
       "2                    TN        82.800000     3.765339\n",
       "3                    FP        30.600000     5.660781\n",
       "4                    FN        17.500000     4.648775\n",
       "5              Accuracy         0.838098     0.025392\n",
       "6             Precision         0.844672     0.027525\n",
       "7           Sensitivity         0.904638     0.025431\n",
       "8           Specificity         0.731080     0.042736\n",
       "9              F1 score         0.873356     0.021205\n",
       "10  F1 score (weighted)         0.835867     0.025882\n",
       "11     F1 score (macro)         0.824300     0.026217\n",
       "12    Balanced Accuracy         0.817859     0.025945\n",
       "13                  MCC         0.653329     0.051721\n",
       "14                  NPV         0.826910     0.038450\n",
       "15              ROC_AUC         0.817859     0.025945"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.615816</td>\n",
       "      <td>0.672816</td>\n",
       "      <td>0.669154</td>\n",
       "      <td>0.659064</td>\n",
       "      <td>0.689168</td>\n",
       "      <td>0.651150</td>\n",
       "      <td>0.650946</td>\n",
       "      <td>0.623832</td>\n",
       "      <td>0.648132</td>\n",
       "      <td>0.665763</td>\n",
       "      <td>0.654584</td>\n",
       "      <td>0.022113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>330.700000</td>\n",
       "      <td>8.042250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>170.900000</td>\n",
       "      <td>5.404730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>8.524475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>34.400000</td>\n",
       "      <td>5.168279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.821849</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.845378</td>\n",
       "      <td>0.860504</td>\n",
       "      <td>0.860504</td>\n",
       "      <td>0.820168</td>\n",
       "      <td>0.831933</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.843025</td>\n",
       "      <td>0.015161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.824742</td>\n",
       "      <td>0.874674</td>\n",
       "      <td>0.841432</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.865285</td>\n",
       "      <td>0.868020</td>\n",
       "      <td>0.818408</td>\n",
       "      <td>0.850394</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.866499</td>\n",
       "      <td>0.848715</td>\n",
       "      <td>0.020684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.893855</td>\n",
       "      <td>0.888594</td>\n",
       "      <td>0.918994</td>\n",
       "      <td>0.901099</td>\n",
       "      <td>0.915068</td>\n",
       "      <td>0.916890</td>\n",
       "      <td>0.906336</td>\n",
       "      <td>0.882834</td>\n",
       "      <td>0.917379</td>\n",
       "      <td>0.917333</td>\n",
       "      <td>0.905838</td>\n",
       "      <td>0.013493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.713100</td>\n",
       "      <td>0.779800</td>\n",
       "      <td>0.738400</td>\n",
       "      <td>0.757600</td>\n",
       "      <td>0.773900</td>\n",
       "      <td>0.765800</td>\n",
       "      <td>0.685300</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.717200</td>\n",
       "      <td>0.759100</td>\n",
       "      <td>0.744020</td>\n",
       "      <td>0.030276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.857909</td>\n",
       "      <td>0.881579</td>\n",
       "      <td>0.878505</td>\n",
       "      <td>0.877005</td>\n",
       "      <td>0.889481</td>\n",
       "      <td>0.891786</td>\n",
       "      <td>0.860131</td>\n",
       "      <td>0.866310</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.891192</td>\n",
       "      <td>0.876182</td>\n",
       "      <td>0.012640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.819412</td>\n",
       "      <td>0.848281</td>\n",
       "      <td>0.844706</td>\n",
       "      <td>0.843947</td>\n",
       "      <td>0.859118</td>\n",
       "      <td>0.858951</td>\n",
       "      <td>0.816501</td>\n",
       "      <td>0.830844</td>\n",
       "      <td>0.832381</td>\n",
       "      <td>0.857120</td>\n",
       "      <td>0.841126</td>\n",
       "      <td>0.015739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.809585</td>\n",
       "      <td>0.836138</td>\n",
       "      <td>0.836078</td>\n",
       "      <td>0.834430</td>\n",
       "      <td>0.850207</td>\n",
       "      <td>0.847784</td>\n",
       "      <td>0.804183</td>\n",
       "      <td>0.820033</td>\n",
       "      <td>0.824587</td>\n",
       "      <td>0.845117</td>\n",
       "      <td>0.830814</td>\n",
       "      <td>0.015821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.803467</td>\n",
       "      <td>0.834205</td>\n",
       "      <td>0.828696</td>\n",
       "      <td>0.829337</td>\n",
       "      <td>0.844491</td>\n",
       "      <td>0.841328</td>\n",
       "      <td>0.795840</td>\n",
       "      <td>0.816417</td>\n",
       "      <td>0.817296</td>\n",
       "      <td>0.838212</td>\n",
       "      <td>0.824929</td>\n",
       "      <td>0.016279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.623816</td>\n",
       "      <td>0.672473</td>\n",
       "      <td>0.678010</td>\n",
       "      <td>0.670997</td>\n",
       "      <td>0.702831</td>\n",
       "      <td>0.698050</td>\n",
       "      <td>0.616443</td>\n",
       "      <td>0.641093</td>\n",
       "      <td>0.657566</td>\n",
       "      <td>0.692976</td>\n",
       "      <td>0.665426</td>\n",
       "      <td>0.030269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.816400</td>\n",
       "      <td>0.801900</td>\n",
       "      <td>0.857800</td>\n",
       "      <td>0.829400</td>\n",
       "      <td>0.851700</td>\n",
       "      <td>0.845800</td>\n",
       "      <td>0.823800</td>\n",
       "      <td>0.799100</td>\n",
       "      <td>0.857800</td>\n",
       "      <td>0.843400</td>\n",
       "      <td>0.832710</td>\n",
       "      <td>0.021961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.803467</td>\n",
       "      <td>0.834205</td>\n",
       "      <td>0.828696</td>\n",
       "      <td>0.829337</td>\n",
       "      <td>0.844491</td>\n",
       "      <td>0.841328</td>\n",
       "      <td>0.795840</td>\n",
       "      <td>0.816417</td>\n",
       "      <td>0.817296</td>\n",
       "      <td>0.838212</td>\n",
       "      <td>0.824929</td>\n",
       "      <td>0.016279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.615816    0.672816    0.669154    0.659064   \n",
       "1                    TP  320.000000  335.000000  329.000000  328.000000   \n",
       "2                    TN  169.000000  170.000000  175.000000  175.000000   \n",
       "3                    FP   68.000000   48.000000   62.000000   56.000000   \n",
       "4                    FN   38.000000   42.000000   29.000000   36.000000   \n",
       "5              Accuracy    0.821849    0.848739    0.847059    0.845378   \n",
       "6             Precision    0.824742    0.874674    0.841432    0.854167   \n",
       "7           Sensitivity    0.893855    0.888594    0.918994    0.901099   \n",
       "8           Specificity    0.713100    0.779800    0.738400    0.757600   \n",
       "9              F1 score    0.857909    0.881579    0.878505    0.877005   \n",
       "10  F1 score (weighted)    0.819412    0.848281    0.844706    0.843947   \n",
       "11     F1 score (macro)    0.809585    0.836138    0.836078    0.834430   \n",
       "12    Balanced Accuracy    0.803467    0.834205    0.828696    0.829337   \n",
       "13                  MCC    0.623816    0.672473    0.678010    0.670997   \n",
       "14                  NPV    0.816400    0.801900    0.857800    0.829400   \n",
       "15              ROC_AUC    0.803467    0.834205    0.828696    0.829337   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.689168    0.651150    0.650946    0.623832    0.648132    0.665763   \n",
       "1   334.000000  342.000000  329.000000  324.000000  322.000000  344.000000   \n",
       "2   178.000000  170.000000  159.000000  171.000000  175.000000  167.000000   \n",
       "3    52.000000   52.000000   73.000000   57.000000   69.000000   53.000000   \n",
       "4    31.000000   31.000000   34.000000   43.000000   29.000000   31.000000   \n",
       "5     0.860504    0.860504    0.820168    0.831933    0.835294    0.858824   \n",
       "6     0.865285    0.868020    0.818408    0.850394    0.823529    0.866499   \n",
       "7     0.915068    0.916890    0.906336    0.882834    0.917379    0.917333   \n",
       "8     0.773900    0.765800    0.685300    0.750000    0.717200    0.759100   \n",
       "9     0.889481    0.891786    0.860131    0.866310    0.867925    0.891192   \n",
       "10    0.859118    0.858951    0.816501    0.830844    0.832381    0.857120   \n",
       "11    0.850207    0.847784    0.804183    0.820033    0.824587    0.845117   \n",
       "12    0.844491    0.841328    0.795840    0.816417    0.817296    0.838212   \n",
       "13    0.702831    0.698050    0.616443    0.641093    0.657566    0.692976   \n",
       "14    0.851700    0.845800    0.823800    0.799100    0.857800    0.843400   \n",
       "15    0.844491    0.841328    0.795840    0.816417    0.817296    0.838212   \n",
       "\n",
       "           ave       std  \n",
       "0     0.654584  0.022113  \n",
       "1   330.700000  8.042250  \n",
       "2   170.900000  5.404730  \n",
       "3    59.000000  8.524475  \n",
       "4    34.400000  5.168279  \n",
       "5     0.843025  0.015161  \n",
       "6     0.848715  0.020684  \n",
       "7     0.905838  0.013493  \n",
       "8     0.744020  0.030276  \n",
       "9     0.876182  0.012640  \n",
       "10    0.841126  0.015739  \n",
       "11    0.830814  0.015821  \n",
       "12    0.824929  0.016279  \n",
       "13    0.665426  0.030269  \n",
       "14    0.832710  0.021961  \n",
       "15    0.824929  0.016279  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test['std'] = mat_met_knn_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_knn0</th>\n",
       "      <th>y_pred_knn1</th>\n",
       "      <th>y_pred_knn2</th>\n",
       "      <th>y_pred_knn3</th>\n",
       "      <th>y_pred_knn4</th>\n",
       "      <th>y_pred_knn_ave</th>\n",
       "      <th>y_pred_knn_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL356066</td>\n",
       "      <td>0</td>\n",
       "      <td>8.02</td>\n",
       "      <td>7.101069</td>\n",
       "      <td>7.101069</td>\n",
       "      <td>7.034960</td>\n",
       "      <td>7.101069</td>\n",
       "      <td>7.101069</td>\n",
       "      <td>7.243206</td>\n",
       "      <td>0.348231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3652228</td>\n",
       "      <td>1</td>\n",
       "      <td>8.05</td>\n",
       "      <td>8.106915</td>\n",
       "      <td>8.021567</td>\n",
       "      <td>8.021567</td>\n",
       "      <td>8.021567</td>\n",
       "      <td>8.021567</td>\n",
       "      <td>8.040530</td>\n",
       "      <td>0.031451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3939518</td>\n",
       "      <td>2</td>\n",
       "      <td>6.87</td>\n",
       "      <td>7.574042</td>\n",
       "      <td>7.583610</td>\n",
       "      <td>7.574042</td>\n",
       "      <td>7.606467</td>\n",
       "      <td>7.655317</td>\n",
       "      <td>7.477246</td>\n",
       "      <td>0.273010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4281792</td>\n",
       "      <td>3</td>\n",
       "      <td>7.22</td>\n",
       "      <td>8.235000</td>\n",
       "      <td>7.850000</td>\n",
       "      <td>7.405000</td>\n",
       "      <td>7.910000</td>\n",
       "      <td>7.850000</td>\n",
       "      <td>7.745000</td>\n",
       "      <td>0.336799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4070232</td>\n",
       "      <td>4</td>\n",
       "      <td>7.15</td>\n",
       "      <td>7.119514</td>\n",
       "      <td>6.928830</td>\n",
       "      <td>6.928830</td>\n",
       "      <td>6.928830</td>\n",
       "      <td>7.119514</td>\n",
       "      <td>7.029253</td>\n",
       "      <td>0.100936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL4202521</td>\n",
       "      <td>2966</td>\n",
       "      <td>7.43</td>\n",
       "      <td>6.978539</td>\n",
       "      <td>7.030952</td>\n",
       "      <td>7.030952</td>\n",
       "      <td>7.030952</td>\n",
       "      <td>6.916975</td>\n",
       "      <td>7.069728</td>\n",
       "      <td>0.166311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL216641</td>\n",
       "      <td>2967</td>\n",
       "      <td>7.35</td>\n",
       "      <td>6.920277</td>\n",
       "      <td>7.263123</td>\n",
       "      <td>7.313291</td>\n",
       "      <td>7.263123</td>\n",
       "      <td>7.261517</td>\n",
       "      <td>7.228555</td>\n",
       "      <td>0.141681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL3693750</td>\n",
       "      <td>2968</td>\n",
       "      <td>7.43</td>\n",
       "      <td>7.515533</td>\n",
       "      <td>7.537293</td>\n",
       "      <td>7.515533</td>\n",
       "      <td>7.515533</td>\n",
       "      <td>7.515533</td>\n",
       "      <td>7.504904</td>\n",
       "      <td>0.034428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL152665</td>\n",
       "      <td>2969</td>\n",
       "      <td>5.96</td>\n",
       "      <td>6.695202</td>\n",
       "      <td>6.388462</td>\n",
       "      <td>6.695202</td>\n",
       "      <td>6.218352</td>\n",
       "      <td>6.218352</td>\n",
       "      <td>6.362595</td>\n",
       "      <td>0.266339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL3693789</td>\n",
       "      <td>2970</td>\n",
       "      <td>8.52</td>\n",
       "      <td>8.272458</td>\n",
       "      <td>8.277536</td>\n",
       "      <td>8.235880</td>\n",
       "      <td>8.363644</td>\n",
       "      <td>8.363644</td>\n",
       "      <td>8.338860</td>\n",
       "      <td>0.093822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_knn0  y_pred_knn1  \\\n",
       "0          CHEMBL356066            0     8.02     7.101069     7.101069   \n",
       "1         CHEMBL3652228            1     8.05     8.106915     8.021567   \n",
       "2         CHEMBL3939518            2     6.87     7.574042     7.583610   \n",
       "3         CHEMBL4281792            3     7.22     8.235000     7.850000   \n",
       "4         CHEMBL4070232            4     7.15     7.119514     6.928830   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "2966      CHEMBL4202521         2966     7.43     6.978539     7.030952   \n",
       "2967       CHEMBL216641         2967     7.35     6.920277     7.263123   \n",
       "2968      CHEMBL3693750         2968     7.43     7.515533     7.537293   \n",
       "2969       CHEMBL152665         2969     5.96     6.695202     6.388462   \n",
       "2970      CHEMBL3693789         2970     8.52     8.272458     8.277536   \n",
       "\n",
       "      y_pred_knn2  y_pred_knn3  y_pred_knn4  y_pred_knn_ave  y_pred_knn_std  \n",
       "0        7.034960     7.101069     7.101069        7.243206        0.348231  \n",
       "1        8.021567     8.021567     8.021567        8.040530        0.031451  \n",
       "2        7.574042     7.606467     7.655317        7.477246        0.273010  \n",
       "3        7.405000     7.910000     7.850000        7.745000        0.336799  \n",
       "4        6.928830     6.928830     7.119514        7.029253        0.100936  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "2966     7.030952     7.030952     6.916975        7.069728        0.166311  \n",
       "2967     7.313291     7.263123     7.261517        7.228555        0.141681  \n",
       "2968     7.515533     7.515533     7.515533        7.504904        0.034428  \n",
       "2969     6.695202     6.218352     6.218352        6.362595        0.266339  \n",
       "2970     8.235880     8.363644     8.363644        8.338860        0.093822  \n",
       "\n",
       "[2971 rows x 10 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_knn_cat = np.where((y_pred_optimized_knn >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_knn_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_knn))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "knn_run0 = data_knn[['y_test_idx0', 'y_test0', 'y_pred_knn0']]\n",
    "knn_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "knn_run0.reset_index(inplace=True, drop=True)\n",
    "knn_run1 = data_knn[['y_test_idx1', 'y_test1', 'y_pred_knn1']]\n",
    "knn_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "knn_run1.reset_index(inplace=True, drop=True)\n",
    "knn_run2 = data_knn[['y_test_idx2', 'y_test2', 'y_pred_knn2']]\n",
    "knn_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "knn_run2.reset_index(inplace=True, drop=True)\n",
    "knn_run3 = data_knn[['y_test_idx3', 'y_test3', 'y_pred_knn3']]\n",
    "knn_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "knn_run3.reset_index(inplace=True, drop=True)\n",
    "knn_run4 = data_knn[['y_test_idx4', 'y_test4', 'y_pred_knn4']]\n",
    "knn_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "knn_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "knn_5preds = pd.concat([chembl_id, knn_run0, knn_run1, knn_run2, knn_run3, knn_run4], axis=1)\n",
    "knn_5preds = knn_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_knn0', 'y_pred_knn1', 'y_pred_knn2', 'y_pred_knn3', 'y_pred_knn4']]\n",
    "knn_5preds['y_pred_knn_ave'] = knn_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "knn_5preds['y_pred_knn_std'] = knn_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "knn_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c149767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_knn.to_csv('mat_met_knn_opt.csv')\n",
    "knn_5preds.to_csv('knn_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0bc43db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model r2_score 0.6403 with a standard deviation of 0.0611\n",
      "KNN optimized model r2_score 0.6490 with a standard deviation of 0.0590\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"KNN baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_knn.joblib']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_reg, \"./knn_reg.joblib\")\n",
    "#joblib.dump(optimized_knn, \"./optimized_knn.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"./optimizedCV_knn.joblib\")\n",
    "#loaded_rf = joblib.load(\"./optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.693773     0.031180\n",
      "1                    TP       165.600000     6.484169\n",
      "2                    TN        86.500000     3.979112\n",
      "3                    FP        26.900000     3.212822\n",
      "4                    FN        18.100000     5.506562\n",
      "5              Accuracy         0.848533     0.021134\n",
      "6             Precision         0.860236     0.016142\n",
      "7           Sensitivity         0.901523     0.029400\n",
      "8           Specificity         0.762940     0.025166\n",
      "9              F1 score         0.880166     0.018100\n",
      "10  F1 score (weighted)         0.847270     0.020787\n",
      "11     F1 score (macro)         0.836962     0.021617\n",
      "12    Balanced Accuracy         0.832225     0.019649\n",
      "13                  MCC         0.676463     0.043865\n",
      "14                  NPV         0.828560     0.043732\n",
      "15              ROC_AUC         0.832225     0.019649\n",
      "CPU times: user 45.1 s, sys: 28 ms, total: 45.1 s\n",
      "Wall time: 45.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    \n",
    "    svm_reg.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:08:32,040]\u001b[0m A new study created in memory with name: SVM_regressor_CV\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:09:03,450]\u001b[0m Trial 0 finished with value: 0.5807661476054035 and parameters: {'C': 64.0, 'gamma': 0.00048828125}. Best is trial 0 with value: 0.5807661476054035.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:09:32,600]\u001b[0m Trial 1 finished with value: 0.6894911052797668 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:10:05,918]\u001b[0m Trial 2 finished with value: 0.03116819693838043 and parameters: {'C': 64.0, 'gamma': 8.0}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:10:37,764]\u001b[0m Trial 3 finished with value: 0.11166867297717506 and parameters: {'C': 0.015625, 'gamma': 0.00390625}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:11:08,284]\u001b[0m Trial 4 finished with value: 0.4137022257645936 and parameters: {'C': 0.125, 'gamma': 0.00390625}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:11:40,261]\u001b[0m Trial 5 finished with value: 0.03817399946528287 and parameters: {'C': 0.5, 'gamma': 0.5}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:12:12,047]\u001b[0m Trial 6 finished with value: 0.2914678511628972 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:12:43,801]\u001b[0m Trial 7 finished with value: 0.033769850157060834 and parameters: {'C': 0.015625, 'gamma': 0.0625}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:13:14,578]\u001b[0m Trial 8 finished with value: 0.3093965977515097 and parameters: {'C': 0.0625, 'gamma': 0.00390625}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:13:46,478]\u001b[0m Trial 9 finished with value: -0.006915831669698702 and parameters: {'C': 0.015625, 'gamma': 0.5}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:14:15,398]\u001b[0m Trial 10 finished with value: 0.6894911052797668 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:14:44,313]\u001b[0m Trial 11 finished with value: 0.6894911052797668 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:15:13,226]\u001b[0m Trial 12 finished with value: 0.6894911052797668 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:15:41,778]\u001b[0m Trial 13 finished with value: 0.6779877820516959 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:16:11,461]\u001b[0m Trial 14 finished with value: 0.6678023317746296 and parameters: {'C': 32.0, 'gamma': 0.0078125}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:16:43,261]\u001b[0m Trial 15 finished with value: 0.036873614533256535 and parameters: {'C': 0.25, 'gamma': 6.103515625e-05}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:17:14,962]\u001b[0m Trial 16 finished with value: 0.10114888858915583 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:17:46,944]\u001b[0m Trial 17 finished with value: 0.031714697563671124 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:18:16,672]\u001b[0m Trial 18 finished with value: 0.5962419866759976 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:18:48,108]\u001b[0m Trial 19 finished with value: 0.12836467452436132 and parameters: {'C': 0.03125, 'gamma': 0.001953125}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:19:17,564]\u001b[0m Trial 20 finished with value: 0.5647083908510981 and parameters: {'C': 2.0, 'gamma': 0.0009765625}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:19:46,447]\u001b[0m Trial 21 finished with value: 0.6894911052797668 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:20:18,335]\u001b[0m Trial 22 finished with value: -0.008336740624216799 and parameters: {'C': 0.0078125, 'gamma': 2.0}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:20:47,229]\u001b[0m Trial 23 finished with value: 0.6894911052797668 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:21:19,195]\u001b[0m Trial 24 finished with value: 0.04467137407641684 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:21:47,964]\u001b[0m Trial 25 finished with value: 0.6775493925688616 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:22:17,686]\u001b[0m Trial 26 finished with value: 0.5951206696931851 and parameters: {'C': 32.0, 'gamma': 0.0001220703125}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:22:46,734]\u001b[0m Trial 27 finished with value: 0.6414098364842119 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:23:18,563]\u001b[0m Trial 28 finished with value: -0.005864113525326009 and parameters: {'C': 0.03125, 'gamma': 3.0517578125e-05}. Best is trial 1 with value: 0.6894911052797668.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:23:47,348]\u001b[0m Trial 29 finished with value: 0.6913114289076366 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6913114289076366.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:24:17,050]\u001b[0m Trial 30 finished with value: 0.607682575567883 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 29 with value: 0.6913114289076366.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:24:45,847]\u001b[0m Trial 31 finished with value: 0.6936052872848771 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:25:14,636]\u001b[0m Trial 32 finished with value: 0.6936052872848771 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:25:47,677]\u001b[0m Trial 33 finished with value: 0.03116861888017707 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:26:16,463]\u001b[0m Trial 34 finished with value: 0.6936052872848771 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:26:45,258]\u001b[0m Trial 35 finished with value: 0.6936052872848771 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:27:14,039]\u001b[0m Trial 36 finished with value: 0.6936052872848771 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:27:45,574]\u001b[0m Trial 37 finished with value: 0.29146785116289775 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:28:17,427]\u001b[0m Trial 38 finished with value: 0.04467149516870089 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:28:46,951]\u001b[0m Trial 39 finished with value: 0.5635592767971116 and parameters: {'C': 64.0, 'gamma': 3.0517578125e-05}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:29:16,877]\u001b[0m Trial 40 finished with value: 0.5665950841142966 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:29:45,656]\u001b[0m Trial 41 finished with value: 0.6936052872848771 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:30:14,438]\u001b[0m Trial 42 finished with value: 0.6936052872848771 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:30:43,222]\u001b[0m Trial 43 finished with value: 0.6936052872848771 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:31:15,184]\u001b[0m Trial 44 finished with value: 0.03171459027439307 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:31:46,937]\u001b[0m Trial 45 finished with value: 0.036642965510159706 and parameters: {'C': 0.125, 'gamma': 0.0001220703125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:32:18,705]\u001b[0m Trial 46 finished with value: 0.0036626968403053683 and parameters: {'C': 0.0625, 'gamma': 0.25}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:32:50,547]\u001b[0m Trial 47 finished with value: 0.06015778026535853 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:33:19,453]\u001b[0m Trial 48 finished with value: 0.6500631629553911 and parameters: {'C': 1.0, 'gamma': 0.0078125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:33:49,167]\u001b[0m Trial 49 finished with value: 0.4967354435300601 and parameters: {'C': 0.25, 'gamma': 0.00390625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6936\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_regressor_CV\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.673671\n",
      "1                    TP  320.000000\n",
      "2                    TN  176.000000\n",
      "3                    FP   61.000000\n",
      "4                    FN   38.000000\n",
      "5              Accuracy    0.833613\n",
      "6             Precision    0.839895\n",
      "7           Sensitivity    0.893855\n",
      "8           Specificity    0.742600\n",
      "9              F1 score    0.866035\n",
      "10  F1 score (weighted)    0.831960\n",
      "11     F1 score (macro)    0.823261\n",
      "12    Balanced Accuracy    0.818235\n",
      "13                  MCC    0.649269\n",
      "14                  NPV    0.822400\n",
      "15              ROC_AUC    0.818235\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_svm_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_0_cat = np.where((y_pred_svm_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_svm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:34:22,969]\u001b[0m Trial 50 finished with value: 0.5015301241588085 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:34:51,940]\u001b[0m Trial 51 finished with value: 0.684073524243196 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:35:20,903]\u001b[0m Trial 52 finished with value: 0.684073524243196 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:35:51,213]\u001b[0m Trial 53 finished with value: 0.4323829398258754 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:36:22,581]\u001b[0m Trial 54 finished with value: 0.16039123129487876 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:36:51,546]\u001b[0m Trial 55 finished with value: 0.684073524243196 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:37:21,049]\u001b[0m Trial 56 finished with value: 0.5956024979410512 and parameters: {'C': 2.0, 'gamma': 0.001953125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:37:52,750]\u001b[0m Trial 57 finished with value: 0.013223520807770905 and parameters: {'C': 0.0078125, 'gamma': 0.0009765625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:38:24,712]\u001b[0m Trial 58 finished with value: 0.0180120482215256 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:38:53,656]\u001b[0m Trial 59 finished with value: 0.6697450994823394 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:39:23,614]\u001b[0m Trial 60 finished with value: 0.48686173183998116 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:39:52,573]\u001b[0m Trial 61 finished with value: 0.684073524243196 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:40:21,541]\u001b[0m Trial 62 finished with value: 0.684073524243196 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:40:50,505]\u001b[0m Trial 63 finished with value: 0.684073524243196 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:41:21,071]\u001b[0m Trial 64 finished with value: 0.3365575233404136 and parameters: {'C': 0.5, 'gamma': 0.00048828125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:41:54,234]\u001b[0m Trial 65 finished with value: 0.014516376588623425 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:42:23,198]\u001b[0m Trial 66 finished with value: 0.684073524243196 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:42:54,816]\u001b[0m Trial 67 finished with value: 0.021512005656662135 and parameters: {'C': 0.0625, 'gamma': 0.125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:43:23,775]\u001b[0m Trial 68 finished with value: 0.684073524243196 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:43:52,578]\u001b[0m Trial 69 finished with value: 0.6656288473767327 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:44:23,695]\u001b[0m Trial 70 finished with value: 0.2508025081324615 and parameters: {'C': 0.03125, 'gamma': 0.0078125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:44:52,657]\u001b[0m Trial 71 finished with value: 0.684073524243196 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:45:22,512]\u001b[0m Trial 72 finished with value: 0.5521231968581922 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:45:51,468]\u001b[0m Trial 73 finished with value: 0.684073524243196 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:46:20,885]\u001b[0m Trial 74 finished with value: 0.5687971302773578 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:46:52,964]\u001b[0m Trial 75 finished with value: 0.014986245739564674 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:47:24,652]\u001b[0m Trial 76 finished with value: -0.005912659712698576 and parameters: {'C': 0.015625, 'gamma': 0.5}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:47:56,361]\u001b[0m Trial 77 finished with value: 0.07518963642614285 and parameters: {'C': 2.0, 'gamma': 0.25}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:48:26,922]\u001b[0m Trial 78 finished with value: 0.3411869253799568 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:48:56,486]\u001b[0m Trial 79 finished with value: 0.6457556229010077 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:49:28,145]\u001b[0m Trial 80 finished with value: -0.001962078936879552 and parameters: {'C': 0.0078125, 'gamma': 0.000244140625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:49:57,128]\u001b[0m Trial 81 finished with value: 0.6830446964931982 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:50:26,129]\u001b[0m Trial 82 finished with value: 0.6830446964931982 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:50:58,210]\u001b[0m Trial 83 finished with value: 0.024550373669319326 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:51:27,180]\u001b[0m Trial 84 finished with value: 0.684073524243196 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:51:56,977]\u001b[0m Trial 85 finished with value: 0.501815632035499 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:52:28,637]\u001b[0m Trial 86 finished with value: 0.03696389070454152 and parameters: {'C': 0.5, 'gamma': 3.0517578125e-05}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:52:57,607]\u001b[0m Trial 87 finished with value: 0.6807932139128755 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:53:27,773]\u001b[0m Trial 88 finished with value: 0.610654616398639 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:53:57,569]\u001b[0m Trial 89 finished with value: 0.6260587334038402 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:54:29,264]\u001b[0m Trial 90 finished with value: -0.0010210525520652713 and parameters: {'C': 0.125, 'gamma': 2.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:54:58,226]\u001b[0m Trial 91 finished with value: 0.6819062220243689 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:55:27,188]\u001b[0m Trial 92 finished with value: 0.6819062220243689 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:55:56,151]\u001b[0m Trial 93 finished with value: 0.6708220202889146 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:56:25,128]\u001b[0m Trial 94 finished with value: 0.6806834220852537 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:56:55,471]\u001b[0m Trial 95 finished with value: 0.3845748131853229 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:57:25,077]\u001b[0m Trial 96 finished with value: 0.5877648804521207 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:57:53,879]\u001b[0m Trial 97 finished with value: 0.6656288473767327 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:58:25,479]\u001b[0m Trial 98 finished with value: 0.26889387164466 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 05:58:56,478]\u001b[0m Trial 99 finished with value: 0.26832587079636583 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6936\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.673671    0.721557\n",
      "1                    TP  320.000000  341.000000\n",
      "2                    TN  176.000000  168.000000\n",
      "3                    FP   61.000000   50.000000\n",
      "4                    FN   38.000000   36.000000\n",
      "5              Accuracy    0.833613    0.855462\n",
      "6             Precision    0.839895    0.872123\n",
      "7           Sensitivity    0.893855    0.904509\n",
      "8           Specificity    0.742600    0.770600\n",
      "9              F1 score    0.866035    0.888021\n",
      "10  F1 score (weighted)    0.831960    0.854382\n",
      "11     F1 score (macro)    0.823261    0.842115\n",
      "12    Balanced Accuracy    0.818235    0.837576\n",
      "13                  MCC    0.649269    0.685325\n",
      "14                  NPV    0.822400    0.823500\n",
      "15              ROC_AUC    0.818235    0.837576\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_svm_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_1_cat = np.where((y_pred_svm_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_svm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 05:59:33,519]\u001b[0m Trial 100 finished with value: 0.01911329847014146 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:00:02,491]\u001b[0m Trial 101 finished with value: 0.6822693744521852 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:00:31,977]\u001b[0m Trial 102 finished with value: 0.5655297146073297 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:01:00,952]\u001b[0m Trial 103 finished with value: 0.6822693744521852 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:01:30,827]\u001b[0m Trial 104 finished with value: 0.5553091855722927 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:01:59,631]\u001b[0m Trial 105 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:02:28,553]\u001b[0m Trial 106 finished with value: 0.6880611334672301 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:02:57,465]\u001b[0m Trial 107 finished with value: 0.674983655929369 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:03:26,264]\u001b[0m Trial 108 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:03:55,069]\u001b[0m Trial 109 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:04:27,022]\u001b[0m Trial 110 finished with value: 0.019762628731190522 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:04:55,829]\u001b[0m Trial 111 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:05:24,633]\u001b[0m Trial 112 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:05:53,432]\u001b[0m Trial 113 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:06:25,268]\u001b[0m Trial 114 finished with value: 0.04444127125723961 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:06:56,845]\u001b[0m Trial 115 finished with value: 0.08356829146951811 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:07:25,408]\u001b[0m Trial 116 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:07:54,641]\u001b[0m Trial 117 finished with value: 0.6428280948906867 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:08:25,259]\u001b[0m Trial 118 finished with value: 0.3445210163912191 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:08:55,086]\u001b[0m Trial 119 finished with value: 0.4993058623087355 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:09:26,591]\u001b[0m Trial 120 finished with value: 0.08850541830251579 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:09:55,392]\u001b[0m Trial 121 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:10:24,205]\u001b[0m Trial 122 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:10:55,940]\u001b[0m Trial 123 finished with value: -0.007607504330183068 and parameters: {'C': 0.015625, 'gamma': 1.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:11:24,671]\u001b[0m Trial 124 finished with value: 0.6888373555442142 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:11:55,528]\u001b[0m Trial 125 finished with value: 0.24025580537917984 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:12:24,453]\u001b[0m Trial 126 finished with value: 0.6880611334672301 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:12:54,223]\u001b[0m Trial 127 finished with value: 0.4995859420415896 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:13:23,028]\u001b[0m Trial 128 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:13:52,915]\u001b[0m Trial 129 finished with value: 0.4942961985703585 and parameters: {'C': 0.5, 'gamma': 0.001953125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:14:22,602]\u001b[0m Trial 130 finished with value: 0.5985650101251363 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:14:51,321]\u001b[0m Trial 131 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:15:20,122]\u001b[0m Trial 132 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:15:48,929]\u001b[0m Trial 133 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:16:20,750]\u001b[0m Trial 134 finished with value: 0.02377608109821434 and parameters: {'C': 64.0, 'gamma': 2.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:16:49,675]\u001b[0m Trial 135 finished with value: 0.6880611334672301 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:17:19,888]\u001b[0m Trial 136 finished with value: 0.43967543487411254 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:17:48,580]\u001b[0m Trial 137 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:18:16,917]\u001b[0m Trial 138 finished with value: 0.6809705665863109 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:18:45,680]\u001b[0m Trial 139 finished with value: 0.6880611334672301 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:19:17,129]\u001b[0m Trial 140 finished with value: 0.07365646805173683 and parameters: {'C': 0.0625, 'gamma': 0.00048828125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:19:45,930]\u001b[0m Trial 141 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:20:14,732]\u001b[0m Trial 142 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:20:43,542]\u001b[0m Trial 143 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 06:21:12,351]\u001b[0m Trial 144 finished with value: 0.6922791334295182 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:21:44,021]\u001b[0m Trial 145 finished with value: 0.007720535589385047 and parameters: {'C': 0.03125, 'gamma': 0.125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:22:16,733]\u001b[0m Trial 146 finished with value: 0.01911321783473118 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:22:45,424]\u001b[0m Trial 147 finished with value: 0.6681547457498589 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:23:13,903]\u001b[0m Trial 148 finished with value: 0.6880611334672301 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:23:43,852]\u001b[0m Trial 149 finished with value: 0.36165633756459814 and parameters: {'C': 0.25, 'gamma': 0.0625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6936\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.673671    0.721557    0.722501\n",
      "1                    TP  320.000000  341.000000  328.000000\n",
      "2                    TN  176.000000  168.000000  191.000000\n",
      "3                    FP   61.000000   50.000000   46.000000\n",
      "4                    FN   38.000000   36.000000   30.000000\n",
      "5              Accuracy    0.833613    0.855462    0.872269\n",
      "6             Precision    0.839895    0.872123    0.877005\n",
      "7           Sensitivity    0.893855    0.904509    0.916201\n",
      "8           Specificity    0.742600    0.770600    0.805900\n",
      "9              F1 score    0.866035    0.888021    0.896175\n",
      "10  F1 score (weighted)    0.831960    0.854382    0.871434\n",
      "11     F1 score (macro)    0.823261    0.842115    0.865118\n",
      "12    Balanced Accuracy    0.818235    0.837576    0.861054\n",
      "13                  MCC    0.649269    0.685325    0.731621\n",
      "14                  NPV    0.822400    0.823500    0.864300\n",
      "15              ROC_AUC    0.818235    0.837576    0.861054\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_svm_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_2_cat = np.where((y_pred_svm_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_svm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 06:24:15,593]\u001b[0m Trial 150 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:24:43,434]\u001b[0m Trial 151 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:25:11,266]\u001b[0m Trial 152 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:25:39,099]\u001b[0m Trial 153 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:26:06,932]\u001b[0m Trial 154 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:26:35,154]\u001b[0m Trial 155 finished with value: 0.6674287255669755 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:27:02,988]\u001b[0m Trial 156 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:27:30,962]\u001b[0m Trial 157 finished with value: 0.6778224583598063 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:28:02,077]\u001b[0m Trial 158 finished with value: 0.015864123789135265 and parameters: {'C': 2.0, 'gamma': 4.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:28:32,915]\u001b[0m Trial 159 finished with value: -0.005683903871876983 and parameters: {'C': 0.015625, 'gamma': 0.5}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:29:03,733]\u001b[0m Trial 160 finished with value: 0.08898828990938333 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:29:31,567]\u001b[0m Trial 161 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:29:59,397]\u001b[0m Trial 162 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:30:27,230]\u001b[0m Trial 163 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:30:58,138]\u001b[0m Trial 164 finished with value: 0.07130160407238859 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:31:27,637]\u001b[0m Trial 165 finished with value: 0.4291487517958785 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:31:55,474]\u001b[0m Trial 166 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:32:24,503]\u001b[0m Trial 167 finished with value: 0.5483342597784057 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:32:55,714]\u001b[0m Trial 168 finished with value: 0.025264612855401603 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:33:23,968]\u001b[0m Trial 169 finished with value: 0.6308571116450234 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:33:51,989]\u001b[0m Trial 170 finished with value: 0.6738491696830634 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:34:19,822]\u001b[0m Trial 171 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:34:48,130]\u001b[0m Trial 172 finished with value: 0.6464481936808206 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:35:15,970]\u001b[0m Trial 173 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:35:43,798]\u001b[0m Trial 174 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:36:13,473]\u001b[0m Trial 175 finished with value: 0.34358624215362543 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:36:41,304]\u001b[0m Trial 176 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:37:10,318]\u001b[0m Trial 177 finished with value: 0.49478500137569786 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:37:38,346]\u001b[0m Trial 178 finished with value: 0.6735281625043903 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:38:08,198]\u001b[0m Trial 179 finished with value: 0.3240458478571083 and parameters: {'C': 0.125, 'gamma': 0.001953125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:38:38,810]\u001b[0m Trial 180 finished with value: 0.13631274715470004 and parameters: {'C': 0.0625, 'gamma': 0.0009765625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:39:06,641]\u001b[0m Trial 181 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:39:34,467]\u001b[0m Trial 182 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:40:02,296]\u001b[0m Trial 183 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:40:30,127]\u001b[0m Trial 184 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:41:01,253]\u001b[0m Trial 185 finished with value: 0.01866392977815342 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:41:29,137]\u001b[0m Trial 186 finished with value: 0.6646221051206491 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:41:59,328]\u001b[0m Trial 187 finished with value: 0.2738768536864457 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:42:27,156]\u001b[0m Trial 188 finished with value: 0.6684729529044351 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:42:55,134]\u001b[0m Trial 189 finished with value: 0.6778224583598063 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:43:22,961]\u001b[0m Trial 190 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:43:50,856]\u001b[0m Trial 191 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:44:18,681]\u001b[0m Trial 192 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:44:46,513]\u001b[0m Trial 193 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 06:45:18,418]\u001b[0m Trial 194 finished with value: 0.0013767323257105613 and parameters: {'C': 0.25, 'gamma': 8.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:45:46,244]\u001b[0m Trial 195 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:46:15,019]\u001b[0m Trial 196 finished with value: 0.5844603624854804 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:46:42,853]\u001b[0m Trial 197 finished with value: 0.6837849755322805 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:47:10,822]\u001b[0m Trial 198 finished with value: 0.6778224583598063 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:47:39,818]\u001b[0m Trial 199 finished with value: 0.5478621898102264 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6936\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.673671    0.721557    0.722501    0.705563\n",
      "1                    TP  320.000000  341.000000  328.000000  327.000000\n",
      "2                    TN  176.000000  168.000000  191.000000  182.000000\n",
      "3                    FP   61.000000   50.000000   46.000000   49.000000\n",
      "4                    FN   38.000000   36.000000   30.000000   37.000000\n",
      "5              Accuracy    0.833613    0.855462    0.872269    0.855462\n",
      "6             Precision    0.839895    0.872123    0.877005    0.869681\n",
      "7           Sensitivity    0.893855    0.904509    0.916201    0.898352\n",
      "8           Specificity    0.742600    0.770600    0.805900    0.787900\n",
      "9              F1 score    0.866035    0.888021    0.896175    0.883784\n",
      "10  F1 score (weighted)    0.831960    0.854382    0.871434    0.854707\n",
      "11     F1 score (macro)    0.823261    0.842115    0.865118    0.846336\n",
      "12    Balanced Accuracy    0.818235    0.837576    0.861054    0.843115\n",
      "13                  MCC    0.649269    0.685325    0.731621    0.693443\n",
      "14                  NPV    0.822400    0.823500    0.864300    0.831100\n",
      "15              ROC_AUC    0.818235    0.837576    0.861054    0.843115\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_svm_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_3_cat = np.where((y_pred_svm_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_svm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 06:48:11,646]\u001b[0m Trial 200 finished with value: 0.6847505472753335 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:48:42,239]\u001b[0m Trial 201 finished with value: 0.2855049984430771 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:49:10,328]\u001b[0m Trial 202 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:49:38,420]\u001b[0m Trial 203 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:50:06,511]\u001b[0m Trial 204 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:50:34,593]\u001b[0m Trial 205 finished with value: 0.6838080142139599 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:51:03,446]\u001b[0m Trial 206 finished with value: 0.6619747005361456 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:51:31,546]\u001b[0m Trial 207 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:52:02,646]\u001b[0m Trial 208 finished with value: -0.003895512280944935 and parameters: {'C': 0.015625, 'gamma': 4.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:52:30,703]\u001b[0m Trial 209 finished with value: 0.6838080142139599 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:52:58,802]\u001b[0m Trial 210 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:53:26,893]\u001b[0m Trial 211 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:53:57,769]\u001b[0m Trial 212 finished with value: 0.09271176869048084 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:54:28,926]\u001b[0m Trial 213 finished with value: 0.046578183007845754 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:54:57,019]\u001b[0m Trial 214 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:55:25,110]\u001b[0m Trial 215 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:55:54,402]\u001b[0m Trial 216 finished with value: 0.43550481546610864 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:56:25,302]\u001b[0m Trial 217 finished with value: 0.0898305115722514 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:56:53,387]\u001b[0m Trial 218 finished with value: 0.6288426477413632 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:57:22,967]\u001b[0m Trial 219 finished with value: 0.583265742192246 and parameters: {'C': 64.0, 'gamma': 0.000244140625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:57:51,439]\u001b[0m Trial 220 finished with value: 0.6437620517373328 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:58:19,549]\u001b[0m Trial 221 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:58:47,644]\u001b[0m Trial 222 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:59:15,735]\u001b[0m Trial 223 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 06:59:43,784]\u001b[0m Trial 224 finished with value: 0.6838080142139599 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:00:11,875]\u001b[0m Trial 225 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:00:43,158]\u001b[0m Trial 226 finished with value: 0.03105860741601805 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:01:12,361]\u001b[0m Trial 227 finished with value: 0.4892111103741539 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:01:42,101]\u001b[0m Trial 228 finished with value: 0.3525118298545374 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:02:10,225]\u001b[0m Trial 229 finished with value: 0.6791206624153354 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:02:41,120]\u001b[0m Trial 230 finished with value: 0.018591797086438867 and parameters: {'C': 0.0625, 'gamma': 0.0001220703125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:03:09,219]\u001b[0m Trial 231 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:03:37,309]\u001b[0m Trial 232 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:04:05,401]\u001b[0m Trial 233 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:04:33,496]\u001b[0m Trial 234 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:05:02,119]\u001b[0m Trial 235 finished with value: 0.6127040125946527 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:05:30,897]\u001b[0m Trial 236 finished with value: 0.6012038252453241 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:05:58,988]\u001b[0m Trial 237 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:06:26,901]\u001b[0m Trial 238 finished with value: 0.6669405366725947 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:06:57,888]\u001b[0m Trial 239 finished with value: -0.0031966126691167383 and parameters: {'C': 0.03125, 'gamma': 2.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:07:25,980]\u001b[0m Trial 240 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:07:54,082]\u001b[0m Trial 241 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:08:22,182]\u001b[0m Trial 242 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:08:50,274]\u001b[0m Trial 243 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 07:09:18,326]\u001b[0m Trial 244 finished with value: 0.6838080142139599 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:09:46,418]\u001b[0m Trial 245 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:10:15,028]\u001b[0m Trial 246 finished with value: 0.5689108683898148 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:10:43,033]\u001b[0m Trial 247 finished with value: 0.6727188228430229 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:11:11,842]\u001b[0m Trial 248 finished with value: 0.5793622791368476 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:11:39,933]\u001b[0m Trial 249 finished with value: 0.6858118314811711 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6936\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.673671    0.721557    0.722501    0.705563   \n",
      "1                    TP  320.000000  341.000000  328.000000  327.000000   \n",
      "2                    TN  176.000000  168.000000  191.000000  182.000000   \n",
      "3                    FP   61.000000   50.000000   46.000000   49.000000   \n",
      "4                    FN   38.000000   36.000000   30.000000   37.000000   \n",
      "5              Accuracy    0.833613    0.855462    0.872269    0.855462   \n",
      "6             Precision    0.839895    0.872123    0.877005    0.869681   \n",
      "7           Sensitivity    0.893855    0.904509    0.916201    0.898352   \n",
      "8           Specificity    0.742600    0.770600    0.805900    0.787900   \n",
      "9              F1 score    0.866035    0.888021    0.896175    0.883784   \n",
      "10  F1 score (weighted)    0.831960    0.854382    0.871434    0.854707   \n",
      "11     F1 score (macro)    0.823261    0.842115    0.865118    0.846336   \n",
      "12    Balanced Accuracy    0.818235    0.837576    0.861054    0.843115   \n",
      "13                  MCC    0.649269    0.685325    0.731621    0.693443   \n",
      "14                  NPV    0.822400    0.823500    0.864300    0.831100   \n",
      "15              ROC_AUC    0.818235    0.837576    0.861054    0.843115   \n",
      "\n",
      "          Set4  \n",
      "0     0.728225  \n",
      "1   328.000000  \n",
      "2   181.000000  \n",
      "3    49.000000  \n",
      "4    37.000000  \n",
      "5     0.855462  \n",
      "6     0.870027  \n",
      "7     0.898630  \n",
      "8     0.787000  \n",
      "9     0.884097  \n",
      "10    0.854695  \n",
      "11    0.846066  \n",
      "12    0.842793  \n",
      "13    0.692905  \n",
      "14    0.830300  \n",
      "15    0.842793  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_svm_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_4_cat = np.where((y_pred_svm_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_svm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 07:12:12,009]\u001b[0m Trial 250 finished with value: 0.6771381472708761 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:12:40,261]\u001b[0m Trial 251 finished with value: 0.6750612859736858 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:13:08,430]\u001b[0m Trial 252 finished with value: 0.6771381472708761 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:13:39,514]\u001b[0m Trial 253 finished with value: 0.2709446352542908 and parameters: {'C': 2.0, 'gamma': 0.125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:14:10,157]\u001b[0m Trial 254 finished with value: 0.16315629629956135 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:14:39,113]\u001b[0m Trial 255 finished with value: 0.5447900759910524 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:15:09,853]\u001b[0m Trial 256 finished with value: 0.08599397526116838 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:15:38,101]\u001b[0m Trial 257 finished with value: 0.6750612859736858 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:16:06,266]\u001b[0m Trial 258 finished with value: 0.6771381472708761 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:16:34,432]\u001b[0m Trial 259 finished with value: 0.6771381472708761 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:17:02,706]\u001b[0m Trial 260 finished with value: 0.6729754573269918 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:17:33,819]\u001b[0m Trial 261 finished with value: 0.0345854373503342 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:18:01,988]\u001b[0m Trial 262 finished with value: 0.6771381472708761 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:18:33,947]\u001b[0m Trial 263 finished with value: 0.004005868431943327 and parameters: {'C': 0.5, 'gamma': 8.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:19:02,272]\u001b[0m Trial 264 finished with value: 0.6641851761455021 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:19:30,517]\u001b[0m Trial 265 finished with value: 0.6750612859736858 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:19:59,418]\u001b[0m Trial 266 finished with value: 0.568674251679739 and parameters: {'C': 64.0, 'gamma': 6.103515625e-05}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:20:30,519]\u001b[0m Trial 267 finished with value: 0.07587991455051159 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:21:01,738]\u001b[0m Trial 268 finished with value: 0.008568783654035827 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:21:30,014]\u001b[0m Trial 269 finished with value: 0.6716438525295543 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:21:59,132]\u001b[0m Trial 270 finished with value: 0.4811818040275881 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:22:27,945]\u001b[0m Trial 271 finished with value: 0.5427975120925908 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:22:59,084]\u001b[0m Trial 272 finished with value: 0.019756726835716498 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:23:27,242]\u001b[0m Trial 273 finished with value: 0.6771381472708761 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:23:57,248]\u001b[0m Trial 274 finished with value: 0.3007694607531014 and parameters: {'C': 0.0625, 'gamma': 0.00390625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:24:25,581]\u001b[0m Trial 275 finished with value: 0.6715278067112023 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:24:53,827]\u001b[0m Trial 276 finished with value: 0.6750612859736858 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:25:23,999]\u001b[0m Trial 277 finished with value: 0.23940101573065403 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:25:53,533]\u001b[0m Trial 278 finished with value: 0.4314976720140389 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:26:21,783]\u001b[0m Trial 279 finished with value: 0.6750612859736858 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:26:51,926]\u001b[0m Trial 280 finished with value: 0.2721326483327866 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:27:20,671]\u001b[0m Trial 281 finished with value: 0.5442868088688398 and parameters: {'C': 1.0, 'gamma': 0.001953125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:27:49,478]\u001b[0m Trial 282 finished with value: 0.576906321069024 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:28:17,642]\u001b[0m Trial 283 finished with value: 0.6771381472708761 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:28:45,895]\u001b[0m Trial 284 finished with value: 0.6750612859736858 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:29:14,677]\u001b[0m Trial 285 finished with value: 0.5606338942794378 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:29:45,868]\u001b[0m Trial 286 finished with value: 0.011678053786908848 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:30:13,948]\u001b[0m Trial 287 finished with value: 0.6579218112305568 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:30:42,194]\u001b[0m Trial 288 finished with value: 0.6750612859736858 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:31:10,359]\u001b[0m Trial 289 finished with value: 0.6771381472708761 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:31:38,517]\u001b[0m Trial 290 finished with value: 0.6771381472708761 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:32:06,764]\u001b[0m Trial 291 finished with value: 0.6750612859736858 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:32:34,927]\u001b[0m Trial 292 finished with value: 0.6771381472708761 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:33:05,677]\u001b[0m Trial 293 finished with value: 0.08599397526116838 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 07:33:37,860]\u001b[0m Trial 294 finished with value: 0.008330012280800447 and parameters: {'C': 2.0, 'gamma': 8.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:34:06,778]\u001b[0m Trial 295 finished with value: 0.5863154003897897 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:34:37,408]\u001b[0m Trial 296 finished with value: 0.2722770977264709 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:35:05,659]\u001b[0m Trial 297 finished with value: 0.6750612859736858 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:35:33,819]\u001b[0m Trial 298 finished with value: 0.6771381472708761 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:36:04,655]\u001b[0m Trial 299 finished with value: 0.03116073567564952 and parameters: {'C': 0.015625, 'gamma': 0.0625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6936\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.673671    0.721557    0.722501    0.705563   \n",
      "1                    TP  320.000000  341.000000  328.000000  327.000000   \n",
      "2                    TN  176.000000  168.000000  191.000000  182.000000   \n",
      "3                    FP   61.000000   50.000000   46.000000   49.000000   \n",
      "4                    FN   38.000000   36.000000   30.000000   37.000000   \n",
      "5              Accuracy    0.833613    0.855462    0.872269    0.855462   \n",
      "6             Precision    0.839895    0.872123    0.877005    0.869681   \n",
      "7           Sensitivity    0.893855    0.904509    0.916201    0.898352   \n",
      "8           Specificity    0.742600    0.770600    0.805900    0.787900   \n",
      "9              F1 score    0.866035    0.888021    0.896175    0.883784   \n",
      "10  F1 score (weighted)    0.831960    0.854382    0.871434    0.854707   \n",
      "11     F1 score (macro)    0.823261    0.842115    0.865118    0.846336   \n",
      "12    Balanced Accuracy    0.818235    0.837576    0.861054    0.843115   \n",
      "13                  MCC    0.649269    0.685325    0.731621    0.693443   \n",
      "14                  NPV    0.822400    0.823500    0.864300    0.831100   \n",
      "15              ROC_AUC    0.818235    0.837576    0.861054    0.843115   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.728225    0.737113  \n",
      "1   328.000000  336.000000  \n",
      "2   181.000000  185.000000  \n",
      "3    49.000000   37.000000  \n",
      "4    37.000000   37.000000  \n",
      "5     0.855462    0.875630  \n",
      "6     0.870027    0.900804  \n",
      "7     0.898630    0.900804  \n",
      "8     0.787000    0.833300  \n",
      "9     0.884097    0.900804  \n",
      "10    0.854695    0.875630  \n",
      "11    0.846066    0.867069  \n",
      "12    0.842793    0.867069  \n",
      "13    0.692905    0.734138  \n",
      "14    0.830300    0.833300  \n",
      "15    0.842793    0.867069  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_svm_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_5_cat = np.where((y_pred_svm_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_svm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 07:36:36,657]\u001b[0m Trial 300 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:37:04,763]\u001b[0m Trial 301 finished with value: 0.6900144967469092 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:37:32,807]\u001b[0m Trial 302 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:38:00,871]\u001b[0m Trial 303 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:38:28,964]\u001b[0m Trial 304 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:38:57,010]\u001b[0m Trial 305 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:39:25,235]\u001b[0m Trial 306 finished with value: 0.684807956229081 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:39:56,149]\u001b[0m Trial 307 finished with value: 0.014801908051526046 and parameters: {'C': 0.5, 'gamma': 4.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:40:24,551]\u001b[0m Trial 308 finished with value: 0.6784272216212055 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:40:52,599]\u001b[0m Trial 309 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:41:20,643]\u001b[0m Trial 310 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:41:49,572]\u001b[0m Trial 311 finished with value: 0.4886205366462746 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:42:20,455]\u001b[0m Trial 312 finished with value: 0.0817006815548894 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:42:51,595]\u001b[0m Trial 313 finished with value: 0.04481246620367514 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:43:19,735]\u001b[0m Trial 314 finished with value: 0.6900144967469092 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:43:50,147]\u001b[0m Trial 315 finished with value: 0.3514578736269384 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:44:20,162]\u001b[0m Trial 316 finished with value: 0.3924616777871333 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:44:49,057]\u001b[0m Trial 317 finished with value: 0.6845368705493827 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:45:17,791]\u001b[0m Trial 318 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:45:47,250]\u001b[0m Trial 319 finished with value: 0.5018677475198609 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:46:15,853]\u001b[0m Trial 320 finished with value: 0.6581532868752584 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:46:47,131]\u001b[0m Trial 321 finished with value: 0.03263309997520618 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:47:17,254]\u001b[0m Trial 322 finished with value: 0.27756306124818997 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:47:45,462]\u001b[0m Trial 323 finished with value: 0.6704269783670967 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:48:14,148]\u001b[0m Trial 324 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:48:44,585]\u001b[0m Trial 325 finished with value: 0.35176835477839896 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:49:14,390]\u001b[0m Trial 326 finished with value: 0.437335584481594 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:49:42,940]\u001b[0m Trial 327 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:50:11,832]\u001b[0m Trial 328 finished with value: 0.5688865454301897 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:50:40,183]\u001b[0m Trial 329 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:51:08,231]\u001b[0m Trial 330 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:51:36,343]\u001b[0m Trial 331 finished with value: 0.6900144967469092 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:52:05,215]\u001b[0m Trial 332 finished with value: 0.6184401518997927 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:52:33,957]\u001b[0m Trial 333 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:53:02,324]\u001b[0m Trial 334 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:53:31,118]\u001b[0m Trial 335 finished with value: 0.6900144967469092 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:54:00,546]\u001b[0m Trial 336 finished with value: 0.5873921162994891 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:54:29,410]\u001b[0m Trial 337 finished with value: 0.68741138289376 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:55:01,290]\u001b[0m Trial 338 finished with value: 0.025674573736849726 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:55:32,435]\u001b[0m Trial 339 finished with value: 0.165706355608711 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:56:01,271]\u001b[0m Trial 340 finished with value: 0.6725843239844911 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:56:30,018]\u001b[0m Trial 341 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:56:58,638]\u001b[0m Trial 342 finished with value: 0.690419811600216 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:57:30,201]\u001b[0m Trial 343 finished with value: 0.0021002224832342266 and parameters: {'C': 0.0078125, 'gamma': 0.00048828125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 07:57:58,767]\u001b[0m Trial 344 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:58:27,516]\u001b[0m Trial 345 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:58:56,254]\u001b[0m Trial 346 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:59:27,590]\u001b[0m Trial 347 finished with value: 0.27097423627476475 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 07:59:56,329]\u001b[0m Trial 348 finished with value: 0.693142561254053 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:00:25,217]\u001b[0m Trial 349 finished with value: 0.6859912850022686 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6936\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.673671    0.721557    0.722501    0.705563   \n",
      "1                    TP  320.000000  341.000000  328.000000  327.000000   \n",
      "2                    TN  176.000000  168.000000  191.000000  182.000000   \n",
      "3                    FP   61.000000   50.000000   46.000000   49.000000   \n",
      "4                    FN   38.000000   36.000000   30.000000   37.000000   \n",
      "5              Accuracy    0.833613    0.855462    0.872269    0.855462   \n",
      "6             Precision    0.839895    0.872123    0.877005    0.869681   \n",
      "7           Sensitivity    0.893855    0.904509    0.916201    0.898352   \n",
      "8           Specificity    0.742600    0.770600    0.805900    0.787900   \n",
      "9              F1 score    0.866035    0.888021    0.896175    0.883784   \n",
      "10  F1 score (weighted)    0.831960    0.854382    0.871434    0.854707   \n",
      "11     F1 score (macro)    0.823261    0.842115    0.865118    0.846336   \n",
      "12    Balanced Accuracy    0.818235    0.837576    0.861054    0.843115   \n",
      "13                  MCC    0.649269    0.685325    0.731621    0.693443   \n",
      "14                  NPV    0.822400    0.823500    0.864300    0.831100   \n",
      "15              ROC_AUC    0.818235    0.837576    0.861054    0.843115   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.728225    0.737113    0.714586  \n",
      "1   328.000000  336.000000  328.000000  \n",
      "2   181.000000  185.000000  172.000000  \n",
      "3    49.000000   37.000000   60.000000  \n",
      "4    37.000000   37.000000   35.000000  \n",
      "5     0.855462    0.875630    0.840336  \n",
      "6     0.870027    0.900804    0.845361  \n",
      "7     0.898630    0.900804    0.903581  \n",
      "8     0.787000    0.833300    0.741400  \n",
      "9     0.884097    0.900804    0.873502  \n",
      "10    0.854695    0.875630    0.838447  \n",
      "11    0.846066    0.867069    0.828551  \n",
      "12    0.842793    0.867069    0.822480  \n",
      "13    0.692905    0.734138    0.660434  \n",
      "14    0.830300    0.833300    0.830900  \n",
      "15    0.842793    0.867069    0.822480  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_svm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_6_cat = np.where((y_pred_svm_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_svm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 08:01:02,143]\u001b[0m Trial 350 finished with value: 0.02373579483380388 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:01:31,548]\u001b[0m Trial 351 finished with value: 0.5668142829212602 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 31 with value: 0.6936052872848771.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:01:59,827]\u001b[0m Trial 352 finished with value: 0.6989360807510077 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:02:28,253]\u001b[0m Trial 353 finished with value: 0.6432445932749933 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:02:56,495]\u001b[0m Trial 354 finished with value: 0.6841380760038571 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:03:24,667]\u001b[0m Trial 355 finished with value: 0.6989360807510077 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:03:53,244]\u001b[0m Trial 356 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:04:21,825]\u001b[0m Trial 357 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:04:52,991]\u001b[0m Trial 358 finished with value: 0.02416239443680048 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:05:21,267]\u001b[0m Trial 359 finished with value: 0.6841261383660431 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:05:49,566]\u001b[0m Trial 360 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:06:20,952]\u001b[0m Trial 361 finished with value: 0.0506268401815552 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:06:49,117]\u001b[0m Trial 362 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:07:20,747]\u001b[0m Trial 363 finished with value: 0.09357794471831324 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:07:48,645]\u001b[0m Trial 364 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:08:16,620]\u001b[0m Trial 365 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:08:46,064]\u001b[0m Trial 366 finished with value: 0.4457028215033101 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:09:15,219]\u001b[0m Trial 367 finished with value: 0.5643744058870709 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:09:44,006]\u001b[0m Trial 368 finished with value: 0.665557191412391 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:10:15,208]\u001b[0m Trial 369 finished with value: 0.034921289542474754 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:10:43,100]\u001b[0m Trial 370 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:11:11,371]\u001b[0m Trial 371 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:11:39,875]\u001b[0m Trial 372 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:12:08,379]\u001b[0m Trial 373 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:12:37,928]\u001b[0m Trial 374 finished with value: 0.35527918988677726 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:13:05,853]\u001b[0m Trial 375 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:13:34,646]\u001b[0m Trial 376 finished with value: 0.5120082634250994 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:14:02,535]\u001b[0m Trial 377 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:14:30,426]\u001b[0m Trial 378 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:14:59,192]\u001b[0m Trial 379 finished with value: 0.6446178951976267 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:15:27,962]\u001b[0m Trial 380 finished with value: 0.6226490189028817 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:15:55,847]\u001b[0m Trial 381 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:16:23,736]\u001b[0m Trial 382 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:16:54,760]\u001b[0m Trial 383 finished with value: 0.027083661724825438 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:17:22,644]\u001b[0m Trial 384 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:17:50,537]\u001b[0m Trial 385 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:18:18,422]\u001b[0m Trial 386 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:18:46,317]\u001b[0m Trial 387 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:19:14,121]\u001b[0m Trial 388 finished with value: 0.6784879602410437 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:19:42,009]\u001b[0m Trial 389 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:20:09,895]\u001b[0m Trial 390 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:20:37,788]\u001b[0m Trial 391 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:21:05,678]\u001b[0m Trial 392 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:21:33,565]\u001b[0m Trial 393 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 08:22:01,450]\u001b[0m Trial 394 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:22:29,342]\u001b[0m Trial 395 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:22:57,281]\u001b[0m Trial 396 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:23:25,187]\u001b[0m Trial 397 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:23:53,832]\u001b[0m Trial 398 finished with value: 0.6016674377326232 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:24:21,720]\u001b[0m Trial 399 finished with value: 0.6954236535729417 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6989\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.673671    0.721557    0.722501    0.705563   \n",
      "1                    TP  320.000000  341.000000  328.000000  327.000000   \n",
      "2                    TN  176.000000  168.000000  191.000000  182.000000   \n",
      "3                    FP   61.000000   50.000000   46.000000   49.000000   \n",
      "4                    FN   38.000000   36.000000   30.000000   37.000000   \n",
      "5              Accuracy    0.833613    0.855462    0.872269    0.855462   \n",
      "6             Precision    0.839895    0.872123    0.877005    0.869681   \n",
      "7           Sensitivity    0.893855    0.904509    0.916201    0.898352   \n",
      "8           Specificity    0.742600    0.770600    0.805900    0.787900   \n",
      "9              F1 score    0.866035    0.888021    0.896175    0.883784   \n",
      "10  F1 score (weighted)    0.831960    0.854382    0.871434    0.854707   \n",
      "11     F1 score (macro)    0.823261    0.842115    0.865118    0.846336   \n",
      "12    Balanced Accuracy    0.818235    0.837576    0.861054    0.843115   \n",
      "13                  MCC    0.649269    0.685325    0.731621    0.693443   \n",
      "14                  NPV    0.822400    0.823500    0.864300    0.831100   \n",
      "15              ROC_AUC    0.818235    0.837576    0.861054    0.843115   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.728225    0.737113    0.714586    0.686813  \n",
      "1   328.000000  336.000000  328.000000  323.000000  \n",
      "2   181.000000  185.000000  172.000000  180.000000  \n",
      "3    49.000000   37.000000   60.000000   48.000000  \n",
      "4    37.000000   37.000000   35.000000   44.000000  \n",
      "5     0.855462    0.875630    0.840336    0.845378  \n",
      "6     0.870027    0.900804    0.845361    0.870620  \n",
      "7     0.898630    0.900804    0.903581    0.880109  \n",
      "8     0.787000    0.833300    0.741400    0.789500  \n",
      "9     0.884097    0.900804    0.873502    0.875339  \n",
      "10    0.854695    0.875630    0.838447    0.845113  \n",
      "11    0.846066    0.867069    0.828551    0.835899  \n",
      "12    0.842793    0.867069    0.822480    0.834791  \n",
      "13    0.692905    0.734138    0.660434    0.671883  \n",
      "14    0.830300    0.833300    0.830900    0.803600  \n",
      "15    0.842793    0.867069    0.822480    0.834791  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_svm_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_7_cat = np.where((y_pred_svm_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_svm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 08:24:57,863]\u001b[0m Trial 400 finished with value: 0.011914237178878406 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:25:26,063]\u001b[0m Trial 401 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:25:56,720]\u001b[0m Trial 402 finished with value: 0.26436386042617904 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:26:24,921]\u001b[0m Trial 403 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:26:53,787]\u001b[0m Trial 404 finished with value: 0.5417357256767776 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:27:21,986]\u001b[0m Trial 405 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:27:50,189]\u001b[0m Trial 406 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:28:18,389]\u001b[0m Trial 407 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:28:46,591]\u001b[0m Trial 408 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:29:15,155]\u001b[0m Trial 409 finished with value: 0.675247422915098 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:29:43,354]\u001b[0m Trial 410 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:30:14,570]\u001b[0m Trial 411 finished with value: 0.012199092816747104 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:30:45,667]\u001b[0m Trial 412 finished with value: 0.032122188134808084 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:31:13,863]\u001b[0m Trial 413 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:31:42,093]\u001b[0m Trial 414 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:32:13,045]\u001b[0m Trial 415 finished with value: 0.07232029357332528 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:32:42,291]\u001b[0m Trial 416 finished with value: 0.4321421285355428 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:33:10,480]\u001b[0m Trial 417 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:33:39,262]\u001b[0m Trial 418 finished with value: 0.5474341059794716 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:34:07,419]\u001b[0m Trial 419 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:34:36,159]\u001b[0m Trial 420 finished with value: 0.6510501723234968 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:35:07,228]\u001b[0m Trial 421 finished with value: 0.019423208287897408 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:35:35,370]\u001b[0m Trial 422 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:36:03,514]\u001b[0m Trial 423 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:36:31,659]\u001b[0m Trial 424 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:37:00,718]\u001b[0m Trial 425 finished with value: 0.49680196415251715 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:37:28,855]\u001b[0m Trial 426 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:37:57,002]\u001b[0m Trial 427 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:38:25,762]\u001b[0m Trial 428 finished with value: 0.6264030646493015 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:38:55,420]\u001b[0m Trial 429 finished with value: 0.3483986504047717 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:39:23,581]\u001b[0m Trial 430 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:39:51,740]\u001b[0m Trial 431 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:40:22,812]\u001b[0m Trial 432 finished with value: 0.014082923496659272 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:40:51,556]\u001b[0m Trial 433 finished with value: 0.6028545502276483 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:41:19,719]\u001b[0m Trial 434 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:41:47,879]\u001b[0m Trial 435 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:42:16,032]\u001b[0m Trial 436 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:42:44,148]\u001b[0m Trial 437 finished with value: 0.660692420363646 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:43:12,350]\u001b[0m Trial 438 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:43:42,789]\u001b[0m Trial 439 finished with value: 0.14252415873126872 and parameters: {'C': 0.125, 'gamma': 0.00048828125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:44:12,392]\u001b[0m Trial 440 finished with value: 0.3904652354028205 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:44:40,790]\u001b[0m Trial 441 finished with value: 0.6721578441523743 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:45:08,924]\u001b[0m Trial 442 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:45:39,501]\u001b[0m Trial 443 finished with value: 0.26436386042617904 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 08:46:11,687]\u001b[0m Trial 444 finished with value: 0.011914237178878406 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:46:41,721]\u001b[0m Trial 445 finished with value: 0.27619614455190267 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:47:09,839]\u001b[0m Trial 446 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:47:37,955]\u001b[0m Trial 447 finished with value: 0.68112229422167 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:48:05,780]\u001b[0m Trial 448 finished with value: 0.6643908077928091 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:48:34,561]\u001b[0m Trial 449 finished with value: 0.5417357256767776 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6989\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.673671    0.721557    0.722501    0.705563   \n",
      "1                    TP  320.000000  341.000000  328.000000  327.000000   \n",
      "2                    TN  176.000000  168.000000  191.000000  182.000000   \n",
      "3                    FP   61.000000   50.000000   46.000000   49.000000   \n",
      "4                    FN   38.000000   36.000000   30.000000   37.000000   \n",
      "5              Accuracy    0.833613    0.855462    0.872269    0.855462   \n",
      "6             Precision    0.839895    0.872123    0.877005    0.869681   \n",
      "7           Sensitivity    0.893855    0.904509    0.916201    0.898352   \n",
      "8           Specificity    0.742600    0.770600    0.805900    0.787900   \n",
      "9              F1 score    0.866035    0.888021    0.896175    0.883784   \n",
      "10  F1 score (weighted)    0.831960    0.854382    0.871434    0.854707   \n",
      "11     F1 score (macro)    0.823261    0.842115    0.865118    0.846336   \n",
      "12    Balanced Accuracy    0.818235    0.837576    0.861054    0.843115   \n",
      "13                  MCC    0.649269    0.685325    0.731621    0.693443   \n",
      "14                  NPV    0.822400    0.823500    0.864300    0.831100   \n",
      "15              ROC_AUC    0.818235    0.837576    0.861054    0.843115   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.728225    0.737113    0.714586    0.686813    0.715745  \n",
      "1   328.000000  336.000000  328.000000  323.000000  312.000000  \n",
      "2   181.000000  185.000000  172.000000  180.000000  182.000000  \n",
      "3    49.000000   37.000000   60.000000   48.000000   62.000000  \n",
      "4    37.000000   37.000000   35.000000   44.000000   39.000000  \n",
      "5     0.855462    0.875630    0.840336    0.845378    0.830252  \n",
      "6     0.870027    0.900804    0.845361    0.870620    0.834225  \n",
      "7     0.898630    0.900804    0.903581    0.880109    0.888889  \n",
      "8     0.787000    0.833300    0.741400    0.789500    0.745900  \n",
      "9     0.884097    0.900804    0.873502    0.875339    0.860690  \n",
      "10    0.854695    0.875630    0.838447    0.845113    0.828747  \n",
      "11    0.846066    0.867069    0.828551    0.835899    0.821743  \n",
      "12    0.842793    0.867069    0.822480    0.834791    0.817395  \n",
      "13    0.692905    0.734138    0.660434    0.671883    0.646170  \n",
      "14    0.830300    0.833300    0.830900    0.803600    0.823500  \n",
      "15    0.842793    0.867069    0.822480    0.834791    0.817395  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_svm_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_8_cat = np.where((y_pred_svm_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_svm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 08:49:07,179]\u001b[0m Trial 450 finished with value: 0.5747021874298335 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:49:35,263]\u001b[0m Trial 451 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:50:03,357]\u001b[0m Trial 452 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:50:31,747]\u001b[0m Trial 453 finished with value: 0.6727144875800344 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:51:02,760]\u001b[0m Trial 454 finished with value: 0.04520393650991376 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:51:33,890]\u001b[0m Trial 455 finished with value: 0.02053960497891223 and parameters: {'C': 2.0, 'gamma': 4.0}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:52:01,979]\u001b[0m Trial 456 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:52:32,823]\u001b[0m Trial 457 finished with value: -0.0033517196274526094 and parameters: {'C': 0.0078125, 'gamma': 0.25}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:53:03,165]\u001b[0m Trial 458 finished with value: 0.16901681239313396 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:53:31,217]\u001b[0m Trial 459 finished with value: 0.6865676138123964 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:53:59,302]\u001b[0m Trial 460 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:54:28,166]\u001b[0m Trial 461 finished with value: 0.5577086656934002 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:54:57,056]\u001b[0m Trial 462 finished with value: 0.5572151151594901 and parameters: {'C': 32.0, 'gamma': 6.103515625e-05}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:55:25,628]\u001b[0m Trial 463 finished with value: 0.6528201346148957 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:55:53,696]\u001b[0m Trial 464 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:56:21,772]\u001b[0m Trial 465 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:56:52,848]\u001b[0m Trial 466 finished with value: 0.03106711007340688 and parameters: {'C': 64.0, 'gamma': 1.0}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:57:22,534]\u001b[0m Trial 467 finished with value: 0.3500805580214822 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:57:50,593]\u001b[0m Trial 468 finished with value: 0.6333726714990949 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:58:18,672]\u001b[0m Trial 469 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:58:46,754]\u001b[0m Trial 470 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:59:15,454]\u001b[0m Trial 471 finished with value: 0.6278287682997334 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 08:59:44,379]\u001b[0m Trial 472 finished with value: 0.5047016438804566 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:00:13,573]\u001b[0m Trial 473 finished with value: 0.4932509587037778 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:00:41,670]\u001b[0m Trial 474 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:01:12,753]\u001b[0m Trial 475 finished with value: 0.02363391177234988 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:01:40,844]\u001b[0m Trial 476 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:02:13,435]\u001b[0m Trial 477 finished with value: 0.5738565373062403 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:02:43,263]\u001b[0m Trial 478 finished with value: 0.3299344308858954 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:03:11,361]\u001b[0m Trial 479 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:03:39,450]\u001b[0m Trial 480 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:04:07,228]\u001b[0m Trial 481 finished with value: 0.6676599748917666 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:04:35,819]\u001b[0m Trial 482 finished with value: 0.5893115423495414 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:05:03,906]\u001b[0m Trial 483 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:05:33,972]\u001b[0m Trial 484 finished with value: 0.2782527824914478 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:06:02,061]\u001b[0m Trial 485 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:06:30,153]\u001b[0m Trial 486 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:07:00,794]\u001b[0m Trial 487 finished with value: 0.27194431753693205 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:07:29,473]\u001b[0m Trial 488 finished with value: 0.5747021874298335 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:08:01,712]\u001b[0m Trial 489 finished with value: 0.019919736852226866 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:08:29,766]\u001b[0m Trial 490 finished with value: 0.6865676138123964 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:08:57,562]\u001b[0m Trial 491 finished with value: 0.6846543630813147 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:09:25,631]\u001b[0m Trial 492 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:09:53,699]\u001b[0m Trial 493 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 09:10:21,770]\u001b[0m Trial 494 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:10:52,541]\u001b[0m Trial 495 finished with value: 0.03803375148296576 and parameters: {'C': 0.015625, 'gamma': 0.0625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:11:20,872]\u001b[0m Trial 496 finished with value: 0.6727144875800344 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:11:48,946]\u001b[0m Trial 497 finished with value: 0.6890571308396714 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:12:19,824]\u001b[0m Trial 498 finished with value: 0.08386893155393074 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 09:12:50,778]\u001b[0m Trial 499 finished with value: -0.004147896098499493 and parameters: {'C': 0.0078125, 'gamma': 4.0}. Best is trial 352 with value: 0.6989360807510077.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6989\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.673671    0.721557    0.722501    0.705563   \n",
      "1                    TP  320.000000  341.000000  328.000000  327.000000   \n",
      "2                    TN  176.000000  168.000000  191.000000  182.000000   \n",
      "3                    FP   61.000000   50.000000   46.000000   49.000000   \n",
      "4                    FN   38.000000   36.000000   30.000000   37.000000   \n",
      "5              Accuracy    0.833613    0.855462    0.872269    0.855462   \n",
      "6             Precision    0.839895    0.872123    0.877005    0.869681   \n",
      "7           Sensitivity    0.893855    0.904509    0.916201    0.898352   \n",
      "8           Specificity    0.742600    0.770600    0.805900    0.787900   \n",
      "9              F1 score    0.866035    0.888021    0.896175    0.883784   \n",
      "10  F1 score (weighted)    0.831960    0.854382    0.871434    0.854707   \n",
      "11     F1 score (macro)    0.823261    0.842115    0.865118    0.846336   \n",
      "12    Balanced Accuracy    0.818235    0.837576    0.861054    0.843115   \n",
      "13                  MCC    0.649269    0.685325    0.731621    0.693443   \n",
      "14                  NPV    0.822400    0.823500    0.864300    0.831100   \n",
      "15              ROC_AUC    0.818235    0.837576    0.861054    0.843115   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.728225    0.737113    0.714586    0.686813    0.715745    0.718249  \n",
      "1   328.000000  336.000000  328.000000  323.000000  312.000000  333.000000  \n",
      "2   181.000000  185.000000  172.000000  180.000000  182.000000  173.000000  \n",
      "3    49.000000   37.000000   60.000000   48.000000   62.000000   47.000000  \n",
      "4    37.000000   37.000000   35.000000   44.000000   39.000000   42.000000  \n",
      "5     0.855462    0.875630    0.840336    0.845378    0.830252    0.850420  \n",
      "6     0.870027    0.900804    0.845361    0.870620    0.834225    0.876316  \n",
      "7     0.898630    0.900804    0.903581    0.880109    0.888889    0.888000  \n",
      "8     0.787000    0.833300    0.741400    0.789500    0.745900    0.786400  \n",
      "9     0.884097    0.900804    0.873502    0.875339    0.860690    0.882119  \n",
      "10    0.854695    0.875630    0.838447    0.845113    0.828747    0.850056  \n",
      "11    0.846066    0.867069    0.828551    0.835899    0.821743    0.838761  \n",
      "12    0.842793    0.867069    0.822480    0.834791    0.817395    0.837182  \n",
      "13    0.692905    0.734138    0.660434    0.671883    0.646170    0.677657  \n",
      "14    0.830300    0.833300    0.830900    0.803600    0.823500    0.804700  \n",
      "15    0.842793    0.867069    0.822480    0.834791    0.817395    0.837182  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_svm_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_9_cat = np.where((y_pred_svm_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_svm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7b0e56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6989\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "#plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.713830</td>\n",
       "      <td>0.034916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>166.100000</td>\n",
       "      <td>5.877452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>89.500000</td>\n",
       "      <td>5.169354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>4.175324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>5.168279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.860312</td>\n",
       "      <td>0.019678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.874495</td>\n",
       "      <td>0.019568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.904297</td>\n",
       "      <td>0.027356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.789290</td>\n",
       "      <td>0.036002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.888838</td>\n",
       "      <td>0.016155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.859458</td>\n",
       "      <td>0.019663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.850220</td>\n",
       "      <td>0.021015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.846791</td>\n",
       "      <td>0.020479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.702395</td>\n",
       "      <td>0.041911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.836950</td>\n",
       "      <td>0.040158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.846791</td>\n",
       "      <td>0.020479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.713830     0.034916\n",
       "1                    TP       166.100000     5.877452\n",
       "2                    TN        89.500000     5.169354\n",
       "3                    FP        23.900000     4.175324\n",
       "4                    FN        17.600000     5.168279\n",
       "5              Accuracy         0.860312     0.019678\n",
       "6             Precision         0.874495     0.019568\n",
       "7           Sensitivity         0.904297     0.027356\n",
       "8           Specificity         0.789290     0.036002\n",
       "9              F1 score         0.888838     0.016155\n",
       "10  F1 score (weighted)         0.859458     0.019663\n",
       "11     F1 score (macro)         0.850220     0.021015\n",
       "12    Balanced Accuracy         0.846791     0.020479\n",
       "13                  MCC         0.702395     0.041911\n",
       "14                  NPV         0.836950     0.040158\n",
       "15              ROC_AUC         0.846791     0.020479"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.673671</td>\n",
       "      <td>0.721557</td>\n",
       "      <td>0.722501</td>\n",
       "      <td>0.705563</td>\n",
       "      <td>0.728225</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>0.714586</td>\n",
       "      <td>0.686813</td>\n",
       "      <td>0.715745</td>\n",
       "      <td>0.718249</td>\n",
       "      <td>0.712402</td>\n",
       "      <td>0.019145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>323.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>327.600000</td>\n",
       "      <td>8.181279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>6.815016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>50.900000</td>\n",
       "      <td>7.866243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>3.807887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.833613</td>\n",
       "      <td>0.855462</td>\n",
       "      <td>0.872269</td>\n",
       "      <td>0.855462</td>\n",
       "      <td>0.855462</td>\n",
       "      <td>0.875630</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.845378</td>\n",
       "      <td>0.830252</td>\n",
       "      <td>0.850420</td>\n",
       "      <td>0.851429</td>\n",
       "      <td>0.014890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.839895</td>\n",
       "      <td>0.872123</td>\n",
       "      <td>0.877005</td>\n",
       "      <td>0.869681</td>\n",
       "      <td>0.870027</td>\n",
       "      <td>0.900804</td>\n",
       "      <td>0.845361</td>\n",
       "      <td>0.870620</td>\n",
       "      <td>0.834225</td>\n",
       "      <td>0.876316</td>\n",
       "      <td>0.865606</td>\n",
       "      <td>0.020119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.893855</td>\n",
       "      <td>0.904509</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>0.898352</td>\n",
       "      <td>0.898630</td>\n",
       "      <td>0.900804</td>\n",
       "      <td>0.903581</td>\n",
       "      <td>0.880109</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.897293</td>\n",
       "      <td>0.010157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.770600</td>\n",
       "      <td>0.805900</td>\n",
       "      <td>0.787900</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.741400</td>\n",
       "      <td>0.789500</td>\n",
       "      <td>0.745900</td>\n",
       "      <td>0.786400</td>\n",
       "      <td>0.779050</td>\n",
       "      <td>0.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.866035</td>\n",
       "      <td>0.888021</td>\n",
       "      <td>0.896175</td>\n",
       "      <td>0.883784</td>\n",
       "      <td>0.884097</td>\n",
       "      <td>0.900804</td>\n",
       "      <td>0.873502</td>\n",
       "      <td>0.875339</td>\n",
       "      <td>0.860690</td>\n",
       "      <td>0.882119</td>\n",
       "      <td>0.881057</td>\n",
       "      <td>0.012539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.831960</td>\n",
       "      <td>0.854382</td>\n",
       "      <td>0.871434</td>\n",
       "      <td>0.854707</td>\n",
       "      <td>0.854695</td>\n",
       "      <td>0.875630</td>\n",
       "      <td>0.838447</td>\n",
       "      <td>0.845113</td>\n",
       "      <td>0.828747</td>\n",
       "      <td>0.850056</td>\n",
       "      <td>0.850517</td>\n",
       "      <td>0.015317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.823261</td>\n",
       "      <td>0.842115</td>\n",
       "      <td>0.865118</td>\n",
       "      <td>0.846336</td>\n",
       "      <td>0.846066</td>\n",
       "      <td>0.867069</td>\n",
       "      <td>0.828551</td>\n",
       "      <td>0.835899</td>\n",
       "      <td>0.821743</td>\n",
       "      <td>0.838761</td>\n",
       "      <td>0.841492</td>\n",
       "      <td>0.015584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.818235</td>\n",
       "      <td>0.837576</td>\n",
       "      <td>0.861054</td>\n",
       "      <td>0.843115</td>\n",
       "      <td>0.842793</td>\n",
       "      <td>0.867069</td>\n",
       "      <td>0.822480</td>\n",
       "      <td>0.834791</td>\n",
       "      <td>0.817395</td>\n",
       "      <td>0.837182</td>\n",
       "      <td>0.838169</td>\n",
       "      <td>0.016642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.649269</td>\n",
       "      <td>0.685325</td>\n",
       "      <td>0.731621</td>\n",
       "      <td>0.693443</td>\n",
       "      <td>0.692905</td>\n",
       "      <td>0.734138</td>\n",
       "      <td>0.660434</td>\n",
       "      <td>0.671883</td>\n",
       "      <td>0.646170</td>\n",
       "      <td>0.677657</td>\n",
       "      <td>0.684285</td>\n",
       "      <td>0.030424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.822400</td>\n",
       "      <td>0.823500</td>\n",
       "      <td>0.864300</td>\n",
       "      <td>0.831100</td>\n",
       "      <td>0.830300</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.830900</td>\n",
       "      <td>0.803600</td>\n",
       "      <td>0.823500</td>\n",
       "      <td>0.804700</td>\n",
       "      <td>0.826760</td>\n",
       "      <td>0.016878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.818235</td>\n",
       "      <td>0.837576</td>\n",
       "      <td>0.861054</td>\n",
       "      <td>0.843115</td>\n",
       "      <td>0.842793</td>\n",
       "      <td>0.867069</td>\n",
       "      <td>0.822480</td>\n",
       "      <td>0.834791</td>\n",
       "      <td>0.817395</td>\n",
       "      <td>0.837182</td>\n",
       "      <td>0.838169</td>\n",
       "      <td>0.016642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.673671    0.721557    0.722501    0.705563   \n",
       "1                    TP  320.000000  341.000000  328.000000  327.000000   \n",
       "2                    TN  176.000000  168.000000  191.000000  182.000000   \n",
       "3                    FP   61.000000   50.000000   46.000000   49.000000   \n",
       "4                    FN   38.000000   36.000000   30.000000   37.000000   \n",
       "5              Accuracy    0.833613    0.855462    0.872269    0.855462   \n",
       "6             Precision    0.839895    0.872123    0.877005    0.869681   \n",
       "7           Sensitivity    0.893855    0.904509    0.916201    0.898352   \n",
       "8           Specificity    0.742600    0.770600    0.805900    0.787900   \n",
       "9              F1 score    0.866035    0.888021    0.896175    0.883784   \n",
       "10  F1 score (weighted)    0.831960    0.854382    0.871434    0.854707   \n",
       "11     F1 score (macro)    0.823261    0.842115    0.865118    0.846336   \n",
       "12    Balanced Accuracy    0.818235    0.837576    0.861054    0.843115   \n",
       "13                  MCC    0.649269    0.685325    0.731621    0.693443   \n",
       "14                  NPV    0.822400    0.823500    0.864300    0.831100   \n",
       "15              ROC_AUC    0.818235    0.837576    0.861054    0.843115   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.728225    0.737113    0.714586    0.686813    0.715745    0.718249   \n",
       "1   328.000000  336.000000  328.000000  323.000000  312.000000  333.000000   \n",
       "2   181.000000  185.000000  172.000000  180.000000  182.000000  173.000000   \n",
       "3    49.000000   37.000000   60.000000   48.000000   62.000000   47.000000   \n",
       "4    37.000000   37.000000   35.000000   44.000000   39.000000   42.000000   \n",
       "5     0.855462    0.875630    0.840336    0.845378    0.830252    0.850420   \n",
       "6     0.870027    0.900804    0.845361    0.870620    0.834225    0.876316   \n",
       "7     0.898630    0.900804    0.903581    0.880109    0.888889    0.888000   \n",
       "8     0.787000    0.833300    0.741400    0.789500    0.745900    0.786400   \n",
       "9     0.884097    0.900804    0.873502    0.875339    0.860690    0.882119   \n",
       "10    0.854695    0.875630    0.838447    0.845113    0.828747    0.850056   \n",
       "11    0.846066    0.867069    0.828551    0.835899    0.821743    0.838761   \n",
       "12    0.842793    0.867069    0.822480    0.834791    0.817395    0.837182   \n",
       "13    0.692905    0.734138    0.660434    0.671883    0.646170    0.677657   \n",
       "14    0.830300    0.833300    0.830900    0.803600    0.823500    0.804700   \n",
       "15    0.842793    0.867069    0.822480    0.834791    0.817395    0.837182   \n",
       "\n",
       "           ave       std  \n",
       "0     0.712402  0.019145  \n",
       "1   327.600000  8.181279  \n",
       "2   179.000000  6.815016  \n",
       "3    50.900000  7.866243  \n",
       "4    37.500000  3.807887  \n",
       "5     0.851429  0.014890  \n",
       "6     0.865606  0.020119  \n",
       "7     0.897293  0.010157  \n",
       "8     0.779050  0.029600  \n",
       "9     0.881057  0.012539  \n",
       "10    0.850517  0.015317  \n",
       "11    0.841492  0.015584  \n",
       "12    0.838169  0.016642  \n",
       "13    0.684285  0.030424  \n",
       "14    0.826760  0.016878  \n",
       "15    0.838169  0.016642  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test['std'] = mat_met_svm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_svm0</th>\n",
       "      <th>y_pred_svm1</th>\n",
       "      <th>y_pred_svm2</th>\n",
       "      <th>y_pred_svm3</th>\n",
       "      <th>y_pred_svm4</th>\n",
       "      <th>y_pred_svm_ave</th>\n",
       "      <th>y_pred_svm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL356066</td>\n",
       "      <td>0</td>\n",
       "      <td>8.02</td>\n",
       "      <td>7.112791</td>\n",
       "      <td>7.158699</td>\n",
       "      <td>7.249718</td>\n",
       "      <td>7.168090</td>\n",
       "      <td>7.228336</td>\n",
       "      <td>7.322939</td>\n",
       "      <td>0.314992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3652228</td>\n",
       "      <td>1</td>\n",
       "      <td>8.05</td>\n",
       "      <td>8.057669</td>\n",
       "      <td>8.010474</td>\n",
       "      <td>7.998303</td>\n",
       "      <td>8.077573</td>\n",
       "      <td>8.073258</td>\n",
       "      <td>8.044546</td>\n",
       "      <td>0.030046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3939518</td>\n",
       "      <td>2</td>\n",
       "      <td>6.87</td>\n",
       "      <td>7.376885</td>\n",
       "      <td>7.318239</td>\n",
       "      <td>7.424905</td>\n",
       "      <td>7.482932</td>\n",
       "      <td>7.362245</td>\n",
       "      <td>7.305868</td>\n",
       "      <td>0.201609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4281792</td>\n",
       "      <td>3</td>\n",
       "      <td>7.22</td>\n",
       "      <td>8.251271</td>\n",
       "      <td>7.829983</td>\n",
       "      <td>7.629744</td>\n",
       "      <td>8.149521</td>\n",
       "      <td>7.829861</td>\n",
       "      <td>7.818397</td>\n",
       "      <td>0.339352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4070232</td>\n",
       "      <td>4</td>\n",
       "      <td>7.15</td>\n",
       "      <td>6.697583</td>\n",
       "      <td>6.501944</td>\n",
       "      <td>6.603923</td>\n",
       "      <td>6.686166</td>\n",
       "      <td>6.670043</td>\n",
       "      <td>6.718277</td>\n",
       "      <td>0.204143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL4202521</td>\n",
       "      <td>2966</td>\n",
       "      <td>7.43</td>\n",
       "      <td>6.956408</td>\n",
       "      <td>6.937631</td>\n",
       "      <td>6.983168</td>\n",
       "      <td>6.970883</td>\n",
       "      <td>6.818440</td>\n",
       "      <td>7.016088</td>\n",
       "      <td>0.192887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL216641</td>\n",
       "      <td>2967</td>\n",
       "      <td>7.35</td>\n",
       "      <td>7.139385</td>\n",
       "      <td>7.184603</td>\n",
       "      <td>7.286296</td>\n",
       "      <td>7.145974</td>\n",
       "      <td>7.426355</td>\n",
       "      <td>7.255436</td>\n",
       "      <td>0.107684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL3693750</td>\n",
       "      <td>2968</td>\n",
       "      <td>7.43</td>\n",
       "      <td>7.710396</td>\n",
       "      <td>7.773071</td>\n",
       "      <td>7.794279</td>\n",
       "      <td>7.790296</td>\n",
       "      <td>7.739841</td>\n",
       "      <td>7.706314</td>\n",
       "      <td>0.126976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL152665</td>\n",
       "      <td>2969</td>\n",
       "      <td>5.96</td>\n",
       "      <td>5.876373</td>\n",
       "      <td>5.937037</td>\n",
       "      <td>5.882080</td>\n",
       "      <td>5.776774</td>\n",
       "      <td>5.883628</td>\n",
       "      <td>5.885982</td>\n",
       "      <td>0.057915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL3693789</td>\n",
       "      <td>2970</td>\n",
       "      <td>8.52</td>\n",
       "      <td>7.960178</td>\n",
       "      <td>8.000967</td>\n",
       "      <td>7.970043</td>\n",
       "      <td>7.987815</td>\n",
       "      <td>8.013317</td>\n",
       "      <td>8.075387</td>\n",
       "      <td>0.199629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_svm0  y_pred_svm1  \\\n",
       "0          CHEMBL356066            0     8.02     7.112791     7.158699   \n",
       "1         CHEMBL3652228            1     8.05     8.057669     8.010474   \n",
       "2         CHEMBL3939518            2     6.87     7.376885     7.318239   \n",
       "3         CHEMBL4281792            3     7.22     8.251271     7.829983   \n",
       "4         CHEMBL4070232            4     7.15     6.697583     6.501944   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "2966      CHEMBL4202521         2966     7.43     6.956408     6.937631   \n",
       "2967       CHEMBL216641         2967     7.35     7.139385     7.184603   \n",
       "2968      CHEMBL3693750         2968     7.43     7.710396     7.773071   \n",
       "2969       CHEMBL152665         2969     5.96     5.876373     5.937037   \n",
       "2970      CHEMBL3693789         2970     8.52     7.960178     8.000967   \n",
       "\n",
       "      y_pred_svm2  y_pred_svm3  y_pred_svm4  y_pred_svm_ave  y_pred_svm_std  \n",
       "0        7.249718     7.168090     7.228336        7.322939        0.314992  \n",
       "1        7.998303     8.077573     8.073258        8.044546        0.030046  \n",
       "2        7.424905     7.482932     7.362245        7.305868        0.201609  \n",
       "3        7.629744     8.149521     7.829861        7.818397        0.339352  \n",
       "4        6.603923     6.686166     6.670043        6.718277        0.204143  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "2966     6.983168     6.970883     6.818440        7.016088        0.192887  \n",
       "2967     7.286296     7.145974     7.426355        7.255436        0.107684  \n",
       "2968     7.794279     7.790296     7.739841        7.706314        0.126976  \n",
       "2969     5.882080     5.776774     5.883628        5.885982        0.057915  \n",
       "2970     7.970043     7.987815     8.013317        8.075387        0.199629  \n",
       "\n",
       "[2971 rows x 10 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_svm_cat = np.where((y_pred_optimized_svm >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_svm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_svm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "svm_run0 = data_svm[['y_test_idx0', 'y_test0', 'y_pred_svm0']]\n",
    "svm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "svm_run0.reset_index(inplace=True, drop=True)\n",
    "svm_run1 = data_svm[['y_test_idx1', 'y_test1', 'y_pred_svm1']]\n",
    "svm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "svm_run1.reset_index(inplace=True, drop=True)\n",
    "svm_run2 = data_svm[['y_test_idx2', 'y_test2', 'y_pred_svm2']]\n",
    "svm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "svm_run2.reset_index(inplace=True, drop=True)\n",
    "svm_run3 = data_svm[['y_test_idx3', 'y_test3', 'y_pred_svm3']]\n",
    "svm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "svm_run3.reset_index(inplace=True, drop=True)\n",
    "svm_run4 = data_svm[['y_test_idx4', 'y_test4', 'y_pred_svm4']]\n",
    "svm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "svm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "svm_5preds = pd.concat([chembl_id, svm_run0, svm_run1, svm_run2, svm_run3, svm_run4], axis=1)\n",
    "svm_5preds = svm_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_svm0', 'y_pred_svm1', 'y_pred_svm2', 'y_pred_svm3', 'y_pred_svm4']]\n",
    "svm_5preds['y_pred_svm_ave'] = svm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "svm_5preds['y_pred_svm_std'] = svm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "svm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6394fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_svm.to_csv('mat_met_svm_opt.csv')\n",
    "svm_5preds.to_csv('svm_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2869d8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model r2_score 0.6860 with a standard deviation of 0.0432\n",
      "SVM optimized model r2_score 0.7065 with a standard deviation of 0.0425\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVR \n",
    "svm_baseline_CVscore = cross_val_score(svm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"SVM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_svm.joblib']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_reg, \"./svm_reg.joblib\")\n",
    "#joblib.dump(optimized_svm, \"./optimized_svm.joblib\")\n",
    "joblib.dump(optimizedCV_svm, \"./optimizedCV_svm.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
