{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/miniforge3/envs/teachopencadd/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "HDAC6 = Path(HERE).resolve().parents[1]/'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4084049</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[2631951, 2609298, 137380, 7125875, 28861, 361...</td>\n",
       "      <td>6.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL2178343</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[42543, 4438892, 868377, 13268142, 10609042, 2...</td>\n",
       "      <td>5.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL454672</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[18278628, 736579, 8934602, 21013631, 1468176,...</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4299417</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[100053, 10511, 1733887, 904525, 692898, 28201...</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3692580</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2532187, 6225575, 4703614, 6282372, 14228323,...</td>\n",
       "      <td>5.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4084049  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL2178343  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2       CHEMBL454672  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL4299417  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL3692580  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  pChEMBL_HDAC6  \n",
       "0  [2631951, 2609298, 137380, 7125875, 28861, 361...           6.26  \n",
       "1  [42543, 4438892, 868377, 13268142, 10609042, 2...           5.96  \n",
       "2  [18278628, 736579, 8934602, 21013631, 1468176,...           6.80  \n",
       "3  [100053, 10511, 1733887, 904525, 692898, 28201...           8.14  \n",
       "4  [2532187, 6225575, 4703614, 6282372, 14228323,...           5.24  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC6/\"HDAC6_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>type</th>\n",
       "      <th>Standard_Value_HDAC6</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4082520</td>\n",
       "      <td>CN1C(=O)C2CN(Cc3c2c2cc(OCc4ccccc4)ccc2n3Cc2ccc...</td>\n",
       "      <td>Ki</td>\n",
       "      <td>0.08</td>\n",
       "      <td>10.10</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4098975</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9.85</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4100534</td>\n",
       "      <td>COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...</td>\n",
       "      <td>Ki</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9.82</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4101480</td>\n",
       "      <td>COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...</td>\n",
       "      <td>Ki</td>\n",
       "      <td>0.16</td>\n",
       "      <td>9.80</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3912061</td>\n",
       "      <td>CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.17</td>\n",
       "      <td>9.77</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL1798006</td>\n",
       "      <td>CC[C@H](C)[C@H](NC(=O)C1CCNCC1)C(=O)N1Cc2cc(OC...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>169.00</td>\n",
       "      <td>6.77</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL470843</td>\n",
       "      <td>O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO</td>\n",
       "      <td>IC50</td>\n",
       "      <td>175.00</td>\n",
       "      <td>6.76</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL1798004</td>\n",
       "      <td>CC[C@H](C)[C@H](NC(=O)[C@@H]1CCCN1)C(=O)N1Cc2c...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>191.00</td>\n",
       "      <td>6.72</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL3215861</td>\n",
       "      <td>CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC</td>\n",
       "      <td>Ki</td>\n",
       "      <td>247.00</td>\n",
       "      <td>6.61</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL3233708</td>\n",
       "      <td>O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>293.60</td>\n",
       "      <td>6.53</td>\n",
       "      <td>Non-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                             smiles  \\\n",
       "0         CHEMBL4082520  CN1C(=O)C2CN(Cc3c2c2cc(OCc4ccccc4)ccc2n3Cc2ccc...   \n",
       "1         CHEMBL4098975  O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...   \n",
       "2         CHEMBL4100534  COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...   \n",
       "3         CHEMBL4101480  COc1ccc2c(c1)c1c(n2Cc2ccc(C(=O)NO)cc2)CN2CC1C(...   \n",
       "4         CHEMBL3912061     CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12   \n",
       "...                 ...                                                ...   \n",
       "2966      CHEMBL1798006  CC[C@H](C)[C@H](NC(=O)C1CCNCC1)C(=O)N1Cc2cc(OC...   \n",
       "2967       CHEMBL470843          O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO   \n",
       "2968      CHEMBL1798004  CC[C@H](C)[C@H](NC(=O)[C@@H]1CCCN1)C(=O)N1Cc2c...   \n",
       "2969      CHEMBL3215861           CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC   \n",
       "2970      CHEMBL3233708  O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...   \n",
       "\n",
       "      type  Standard_Value_HDAC6  pChEMBL_HDAC6            label  \n",
       "0       Ki                  0.08          10.10    Single points  \n",
       "1     IC50                  0.14           9.85  HDAC6-selective  \n",
       "2       Ki                  0.15           9.82    Single points  \n",
       "3       Ki                  0.16           9.80    Single points  \n",
       "4     IC50                  0.17           9.77  HDAC6-selective  \n",
       "...    ...                   ...            ...              ...  \n",
       "2966  IC50                169.00           6.77    Single points  \n",
       "2967  IC50                175.00           6.76   Semi-selective  \n",
       "2968  IC50                191.00           6.72    Single points  \n",
       "2969    Ki                247.00           6.61      Dual-binder  \n",
       "2970  IC50                293.60           6.53       Non-binder  \n",
       "\n",
       "[2971 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC6/\"HDAC6_dataset.csv\",)\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4084049</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[2631951, 2609298, 137380, 7125875, 28861, 361...</td>\n",
       "      <td>6.26</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL2178343</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[42543, 4438892, 868377, 13268142, 10609042, 2...</td>\n",
       "      <td>5.96</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL454672</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[18278628, 736579, 8934602, 21013631, 1468176,...</td>\n",
       "      <td>6.80</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4299417</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[100053, 10511, 1733887, 904525, 692898, 28201...</td>\n",
       "      <td>8.14</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3692580</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2532187, 6225575, 4703614, 6282372, 14228323,...</td>\n",
       "      <td>5.24</td>\n",
       "      <td>Single points</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4084049  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL2178343  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2       CHEMBL454672  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL4299417  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL3692580  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  pChEMBL_HDAC6  \\\n",
       "0  [2631951, 2609298, 137380, 7125875, 28861, 361...           6.26   \n",
       "1  [42543, 4438892, 868377, 13268142, 10609042, 2...           5.96   \n",
       "2  [18278628, 736579, 8934602, 21013631, 1468176,...           6.80   \n",
       "3  [100053, 10511, 1733887, 904525, 692898, 28201...           8.14   \n",
       "4  [2532187, 6225575, 4703614, 6282372, 14228323,...           5.24   \n",
       "\n",
       "            label  \n",
       "0   Single points  \n",
       "1   Single points  \n",
       "2  Semi-selective  \n",
       "3     Dual-binder  \n",
       "4   Single points  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4084049</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[2631951, 2609298, 137380, 7125875, 28861, 361...</td>\n",
       "      <td>6.26</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL2178343</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[42543, 4438892, 868377, 13268142, 10609042, 2...</td>\n",
       "      <td>5.96</td>\n",
       "      <td>Single points</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL454672</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[18278628, 736579, 8934602, 21013631, 1468176,...</td>\n",
       "      <td>6.80</td>\n",
       "      <td>Semi-selective</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4299417</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[100053, 10511, 1733887, 904525, 692898, 28201...</td>\n",
       "      <td>8.14</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4084049  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL2178343  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2       CHEMBL454672  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL4299417  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  pChEMBL_HDAC6  \\\n",
       "0  [2631951, 2609298, 137380, 7125875, 28861, 361...           6.26   \n",
       "1  [42543, 4438892, 868377, 13268142, 10609042, 2...           5.96   \n",
       "2  [18278628, 736579, 8934602, 21013631, 1468176,...           6.80   \n",
       "3  [100053, 10511, 1733887, 904525, 692898, 28201...           8.14   \n",
       "\n",
       "            label  Class  \n",
       "0   Single points    0.0  \n",
       "1   Single points    0.0  \n",
       "2  Semi-selective    5.0  \n",
       "3     Dual-binder    3.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'hDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'hDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'hDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "df.loc[df[df.label == 'Semi-selective'].index, \"Class\"] = 5.0\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for activity\n",
    "df[\"activity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as active if pChEMBL_HDAC6 value is >=6.6 0 otherwise\n",
    "df.loc[df[df.pChEMBL_HDAC6 >= 6.6].index, \"activity\"] = 1.0\n",
    "\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y = df[\"pChEMBL_HDAC6\"].values\n",
    "Y_cat =  df[\"activity\"].values\n",
    "Y_class = df['Class'].values\n",
    "indices =  np.array(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['pChEMBL_HDAC6'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_cat\"] = np.array(list(df.iloc[index_tr_all[i]]['activity'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['pChEMBL_HDAC6'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_cat\"] = np.array(list(df.iloc[index_te_all[i]]['activity'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.691480     0.025589\n",
      "1                    TP       164.800000    11.213088\n",
      "2                    TN        89.000000     6.616478\n",
      "3                    FP        24.400000     4.550946\n",
      "4                    FN        18.900000     5.896327\n",
      "5              Accuracy         0.854262     0.031203\n",
      "6             Precision         0.870435     0.027507\n",
      "7           Sensitivity         0.896454     0.035669\n",
      "8           Specificity         0.785070     0.035944\n",
      "9              F1 score         0.883109     0.029392\n",
      "10  F1 score (weighted)         0.853511     0.031112\n",
      "11     F1 score (macro)         0.843952     0.031070\n",
      "12    Balanced Accuracy         0.840759     0.030357\n",
      "13                  MCC         0.689070     0.062542\n",
      "14                  NPV         0.826330     0.043867\n",
      "15              ROC_AUC         0.840759     0.030357\n",
      "CPU times: user 2min 20s, sys: 250 ms, total: 2min 21s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_reg =  RandomForestRegressor(random_state=1121218, max_features = None, n_jobs=8,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_reg.fit(x_train, y_train)\n",
    "        y_pred = rf_reg.predict(x_test)  \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6) , 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=8, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=8, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 15:46:09,183] A new study created in memory with name: RFRegressor\n",
      "[I 2023-12-19 15:47:23,478] Trial 0 finished with value: 0.6684480541622599 and parameters: {'n_estimators': 463}. Best is trial 0 with value: 0.6684480541622599.\n",
      "[I 2023-12-19 15:50:12,175] Trial 1 finished with value: 0.6690088074637214 and parameters: {'n_estimators': 957}. Best is trial 1 with value: 0.6690088074637214.\n",
      "[I 2023-12-19 15:52:48,603] Trial 2 finished with value: 0.6690400225631468 and parameters: {'n_estimators': 754}. Best is trial 2 with value: 0.6690400225631468.\n",
      "[I 2023-12-19 15:54:04,443] Trial 3 finished with value: 0.6681229338474395 and parameters: {'n_estimators': 351}. Best is trial 2 with value: 0.6690400225631468.\n",
      "[I 2023-12-19 15:57:32,173] Trial 4 finished with value: 0.668984909599482 and parameters: {'n_estimators': 983}. Best is trial 2 with value: 0.6690400225631468.\n",
      "[I 2023-12-19 15:59:28,988] Trial 5 finished with value: 0.6684805313880693 and parameters: {'n_estimators': 540}. Best is trial 2 with value: 0.6690400225631468.\n",
      "[I 2023-12-19 16:01:55,081] Trial 6 finished with value: 0.6691172838871083 and parameters: {'n_estimators': 697}. Best is trial 6 with value: 0.6691172838871083.\n",
      "[I 2023-12-19 16:04:02,249] Trial 7 finished with value: 0.6690312439667019 and parameters: {'n_estimators': 622}. Best is trial 6 with value: 0.6691172838871083.\n",
      "[I 2023-12-19 16:07:25,357] Trial 8 finished with value: 0.6689954593329943 and parameters: {'n_estimators': 996}. Best is trial 6 with value: 0.6691172838871083.\n",
      "[I 2023-12-19 16:08:43,426] Trial 9 finished with value: 0.6678839831979614 and parameters: {'n_estimators': 361}. Best is trial 6 with value: 0.6691172838871083.\n",
      "[I 2023-12-19 16:09:05,401] Trial 10 finished with value: 0.6660227888128005 and parameters: {'n_estimators': 107}. Best is trial 6 with value: 0.6691172838871083.\n",
      "[I 2023-12-19 16:11:31,997] Trial 11 finished with value: 0.669094540138534 and parameters: {'n_estimators': 710}. Best is trial 6 with value: 0.6691172838871083.\n",
      "[I 2023-12-19 16:14:09,339] Trial 12 finished with value: 0.6689711618732102 and parameters: {'n_estimators': 778}. Best is trial 6 with value: 0.6691172838871083.\n",
      "[I 2023-12-19 16:16:41,060] Trial 13 finished with value: 0.6691701013472153 and parameters: {'n_estimators': 736}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:19:35,201] Trial 14 finished with value: 0.6691265825177223 and parameters: {'n_estimators': 837}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:22:28,201] Trial 15 finished with value: 0.6691358536719003 and parameters: {'n_estimators': 844}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:25:23,641] Trial 16 finished with value: 0.6691372052707517 and parameters: {'n_estimators': 851}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:27:30,171] Trial 17 finished with value: 0.6690745116653418 and parameters: {'n_estimators': 616}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:30:25,999] Trial 18 finished with value: 0.6690354727387773 and parameters: {'n_estimators': 885}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:33:30,573] Trial 19 finished with value: 0.6690256790365761 and parameters: {'n_estimators': 892}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:35:43,984] Trial 20 finished with value: 0.6689438284491018 and parameters: {'n_estimators': 654}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:38:37,498] Trial 21 finished with value: 0.6691251094681853 and parameters: {'n_estimators': 849}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:41:20,160] Trial 22 finished with value: 0.6689772163945558 and parameters: {'n_estimators': 786}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:44:21,812] Trial 23 finished with value: 0.6689724887028555 and parameters: {'n_estimators': 905}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:46:12,247] Trial 24 finished with value: 0.668485666428847 and parameters: {'n_estimators': 543}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:48:50,351] Trial 25 finished with value: 0.6689840396757679 and parameters: {'n_estimators': 784}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:51:17,404] Trial 26 finished with value: 0.6690165300609484 and parameters: {'n_estimators': 713}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:54:02,084] Trial 27 finished with value: 0.6691258530673023 and parameters: {'n_estimators': 824}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:54:36,027] Trial 28 finished with value: 0.6659490579280476 and parameters: {'n_estimators': 169}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:56:21,637] Trial 29 finished with value: 0.6684141283773796 and parameters: {'n_estimators': 509}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 16:59:28,737] Trial 30 finished with value: 0.6689595022844381 and parameters: {'n_estimators': 927}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 17:02:26,208] Trial 31 finished with value: 0.6690527247377248 and parameters: {'n_estimators': 862}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 17:05:12,319] Trial 32 finished with value: 0.6689299170188721 and parameters: {'n_estimators': 811}. Best is trial 13 with value: 0.6691701013472153.\n",
      "[I 2023-12-19 17:07:38,935] Trial 33 finished with value: 0.6692033552227884 and parameters: {'n_estimators': 729}. Best is trial 33 with value: 0.6692033552227884.\n",
      "[I 2023-12-19 17:10:06,743] Trial 34 finished with value: 0.6692076873028635 and parameters: {'n_estimators': 730}. Best is trial 34 with value: 0.6692076873028635.\n",
      "[I 2023-12-19 17:11:33,474] Trial 35 finished with value: 0.6680260203014943 and parameters: {'n_estimators': 431}. Best is trial 34 with value: 0.6692076873028635.\n",
      "[I 2023-12-19 17:13:37,499] Trial 36 finished with value: 0.6689240093086058 and parameters: {'n_estimators': 608}. Best is trial 34 with value: 0.6692076873028635.\n",
      "[I 2023-12-19 17:16:02,834] Trial 37 finished with value: 0.6691737874797343 and parameters: {'n_estimators': 735}. Best is trial 34 with value: 0.6692076873028635.\n",
      "[I 2023-12-19 17:18:33,086] Trial 38 finished with value: 0.6692076873028635 and parameters: {'n_estimators': 730}. Best is trial 34 with value: 0.6692076873028635.\n",
      "[I 2023-12-19 17:20:53,066] Trial 39 finished with value: 0.6690504303508413 and parameters: {'n_estimators': 683}. Best is trial 34 with value: 0.6692076873028635.\n",
      "[I 2023-12-19 17:22:50,032] Trial 40 finished with value: 0.6690458772782506 and parameters: {'n_estimators': 588}. Best is trial 34 with value: 0.6692076873028635.\n",
      "[I 2023-12-19 17:25:18,706] Trial 41 finished with value: 0.6690782269640293 and parameters: {'n_estimators': 743}. Best is trial 34 with value: 0.6692076873028635.\n",
      "[I 2023-12-19 17:27:50,401] Trial 42 finished with value: 0.6691234834428541 and parameters: {'n_estimators': 740}. Best is trial 34 with value: 0.6692076873028635.\n",
      "[I 2023-12-19 17:30:05,842] Trial 43 finished with value: 0.6687764075872078 and parameters: {'n_estimators': 661}. Best is trial 34 with value: 0.6692076873028635.\n",
      "[I 2023-12-19 17:32:34,408] Trial 44 finished with value: 0.6691701013472153 and parameters: {'n_estimators': 736}. Best is trial 34 with value: 0.6692076873028635.\n",
      "[I 2023-12-19 17:34:45,816] Trial 45 finished with value: 0.6688987140220359 and parameters: {'n_estimators': 655}. Best is trial 34 with value: 0.6692076873028635.\n",
      "[I 2023-12-19 17:37:58,915] Trial 46 finished with value: 0.6690336439383293 and parameters: {'n_estimators': 956}. Best is trial 34 with value: 0.6692076873028635.\n",
      "[I 2023-12-19 17:40:22,709] Trial 47 finished with value: 0.6691223604128196 and parameters: {'n_estimators': 699}. Best is trial 34 with value: 0.6692076873028635.\n",
      "[I 2023-12-19 17:41:58,884] Trial 48 finished with value: 0.6684598461401373 and parameters: {'n_estimators': 480}. Best is trial 34 with value: 0.6692076873028635.\n",
      "[I 2023-12-19 17:44:36,970] Trial 49 finished with value: 0.6689552432100229 and parameters: {'n_estimators': 783}. Best is trial 34 with value: 0.6692076873028635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6692\n",
      "\tBest params:\n",
      "\t\tn_estimators: 730\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFRegressor\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.701707\n",
      "1                    TP  345.000000\n",
      "2                    TN  170.000000\n",
      "3                    FP   42.000000\n",
      "4                    FN   38.000000\n",
      "5              Accuracy    0.865546\n",
      "6             Precision    0.891473\n",
      "7           Sensitivity    0.900783\n",
      "8           Specificity    0.801900\n",
      "9              F1 score    0.896104\n",
      "10  F1 score (weighted)    0.865255\n",
      "11     F1 score (macro)    0.852814\n",
      "12    Balanced Accuracy    0.851335\n",
      "13                  MCC    0.705719\n",
      "14                  NPV    0.817300\n",
      "15              ROC_AUC    0.851335\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_rf_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_0_cat = np.where((y_pred_rf_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_rf_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 17:46:48,442] Trial 50 finished with value: 0.6707941325526378 and parameters: {'n_estimators': 569}. Best is trial 50 with value: 0.6707941325526378.\n",
      "[I 2023-12-19 17:48:42,275] Trial 51 finished with value: 0.6707617402922468 and parameters: {'n_estimators': 568}. Best is trial 50 with value: 0.6707941325526378.\n",
      "[I 2023-12-19 17:50:44,877] Trial 52 finished with value: 0.6708063351000184 and parameters: {'n_estimators': 561}. Best is trial 52 with value: 0.6708063351000184.\n",
      "[I 2023-12-19 17:52:08,701] Trial 53 finished with value: 0.670289646680386 and parameters: {'n_estimators': 406}. Best is trial 52 with value: 0.6708063351000184.\n",
      "[I 2023-12-19 17:53:36,357] Trial 54 finished with value: 0.6704915005116191 and parameters: {'n_estimators': 419}. Best is trial 52 with value: 0.6708063351000184.\n",
      "[I 2023-12-19 17:55:00,813] Trial 55 finished with value: 0.6701961072285 and parameters: {'n_estimators': 399}. Best is trial 52 with value: 0.6708063351000184.\n",
      "[I 2023-12-19 17:56:27,029] Trial 56 finished with value: 0.6705394111159806 and parameters: {'n_estimators': 410}. Best is trial 52 with value: 0.6708063351000184.\n",
      "[I 2023-12-19 17:57:35,598] Trial 57 finished with value: 0.6699204609228264 and parameters: {'n_estimators': 326}. Best is trial 52 with value: 0.6708063351000184.\n",
      "[I 2023-12-19 17:58:37,428] Trial 58 finished with value: 0.6702704427879739 and parameters: {'n_estimators': 298}. Best is trial 52 with value: 0.6708063351000184.\n",
      "[I 2023-12-19 18:00:41,180] Trial 59 finished with value: 0.6707445743438529 and parameters: {'n_estimators': 575}. Best is trial 52 with value: 0.6708063351000184.\n",
      "[I 2023-12-19 18:02:37,617] Trial 60 finished with value: 0.6707682420201015 and parameters: {'n_estimators': 573}. Best is trial 52 with value: 0.6708063351000184.\n",
      "[I 2023-12-19 18:04:37,158] Trial 61 finished with value: 0.6707365149373373 and parameters: {'n_estimators': 574}. Best is trial 52 with value: 0.6708063351000184.\n",
      "[I 2023-12-19 18:06:35,826] Trial 62 finished with value: 0.6708004537135615 and parameters: {'n_estimators': 572}. Best is trial 52 with value: 0.6708063351000184.\n",
      "[I 2023-12-19 18:08:34,996] Trial 63 finished with value: 0.6707574156771628 and parameters: {'n_estimators': 567}. Best is trial 52 with value: 0.6708063351000184.\n",
      "[I 2023-12-19 18:10:22,041] Trial 64 finished with value: 0.6707443228910035 and parameters: {'n_estimators': 512}. Best is trial 52 with value: 0.6708063351000184.\n",
      "[I 2023-12-19 18:12:21,293] Trial 65 finished with value: 0.6707682420201014 and parameters: {'n_estimators': 573}. Best is trial 52 with value: 0.6708063351000184.\n",
      "[I 2023-12-19 18:14:04,270] Trial 66 finished with value: 0.6708548242972941 and parameters: {'n_estimators': 519}. Best is trial 66 with value: 0.6708548242972941.\n",
      "[I 2023-12-19 18:15:50,902] Trial 67 finished with value: 0.6708750207071035 and parameters: {'n_estimators': 523}. Best is trial 67 with value: 0.6708750207071035.\n",
      "[I 2023-12-19 18:17:33,136] Trial 68 finished with value: 0.6707660333821572 and parameters: {'n_estimators': 514}. Best is trial 67 with value: 0.6708750207071035.\n",
      "[I 2023-12-19 18:19:07,671] Trial 69 finished with value: 0.6706210688862418 and parameters: {'n_estimators': 457}. Best is trial 67 with value: 0.6708750207071035.\n",
      "[I 2023-12-19 18:20:45,031] Trial 70 finished with value: 0.6705354133237931 and parameters: {'n_estimators': 482}. Best is trial 67 with value: 0.6708750207071035.\n",
      "[I 2023-12-19 18:22:31,323] Trial 71 finished with value: 0.6707927421378802 and parameters: {'n_estimators': 527}. Best is trial 67 with value: 0.6708750207071035.\n",
      "[I 2023-12-19 18:24:40,644] Trial 72 finished with value: 0.6710955588825246 and parameters: {'n_estimators': 632}. Best is trial 72 with value: 0.6710955588825246.\n",
      "[I 2023-12-19 18:26:46,512] Trial 73 finished with value: 0.6710800159032201 and parameters: {'n_estimators': 621}. Best is trial 72 with value: 0.6710955588825246.\n",
      "[I 2023-12-19 18:28:34,341] Trial 74 finished with value: 0.6708635040455075 and parameters: {'n_estimators': 532}. Best is trial 72 with value: 0.6710955588825246.\n",
      "[I 2023-12-19 18:30:41,781] Trial 75 finished with value: 0.6711523316876356 and parameters: {'n_estimators': 628}. Best is trial 75 with value: 0.6711523316876356.\n",
      "[I 2023-12-19 18:32:53,804] Trial 76 finished with value: 0.6711523316876357 and parameters: {'n_estimators': 628}. Best is trial 76 with value: 0.6711523316876357.\n",
      "[I 2023-12-19 18:35:03,816] Trial 77 finished with value: 0.6710928131659769 and parameters: {'n_estimators': 631}. Best is trial 76 with value: 0.6711523316876357.\n",
      "[I 2023-12-19 18:37:08,101] Trial 78 finished with value: 0.671121550109358 and parameters: {'n_estimators': 626}. Best is trial 76 with value: 0.6711523316876357.\n",
      "[I 2023-12-19 18:39:11,565] Trial 79 finished with value: 0.670979393375037 and parameters: {'n_estimators': 609}. Best is trial 76 with value: 0.6711523316876357.\n",
      "[I 2023-12-19 18:41:18,976] Trial 80 finished with value: 0.6711221440422745 and parameters: {'n_estimators': 625}. Best is trial 76 with value: 0.6711523316876357.\n",
      "[I 2023-12-19 18:43:24,821] Trial 81 finished with value: 0.6710596367910125 and parameters: {'n_estimators': 616}. Best is trial 76 with value: 0.6711523316876357.\n",
      "[I 2023-12-19 18:45:34,241] Trial 82 finished with value: 0.6711301720560662 and parameters: {'n_estimators': 629}. Best is trial 76 with value: 0.6711523316876357.\n",
      "[I 2023-12-19 18:47:41,741] Trial 83 finished with value: 0.6711215607310308 and parameters: {'n_estimators': 627}. Best is trial 76 with value: 0.6711523316876357.\n",
      "[I 2023-12-19 18:49:51,504] Trial 84 finished with value: 0.6711382209074253 and parameters: {'n_estimators': 634}. Best is trial 76 with value: 0.6711523316876357.\n",
      "[I 2023-12-19 18:52:09,025] Trial 85 finished with value: 0.6710705970438309 and parameters: {'n_estimators': 674}. Best is trial 76 with value: 0.6711523316876357.\n",
      "[I 2023-12-19 18:54:16,472] Trial 86 finished with value: 0.6712437562186666 and parameters: {'n_estimators': 638}. Best is trial 86 with value: 0.6712437562186666.\n",
      "[I 2023-12-19 18:56:27,061] Trial 87 finished with value: 0.6712031185572702 and parameters: {'n_estimators': 648}. Best is trial 86 with value: 0.6712437562186666.\n",
      "[I 2023-12-19 18:58:36,166] Trial 88 finished with value: 0.6711934472792034 and parameters: {'n_estimators': 649}. Best is trial 86 with value: 0.6712437562186666.\n",
      "[I 2023-12-19 19:00:50,868] Trial 89 finished with value: 0.6711934472792033 and parameters: {'n_estimators': 649}. Best is trial 86 with value: 0.6712437562186666.\n",
      "[I 2023-12-19 19:03:12,475] Trial 90 finished with value: 0.6711249836350658 and parameters: {'n_estimators': 685}. Best is trial 86 with value: 0.6712437562186666.\n",
      "[I 2023-12-19 19:05:34,249] Trial 91 finished with value: 0.6710962965471985 and parameters: {'n_estimators': 684}. Best is trial 86 with value: 0.6712437562186666.\n",
      "[I 2023-12-19 19:07:47,597] Trial 92 finished with value: 0.6711841130963165 and parameters: {'n_estimators': 647}. Best is trial 86 with value: 0.6712437562186666.\n",
      "[I 2023-12-19 19:10:14,474] Trial 93 finished with value: 0.6711306870486937 and parameters: {'n_estimators': 705}. Best is trial 86 with value: 0.6712437562186666.\n",
      "[I 2023-12-19 19:12:50,311] Trial 94 finished with value: 0.6712344531106818 and parameters: {'n_estimators': 761}. Best is trial 86 with value: 0.6712437562186666.\n",
      "[I 2023-12-19 19:15:07,488] Trial 95 finished with value: 0.67108893509862 and parameters: {'n_estimators': 668}. Best is trial 86 with value: 0.6712437562186666.\n",
      "[I 2023-12-19 19:17:45,110] Trial 96 finished with value: 0.6712410718158672 and parameters: {'n_estimators': 770}. Best is trial 86 with value: 0.6712437562186666.\n",
      "[I 2023-12-19 19:20:19,990] Trial 97 finished with value: 0.6712249412369715 and parameters: {'n_estimators': 759}. Best is trial 86 with value: 0.6712437562186666.\n",
      "[I 2023-12-19 19:22:56,551] Trial 98 finished with value: 0.6711625441460826 and parameters: {'n_estimators': 764}. Best is trial 86 with value: 0.6712437562186666.\n",
      "[I 2023-12-19 19:25:29,372] Trial 99 finished with value: 0.6711693972807473 and parameters: {'n_estimators': 767}. Best is trial 86 with value: 0.6712437562186666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6712\n",
      "\tBest params:\n",
      "\t\tn_estimators: 638\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.701707    0.700515\n",
      "1                    TP  345.000000  333.000000\n",
      "2                    TN  170.000000  182.000000\n",
      "3                    FP   42.000000   48.000000\n",
      "4                    FN   38.000000   32.000000\n",
      "5              Accuracy    0.865546    0.865546\n",
      "6             Precision    0.891473    0.874016\n",
      "7           Sensitivity    0.900783    0.912329\n",
      "8           Specificity    0.801900    0.791300\n",
      "9              F1 score    0.896104    0.892761\n",
      "10  F1 score (weighted)    0.865255    0.864565\n",
      "11     F1 score (macro)    0.852814    0.856291\n",
      "12    Balanced Accuracy    0.851335    0.851817\n",
      "13                  MCC    0.705719    0.713982\n",
      "14                  NPV    0.817300    0.850500\n",
      "15              ROC_AUC    0.851335    0.851817\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_rf_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_1_cat = np.where((y_pred_rf_1 >= 6.6), 1, 0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_rf_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 19:28:17,276] Trial 100 finished with value: 0.6815378035463577 and parameters: {'n_estimators': 761}. Best is trial 100 with value: 0.6815378035463577.\n",
      "[I 2023-12-19 19:30:55,225] Trial 101 finished with value: 0.6815365061516063 and parameters: {'n_estimators': 809}. Best is trial 100 with value: 0.6815378035463577.\n",
      "[I 2023-12-19 19:33:34,476] Trial 102 finished with value: 0.6816145660536908 and parameters: {'n_estimators': 797}. Best is trial 102 with value: 0.6816145660536908.\n",
      "[I 2023-12-19 19:36:16,689] Trial 103 finished with value: 0.6815202442860991 and parameters: {'n_estimators': 808}. Best is trial 102 with value: 0.6816145660536908.\n",
      "[I 2023-12-19 19:39:00,454] Trial 104 finished with value: 0.6815207067829708 and parameters: {'n_estimators': 817}. Best is trial 102 with value: 0.6816145660536908.\n",
      "[I 2023-12-19 19:41:41,117] Trial 105 finished with value: 0.6815924841283003 and parameters: {'n_estimators': 803}. Best is trial 102 with value: 0.6816145660536908.\n",
      "[I 2023-12-19 19:44:18,467] Trial 106 finished with value: 0.6815895816661077 and parameters: {'n_estimators': 804}. Best is trial 102 with value: 0.6816145660536908.\n",
      "[I 2023-12-19 19:47:00,271] Trial 107 finished with value: 0.6815506199993975 and parameters: {'n_estimators': 806}. Best is trial 102 with value: 0.6816145660536908.\n",
      "[I 2023-12-19 19:49:42,707] Trial 108 finished with value: 0.6815801717351644 and parameters: {'n_estimators': 805}. Best is trial 102 with value: 0.6816145660536908.\n",
      "[I 2023-12-19 19:52:27,819] Trial 109 finished with value: 0.6815391715769573 and parameters: {'n_estimators': 807}. Best is trial 102 with value: 0.6816145660536908.\n",
      "[I 2023-12-19 19:55:10,733] Trial 110 finished with value: 0.6815801717351644 and parameters: {'n_estimators': 805}. Best is trial 102 with value: 0.6816145660536908.\n",
      "[I 2023-12-19 19:57:48,114] Trial 111 finished with value: 0.6815835636850798 and parameters: {'n_estimators': 802}. Best is trial 102 with value: 0.6816145660536908.\n",
      "[I 2023-12-19 20:00:26,180] Trial 112 finished with value: 0.6815802811530649 and parameters: {'n_estimators': 800}. Best is trial 102 with value: 0.6816145660536908.\n",
      "[I 2023-12-19 20:03:16,977] Trial 113 finished with value: 0.681618540258212 and parameters: {'n_estimators': 869}. Best is trial 113 with value: 0.681618540258212.\n",
      "[I 2023-12-19 20:06:10,445] Trial 114 finished with value: 0.6816368878212768 and parameters: {'n_estimators': 872}. Best is trial 114 with value: 0.6816368878212768.\n",
      "[I 2023-12-19 20:09:01,617] Trial 115 finished with value: 0.6816134309609934 and parameters: {'n_estimators': 868}. Best is trial 114 with value: 0.6816368878212768.\n",
      "[I 2023-12-19 20:11:54,156] Trial 116 finished with value: 0.6816302537766352 and parameters: {'n_estimators': 875}. Best is trial 114 with value: 0.6816368878212768.\n",
      "[I 2023-12-19 20:14:47,048] Trial 117 finished with value: 0.6816382985497563 and parameters: {'n_estimators': 870}. Best is trial 117 with value: 0.6816382985497563.\n",
      "[I 2023-12-19 20:17:38,396] Trial 118 finished with value: 0.6815913985606735 and parameters: {'n_estimators': 867}. Best is trial 117 with value: 0.6816382985497563.\n",
      "[I 2023-12-19 20:20:32,626] Trial 119 finished with value: 0.6816368878212768 and parameters: {'n_estimators': 872}. Best is trial 117 with value: 0.6816382985497563.\n",
      "[I 2023-12-19 20:23:24,578] Trial 120 finished with value: 0.681648890054104 and parameters: {'n_estimators': 876}. Best is trial 120 with value: 0.681648890054104.\n",
      "[I 2023-12-19 20:26:19,114] Trial 121 finished with value: 0.6816618187436873 and parameters: {'n_estimators': 877}. Best is trial 121 with value: 0.6816618187436873.\n",
      "[I 2023-12-19 20:29:12,043] Trial 122 finished with value: 0.681642684051474 and parameters: {'n_estimators': 871}. Best is trial 121 with value: 0.6816618187436873.\n",
      "[I 2023-12-19 20:32:05,565] Trial 123 finished with value: 0.6816237009999758 and parameters: {'n_estimators': 873}. Best is trial 121 with value: 0.6816618187436873.\n",
      "[I 2023-12-19 20:34:57,173] Trial 124 finished with value: 0.681642684051474 and parameters: {'n_estimators': 871}. Best is trial 121 with value: 0.6816618187436873.\n",
      "[I 2023-12-19 20:38:03,332] Trial 125 finished with value: 0.6818528237726073 and parameters: {'n_estimators': 931}. Best is trial 125 with value: 0.6818528237726073.\n",
      "[I 2023-12-19 20:41:04,946] Trial 126 finished with value: 0.6817929363616275 and parameters: {'n_estimators': 917}. Best is trial 125 with value: 0.6818528237726073.\n",
      "[I 2023-12-19 20:44:06,989] Trial 127 finished with value: 0.6817968245826401 and parameters: {'n_estimators': 916}. Best is trial 125 with value: 0.6818528237726073.\n",
      "[I 2023-12-19 20:47:10,277] Trial 128 finished with value: 0.6818294832288263 and parameters: {'n_estimators': 922}. Best is trial 125 with value: 0.6818528237726073.\n",
      "[I 2023-12-19 20:50:13,707] Trial 129 finished with value: 0.6818377224422105 and parameters: {'n_estimators': 921}. Best is trial 125 with value: 0.6818528237726073.\n",
      "[I 2023-12-19 20:53:15,110] Trial 130 finished with value: 0.6817805987949069 and parameters: {'n_estimators': 915}. Best is trial 125 with value: 0.6818528237726073.\n",
      "[I 2023-12-19 20:56:14,279] Trial 131 finished with value: 0.681803732110307 and parameters: {'n_estimators': 912}. Best is trial 125 with value: 0.6818528237726073.\n",
      "[I 2023-12-19 20:59:16,947] Trial 132 finished with value: 0.6818248763193455 and parameters: {'n_estimators': 924}. Best is trial 125 with value: 0.6818528237726073.\n",
      "[I 2023-12-19 21:02:15,382] Trial 133 finished with value: 0.6818279394596198 and parameters: {'n_estimators': 909}. Best is trial 125 with value: 0.6818528237726073.\n",
      "[I 2023-12-19 21:05:14,259] Trial 134 finished with value: 0.6818294832288263 and parameters: {'n_estimators': 922}. Best is trial 125 with value: 0.6818528237726073.\n",
      "[I 2023-12-19 21:08:16,067] Trial 135 finished with value: 0.681816057016469 and parameters: {'n_estimators': 923}. Best is trial 125 with value: 0.6818528237726073.\n",
      "[I 2023-12-19 21:11:16,953] Trial 136 finished with value: 0.6818211787504034 and parameters: {'n_estimators': 919}. Best is trial 125 with value: 0.6818528237726073.\n",
      "[I 2023-12-19 21:14:20,337] Trial 137 finished with value: 0.6818377224422105 and parameters: {'n_estimators': 921}. Best is trial 125 with value: 0.6818528237726073.\n",
      "[I 2023-12-19 21:17:23,035] Trial 138 finished with value: 0.681816057016469 and parameters: {'n_estimators': 923}. Best is trial 125 with value: 0.6818528237726073.\n",
      "[I 2023-12-19 21:20:27,607] Trial 139 finished with value: 0.6818285623505447 and parameters: {'n_estimators': 928}. Best is trial 125 with value: 0.6818528237726073.\n",
      "[I 2023-12-19 21:23:35,347] Trial 140 finished with value: 0.6818561357575683 and parameters: {'n_estimators': 952}. Best is trial 140 with value: 0.6818561357575683.\n",
      "[I 2023-12-19 21:26:43,364] Trial 141 finished with value: 0.6818697705137722 and parameters: {'n_estimators': 956}. Best is trial 141 with value: 0.6818697705137722.\n",
      "[I 2023-12-19 21:29:51,816] Trial 142 finished with value: 0.6818410870377265 and parameters: {'n_estimators': 950}. Best is trial 141 with value: 0.6818697705137722.\n",
      "[I 2023-12-19 21:33:08,188] Trial 143 finished with value: 0.681918629490985 and parameters: {'n_estimators': 973}. Best is trial 143 with value: 0.681918629490985.\n",
      "[I 2023-12-19 21:36:17,182] Trial 144 finished with value: 0.6818587736319865 and parameters: {'n_estimators': 953}. Best is trial 143 with value: 0.681918629490985.\n",
      "[I 2023-12-19 21:39:29,767] Trial 145 finished with value: 0.6818968488076476 and parameters: {'n_estimators': 964}. Best is trial 143 with value: 0.681918629490985.\n",
      "[I 2023-12-19 21:42:38,385] Trial 146 finished with value: 0.6818461860269964 and parameters: {'n_estimators': 959}. Best is trial 143 with value: 0.681918629490985.\n",
      "[I 2023-12-19 21:45:50,186] Trial 147 finished with value: 0.6818461860269964 and parameters: {'n_estimators': 959}. Best is trial 143 with value: 0.681918629490985.\n",
      "[I 2023-12-19 21:49:05,061] Trial 148 finished with value: 0.6818749841814805 and parameters: {'n_estimators': 967}. Best is trial 143 with value: 0.681918629490985.\n",
      "[I 2023-12-19 21:52:16,804] Trial 149 finished with value: 0.6819404343724317 and parameters: {'n_estimators': 974}. Best is trial 149 with value: 0.6819404343724317.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6819\n",
      "\tBest params:\n",
      "\t\tn_estimators: 974\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.701707    0.700515    0.676903\n",
      "1                    TP  345.000000  333.000000  339.000000\n",
      "2                    TN  170.000000  182.000000  167.000000\n",
      "3                    FP   42.000000   48.000000   55.000000\n",
      "4                    FN   38.000000   32.000000   34.000000\n",
      "5              Accuracy    0.865546    0.865546    0.850420\n",
      "6             Precision    0.891473    0.874016    0.860406\n",
      "7           Sensitivity    0.900783    0.912329    0.908847\n",
      "8           Specificity    0.801900    0.791300    0.752300\n",
      "9              F1 score    0.896104    0.892761    0.883963\n",
      "10  F1 score (weighted)    0.865255    0.864565    0.848755\n",
      "11     F1 score (macro)    0.852814    0.856291    0.836781\n",
      "12    Balanced Accuracy    0.851335    0.851817    0.830550\n",
      "13                  MCC    0.705719    0.713982    0.676008\n",
      "14                  NPV    0.817300    0.850500    0.830800\n",
      "15              ROC_AUC    0.851335    0.851817    0.830550\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_rf_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_2_cat = np.where((y_pred_rf_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_rf_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 21:55:56,206] Trial 150 finished with value: 0.6728330462871506 and parameters: {'n_estimators': 999}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 21:59:03,938] Trial 151 finished with value: 0.6727642742980273 and parameters: {'n_estimators': 960}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:02:11,310] Trial 152 finished with value: 0.6727752413832462 and parameters: {'n_estimators': 964}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:05:19,807] Trial 153 finished with value: 0.6728911606790416 and parameters: {'n_estimators': 982}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:08:24,644] Trial 154 finished with value: 0.6726961575395622 and parameters: {'n_estimators': 946}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:11:28,569] Trial 155 finished with value: 0.6727060319535261 and parameters: {'n_estimators': 941}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:14:42,961] Trial 156 finished with value: 0.6728823435762402 and parameters: {'n_estimators': 983}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:17:56,039] Trial 157 finished with value: 0.6728315134382435 and parameters: {'n_estimators': 970}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:21:03,427] Trial 158 finished with value: 0.6727510566523969 and parameters: {'n_estimators': 939}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:24:17,840] Trial 159 finished with value: 0.6728565144264033 and parameters: {'n_estimators': 975}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:27:21,258] Trial 160 finished with value: 0.6726984326060366 and parameters: {'n_estimators': 947}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:30:39,194] Trial 161 finished with value: 0.67285202329875 and parameters: {'n_estimators': 997}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:33:48,649] Trial 162 finished with value: 0.67272103784228 and parameters: {'n_estimators': 957}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:36:44,210] Trial 163 finished with value: 0.6726098587501919 and parameters: {'n_estimators': 905}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:39:45,451] Trial 164 finished with value: 0.6727234713517818 and parameters: {'n_estimators': 940}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:42:56,116] Trial 165 finished with value: 0.6728565144264033 and parameters: {'n_estimators': 975}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:45:50,138] Trial 166 finished with value: 0.6725505305188945 and parameters: {'n_estimators': 890}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:48:54,079] Trial 167 finished with value: 0.67272103784228 and parameters: {'n_estimators': 957}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:52:06,807] Trial 168 finished with value: 0.6728461330194508 and parameters: {'n_estimators': 985}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:55:02,103] Trial 169 finished with value: 0.6725357438031448 and parameters: {'n_estimators': 893}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 22:57:41,966] Trial 170 finished with value: 0.6725385913282567 and parameters: {'n_estimators': 836}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:00:41,809] Trial 171 finished with value: 0.6726659981819403 and parameters: {'n_estimators': 934}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:03:37,289] Trial 172 finished with value: 0.6725351184660019 and parameters: {'n_estimators': 894}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:06:43,986] Trial 173 finished with value: 0.6727226628050056 and parameters: {'n_estimators': 956}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:09:48,607] Trial 174 finished with value: 0.6726952523436467 and parameters: {'n_estimators': 932}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:12:59,316] Trial 175 finished with value: 0.6728163428752015 and parameters: {'n_estimators': 971}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:15:56,662] Trial 176 finished with value: 0.6726540251678724 and parameters: {'n_estimators': 906}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:19:14,671] Trial 177 finished with value: 0.6728330462871506 and parameters: {'n_estimators': 999}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:22:20,138] Trial 178 finished with value: 0.6727510566523969 and parameters: {'n_estimators': 939}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:25:29,747] Trial 179 finished with value: 0.672813212999268 and parameters: {'n_estimators': 963}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:28:27,855] Trial 180 finished with value: 0.6727056872567816 and parameters: {'n_estimators': 936}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:31:28,790] Trial 181 finished with value: 0.6726607252026338 and parameters: {'n_estimators': 926}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:34:24,531] Trial 182 finished with value: 0.6726291876479762 and parameters: {'n_estimators': 908}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:37:25,236] Trial 183 finished with value: 0.6727266350638066 and parameters: {'n_estimators': 950}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:40:36,017] Trial 184 finished with value: 0.6728559831330851 and parameters: {'n_estimators': 976}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:43:36,658] Trial 185 finished with value: 0.6725233542135873 and parameters: {'n_estimators': 901}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:46:49,128] Trial 186 finished with value: 0.672633175145514 and parameters: {'n_estimators': 925}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:50:06,890] Trial 187 finished with value: 0.6727266350638066 and parameters: {'n_estimators': 950}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:53:25,462] Trial 188 finished with value: 0.6728461330194508 and parameters: {'n_estimators': 985}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:56:01,676] Trial 189 finished with value: 0.6727568760365227 and parameters: {'n_estimators': 959}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-19 23:58:33,281] Trial 190 finished with value: 0.6726243249607771 and parameters: {'n_estimators': 929}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:01:03,131] Trial 191 finished with value: 0.6725850575437355 and parameters: {'n_estimators': 918}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:03:30,891] Trial 192 finished with value: 0.6725278167435247 and parameters: {'n_estimators': 898}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:06:01,049] Trial 193 finished with value: 0.6726243249607772 and parameters: {'n_estimators': 929}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:06:41,989] Trial 194 finished with value: 0.6719684659569979 and parameters: {'n_estimators': 241}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:09:22,808] Trial 195 finished with value: 0.6728409531810906 and parameters: {'n_estimators': 969}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:11:54,867] Trial 196 finished with value: 0.6726961575395622 and parameters: {'n_estimators': 946}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:14:36,852] Trial 197 finished with value: 0.6728248219622142 and parameters: {'n_estimators': 987}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:17:08,011] Trial 198 finished with value: 0.6726058356577637 and parameters: {'n_estimators': 916}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:19:42,804] Trial 199 finished with value: 0.6727517317135367 and parameters: {'n_estimators': 951}. Best is trial 149 with value: 0.6819404343724317.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6819\n",
      "\tBest params:\n",
      "\t\tn_estimators: 974\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.701707    0.700515    0.676903    0.718249\n",
      "1                    TP  345.000000  333.000000  339.000000  336.000000\n",
      "2                    TN  170.000000  182.000000  167.000000  174.000000\n",
      "3                    FP   42.000000   48.000000   55.000000   52.000000\n",
      "4                    FN   38.000000   32.000000   34.000000   33.000000\n",
      "5              Accuracy    0.865546    0.865546    0.850420    0.857143\n",
      "6             Precision    0.891473    0.874016    0.860406    0.865979\n",
      "7           Sensitivity    0.900783    0.912329    0.908847    0.910569\n",
      "8           Specificity    0.801900    0.791300    0.752300    0.769900\n",
      "9              F1 score    0.896104    0.892761    0.883963    0.887715\n",
      "10  F1 score (weighted)    0.865255    0.864565    0.848755    0.855801\n",
      "11     F1 score (macro)    0.852814    0.856291    0.836781    0.845705\n",
      "12    Balanced Accuracy    0.851335    0.851817    0.830550    0.840240\n",
      "13                  MCC    0.705719    0.713982    0.676008    0.693397\n",
      "14                  NPV    0.817300    0.850500    0.830800    0.840600\n",
      "15              ROC_AUC    0.851335    0.851817    0.830550    0.840240\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_rf_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_3_cat = np.where((y_pred_rf_3 >= 6.6) , 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_rf_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 00:22:21,098] Trial 200 finished with value: 0.6762553340276423 and parameters: {'n_estimators': 849}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:24:55,038] Trial 201 finished with value: 0.6761521643861699 and parameters: {'n_estimators': 925}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:27:22,538] Trial 202 finished with value: 0.6761811425353205 and parameters: {'n_estimators': 891}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:30:05,951] Trial 203 finished with value: 0.6761150464509045 and parameters: {'n_estimators': 969}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:32:38,283] Trial 204 finished with value: 0.6761402498595366 and parameters: {'n_estimators': 930}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:35:24,830] Trial 205 finished with value: 0.6762708876595058 and parameters: {'n_estimators': 1000}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:37:54,422] Trial 206 finished with value: 0.6761924289195591 and parameters: {'n_estimators': 911}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:40:29,121] Trial 207 finished with value: 0.6762289265285085 and parameters: {'n_estimators': 947}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:43:07,938] Trial 208 finished with value: 0.6761281211117077 and parameters: {'n_estimators': 964}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:45:48,149] Trial 209 finished with value: 0.6761945845788659 and parameters: {'n_estimators': 940}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:48:22,894] Trial 210 finished with value: 0.6761857484708458 and parameters: {'n_estimators': 924}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:50:52,754] Trial 211 finished with value: 0.6761882683434405 and parameters: {'n_estimators': 913}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:53:19,569] Trial 212 finished with value: 0.6760891408136429 and parameters: {'n_estimators': 898}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:55:58,244] Trial 213 finished with value: 0.6761733145334313 and parameters: {'n_estimators': 976}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 00:58:29,898] Trial 214 finished with value: 0.6762274295637859 and parameters: {'n_estimators': 944}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:01:03,645] Trial 215 finished with value: 0.6761609159556727 and parameters: {'n_estimators': 916}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:03:42,866] Trial 216 finished with value: 0.6761283486734107 and parameters: {'n_estimators': 958}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:06:09,991] Trial 217 finished with value: 0.6761569446755524 and parameters: {'n_estimators': 890}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:08:35,098] Trial 218 finished with value: 0.6761320572920356 and parameters: {'n_estimators': 935}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:11:04,296] Trial 219 finished with value: 0.6762003518390973 and parameters: {'n_estimators': 980}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:13:31,926] Trial 220 finished with value: 0.6761486965577881 and parameters: {'n_estimators': 907}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:15:56,316] Trial 221 finished with value: 0.6762051893647927 and parameters: {'n_estimators': 918}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:18:29,065] Trial 222 finished with value: 0.6761283486734107 and parameters: {'n_estimators': 958}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:21:01,218] Trial 223 finished with value: 0.6761328357099405 and parameters: {'n_estimators': 933}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:23:27,324] Trial 224 finished with value: 0.676170968071353 and parameters: {'n_estimators': 908}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:26:01,718] Trial 225 finished with value: 0.6761645629915243 and parameters: {'n_estimators': 953}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:28:29,972] Trial 226 finished with value: 0.6761857484708458 and parameters: {'n_estimators': 924}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:31:07,364] Trial 227 finished with value: 0.6761281211117077 and parameters: {'n_estimators': 964}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:33:27,557] Trial 228 finished with value: 0.6761390474837735 and parameters: {'n_estimators': 889}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:35:56,418] Trial 229 finished with value: 0.6761998460688848 and parameters: {'n_estimators': 942}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:38:31,645] Trial 230 finished with value: 0.676201631899052 and parameters: {'n_estimators': 979}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:40:57,614] Trial 231 finished with value: 0.6761924289195591 and parameters: {'n_estimators': 911}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:43:29,069] Trial 232 finished with value: 0.6761521643861699 and parameters: {'n_estimators': 925}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:46:00,328] Trial 233 finished with value: 0.6762106727766655 and parameters: {'n_estimators': 943}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:48:28,466] Trial 234 finished with value: 0.6762179847174968 and parameters: {'n_estimators': 920}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:50:50,732] Trial 235 finished with value: 0.6760801954429365 and parameters: {'n_estimators': 899}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:53:24,020] Trial 236 finished with value: 0.6761106486279302 and parameters: {'n_estimators': 962}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:55:54,271] Trial 237 finished with value: 0.6762110739127908 and parameters: {'n_estimators': 939}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 01:58:29,868] Trial 238 finished with value: 0.6762080059596788 and parameters: {'n_estimators': 988}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:00:53,164] Trial 239 finished with value: 0.6762179847174968 and parameters: {'n_estimators': 920}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:03:24,990] Trial 240 finished with value: 0.6761911667616397 and parameters: {'n_estimators': 951}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:05:49,798] Trial 241 finished with value: 0.676042525531466 and parameters: {'n_estimators': 903}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:08:15,318] Trial 242 finished with value: 0.6762051893647927 and parameters: {'n_estimators': 918}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:10:43,454] Trial 243 finished with value: 0.6761318260286371 and parameters: {'n_estimators': 934}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:13:18,921] Trial 244 finished with value: 0.6761517429075354 and parameters: {'n_estimators': 966}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:15:39,997] Trial 245 finished with value: 0.6761569446755524 and parameters: {'n_estimators': 890}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:18:11,561] Trial 246 finished with value: 0.6761320572920356 and parameters: {'n_estimators': 935}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:20:36,795] Trial 247 finished with value: 0.6761849175464635 and parameters: {'n_estimators': 912}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:23:05,751] Trial 248 finished with value: 0.6761958508109417 and parameters: {'n_estimators': 948}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:25:39,892] Trial 249 finished with value: 0.6761923331627797 and parameters: {'n_estimators': 974}. Best is trial 149 with value: 0.6819404343724317.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6819\n",
      "\tBest params:\n",
      "\t\tn_estimators: 974\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77894dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.701707    0.700515    0.676903    0.718249   \n",
      "1                    TP  345.000000  333.000000  339.000000  336.000000   \n",
      "2                    TN  170.000000  182.000000  167.000000  174.000000   \n",
      "3                    FP   42.000000   48.000000   55.000000   52.000000   \n",
      "4                    FN   38.000000   32.000000   34.000000   33.000000   \n",
      "5              Accuracy    0.865546    0.865546    0.850420    0.857143   \n",
      "6             Precision    0.891473    0.874016    0.860406    0.865979   \n",
      "7           Sensitivity    0.900783    0.912329    0.908847    0.910569   \n",
      "8           Specificity    0.801900    0.791300    0.752300    0.769900   \n",
      "9              F1 score    0.896104    0.892761    0.883963    0.887715   \n",
      "10  F1 score (weighted)    0.865255    0.864565    0.848755    0.855801   \n",
      "11     F1 score (macro)    0.852814    0.856291    0.836781    0.845705   \n",
      "12    Balanced Accuracy    0.851335    0.851817    0.830550    0.840240   \n",
      "13                  MCC    0.705719    0.713982    0.676008    0.693397   \n",
      "14                  NPV    0.817300    0.850500    0.830800    0.840600   \n",
      "15              ROC_AUC    0.851335    0.851817    0.830550    0.840240   \n",
      "\n",
      "          Set4  \n",
      "0     0.663945  \n",
      "1   333.000000  \n",
      "2   170.000000  \n",
      "3    61.000000  \n",
      "4    31.000000  \n",
      "5     0.845378  \n",
      "6     0.845178  \n",
      "7     0.914835  \n",
      "8     0.735900  \n",
      "9     0.878628  \n",
      "10    0.843069  \n",
      "11    0.832833  \n",
      "12    0.825383  \n",
      "13    0.670556  \n",
      "14    0.845800  \n",
      "15    0.825383  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_rf_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_4_cat = np.where((y_pred_rf_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_rf_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 02:28:22,967] Trial 250 finished with value: 0.6678566310796008 and parameters: {'n_estimators': 929}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:30:49,933] Trial 251 finished with value: 0.6678634685931628 and parameters: {'n_estimators': 955}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:33:10,885] Trial 252 finished with value: 0.6679753621893405 and parameters: {'n_estimators': 910}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:35:28,053] Trial 253 finished with value: 0.6678492883451326 and parameters: {'n_estimators': 886}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:37:56,406] Trial 254 finished with value: 0.6678273971715203 and parameters: {'n_estimators': 986}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:40:22,924] Trial 255 finished with value: 0.6677938662024976 and parameters: {'n_estimators': 948}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:42:48,541] Trial 256 finished with value: 0.6679200928316241 and parameters: {'n_estimators': 923}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:45:22,390] Trial 257 finished with value: 0.6678693637818349 and parameters: {'n_estimators': 969}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:47:44,526] Trial 258 finished with value: 0.6679989447949317 and parameters: {'n_estimators': 908}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:50:23,556] Trial 259 finished with value: 0.6679238509260335 and parameters: {'n_estimators': 999}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:52:50,911] Trial 260 finished with value: 0.6677858862337186 and parameters: {'n_estimators': 938}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:55:18,109] Trial 261 finished with value: 0.6678733249177327 and parameters: {'n_estimators': 963}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:57:36,508] Trial 262 finished with value: 0.6678731354995925 and parameters: {'n_estimators': 887}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 02:59:57,287] Trial 263 finished with value: 0.6678566310796008 and parameters: {'n_estimators': 929}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:02:22,050] Trial 264 finished with value: 0.6678018031574229 and parameters: {'n_estimators': 949}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:04:35,338] Trial 265 finished with value: 0.6679539597615965 and parameters: {'n_estimators': 851}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:06:53,491] Trial 266 finished with value: 0.6679808151986998 and parameters: {'n_estimators': 902}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:07:12,321] Trial 267 finished with value: 0.6628299035803054 and parameters: {'n_estimators': 115}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:09:44,243] Trial 268 finished with value: 0.6678680946721993 and parameters: {'n_estimators': 975}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:12:03,544] Trial 269 finished with value: 0.6679344430533517 and parameters: {'n_estimators': 921}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:14:30,432] Trial 270 finished with value: 0.667782310858619 and parameters: {'n_estimators': 943}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:17:00,448] Trial 271 finished with value: 0.6678733249177327 and parameters: {'n_estimators': 963}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:19:20,135] Trial 272 finished with value: 0.6679348327164011 and parameters: {'n_estimators': 899}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:21:45,764] Trial 273 finished with value: 0.6678566310796008 and parameters: {'n_estimators': 929}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:24:20,285] Trial 274 finished with value: 0.6678530050525918 and parameters: {'n_estimators': 985}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:26:47,387] Trial 275 finished with value: 0.6678198200025042 and parameters: {'n_estimators': 951}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:29:09,554] Trial 276 finished with value: 0.667958733699622 and parameters: {'n_estimators': 915}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:31:27,607] Trial 277 finished with value: 0.6678492883451326 and parameters: {'n_estimators': 886}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:33:52,190] Trial 278 finished with value: 0.6678007858092956 and parameters: {'n_estimators': 936}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:36:20,585] Trial 279 finished with value: 0.6678525170156658 and parameters: {'n_estimators': 964}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:38:40,075] Trial 280 finished with value: 0.6679675231733703 and parameters: {'n_estimators': 912}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:41:06,405] Trial 281 finished with value: 0.667801803157423 and parameters: {'n_estimators': 949}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:43:35,430] Trial 282 finished with value: 0.6678093898566083 and parameters: {'n_estimators': 978}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:46:00,668] Trial 283 finished with value: 0.6678608620547258 and parameters: {'n_estimators': 928}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:48:20,454] Trial 284 finished with value: 0.6679348327164011 and parameters: {'n_estimators': 899}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:50:48,781] Trial 285 finished with value: 0.6677858862337186 and parameters: {'n_estimators': 938}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:53:22,835] Trial 286 finished with value: 0.6678955902794317 and parameters: {'n_estimators': 995}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:55:51,049] Trial 287 finished with value: 0.6678733249177327 and parameters: {'n_estimators': 963}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 03:58:05,619] Trial 288 finished with value: 0.667895386329622 and parameters: {'n_estimators': 857}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:00:26,610] Trial 289 finished with value: 0.6679675231733703 and parameters: {'n_estimators': 912}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:02:54,139] Trial 290 finished with value: 0.6678384221383398 and parameters: {'n_estimators': 954}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:05:11,236] Trial 291 finished with value: 0.6678492883451326 and parameters: {'n_estimators': 886}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:07:31,852] Trial 292 finished with value: 0.6678895118897243 and parameters: {'n_estimators': 926}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:10:04,697] Trial 293 finished with value: 0.6678093898566083 and parameters: {'n_estimators': 978}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:12:32,790] Trial 294 finished with value: 0.6677665249298592 and parameters: {'n_estimators': 941}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:13:30,849] Trial 295 finished with value: 0.6673123338507179 and parameters: {'n_estimators': 370}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:15:53,370] Trial 296 finished with value: 0.667958733699622 and parameters: {'n_estimators': 915}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:18:21,559] Trial 297 finished with value: 0.6678543049120534 and parameters: {'n_estimators': 957}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:20:43,706] Trial 298 finished with value: 0.6678380376725617 and parameters: {'n_estimators': 932}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:23:02,695] Trial 299 finished with value: 0.6679134064233496 and parameters: {'n_estimators': 897}. Best is trial 149 with value: 0.6819404343724317.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6819\n",
      "\tBest params:\n",
      "\t\tn_estimators: 974\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.701707    0.700515    0.676903    0.718249   \n",
      "1                    TP  345.000000  333.000000  339.000000  336.000000   \n",
      "2                    TN  170.000000  182.000000  167.000000  174.000000   \n",
      "3                    FP   42.000000   48.000000   55.000000   52.000000   \n",
      "4                    FN   38.000000   32.000000   34.000000   33.000000   \n",
      "5              Accuracy    0.865546    0.865546    0.850420    0.857143   \n",
      "6             Precision    0.891473    0.874016    0.860406    0.865979   \n",
      "7           Sensitivity    0.900783    0.912329    0.908847    0.910569   \n",
      "8           Specificity    0.801900    0.791300    0.752300    0.769900   \n",
      "9              F1 score    0.896104    0.892761    0.883963    0.887715   \n",
      "10  F1 score (weighted)    0.865255    0.864565    0.848755    0.855801   \n",
      "11     F1 score (macro)    0.852814    0.856291    0.836781    0.845705   \n",
      "12    Balanced Accuracy    0.851335    0.851817    0.830550    0.840240   \n",
      "13                  MCC    0.705719    0.713982    0.676008    0.693397   \n",
      "14                  NPV    0.817300    0.850500    0.830800    0.840600   \n",
      "15              ROC_AUC    0.851335    0.851817    0.830550    0.840240   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.663945    0.707662  \n",
      "1   333.000000  344.000000  \n",
      "2   170.000000  167.000000  \n",
      "3    61.000000   51.000000  \n",
      "4    31.000000   33.000000  \n",
      "5     0.845378    0.858824  \n",
      "6     0.845178    0.870886  \n",
      "7     0.914835    0.912467  \n",
      "8     0.735900    0.766100  \n",
      "9     0.878628    0.891192  \n",
      "10    0.843069    0.857430  \n",
      "11    0.832833    0.845117  \n",
      "12    0.825383    0.839261  \n",
      "13    0.670556    0.692069  \n",
      "14    0.845800    0.835000  \n",
      "15    0.825383    0.839261  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_rf_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_5_cat = np.where((y_pred_rf_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_rf_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 04:25:50,718] Trial 300 finished with value: 0.6688723272516598 and parameters: {'n_estimators': 967}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:26:32,229] Trial 301 finished with value: 0.6664618891644787 and parameters: {'n_estimators': 273}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:29:08,804] Trial 302 finished with value: 0.6688728761488456 and parameters: {'n_estimators': 998}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:31:36,960] Trial 303 finished with value: 0.6685190037158563 and parameters: {'n_estimators': 942}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:34:03,361] Trial 304 finished with value: 0.6684635997282679 and parameters: {'n_estimators': 919}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:36:19,832] Trial 305 finished with value: 0.6682892775616229 and parameters: {'n_estimators': 883}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:38:56,061] Trial 306 finished with value: 0.6688219898467869 and parameters: {'n_estimators': 976}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:41:23,930] Trial 307 finished with value: 0.6686256882462805 and parameters: {'n_estimators': 951}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:43:45,552] Trial 308 finished with value: 0.6684386448453041 and parameters: {'n_estimators': 903}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:46:11,013] Trial 309 finished with value: 0.6684823453864674 and parameters: {'n_estimators': 922}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:48:40,168] Trial 310 finished with value: 0.6685166700321327 and parameters: {'n_estimators': 938}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:51:09,973] Trial 311 finished with value: 0.6688482606749739 and parameters: {'n_estimators': 963}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:53:30,631] Trial 312 finished with value: 0.6684486434551614 and parameters: {'n_estimators': 902}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:55:54,130] Trial 313 finished with value: 0.6684917620795486 and parameters: {'n_estimators': 930}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 04:58:26,047] Trial 314 finished with value: 0.6688135380254423 and parameters: {'n_estimators': 981}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:00:55,326] Trial 315 finished with value: 0.6687061206129817 and parameters: {'n_estimators': 955}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:03:22,268] Trial 316 finished with value: 0.6684674322238171 and parameters: {'n_estimators': 921}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:05:39,749] Trial 317 finished with value: 0.6682395395842947 and parameters: {'n_estimators': 881}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:08:08,615] Trial 318 finished with value: 0.6685500949138975 and parameters: {'n_estimators': 944}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:10:43,192] Trial 319 finished with value: 0.668867768473379 and parameters: {'n_estimators': 972}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:13:04,901] Trial 320 finished with value: 0.6684861662083744 and parameters: {'n_estimators': 907}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:15:33,308] Trial 321 finished with value: 0.6685195488314655 and parameters: {'n_estimators': 940}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:17:56,978] Trial 322 finished with value: 0.6684357731939364 and parameters: {'n_estimators': 920}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:20:24,022] Trial 323 finished with value: 0.6687509729713815 and parameters: {'n_estimators': 958}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:22:39,215] Trial 324 finished with value: 0.668492616995662 and parameters: {'n_estimators': 861}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:25:15,839] Trial 325 finished with value: 0.6687897248557534 and parameters: {'n_estimators': 988}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:27:35,235] Trial 326 finished with value: 0.6683293055074102 and parameters: {'n_estimators': 898}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:30:02,632] Trial 327 finished with value: 0.6684921863710482 and parameters: {'n_estimators': 933}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:32:32,436] Trial 328 finished with value: 0.6688255728399108 and parameters: {'n_estimators': 960}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:35:07,547] Trial 329 finished with value: 0.6688689204793344 and parameters: {'n_estimators': 999}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:36:18,910] Trial 330 finished with value: 0.6681831271643016 and parameters: {'n_estimators': 454}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:38:45,682] Trial 331 finished with value: 0.6684605884860837 and parameters: {'n_estimators': 914}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:41:19,190] Trial 332 finished with value: 0.6685512904489266 and parameters: {'n_estimators': 945}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:43:45,307] Trial 333 finished with value: 0.6682843663393463 and parameters: {'n_estimators': 882}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:46:36,832] Trial 334 finished with value: 0.6688416269954228 and parameters: {'n_estimators': 978}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:49:41,594] Trial 335 finished with value: 0.6684917620795486 and parameters: {'n_estimators': 930}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:52:59,039] Trial 336 finished with value: 0.6685837606778529 and parameters: {'n_estimators': 949}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:55:59,011] Trial 337 finished with value: 0.6684769584554722 and parameters: {'n_estimators': 905}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 05:58:21,156] Trial 338 finished with value: 0.6688658791673261 and parameters: {'n_estimators': 970}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:00:24,374] Trial 339 finished with value: 0.6684823453864674 and parameters: {'n_estimators': 922}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:02:30,171] Trial 340 finished with value: 0.6685824669788725 and parameters: {'n_estimators': 947}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:04:21,229] Trial 341 finished with value: 0.6684761329652715 and parameters: {'n_estimators': 829}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:06:15,324] Trial 342 finished with value: 0.6683484850576059 and parameters: {'n_estimators': 891}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:08:17,101] Trial 343 finished with value: 0.6684985333803402 and parameters: {'n_estimators': 927}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:10:21,348] Trial 344 finished with value: 0.6688294644954202 and parameters: {'n_estimators': 961}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:12:14,201] Trial 345 finished with value: 0.6683487560420402 and parameters: {'n_estimators': 871}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:14:15,635] Trial 346 finished with value: 0.6685024002827336 and parameters: {'n_estimators': 909}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:16:25,516] Trial 347 finished with value: 0.6688213356479284 and parameters: {'n_estimators': 982}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:18:29,008] Trial 348 finished with value: 0.6685166700321326 and parameters: {'n_estimators': 938}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:20:33,202] Trial 349 finished with value: 0.6686981195116639 and parameters: {'n_estimators': 956}. Best is trial 149 with value: 0.6819404343724317.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6819\n",
      "\tBest params:\n",
      "\t\tn_estimators: 974\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.701707    0.700515    0.676903    0.718249   \n",
      "1                    TP  345.000000  333.000000  339.000000  336.000000   \n",
      "2                    TN  170.000000  182.000000  167.000000  174.000000   \n",
      "3                    FP   42.000000   48.000000   55.000000   52.000000   \n",
      "4                    FN   38.000000   32.000000   34.000000   33.000000   \n",
      "5              Accuracy    0.865546    0.865546    0.850420    0.857143   \n",
      "6             Precision    0.891473    0.874016    0.860406    0.865979   \n",
      "7           Sensitivity    0.900783    0.912329    0.908847    0.910569   \n",
      "8           Specificity    0.801900    0.791300    0.752300    0.769900   \n",
      "9              F1 score    0.896104    0.892761    0.883963    0.887715   \n",
      "10  F1 score (weighted)    0.865255    0.864565    0.848755    0.855801   \n",
      "11     F1 score (macro)    0.852814    0.856291    0.836781    0.845705   \n",
      "12    Balanced Accuracy    0.851335    0.851817    0.830550    0.840240   \n",
      "13                  MCC    0.705719    0.713982    0.676008    0.693397   \n",
      "14                  NPV    0.817300    0.850500    0.830800    0.840600   \n",
      "15              ROC_AUC    0.851335    0.851817    0.830550    0.840240   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.663945    0.707662    0.702829  \n",
      "1   333.000000  344.000000  333.000000  \n",
      "2   170.000000  167.000000  182.000000  \n",
      "3    61.000000   51.000000   46.000000  \n",
      "4    31.000000   33.000000   34.000000  \n",
      "5     0.845378    0.858824    0.865546  \n",
      "6     0.845178    0.870886    0.878628  \n",
      "7     0.914835    0.912467    0.907357  \n",
      "8     0.735900    0.766100    0.798200  \n",
      "9     0.878628    0.891192    0.892761  \n",
      "10    0.843069    0.857430    0.864811  \n",
      "11    0.832833    0.845117    0.856291  \n",
      "12    0.825383    0.839261    0.852801  \n",
      "13    0.670556    0.692069    0.713369  \n",
      "14    0.845800    0.835000    0.842600  \n",
      "15    0.825383    0.839261    0.852801  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_rf_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_6_cat = np.where((y_pred_rf_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_rf_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 06:22:46,812] Trial 350 finished with value: 0.6808030924405266 and parameters: {'n_estimators': 916}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:24:48,984] Trial 351 finished with value: 0.6808431103947848 and parameters: {'n_estimators': 935}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:26:57,939] Trial 352 finished with value: 0.6810314066776807 and parameters: {'n_estimators': 970}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:28:56,652] Trial 353 finished with value: 0.6808754049931609 and parameters: {'n_estimators': 898}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:30:57,407] Trial 354 finished with value: 0.6808446816965411 and parameters: {'n_estimators': 944}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:32:57,940] Trial 355 finished with value: 0.6807836009258581 and parameters: {'n_estimators': 923}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:35:02,841] Trial 356 finished with value: 0.6810914780657008 and parameters: {'n_estimators': 986}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:37:08,964] Trial 357 finished with value: 0.6809009719056768 and parameters: {'n_estimators': 955}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:37:36,820] Trial 358 finished with value: 0.6797741300519233 and parameters: {'n_estimators': 200}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:39:34,481] Trial 359 finished with value: 0.6807923334273042 and parameters: {'n_estimators': 907}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:41:28,644] Trial 360 finished with value: 0.6808186799270501 and parameters: {'n_estimators': 879}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:43:30,721] Trial 361 finished with value: 0.6808933646265583 and parameters: {'n_estimators': 931}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:45:37,536] Trial 362 finished with value: 0.6810333298037603 and parameters: {'n_estimators': 969}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:47:41,698] Trial 363 finished with value: 0.6808498722697522 and parameters: {'n_estimators': 946}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:49:36,750] Trial 364 finished with value: 0.6807836258407178 and parameters: {'n_estimators': 893}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:51:36,118] Trial 365 finished with value: 0.6807733339861088 and parameters: {'n_estimators': 918}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:53:43,545] Trial 366 finished with value: 0.6812336375153093 and parameters: {'n_estimators': 999}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:55:48,931] Trial 367 finished with value: 0.6810384668698637 and parameters: {'n_estimators': 964}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:57:50,284] Trial 368 finished with value: 0.6808933646265583 and parameters: {'n_estimators': 931}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 06:59:39,835] Trial 369 finished with value: 0.6807646976616355 and parameters: {'n_estimators': 852}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:01:36,837] Trial 370 finished with value: 0.6807909517244324 and parameters: {'n_estimators': 908}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:03:39,100] Trial 371 finished with value: 0.6808765350594583 and parameters: {'n_estimators': 952}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:05:43,698] Trial 372 finished with value: 0.6810737011177517 and parameters: {'n_estimators': 983}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:07:58,118] Trial 373 finished with value: 0.6808933646265583 and parameters: {'n_estimators': 931}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:10:32,372] Trial 374 finished with value: 0.6808202791356475 and parameters: {'n_estimators': 943}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:12:51,434] Trial 375 finished with value: 0.6808232203733982 and parameters: {'n_estimators': 895}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:14:56,331] Trial 376 finished with value: 0.6810391183183425 and parameters: {'n_estimators': 967}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:16:51,039] Trial 377 finished with value: 0.6808030924405266 and parameters: {'n_estimators': 916}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:18:42,041] Trial 378 finished with value: 0.6807171074833973 and parameters: {'n_estimators': 872}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:20:43,312] Trial 379 finished with value: 0.6808919502746733 and parameters: {'n_estimators': 951}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:22:41,779] Trial 380 finished with value: 0.6808590297865543 and parameters: {'n_estimators': 929}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:24:48,307] Trial 381 finished with value: 0.6811059664858777 and parameters: {'n_estimators': 979}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:26:44,182] Trial 382 finished with value: 0.680796473446981 and parameters: {'n_estimators': 906}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:28:44,685] Trial 383 finished with value: 0.6808202791356476 and parameters: {'n_estimators': 943}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:30:48,883] Trial 384 finished with value: 0.6810318869716305 and parameters: {'n_estimators': 963}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:32:49,068] Trial 385 finished with value: 0.6807997424332651 and parameters: {'n_estimators': 924}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:34:46,004] Trial 386 finished with value: 0.6807836258407177 and parameters: {'n_estimators': 893}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:36:54,911] Trial 387 finished with value: 0.6810713446833526 and parameters: {'n_estimators': 985}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:38:57,508] Trial 388 finished with value: 0.6808507914082114 and parameters: {'n_estimators': 938}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:40:58,629] Trial 389 finished with value: 0.6808256408350754 and parameters: {'n_estimators': 915}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:43:04,016] Trial 390 finished with value: 0.6809009719056767 and parameters: {'n_estimators': 955}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:44:56,059] Trial 391 finished with value: 0.6808355242209666 and parameters: {'n_estimators': 881}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:47:03,181] Trial 392 finished with value: 0.6810506035502393 and parameters: {'n_estimators': 972}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:49:00,481] Trial 393 finished with value: 0.680843110394785 and parameters: {'n_estimators': 935}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:50:57,775] Trial 394 finished with value: 0.6808255347086745 and parameters: {'n_estimators': 911}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:53:02,776] Trial 395 finished with value: 0.6809129252885008 and parameters: {'n_estimators': 956}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:55:10,476] Trial 396 finished with value: 0.6812252851781035 and parameters: {'n_estimators': 997}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:57:09,413] Trial 397 finished with value: 0.6807997424332651 and parameters: {'n_estimators': 924}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 07:59:04,661] Trial 398 finished with value: 0.6808604022051756 and parameters: {'n_estimators': 897}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:01:09,283] Trial 399 finished with value: 0.6808498722697522 and parameters: {'n_estimators': 946}. Best is trial 149 with value: 0.6819404343724317.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6819\n",
      "\tBest params:\n",
      "\t\tn_estimators: 974\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.701707    0.700515    0.676903    0.718249   \n",
      "1                    TP  345.000000  333.000000  339.000000  336.000000   \n",
      "2                    TN  170.000000  182.000000  167.000000  174.000000   \n",
      "3                    FP   42.000000   48.000000   55.000000   52.000000   \n",
      "4                    FN   38.000000   32.000000   34.000000   33.000000   \n",
      "5              Accuracy    0.865546    0.865546    0.850420    0.857143   \n",
      "6             Precision    0.891473    0.874016    0.860406    0.865979   \n",
      "7           Sensitivity    0.900783    0.912329    0.908847    0.910569   \n",
      "8           Specificity    0.801900    0.791300    0.752300    0.769900   \n",
      "9              F1 score    0.896104    0.892761    0.883963    0.887715   \n",
      "10  F1 score (weighted)    0.865255    0.864565    0.848755    0.855801   \n",
      "11     F1 score (macro)    0.852814    0.856291    0.836781    0.845705   \n",
      "12    Balanced Accuracy    0.851335    0.851817    0.830550    0.840240   \n",
      "13                  MCC    0.705719    0.713982    0.676008    0.693397   \n",
      "14                  NPV    0.817300    0.850500    0.830800    0.840600   \n",
      "15              ROC_AUC    0.851335    0.851817    0.830550    0.840240   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.663945    0.707662    0.702829    0.685792  \n",
      "1   333.000000  344.000000  333.000000  340.000000  \n",
      "2   170.000000  167.000000  182.000000  162.000000  \n",
      "3    61.000000   51.000000   46.000000   58.000000  \n",
      "4    31.000000   33.000000   34.000000   35.000000  \n",
      "5     0.845378    0.858824    0.865546    0.843697  \n",
      "6     0.845178    0.870886    0.878628    0.854271  \n",
      "7     0.914835    0.912467    0.907357    0.906667  \n",
      "8     0.735900    0.766100    0.798200    0.736400  \n",
      "9     0.878628    0.891192    0.892761    0.879690  \n",
      "10    0.843069    0.857430    0.864811    0.841712  \n",
      "11    0.832833    0.845117    0.856291    0.828334  \n",
      "12    0.825383    0.839261    0.852801    0.821515  \n",
      "13    0.670556    0.692069    0.713369    0.659605  \n",
      "14    0.845800    0.835000    0.842600    0.822300  \n",
      "15    0.825383    0.839261    0.852801    0.821515  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_rf_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_7_cat = np.where((y_pred_rf_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_rf_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:03:31,559] Trial 400 finished with value: 0.6732059795794759 and parameters: {'n_estimators': 971}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:05:35,612] Trial 401 finished with value: 0.6732668230804988 and parameters: {'n_estimators': 917}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:07:41,813] Trial 402 finished with value: 0.6731671183494086 and parameters: {'n_estimators': 940}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:09:48,221] Trial 403 finished with value: 0.6732179808647143 and parameters: {'n_estimators': 960}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:11:44,210] Trial 404 finished with value: 0.6731838280831571 and parameters: {'n_estimators': 862}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:13:40,876] Trial 405 finished with value: 0.673143344561125 and parameters: {'n_estimators': 893}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:15:41,532] Trial 406 finished with value: 0.6731819429954036 and parameters: {'n_estimators': 929}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:17:52,943] Trial 407 finished with value: 0.6732457140812611 and parameters: {'n_estimators': 1000}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:18:37,208] Trial 408 finished with value: 0.6712006096634215 and parameters: {'n_estimators': 326}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:20:46,687] Trial 409 finished with value: 0.6732025765338594 and parameters: {'n_estimators': 973}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:22:47,753] Trial 410 finished with value: 0.6732715122410042 and parameters: {'n_estimators': 907}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:24:22,768] Trial 411 finished with value: 0.6729422844318516 and parameters: {'n_estimators': 712}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:25:42,044] Trial 412 finished with value: 0.6724172016770812 and parameters: {'n_estimators': 598}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:27:46,817] Trial 413 finished with value: 0.6731049789538436 and parameters: {'n_estimators': 950}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:29:48,942] Trial 414 finished with value: 0.673165920299193 and parameters: {'n_estimators': 930}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:32:13,655] Trial 415 finished with value: 0.6731803747059307 and parameters: {'n_estimators': 957}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:34:37,408] Trial 416 finished with value: 0.6730961140450927 and parameters: {'n_estimators': 879}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:37:09,720] Trial 417 finished with value: 0.6732688175206767 and parameters: {'n_estimators': 982}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:39:10,649] Trial 418 finished with value: 0.6732432535457754 and parameters: {'n_estimators': 916}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:41:11,933] Trial 419 finished with value: 0.6731552045911846 and parameters: {'n_estimators': 936}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:43:06,300] Trial 420 finished with value: 0.6731402912109844 and parameters: {'n_estimators': 895}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:45:11,119] Trial 421 finished with value: 0.6731445543105437 and parameters: {'n_estimators': 967}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:47:11,180] Trial 422 finished with value: 0.673185543690612 and parameters: {'n_estimators': 944}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:49:08,343] Trial 423 finished with value: 0.6732668230804988 and parameters: {'n_estimators': 917}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:51:05,493] Trial 424 finished with value: 0.6732566168616538 and parameters: {'n_estimators': 904}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:53:06,633] Trial 425 finished with value: 0.6731049789538436 and parameters: {'n_estimators': 950}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:55:03,281] Trial 426 finished with value: 0.6731819429954035 and parameters: {'n_estimators': 929}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:57:08,968] Trial 427 finished with value: 0.6732768602711398 and parameters: {'n_estimators': 983}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 08:59:12,613] Trial 428 finished with value: 0.6731505682968516 and parameters: {'n_estimators': 965}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:01:03,932] Trial 429 finished with value: 0.6730765952336988 and parameters: {'n_estimators': 883}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:02:09,168] Trial 430 finished with value: 0.6719247986604321 and parameters: {'n_estimators': 499}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:04:08,242] Trial 431 finished with value: 0.673131627713356 and parameters: {'n_estimators': 935}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:06:02,013] Trial 432 finished with value: 0.6733025712864897 and parameters: {'n_estimators': 908}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:08:01,682] Trial 433 finished with value: 0.6731155542198061 and parameters: {'n_estimators': 951}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:09:59,784] Trial 434 finished with value: 0.6732498917522514 and parameters: {'n_estimators': 922}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:11:47,966] Trial 435 finished with value: 0.6731011396861349 and parameters: {'n_estimators': 844}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:13:52,448] Trial 436 finished with value: 0.6732025765338594 and parameters: {'n_estimators': 973}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:15:53,653] Trial 437 finished with value: 0.6731745510793509 and parameters: {'n_estimators': 941}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:18:00,045] Trial 438 finished with value: 0.6732337174434604 and parameters: {'n_estimators': 986}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:19:58,260] Trial 439 finished with value: 0.6732389973503474 and parameters: {'n_estimators': 918}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:21:49,038] Trial 440 finished with value: 0.6731426354713005 and parameters: {'n_estimators': 865}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:23:44,316] Trial 441 finished with value: 0.6732026301023872 and parameters: {'n_estimators': 898}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:25:46,318] Trial 442 finished with value: 0.6732030298886427 and parameters: {'n_estimators': 958}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:27:44,718] Trial 443 finished with value: 0.673131627713356 and parameters: {'n_estimators': 935}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:29:48,000] Trial 444 finished with value: 0.6732179808647143 and parameters: {'n_estimators': 960}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:31:41,218] Trial 445 finished with value: 0.6730765952336988 and parameters: {'n_estimators': 883}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:33:35,055] Trial 446 finished with value: 0.6732894704049764 and parameters: {'n_estimators': 913}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:35:33,011] Trial 447 finished with value: 0.6732096246231378 and parameters: {'n_estimators': 943}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:37:38,688] Trial 448 finished with value: 0.6732561075944214 and parameters: {'n_estimators': 980}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:39:37,152] Trial 449 finished with value: 0.6731815626269274 and parameters: {'n_estimators': 926}. Best is trial 149 with value: 0.6819404343724317.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6819\n",
      "\tBest params:\n",
      "\t\tn_estimators: 974\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.701707    0.700515    0.676903    0.718249   \n",
      "1                    TP  345.000000  333.000000  339.000000  336.000000   \n",
      "2                    TN  170.000000  182.000000  167.000000  174.000000   \n",
      "3                    FP   42.000000   48.000000   55.000000   52.000000   \n",
      "4                    FN   38.000000   32.000000   34.000000   33.000000   \n",
      "5              Accuracy    0.865546    0.865546    0.850420    0.857143   \n",
      "6             Precision    0.891473    0.874016    0.860406    0.865979   \n",
      "7           Sensitivity    0.900783    0.912329    0.908847    0.910569   \n",
      "8           Specificity    0.801900    0.791300    0.752300    0.769900   \n",
      "9              F1 score    0.896104    0.892761    0.883963    0.887715   \n",
      "10  F1 score (weighted)    0.865255    0.864565    0.848755    0.855801   \n",
      "11     F1 score (macro)    0.852814    0.856291    0.836781    0.845705   \n",
      "12    Balanced Accuracy    0.851335    0.851817    0.830550    0.840240   \n",
      "13                  MCC    0.705719    0.713982    0.676008    0.693397   \n",
      "14                  NPV    0.817300    0.850500    0.830800    0.840600   \n",
      "15              ROC_AUC    0.851335    0.851817    0.830550    0.840240   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.663945    0.707662    0.702829    0.685792    0.702509  \n",
      "1   333.000000  344.000000  333.000000  340.000000  320.000000  \n",
      "2   170.000000  167.000000  182.000000  162.000000  184.000000  \n",
      "3    61.000000   51.000000   46.000000   58.000000   58.000000  \n",
      "4    31.000000   33.000000   34.000000   35.000000   33.000000  \n",
      "5     0.845378    0.858824    0.865546    0.843697    0.847059  \n",
      "6     0.845178    0.870886    0.878628    0.854271    0.846561  \n",
      "7     0.914835    0.912467    0.907357    0.906667    0.906516  \n",
      "8     0.735900    0.766100    0.798200    0.736400    0.760300  \n",
      "9     0.878628    0.891192    0.892761    0.879690    0.875513  \n",
      "10    0.843069    0.857430    0.864811    0.841712    0.845509  \n",
      "11    0.832833    0.845117    0.856291    0.828334    0.838628  \n",
      "12    0.825383    0.839261    0.852801    0.821515    0.833423  \n",
      "13    0.670556    0.692069    0.713369    0.659605    0.680526  \n",
      "14    0.845800    0.835000    0.842600    0.822300    0.847900  \n",
      "15    0.825383    0.839261    0.852801    0.821515    0.833423  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_rf_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_8_cat = np.where((y_pred_rf_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_rf_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 09:41:45,499] Trial 450 finished with value: 0.6759801247453039 and parameters: {'n_estimators': 904}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:43:45,700] Trial 451 finished with value: 0.6759434593103574 and parameters: {'n_estimators': 965}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:45:43,826] Trial 452 finished with value: 0.6759628722152234 and parameters: {'n_estimators': 945}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:47:40,547] Trial 453 finished with value: 0.6759591396740738 and parameters: {'n_estimators': 928}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:49:31,173] Trial 454 finished with value: 0.6759205411200735 and parameters: {'n_estimators': 890}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:51:36,840] Trial 455 finished with value: 0.6757971707649917 and parameters: {'n_estimators': 999}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:53:35,208] Trial 456 finished with value: 0.6759654293066605 and parameters: {'n_estimators': 950}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:55:31,758] Trial 457 finished with value: 0.6759558846182658 and parameters: {'n_estimators': 914}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:57:34,868] Trial 458 finished with value: 0.675940177461525 and parameters: {'n_estimators': 972}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 09:59:29,606] Trial 459 finished with value: 0.6759964892550372 and parameters: {'n_estimators': 931}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:01:24,861] Trial 460 finished with value: 0.6759467801295952 and parameters: {'n_estimators': 902}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:03:25,759] Trial 461 finished with value: 0.6759971640952048 and parameters: {'n_estimators': 956}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:05:24,533] Trial 462 finished with value: 0.6759807175697881 and parameters: {'n_estimators': 938}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:07:28,580] Trial 463 finished with value: 0.6759340694085164 and parameters: {'n_estimators': 980}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:09:18,730] Trial 464 finished with value: 0.6760604923126887 and parameters: {'n_estimators': 871}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:11:15,108] Trial 465 finished with value: 0.6759247874361229 and parameters: {'n_estimators': 919}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:12:10,236] Trial 466 finished with value: 0.6760130725266654 and parameters: {'n_estimators': 433}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:14:08,946] Trial 467 finished with value: 0.6760124295113792 and parameters: {'n_estimators': 955}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:16:00,842] Trial 468 finished with value: 0.6759346504465974 and parameters: {'n_estimators': 898}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:17:57,373] Trial 469 finished with value: 0.6759582787641419 and parameters: {'n_estimators': 927}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:20:02,659] Trial 470 finished with value: 0.6757997709219058 and parameters: {'n_estimators': 1000}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:22:03,191] Trial 471 finished with value: 0.6759430202070849 and parameters: {'n_estimators': 963}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:23:59,063] Trial 472 finished with value: 0.6759628722152234 and parameters: {'n_estimators': 945}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:25:52,178] Trial 473 finished with value: 0.6759963296822634 and parameters: {'n_estimators': 913}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:27:52,753] Trial 474 finished with value: 0.6759447681313207 and parameters: {'n_estimators': 973}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:29:45,603] Trial 475 finished with value: 0.6760037399324414 and parameters: {'n_estimators': 885}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:29:58,680] Trial 476 finished with value: 0.6697735171383327 and parameters: {'n_estimators': 100}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:31:54,695] Trial 477 finished with value: 0.6759934193688901 and parameters: {'n_estimators': 937}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:33:51,334] Trial 478 finished with value: 0.6759293065734702 and parameters: {'n_estimators': 922}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:34:57,462] Trial 479 finished with value: 0.6758563991162833 and parameters: {'n_estimators': 546}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:36:55,937] Trial 480 finished with value: 0.6759698520559758 and parameters: {'n_estimators': 952}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:38:59,965] Trial 481 finished with value: 0.6759471622403753 and parameters: {'n_estimators': 984}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:40:52,692] Trial 482 finished with value: 0.6759317770387636 and parameters: {'n_estimators': 901}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:41:09,590] Trial 483 finished with value: 0.6704762167252915 and parameters: {'n_estimators': 130}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:43:06,750] Trial 484 finished with value: 0.6759644507170843 and parameters: {'n_estimators': 939}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:45:11,968] Trial 485 finished with value: 0.6759078318628686 and parameters: {'n_estimators': 968}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:47:10,020] Trial 486 finished with value: 0.6759139834262717 and parameters: {'n_estimators': 918}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:49:03,346] Trial 487 finished with value: 0.6760579421896914 and parameters: {'n_estimators': 879}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:51:03,553] Trial 488 finished with value: 0.675989708877158 and parameters: {'n_estimators': 936}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:53:07,817] Trial 489 finished with value: 0.675935429021332 and parameters: {'n_estimators': 962}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:55:05,873] Trial 490 finished with value: 0.6759912998116359 and parameters: {'n_estimators': 909}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:57:08,355] Trial 491 finished with value: 0.6759867313943813 and parameters: {'n_estimators': 953}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 10:59:11,618] Trial 492 finished with value: 0.6759539619269057 and parameters: {'n_estimators': 987}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 11:01:10,856] Trial 493 finished with value: 0.6759687203554348 and parameters: {'n_estimators': 930}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 11:03:05,159] Trial 494 finished with value: 0.6759852266421319 and parameters: {'n_estimators': 896}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 11:05:02,666] Trial 495 finished with value: 0.6759876619135504 and parameters: {'n_estimators': 944}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 11:06:59,850] Trial 496 finished with value: 0.6759558846182658 and parameters: {'n_estimators': 914}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 11:09:01,215] Trial 497 finished with value: 0.675940177461525 and parameters: {'n_estimators': 972}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 11:10:58,248] Trial 498 finished with value: 0.6759964892550372 and parameters: {'n_estimators': 931}. Best is trial 149 with value: 0.6819404343724317.\n",
      "[I 2023-12-20 11:12:45,166] Trial 499 finished with value: 0.6760803411225214 and parameters: {'n_estimators': 864}. Best is trial 149 with value: 0.6819404343724317.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6819\n",
      "\tBest params:\n",
      "\t\tn_estimators: 974\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.701707    0.700515    0.676903    0.718249   \n",
      "1                    TP  345.000000  333.000000  339.000000  336.000000   \n",
      "2                    TN  170.000000  182.000000  167.000000  174.000000   \n",
      "3                    FP   42.000000   48.000000   55.000000   52.000000   \n",
      "4                    FN   38.000000   32.000000   34.000000   33.000000   \n",
      "5              Accuracy    0.865546    0.865546    0.850420    0.857143   \n",
      "6             Precision    0.891473    0.874016    0.860406    0.865979   \n",
      "7           Sensitivity    0.900783    0.912329    0.908847    0.910569   \n",
      "8           Specificity    0.801900    0.791300    0.752300    0.769900   \n",
      "9              F1 score    0.896104    0.892761    0.883963    0.887715   \n",
      "10  F1 score (weighted)    0.865255    0.864565    0.848755    0.855801   \n",
      "11     F1 score (macro)    0.852814    0.856291    0.836781    0.845705   \n",
      "12    Balanced Accuracy    0.851335    0.851817    0.830550    0.840240   \n",
      "13                  MCC    0.705719    0.713982    0.676008    0.693397   \n",
      "14                  NPV    0.817300    0.850500    0.830800    0.840600   \n",
      "15              ROC_AUC    0.851335    0.851817    0.830550    0.840240   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.663945    0.707662    0.702829    0.685792    0.702509    0.712557  \n",
      "1   333.000000  344.000000  333.000000  340.000000  320.000000  329.000000  \n",
      "2   170.000000  167.000000  182.000000  162.000000  184.000000  185.000000  \n",
      "3    61.000000   51.000000   46.000000   58.000000   58.000000   46.000000  \n",
      "4    31.000000   33.000000   34.000000   35.000000   33.000000   35.000000  \n",
      "5     0.845378    0.858824    0.865546    0.843697    0.847059    0.863866  \n",
      "6     0.845178    0.870886    0.878628    0.854271    0.846561    0.877333  \n",
      "7     0.914835    0.912467    0.907357    0.906667    0.906516    0.903846  \n",
      "8     0.735900    0.766100    0.798200    0.736400    0.760300    0.800900  \n",
      "9     0.878628    0.891192    0.892761    0.879690    0.875513    0.890392  \n",
      "10    0.843069    0.857430    0.864811    0.841712    0.845509    0.863219  \n",
      "11    0.832833    0.845117    0.856291    0.828334    0.838628    0.855396  \n",
      "12    0.825383    0.839261    0.852801    0.821515    0.833423    0.852356  \n",
      "13    0.670556    0.692069    0.713369    0.659605    0.680526    0.711445  \n",
      "14    0.845800    0.835000    0.842600    0.822300    0.847900    0.840900  \n",
      "15    0.825383    0.839261    0.852801    0.821515    0.833423    0.852356  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_rf_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_rf_9_cat = np.where((y_pred_rf_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_rf_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6819\n",
      "\tBest params:\n",
      "\t\tn_estimators: 974\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.696147</td>\n",
       "      <td>0.027266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>165.300000</td>\n",
       "      <td>10.873515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>88.800000</td>\n",
       "      <td>5.865151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>4.948625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>5.059644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.855272</td>\n",
       "      <td>0.031640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.869794</td>\n",
       "      <td>0.030139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.899167</td>\n",
       "      <td>0.031153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.783740</td>\n",
       "      <td>0.035009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.884169</td>\n",
       "      <td>0.029549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.854448</td>\n",
       "      <td>0.031680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.844958</td>\n",
       "      <td>0.031089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.841450</td>\n",
       "      <td>0.030469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.690990</td>\n",
       "      <td>0.062231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.829420</td>\n",
       "      <td>0.038103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.841450</td>\n",
       "      <td>0.030469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.696147     0.027266\n",
       "1                    TP       165.300000    10.873515\n",
       "2                    TN        88.800000     5.865151\n",
       "3                    FP        24.600000     4.948625\n",
       "4                    FN        18.400000     5.059644\n",
       "5              Accuracy         0.855272     0.031640\n",
       "6             Precision         0.869794     0.030139\n",
       "7           Sensitivity         0.899167     0.031153\n",
       "8           Specificity         0.783740     0.035009\n",
       "9              F1 score         0.884169     0.029549\n",
       "10  F1 score (weighted)         0.854448     0.031680\n",
       "11     F1 score (macro)         0.844958     0.031089\n",
       "12    Balanced Accuracy         0.841450     0.030469\n",
       "13                  MCC         0.690990     0.062231\n",
       "14                  NPV         0.829420     0.038103\n",
       "15              ROC_AUC         0.841450     0.030469"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.701707</td>\n",
       "      <td>0.700515</td>\n",
       "      <td>0.676903</td>\n",
       "      <td>0.718249</td>\n",
       "      <td>0.663945</td>\n",
       "      <td>0.707662</td>\n",
       "      <td>0.702829</td>\n",
       "      <td>0.685792</td>\n",
       "      <td>0.702509</td>\n",
       "      <td>0.712557</td>\n",
       "      <td>0.697267</td>\n",
       "      <td>0.016758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>335.200000</td>\n",
       "      <td>7.420692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>174.300000</td>\n",
       "      <td>8.313978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>51.700000</td>\n",
       "      <td>6.236986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>33.800000</td>\n",
       "      <td>1.932184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.865546</td>\n",
       "      <td>0.865546</td>\n",
       "      <td>0.850420</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.845378</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.865546</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.863866</td>\n",
       "      <td>0.856303</td>\n",
       "      <td>0.008937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.891473</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.860406</td>\n",
       "      <td>0.865979</td>\n",
       "      <td>0.845178</td>\n",
       "      <td>0.870886</td>\n",
       "      <td>0.878628</td>\n",
       "      <td>0.854271</td>\n",
       "      <td>0.846561</td>\n",
       "      <td>0.877333</td>\n",
       "      <td>0.866473</td>\n",
       "      <td>0.014918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.900783</td>\n",
       "      <td>0.912329</td>\n",
       "      <td>0.908847</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.914835</td>\n",
       "      <td>0.912467</td>\n",
       "      <td>0.907357</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.906516</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.908422</td>\n",
       "      <td>0.004278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.801900</td>\n",
       "      <td>0.791300</td>\n",
       "      <td>0.752300</td>\n",
       "      <td>0.769900</td>\n",
       "      <td>0.735900</td>\n",
       "      <td>0.766100</td>\n",
       "      <td>0.798200</td>\n",
       "      <td>0.736400</td>\n",
       "      <td>0.760300</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>0.771320</td>\n",
       "      <td>0.025643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.896104</td>\n",
       "      <td>0.892761</td>\n",
       "      <td>0.883963</td>\n",
       "      <td>0.887715</td>\n",
       "      <td>0.878628</td>\n",
       "      <td>0.891192</td>\n",
       "      <td>0.892761</td>\n",
       "      <td>0.879690</td>\n",
       "      <td>0.875513</td>\n",
       "      <td>0.890392</td>\n",
       "      <td>0.886872</td>\n",
       "      <td>0.007019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.865255</td>\n",
       "      <td>0.864565</td>\n",
       "      <td>0.848755</td>\n",
       "      <td>0.855801</td>\n",
       "      <td>0.843069</td>\n",
       "      <td>0.857430</td>\n",
       "      <td>0.864811</td>\n",
       "      <td>0.841712</td>\n",
       "      <td>0.845509</td>\n",
       "      <td>0.863219</td>\n",
       "      <td>0.855013</td>\n",
       "      <td>0.009516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.852814</td>\n",
       "      <td>0.856291</td>\n",
       "      <td>0.836781</td>\n",
       "      <td>0.845705</td>\n",
       "      <td>0.832833</td>\n",
       "      <td>0.845117</td>\n",
       "      <td>0.856291</td>\n",
       "      <td>0.828334</td>\n",
       "      <td>0.838628</td>\n",
       "      <td>0.855396</td>\n",
       "      <td>0.844819</td>\n",
       "      <td>0.010318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.851335</td>\n",
       "      <td>0.851817</td>\n",
       "      <td>0.830550</td>\n",
       "      <td>0.840240</td>\n",
       "      <td>0.825383</td>\n",
       "      <td>0.839261</td>\n",
       "      <td>0.852801</td>\n",
       "      <td>0.821515</td>\n",
       "      <td>0.833423</td>\n",
       "      <td>0.852356</td>\n",
       "      <td>0.839868</td>\n",
       "      <td>0.011893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.705719</td>\n",
       "      <td>0.713982</td>\n",
       "      <td>0.676008</td>\n",
       "      <td>0.693397</td>\n",
       "      <td>0.670556</td>\n",
       "      <td>0.692069</td>\n",
       "      <td>0.713369</td>\n",
       "      <td>0.659605</td>\n",
       "      <td>0.680526</td>\n",
       "      <td>0.711445</td>\n",
       "      <td>0.691668</td>\n",
       "      <td>0.019447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.817300</td>\n",
       "      <td>0.850500</td>\n",
       "      <td>0.830800</td>\n",
       "      <td>0.840600</td>\n",
       "      <td>0.845800</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.842600</td>\n",
       "      <td>0.822300</td>\n",
       "      <td>0.847900</td>\n",
       "      <td>0.840900</td>\n",
       "      <td>0.837370</td>\n",
       "      <td>0.010971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.851335</td>\n",
       "      <td>0.851817</td>\n",
       "      <td>0.830550</td>\n",
       "      <td>0.840240</td>\n",
       "      <td>0.825383</td>\n",
       "      <td>0.839261</td>\n",
       "      <td>0.852801</td>\n",
       "      <td>0.821515</td>\n",
       "      <td>0.833423</td>\n",
       "      <td>0.852356</td>\n",
       "      <td>0.839868</td>\n",
       "      <td>0.011893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.701707    0.700515    0.676903    0.718249   \n",
       "1                    TP  345.000000  333.000000  339.000000  336.000000   \n",
       "2                    TN  170.000000  182.000000  167.000000  174.000000   \n",
       "3                    FP   42.000000   48.000000   55.000000   52.000000   \n",
       "4                    FN   38.000000   32.000000   34.000000   33.000000   \n",
       "5              Accuracy    0.865546    0.865546    0.850420    0.857143   \n",
       "6             Precision    0.891473    0.874016    0.860406    0.865979   \n",
       "7           Sensitivity    0.900783    0.912329    0.908847    0.910569   \n",
       "8           Specificity    0.801900    0.791300    0.752300    0.769900   \n",
       "9              F1 score    0.896104    0.892761    0.883963    0.887715   \n",
       "10  F1 score (weighted)    0.865255    0.864565    0.848755    0.855801   \n",
       "11     F1 score (macro)    0.852814    0.856291    0.836781    0.845705   \n",
       "12    Balanced Accuracy    0.851335    0.851817    0.830550    0.840240   \n",
       "13                  MCC    0.705719    0.713982    0.676008    0.693397   \n",
       "14                  NPV    0.817300    0.850500    0.830800    0.840600   \n",
       "15              ROC_AUC    0.851335    0.851817    0.830550    0.840240   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.663945    0.707662    0.702829    0.685792    0.702509    0.712557   \n",
       "1   333.000000  344.000000  333.000000  340.000000  320.000000  329.000000   \n",
       "2   170.000000  167.000000  182.000000  162.000000  184.000000  185.000000   \n",
       "3    61.000000   51.000000   46.000000   58.000000   58.000000   46.000000   \n",
       "4    31.000000   33.000000   34.000000   35.000000   33.000000   35.000000   \n",
       "5     0.845378    0.858824    0.865546    0.843697    0.847059    0.863866   \n",
       "6     0.845178    0.870886    0.878628    0.854271    0.846561    0.877333   \n",
       "7     0.914835    0.912467    0.907357    0.906667    0.906516    0.903846   \n",
       "8     0.735900    0.766100    0.798200    0.736400    0.760300    0.800900   \n",
       "9     0.878628    0.891192    0.892761    0.879690    0.875513    0.890392   \n",
       "10    0.843069    0.857430    0.864811    0.841712    0.845509    0.863219   \n",
       "11    0.832833    0.845117    0.856291    0.828334    0.838628    0.855396   \n",
       "12    0.825383    0.839261    0.852801    0.821515    0.833423    0.852356   \n",
       "13    0.670556    0.692069    0.713369    0.659605    0.680526    0.711445   \n",
       "14    0.845800    0.835000    0.842600    0.822300    0.847900    0.840900   \n",
       "15    0.825383    0.839261    0.852801    0.821515    0.833423    0.852356   \n",
       "\n",
       "           ave       std  \n",
       "0     0.697267  0.016758  \n",
       "1   335.200000  7.420692  \n",
       "2   174.300000  8.313978  \n",
       "3    51.700000  6.236986  \n",
       "4    33.800000  1.932184  \n",
       "5     0.856303  0.008937  \n",
       "6     0.866473  0.014918  \n",
       "7     0.908422  0.004278  \n",
       "8     0.771320  0.025643  \n",
       "9     0.886872  0.007019  \n",
       "10    0.855013  0.009516  \n",
       "11    0.844819  0.010318  \n",
       "12    0.839868  0.011893  \n",
       "13    0.691668  0.019447  \n",
       "14    0.837370  0.010971  \n",
       "15    0.839868  0.011893  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test['std'] = mat_met_rf_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_rf0</th>\n",
       "      <th>y_pred_rf1</th>\n",
       "      <th>y_pred_rf2</th>\n",
       "      <th>y_pred_rf3</th>\n",
       "      <th>y_pred_rf4</th>\n",
       "      <th>y_pred_rf_ave</th>\n",
       "      <th>y_pred_rf_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4084049</td>\n",
       "      <td>0</td>\n",
       "      <td>6.26</td>\n",
       "      <td>8.837534</td>\n",
       "      <td>8.719624</td>\n",
       "      <td>8.966145</td>\n",
       "      <td>8.831776</td>\n",
       "      <td>8.896641</td>\n",
       "      <td>8.418620</td>\n",
       "      <td>0.968222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL2178343</td>\n",
       "      <td>1</td>\n",
       "      <td>5.96</td>\n",
       "      <td>5.878593</td>\n",
       "      <td>6.097581</td>\n",
       "      <td>5.952685</td>\n",
       "      <td>5.950197</td>\n",
       "      <td>5.901396</td>\n",
       "      <td>5.956742</td>\n",
       "      <td>0.069613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL454672</td>\n",
       "      <td>2</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.447077</td>\n",
       "      <td>6.419462</td>\n",
       "      <td>6.404847</td>\n",
       "      <td>6.539621</td>\n",
       "      <td>6.554507</td>\n",
       "      <td>6.527586</td>\n",
       "      <td>0.134360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4299417</td>\n",
       "      <td>3</td>\n",
       "      <td>8.14</td>\n",
       "      <td>7.364052</td>\n",
       "      <td>7.362639</td>\n",
       "      <td>7.604408</td>\n",
       "      <td>7.729317</td>\n",
       "      <td>7.330781</td>\n",
       "      <td>7.588533</td>\n",
       "      <td>0.286369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3692580</td>\n",
       "      <td>4</td>\n",
       "      <td>5.24</td>\n",
       "      <td>5.632797</td>\n",
       "      <td>5.728714</td>\n",
       "      <td>5.685316</td>\n",
       "      <td>5.657134</td>\n",
       "      <td>5.777915</td>\n",
       "      <td>5.620313</td>\n",
       "      <td>0.176524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL3775269</td>\n",
       "      <td>2966</td>\n",
       "      <td>7.40</td>\n",
       "      <td>6.972033</td>\n",
       "      <td>7.017793</td>\n",
       "      <td>6.963299</td>\n",
       "      <td>7.058152</td>\n",
       "      <td>7.116940</td>\n",
       "      <td>7.088036</td>\n",
       "      <td>0.148885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL3339019</td>\n",
       "      <td>2967</td>\n",
       "      <td>8.07</td>\n",
       "      <td>7.988166</td>\n",
       "      <td>7.811777</td>\n",
       "      <td>7.967202</td>\n",
       "      <td>7.998701</td>\n",
       "      <td>8.012441</td>\n",
       "      <td>7.974714</td>\n",
       "      <td>0.079440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL3771312</td>\n",
       "      <td>2968</td>\n",
       "      <td>6.52</td>\n",
       "      <td>6.292977</td>\n",
       "      <td>6.173414</td>\n",
       "      <td>6.232690</td>\n",
       "      <td>6.270167</td>\n",
       "      <td>6.259155</td>\n",
       "      <td>6.291400</td>\n",
       "      <td>0.108872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL3589701</td>\n",
       "      <td>2969</td>\n",
       "      <td>6.49</td>\n",
       "      <td>7.115708</td>\n",
       "      <td>7.219419</td>\n",
       "      <td>7.072490</td>\n",
       "      <td>7.097079</td>\n",
       "      <td>7.067160</td>\n",
       "      <td>7.010309</td>\n",
       "      <td>0.238112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL4092997</td>\n",
       "      <td>2970</td>\n",
       "      <td>7.47</td>\n",
       "      <td>7.352065</td>\n",
       "      <td>7.396794</td>\n",
       "      <td>7.324820</td>\n",
       "      <td>7.347172</td>\n",
       "      <td>7.338741</td>\n",
       "      <td>7.371599</td>\n",
       "      <td>0.049268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_rf0  y_pred_rf1  \\\n",
       "0         CHEMBL4084049            0     6.26    8.837534    8.719624   \n",
       "1         CHEMBL2178343            1     5.96    5.878593    6.097581   \n",
       "2          CHEMBL454672            2     6.80    6.447077    6.419462   \n",
       "3         CHEMBL4299417            3     8.14    7.364052    7.362639   \n",
       "4         CHEMBL3692580            4     5.24    5.632797    5.728714   \n",
       "...                 ...          ...      ...         ...         ...   \n",
       "2966      CHEMBL3775269         2966     7.40    6.972033    7.017793   \n",
       "2967      CHEMBL3339019         2967     8.07    7.988166    7.811777   \n",
       "2968      CHEMBL3771312         2968     6.52    6.292977    6.173414   \n",
       "2969      CHEMBL3589701         2969     6.49    7.115708    7.219419   \n",
       "2970      CHEMBL4092997         2970     7.47    7.352065    7.396794   \n",
       "\n",
       "      y_pred_rf2  y_pred_rf3  y_pred_rf4  y_pred_rf_ave  y_pred_rf_std  \n",
       "0       8.966145    8.831776    8.896641       8.418620       0.968222  \n",
       "1       5.952685    5.950197    5.901396       5.956742       0.069613  \n",
       "2       6.404847    6.539621    6.554507       6.527586       0.134360  \n",
       "3       7.604408    7.729317    7.330781       7.588533       0.286369  \n",
       "4       5.685316    5.657134    5.777915       5.620313       0.176524  \n",
       "...          ...         ...         ...            ...            ...  \n",
       "2966    6.963299    7.058152    7.116940       7.088036       0.148885  \n",
       "2967    7.967202    7.998701    8.012441       7.974714       0.079440  \n",
       "2968    6.232690    6.270167    6.259155       6.291400       0.108872  \n",
       "2969    7.072490    7.097079    7.067160       7.010309       0.238112  \n",
       "2970    7.324820    7.347172    7.338741       7.371599       0.049268  \n",
       "\n",
       "[2971 rows x 10 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_rf_cat = np.where((y_pred_optimized_rf >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_rf_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_rf))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "rf_run0 = data_rf[['y_test_idx0', 'y_test0', 'y_pred_rf0']]\n",
    "rf_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "rf_run0.reset_index(inplace=True, drop=True)\n",
    "rf_run1 = data_rf[['y_test_idx1', 'y_test1', 'y_pred_rf1']]\n",
    "rf_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "rf_run1.reset_index(inplace=True, drop=True)\n",
    "rf_run2 = data_rf[['y_test_idx2', 'y_test2', 'y_pred_rf2']]\n",
    "rf_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "rf_run2.reset_index(inplace=True, drop=True)\n",
    "rf_run3 = data_rf[['y_test_idx3', 'y_test3', 'y_pred_rf3']]\n",
    "rf_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "rf_run3.reset_index(inplace=True, drop=True)\n",
    "rf_run4 = data_rf[['y_test_idx4', 'y_test4', 'y_pred_rf4']]\n",
    "rf_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "rf_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "rf_5preds = pd.concat([chembl_id, rf_run0, rf_run1, rf_run2, rf_run3, rf_run4], axis=1)\n",
    "rf_5preds = rf_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_rf0', 'y_pred_rf1', 'y_pred_rf2', 'y_pred_rf3', 'y_pred_rf4']]\n",
    "rf_5preds['y_pred_rf_ave'] = rf_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "rf_5preds['y_pred_rf_std'] = rf_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "rf_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47203ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.699620</td>\n",
       "      <td>0.032128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.855941</td>\n",
       "      <td>0.020113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.870435</td>\n",
       "      <td>0.024364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.901485</td>\n",
       "      <td>0.023598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.782290</td>\n",
       "      <td>0.041462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.885362</td>\n",
       "      <td>0.016877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.854994</td>\n",
       "      <td>0.020344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.845237</td>\n",
       "      <td>0.021377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.841885</td>\n",
       "      <td>0.022128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.692580</td>\n",
       "      <td>0.042335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.831228</td>\n",
       "      <td>0.035791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.841885</td>\n",
       "      <td>0.022128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.699620     0.032128\n",
       "1              Accuracy         0.855941     0.020113\n",
       "2             Precision         0.870435     0.024364\n",
       "3           Sensitivity         0.901485     0.023598\n",
       "4           Specificity         0.782290     0.041462\n",
       "5              F1 score         0.885362     0.016877\n",
       "6   F1 score (weighted)         0.854994     0.020344\n",
       "7      F1 score (macro)         0.845237     0.021377\n",
       "8     Balanced Accuracy         0.841885     0.022128\n",
       "9                   MCC         0.692580     0.042335\n",
       "10                  NPV         0.831228     0.035791\n",
       "11              ROC_AUC         0.841885     0.022128"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_optimized_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfc78124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAG8CAYAAADaV3/fAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB8cUlEQVR4nO3deXhTVf4/8PfN0o3SjVJK2VosMICCKF/9jajgOqMygyjiMoyjojKAjo4bm4gMYikoLih8HXXEZVyQRRwXvqIjKuqjuIsoUqGK0NKGbpRuWe7vj9ukuTf3JjfJbZPcvl/Pw6NJbm7OSdLeT8/5nPMRRFEUQURERGRillg3gIiIiKizMeAhIiIi02PAQ0RERKbHgIeIiIhMjwEPERERmR4DHiIiIjI9BjxERERkegx4iIiIyPQY8BAREZHpMeAJ4ZZbboEgCLjooovgdrtj3RwiIiKKQLcKeK666ioIggBBEGCz2TBw4EDMnDkTtbW1qscvXboUjz/+OB577DF8/PHHmDFjRsAx27Ztw6RJk9C3b1/06NEDxx9/PP797393dlfQ2tqKG2+8Ebm5uejRowf++Mc/4tdffw36nMLCQl///f/Nnj3bd8yhQ4dw1VVXoaCgAGlpafj973+PPXv2BJzr448/xplnnokePXogKysLEyZMQHNzs+H9JCIiMkK3CngA4Pe//z0qKipQXl6OJ554Av/5z38wa9asgOP++c9/4v7778fWrVtx/fXX4/3338fWrVsxZ84c2XEfffQRRo0ahQ0bNuCbb77BNddcgyuvvBL/+c9/OrUfN998MzZt2oQXX3wR27dvR2NjIyZOnBh0FGrHjh2oqKjw/du6dSsA4JJLLgEAiKKICy+8EHv37sXmzZvx5ZdfYtCgQTj77LNx9OhR33k+/vhj/P73v8e5556LTz/9FDt27MANN9wAi6XbfZ2IiChRiN3IX/7yF3HSpEmy+2655RYxJydHdt/LL78s5ufni19++aXs/p9//lksLi4WS0tLg77O+eefL1599dVGNFlVXV2daLfbxRdffNF334EDB0SLxSJu2bJF93luuukm8ZhjjhE9Ho8oiqK4e/duEYC4c+dO3zEul0vMyckRH3/8cd99J598snjnnXca0BMiIqKu0a3/JN+7dy+2bNkCu90uu3/KlCmoqKjA8ccfL7t/4MCB2LNnD+64446g562vr0dOTk7QY0aOHIn09HTNfyNHjtR87ueffw6n04lzzz3Xd19BQQGOPfZYfPTRR0Ff16utrQ3PPfccrrnmGgiCAECaJgOAlJQU33FWqxVJSUnYvn07AKCqqgqffPIJ8vLycMopp6BPnz4YP36873EiIqJ4ZIt1A7raa6+9hvT0dLjdbrS0tAAAVq5cadj5169fjx07duCxxx4Letwbb7wBp9Op+bgyCPNXWVmJpKQkZGdny+7v06cPKisrdbXzlVdeQV1dHa666irffb/5zW8waNAgzJs3D4899hh69OiBlStXorKyEhUVFQCkIBEA7r77btx33304/vjj8cwzz+Css87Czp07MWTIEF2vT0RE1JViHvDs2rULr776Kvbt24fa2lrcdtttOOmkkwAALpcLL774Ir788ktUVVUhLS0Nxx13HK644oqQIyhazjjjDKxZswZNTU144okn8OOPP+LGG280pC/btm3DVVddhccffzzoCA0ADBo0yJDX9CeKom+0JpQnn3wS5513HgoKCnz32e12bNiwAdOnT0dOTg6sVivOPvtsnHfeeb5jPB4PAGDGjBm4+uqrAQBjxozBO++8g3/9618oKSkxsEdERETGiPmUVmtrKwoLC3HNNdcEPNbW1oZ9+/bh4osvRmlpKW699VZUVFRg+fLlEb9ejx49UFxcjFGjRuHhhx9Ga2srFi9eHE0XAADvvfce/vCHP2DlypW48sorQx4fzZRWfn4+2traAlaXVVVVoU+fPiFf++eff8bbb7+Na6+9NuCxE088EV999RXq6upQUVGBLVu24PDhwygqKgIA9O3bFwAwYsQI2fOGDx+OX375JeRrExERxULMR3jGjBmDMWPGqD6WlpaGhQsXyu67+uqrMX/+fDgcDuTm5kb9+osWLcJ5552HmTNnykY7wrFt2zZMnDgRpaWluP7663U9J5oprRNPPBF2ux1bt27F1KlTAQAVFRXYuXOnrmDwqaeeQl5eHi644ALNYzIzMwEAe/bswWeffYYlS5YAkJa2FxQUYPfu3bLjf/zxR9lIEBERUTyJecATrqamJgiCgLS0NM1jnE5nQDChFUBMmDABI0eOxL333otHHnkk7PZs27YNF1xwAW666SZcfPHFvhyapKSkoNNu0UxpZWZmYvr06bj11lvRq1cv5OTk4LbbbsNxxx2Hs88+23fcWWedhcmTJ+OGG27w3efxePDUU0/hL3/5C2y2wI//5ZdfRu/evTFw4EB8++23uOmmm3DhhRf6EqQFQcDtt9+ORYsWYfTo0Tj++OPx9NNP44cffsD69esj7hMREVFnSqiAp62tDc8//zzGjRsXNODZtGmT7OI7btw43HTTTZrH33LLLbj66qsxZ84cDBgwIKw2rV27Fk1NTSgpKZHlr4wfPx7btm0L61zheOCBB2Cz2TB16lQ0NzfjrLPOwtq1a2G1Wn3H/PTTT3A4HLLnvf322/jll19UpxABaaTolltuwaFDh9C3b19ceeWVAaNsN998M1paWvD3v/8dNTU1GD16NLZu3YpjjjnG+I4SEREZQBBFUYx1I7ymTp0qS1r253K5sHLlShw+fBiLFi0Ka4RHEASkpqaitrYWLperU9oeK4IgIDc3Fw6HA3H0URqCfUtMZu4bYO7+sW+Jycx9s9lsASuSIz6XIWfpZC6XCw888ACqq6tx1113BQ12AGn6Sm0Ky+VyBc2bSUTeVVlOp9N0X3T2LTGZuW+AufvHviUmM/fNSDFfpRWKN9iprKzEwoUL0bNnz1g3iYiIiBJMzEd4WlpaZJvlVVVVoby8HOnp6cjOzsbKlSuxb98+zJkzBx6PB3V1dQCA9PR01aRbIiIiIqWYRww//fSTbB+cZ555BoCU9HvJJZfgs88+A4CAcg6LFi0KubkfERERERAHAc/IkSOxbt06zceDPUZERESkR9zn8BARERFFiwEPERERmR4DHiIiIjI9BjxERERkegx4iIiIyPQY8BAREZHpMeAhIiIi02PAQ0RERKbHgIeIiIhMjwEPERERmR4DHiIiIjI9BjxERERkegx4iIiIyPQY8BAREZHpMeAhIiIi02PAQ0RERKbHgIeIiIhMjwEPERERmR4DHiIiIjI9BjxERERkegx4iIiIyPQY8BAREZHpMeAhIiIi02PAQ0RERKbHgIeIiIhMjwEPERERmR4DHiIiIjI9BjxERERkegx4iIiIyPQY8BAREZHpMeAhIiIi07PFugG7du3Cq6++in379qG2tha33XYbTjrpJN/jn3zyCd5++23s3bsXR44cwfLly1FYWBi7BhMREVHCifkIT2trKwoLC3HNNddoPj5s2DBcccUVXdwyIiIiMouYj/CMGTMGY8aM0Xz89NNPBwBUVVXpPqfT6YTT6fTdFgQBqampEAQBgiBE3tg45O2P2foFsG+Jysx9A8zdP/YtMXWHvhkh5gFPZ9i0aRPWr1/vu11UVITS0lLk5ubGsFWdKz8/P9ZN6DTsW2Iyc98Ac/ePfUtMZu6bEUwZ8EyePBkTJ0703fZGiA6HQzbyYwaCICA/Px+VlZUQRTHWzTEU+5aYzNw3wNz9Y98Sk5n7ZrfbDRusMGXAY7fbYbfbA+4XRdF0XwYv9i0xsW+Jy8z9Y98Skxn7ZmR/Yp60TERERNTZGPAQERGR6cV8SqulpQWVlZW+21VVVSgvL0d6ejpyc3PR2NgIh8OBmpoaAMDBgwcBAFlZWcjKyopFk4mIiCjBxDzg+emnn7B48WLf7WeeeQYAMH78eMyePRufffYZVq9e7Xv8wQcfBABMmTIFU6dO7dK2EhERUWKKecAzcuRIrFu3TvPxCRMmYMKECV3XICIiIjId5vAQERGR6THgISIiItNjwENERESmx4CHiIiITI8BDxEREZkeAx4iIiIyPQY8REREZHoMeIiIiMj0GPAQERGR6THgISIiItNjwENERESmx4CHiIiITI8BDxEREZkeAx4iIiIyPQY8REREZHoMeIiIiMj0GPAQERGR6THgISIiItNjwENERESmx4CHiIiITI8BDxEREZkeAx4iIiIyPQY8REREZHoMeIiIiMj0GPAQERGR6THgISIiItNjwENERESmx4CHiIiITI8BDxEREZkeAx4iIiIyPVusG7Br1y68+uqr2LdvH2pra3HbbbfhpJNO8j0uiiJefvllvPPOO2hsbMSQIUMwffp0DBgwIIatJiIiokQS8xGe1tZWFBYW4pprrlF9fPPmzXj99ddxzTXXoKSkBFlZWbjnnnvQ3NzcxS0lIiKiRBXzEZ4xY8ZgzJgxqo+Joog33ngDkydPxsknnwwAmD17Nq677jps374d55xzjurznE4nnE6n77YgCEhNTYUgCBAEwfhOxJC3P2brF8C+JSoz9w0wd//Yt8TUHfpmhJgHPMFUVVWhrq4Oo0eP9t1nt9sxYsQI7N69WzPg2bRpE9avX++7XVRUhNLSUuTm5nZ6m2MlPz8/1k3oNOxbYjJz3wBz9499S0xm7psR4jrgqaurAwBkZmbK7s/MzITD4dB83uTJkzFx4kTfbW+E6HA4ZCM/ZiAIAvLz81FZWQlRFGPdHEOxb4nJzH0DzN0/9i0xmblvdrvdsMGKuA54vJRDWqE+ULvdDrvdHnC/KIqm+zJ4sW+JiX1LXGbuH/uWmMzYNyP7E/Ok5WCysrIAdIz0eDU0NASM+hARERFpieuAJy8vD1lZWfjmm29897lcLuzatQvDhg2LYcuIiIgokcR8SqulpQWVlZW+21VVVSgvL0d6ejpyc3Nx/vnnY9OmTejbty/y8/OxadMmJCcn49RTT41hq4mIiCiRxDzg+emnn7B48WLf7WeeeQYAMH78eMyePRuTJk1CW1sbnnjiCRw9ehTFxcVYsGABUlNTY9VkIiIiSjAxD3hGjhyJdevWaT4uCAKmTp2KqVOndmGriIiIyEziOoeHiIiIyAgMeIiIiMj0GPAQERGR6THgISIiItNjwENERESmx4CHiIiITI8BDxEREZlezPfhISIicxAbauFZswyoqwGycmCZOQ9CRlasm0UEgCM8RERkEM+aZUDZ94DjEFD2PTxrSmLdJCKfiEZ4qqqq8MUXX2D37t2oqalBW1sbMjIy0K9fPxx77LEYNWoUbDYOHhERdSt1NcFvE8VQWFHJd999h1deeQXffvstRFFETk4OMjIykJSUhKqqKuzatQuvv/46MjIycPbZZ+MPf/gD0tLSOqvtREQUT7JypNEd/9tEcUJ3wLNixQp88cUXOP7443HTTTdh5MiRyMjIkB3j8Xjw888/49NPP8UHH3yAt99+GzfeeCNGjRpleMOJiCi+WGbOk6ax/HJ4iOKF7oAnNTUVDz74IPr06aN5jMViQVFREYqKinDJJZfg/fffR00NhzSJiLoDISML1jmlsW4GkSrdAc8NN9wQ1oktFgsmTJgQbnuIiIiIDMfMYiIiSkhcBk/h0L0sfd68efj1119l9+3cuRMtLS2GN4qIiCiUaJbBiw21cJfOgXvedXCXzoHYUNd5DaW4oDvg2bt3ryy48Xg8WLJkCQ4ePNgpDSMiIgoqimXw3DOo++GUFhERdYpOn3KKZhk89wzqdrjTMhERdYrOHkWxzJwHFA8HcvsAxcPDWwavDI64Z5DpcYSHiIg6RyePokSzDJ57BnU/YQU827dvxw8//ABAyuHx3rdr166AYydOnGhA84iIKGHF8c7L3DOo+wkr4HnzzTcD7nv99ddVj2XAQ0TUvXEUheKJ7oDnkUce6cx2EBGRyXTmKAr34KFw6Q54evfu3ZntICIi0s2XEA0AjkPwrCnhFBUFZfgqrZaWFmzYsMHo0xIREXXgsnIKU9irtFwuF44ePYqMjAwIguC7v7W1FW+++Sb+85//oLGxERdffLGhDSUiIvKJ44Roik+6Ax6Xy4V//etfeO+99+ByudCjRw/86U9/wllnnYWPPvoITz/9NOrq6jBw4MCwC40SERGFgwnRFC7dAc+rr76Kd955B/n5+SgsLERVVRX++c9/orq6Gps2bUJmZiZmzpyJ8ePHy0Z+iIioc3XHBF4uK6dw6Q54tm/fjrFjx+LWW2+FxSKl/qxbtw4bNmxAYWEhFi5ciPT09E5rKBERqWMCL1FoupOWDx06hLPOOssX7ADAueeeCwC46KKLGOwQEcUKE3iJQgorhycjI0N2n/d2Zy9Zb25uxksvvYRPP/0U9fX1KCoqwlVXXYXi4uJOfV0iooTQBQm8ngPlEJfNAdpagaRkCHNXwNJvoOGvQ9RZDFmW3tk5O//7v/+Lb775BjfccAPuv/9+jBo1CkuWLEFNDf+KISKKqoimTuKyOUBLM+DxAC3NEJfdrn1sQy3cpXPgnncd3KVzIDbUGd4eonCFtSz94YcfRlJSUsD9Dz74IOx2u++2IAhYsWJF9K0D0NbWhk8++QR33HEHRowYAQCYOnUqduzYgbfeeguXXXZZwHOcTiecTqesPampqRAEwXQJ1d7+mK1fAPuWqMzcNyA++ydkZsMyd3n05/H2qaEO7tX3+pKgrbPmSyM7/tpaNd8Dt0pOkc2A9kUjHj83o3SHvhlBd8AzfPhw1Rf2BiGdxe12w+PxyAIqAEhKSvIVMlXatGkT1q9f77tdVFSE0tJS5ObmdmpbYyk/Pz/WTeg07FtiMnPfAHP3z/LEfbKAxfrEffAkp0BsbvIdIySnoG/fvqrPP9jYALffbWtjg+axXc3Mn5uZ+2YE3QHP3Xff3YnN0JaamoqhQ4diw4YN6NevH7KysrB9+3aUlZVpfriTJ0+WFS/1BmoOh0M28mMGgiAgPz8flZWVEEUx1s0xFPuWmMzcN8Dc/fP2ra3qkOz+tqpKWOYuh1hyu18Oz3JUVFSonsedngHggOy21rFdpTt8bmbsm91uN2ywIuydlmPhhhtuwJo1a/DXv/4VFosFRUVFGDduHPbt26d6vN1uDxgRAgBRFE33ZfBi3xIT+5a4TN2/rGzAUel3OwdCwUBYV70kO0yr/2qbAobzXnXmvkJm/tzM2Dcj+6M74HE4HBFFWTU1NcjJiW7FQH5+PhYvXoyWlhY0NzcjOzsbDzzwAPLy8qI6LxERBbLOmi/L4Qk3CTraTQG5rxB1Bt0Bz0033YSzzz4b5513Xsh5QpfLhR07dmDjxo04+eSTMWXKlKgbCgApKSlISUlBY2Mjvv76a0ybNs2Q8xIRUYeY72LMfYWoE+gOeO688048/fTT2LJlC4qLizFy5EgUFRUhMzMTdrsdjY2NOHToEH788Ud8/fXXaGlpwfnnny/LpYnUV199BQAoKChAZWUlnn32WRQUFGDChAlRn5uIiOIMC4NSJwhrldayZcvw5ZdfYuvWrXjzzTfR1tYWcFxeXh5+97vf4ZxzzkF2drYhjWxqasILL7yAw4cPIz09HSeffDIuv/xy2GwJkYJERKSpO9bBCoWFQakzhB0xjBkzBmPGjIHL5UJ5eTlqa2vR1taGnj17on///lHn66g55ZRTcMoppxh+XiKiWEvUfJVoArVQz435lBqZUsRDJDabjaUdiIii1QX5Kp0xihRNoNbVQZ7YUAv3mmXS/kDpGRxF66YMKS1BREQRUuandEYdLG+A4TgElH0vTRdFK5pArYuTkr39d1ceMK7/lHCYBENEFENdkq8SYYARdGRII7FY12hSVyclc9UXgQEPEZHhwplC6pJ8FR0Bhtq0T7CpJ2WgJkybBXfpHKC8DHA5VZ/j1eVJyVz1RWDAQ0RkOKNyVPQGTmJDLTyr7gF+LZfu6F8Iy40LfcfqCTB80z4AgAMdx/vzu60M1Nylczr6rPEcrecGY0T+kbf/Vr9gjrof5vAQERnNoCkUz6p75Lk3q5ZoH1e+RxpZcTmB8j2KPBUd2/OrtTk9Q36f8naw53tFOZpiRP6RkJEF29zlKHhyM2xzlzNhuZsyLOBpa2vDgQMH4PF4jDolEVFiMioR2Tti41VeBrGhLvRxfseKDbXwLPhr6KAh2jYrj7fZgeLhYY2miA21cJfOgXvedXCXzpH6yvwbMkhEU1pvvvkmjh496isZsXfvXixduhSNjY3Iy8vDokWLDKtuSkSUaDovR0WEZ00JLDPnyqZ5oPaHpsspjQjZbEBLs/yx8jK4510nnyK6cBpw/0JA9ACCADQeASp/lT+vsUGzZWp9DnckRW0qkPk3ZJSIAp7//ve/OPPMM323//3vfyM9PR0XX3wx3njjDWzcuBHXX3+9YY0kIooXYn0tDq28E66qSs0Le7iJyJ4D5RBL7gBaWwAIwIAiWG6+G+hfKE1V+auphmf+jPZjIQUDgqB+YuVzvVxO6Xn++UWP3CMFOwAgioHBDhA02DAk+VplNMcybwV3XSZDRBTwOBwO9OvXDwDQ3NyMXbt24eabb8bJJ5+M9PR0vPTSS4Y2kojICEYkwLrXlMBt8KZ54rI5HQEMRGD/XnjWlEC46m8QF9/UEYgAQNNRv2O9T9GRo+MjQJbT4w0y2lqDP81mj8lqKu66TEaJKIfH6XTCarUCAH788UeIoojjjjsOANC7d2/U1dUZ1kAiIqMES4BVzR9R0xk5JcoApv284pMr5cGOPaljyXekChU75HtHbULlXxYWd3qyr2XmPKB4OJDbJ+z8H6JQIhrhyc3Nxffff4+RI0dix44dKCwsRFpaGgCgoaHB9/9ERHElSLCieym5chSisSEwH0aD9giTYtTF+zrKZd7OwILNYUlJheXGhR1TROkZgMsltV+TABQWd0nwwdEc6kwRjfCcdtpp2LBhA+bMmYO3334bp512mu+xn376CX379jWsgUREhgm2EinEyI13BAg1DgipaUCv3kBKqpQQrHPJtOYIU0am/ECrTX+AkZSs77jkFAhzV/iCCmvJ41JCc/keeQCnlJML64L7uZSbEl5EIzwXXXQRrFYrdu/ejZNOOgnnnXee77H9+/fj5JNPNqyBRETR8o2s1DikICUtHcjJlQcVIVYD+Y8AiQDgVJlaal8KrhkcaAVVzU3y+61WfQFGSirQOx/Yvy/4cTm9gZxciI8sgdu7l05jg77puKbG0MdEqDOKmhJpiSjgEQQBF154oepjc+bMiaY9RBQELxDh8b1f/uUOAKB/YfjlDpTBgVoujcsZPIk5PUMeVB2pl6aTlAnDQROI/aa/WpqBCpXVVEpHjwA11dL/BxvNUZOWruuwSL6bXV01nbq3qEpLNDU14ccff8SRI0cwZswYpKfr+8EgosjwAhEe2fvlL5JyB8oRIC3lZXDPuUZaTZWaJo3etI8oBYyWtLaoJywDcC+9VeMFFLk+epKYXa7gj9ts2sfkSHuqyQIa/1Gi9uAmou8mNxWkLhRxwLN+/Xps3rwZbW1SEl1JSQnS09Pxj3/8A6NGjdIcASKiKCTYBSLmI1I6yx3oaadvBEg5WqTkckpTZ0DHhn8tzR0jLHpp7aETCa19ejoOUH9OcgpQ45Byl1yujjb5B37eDQIj+W4atKlgzL9nlBAiSlr+v//7P6xfvx5nnHEG5s6dK3vshBNOwBdffGFI44hIwaiSBV3EiDpIUdFZ7kBPO4WMLFhmzgUGFHZigztJ/8KO5d6FQ6RAJpTklI5Arex79fIVXt4dn/3p+G4atQw95t8zSggRjfBs2bIFEydOxLRp0wJqZ/Xt2xcVFRWGNI6I5DqvZEEnifGIlO5yB8p2aSQf+4p0JoKkZGnqqT05278vYkOd7H2Rjd4AgGABUtICS1Jo8U1rhffdNGwZeoKNfFJsRBTwVFVVYfTo0aqPpaamoqmpSfUxIopOwu1TEuM6SLrfL2U7XU54FsyAZeljkOpXtU+XOKoCn5uUHHqX4hiwlDyuGtypTf8AgGfBjI4AR/QALYrf4/0LpVwfbw6P2w1U7Jcea8//idl3k/W2SIeIprTS0tJQX1+v+lhVVRUyMjKiahQRmUO87ZyrtZuyZeY8abrLX0szPHOmyyuNKxOGbXYd+TGxoTWtozb9I2RkdSQie6Wlyz+7Gxf69u+xLrgfSE6W8pVcTqB8T0ynkeLte0bxKaIRnmOPPRabN2/G2LFjkZSUBEBaqu52u7F161bN0R8i6l5iOSKlNpKhtZJIyMiSSi4oV3R5L+ha+hcCv/zUWV2Ijt+0juy90Jr+UY6S5OTKqrJLVdr9psbiaBop4UY+KSYiGuG59NJL4XA4cMstt+CZZ54BIOX1zJ8/H5WVlZgyZYqhjSQiCpdqIqvKRdp/B2UIOn4lpqTCmt9PSv4FQtegipUj9b4RLNl7oQzg2qd/vKMk1vx+vlGSoMnACZZATxRRwJOfn48lS5agX79++L//+z8AwPvvv4+ePXti8eLFyM3NNbSRRERhUxuBULlI+y7qNdVS7kpKqvr5vCuc+vaHx9kmJfnGcwJza0tHgKJ8L2z2gOkfISMLtrnLUfDkZtjmLpdGcoKM4nAaiRJNxPvw9O/fHwsWLIDT6cSRI0eQnp7um94iIupqyimsgF2NHYeAI/XAgCJpM0DvNFfJ7fITpWdIozb+ichJybCWPC6NBJV9r8zkiV9a01WFxZElc/sFjJ0xjcT9dKgzRTTC489utyMnJ4fBDhHFlHL6BYA0AuG/qV5rC1BdKSXeenN31KZm3Ipdh9ta4f7HzUBVZSf2QCflHjrJKdoFRBXTVeGOxnT1KA7306HOFNEIz/r160MewzweIupSyumXxgZpVOb6SfLFVYpSDmr7x3hu/Uvg+ffvNb7NkVCWohhQJOUfKXdyLhwim64KNRrjOVAOcdkc7G9rA5KSIMxdAUu/gV2bDBxHidBkPhEFPC+//HLIYxjwEJERdE9zaE6/+BXbBABR9G0qKKuifvQIUHsYnjnTEbD8PJ7V1Uj1rvwDnuLhYQcq4rI5slIY4rLbgVUvGdjQ9tcJ9nlyPx3qRBEFPC+9FPhD0NjYiE8//RRvvPFGQLkJIqJI6S1KKRupSc8AXC6piKcYuIrKu6mgZnHRRBJkl+OwcmLCqtgeuWCfZ8LtJE4JJapq6f7S09Nx5plnoqGhAU899RRuv/320E8iIgrFW4jTa+9uuEvnBFy8/adtvMnFmlqa4bntKiAr2/j2diVBAGocgXvktAurgnlSsryUhFZeULSCTFtxPx3qTIYFPF7FxcXYtGmTYedzu914+eWX8cEHH6Curg7Z2dmYMGECLrroIlgsUedcE1G8a2qU3/Z4pITW+dcDfQcAjQ2Boxd6cj9ED1B72PDmdh0BEEVpKqumWj2YUb4PGsEiAAhzV0jTWG1tgN0O9O4rjZA1HZV2XVapyRURTltRjBgeMZSXlyMlRUclXp02b96MrVu3Yvr06XjggQcwbdo0vPrqq9iyZYthr0FEcSwtXf3+1hZpHxw9m+J1B+3BjHezQQCB74M3WFRZ/WTpNxC2R9ah4Jk3pJGj/Xul0TW/iulGrJri/j0UKxGN8Lz33nsB9zmdTvzyyy949913cdppp0XdMK8ff/wRY8eOxQknnAAAyMvLw/bt2/HTT9rbuTudTjidHbuJCoKA1NRUCIIAIU7r3kTK2x+z9Qtg32JNrK+F2y+fwjprvq6/7o3sm1hfGzjCo6X9gm+dNR/WWfPhXn2vdMFubIjL4p7RUyRWe4OZBTOkHKasHFj+PBueZx8F9u6W7whdV6P6+QiCAMe9d2hXSdd4XjiEzGxY5i6P6hwRvW4C/MxFqjv0zQgRBTyrV69Wvd9ut+O0007Dn//856ga5e83v/kNtm7dioMHD6KgoADl5eXYvXs3/vIXlWWj7TZt2iRbOl9UVITS0lJT7wCdn58f6yZ0GvYtNg6tvBNuv/wP6xP3oc+KJ3U/34i+HVp5J9xaF1+l9gu+r50PPQsA+HXK6Ym05ip6Lc3SP8chiMvuQMETr8Cx9Ha07frad0hSXj769O2r+vSDypwpP8Gelyji+WcuWmbumxEEURTD/l1QXV0dcJ/dbkdWVpYRbZIRRREvvPACNm/eDIvFAo/Hg8suuwyTJ0/WfI7WCI/D4ZDdbwaCICA/Px+VlZWI4KOMa+xbbLnmXivPtcjtA9uyJ0KO/BjZt4A2WCxSMm1aurSMXLknTXs7rfNWSG2sqQ5MevZnswEWa2xHgIT2XJxI2ezBC5wWD+8Y8QoyWifW18KzpgTizz8Bzja/9lmAnF5Adq7uUb54lAg/c5Eyc9/sdrthgxURjfD07t3bkBfX46OPPsIHH3yAv/3tbxgwYADKy8uxdu1aX/KyGrvdDrvdHnC/KIqm+zJ4sW+JKa77ppJcKoqiFEj4jfy4V9+rurLGU1cjC4zCSXiVLaf2N3hYx0qsedepBzyNDXDPmR48CPDqmSmVm4glizVwZ+dwWK3y/1e+J3U1QM/MgM9I+b1zP7gI2L9P3q7BQwM+NyO+r7EsIRHXP3NRMmPfjOxP3C9zeu655zBp0iSMGzcOAwcOxOmnn44LLrgAr7zySqybRmRqvuTSnFypoGaNo6OquD+NFVG+wCiCMgG+5dTeoMVmD0xwVSbk2uxSmYWWZn3BDgDU1QIut+52dYpQwU5KqpTga9P4+7S1Reqvywnk9Q2s+K43gXt/ufy2x9NRfsNgLCFBsaB7hGf27Nm6k4cEQcCqVasibpS/1tbWgOXnFovFdFEsUbzx7oniC3K8q3WU1cS1LqjRlAlQHtt+0fWU3O4bEfBtUlfjkBKb09KDT1+pUdmU0HCCBbBaAFcEozjevXH05jH5j9AAQEpqGKuglL9To/sdG3QUhyUkKAZ0BzwjRoyISQb4iSeeiI0bNyI3Nxf9+/dHeXk5XnvtNZxxxhld3haibkl5MUpLB/oXht4NN4L9Vrz1nAIu8O17zQCQbaBnnVMK99Jbpcf0BgVdTows2AGizy1Kz9A/QpOcIp8OUxYpDVPQTQ+5Fw/FQFgjPLFwzTXX4KWXXsITTzyB+vp65OTk4JxzzmGtrm7CXXsYrmV3xGSun9opL045ubp2w+1YGl4tbV7XPiUW7DNUDXbU1NV0jCCU79E+bsDg2Bf9DDUabWvPN9Q7DReOMAIJYd59HRsPthcPjUqQURyWkKBYiGiVVqKqrq425Sqtvn37oqKiwnTTfIIgwLLyTtly2kiKIsajRPrcxIa6gItTsKBT2beAMg9BPkP3jAvl+8VoKRwCVOxXT1oGgKxeQEsT4HRGlxDcBSz3PwPPqiXBA7dIDBgMy813h/UHgp7vpd6E43A+966QSD9z4TJz3+x2u2ELpaIqLdHU1ISDBw+ira0t4LERI0ZEc2oiAIBbZ4IsdZ6o6xspPzNv8nP7BVOYNgvic6ul23p+V3unWrSCHQCor4luqXcX8qwp0ReUCRagYABw4Gd9J66u6NyEY0B/MVeO4lAciCjgcbvdePzxx/Hee+/Bo/HXmFpFdaJwWXNy4a480HEH5/oThmy/Hn9NjbJ8HHFZkJ191XhLSgR98cQIdgBIwYMtcBuNAKIHqDwgBT7+ydZa+/B01t5COhOOgwXKsVyWTt1XRAHP66+/js8//xwzZ87Eo48+iunTp8NqteKdd95BU1MTrr76aqPbSd1U7oIVOLjoJv6VqEMsLiKqr5kpVSCX7dcDSBfmwuKOFV9epiz7EC6dAZp3JCglVUoe965OUyuf0VnVzg1IOA6rijuRQSIKeN5//31MnjwZp556Kh599FEUFxdj8ODBOOuss7B06VJ89913GD16tNFtpW7ImpUD29zlppuX7gzRXETUAhdADBpAiQ218Cz4a0fw0v6avjpJan/51zgCK5Tbk4JPT3UHFiuAMHKN2mtlyVanJSVLCccQpf/v3VfanNHg4NeQqSouS6cYiCjgOXToEAoLC33L1P0Tgc855xw89dRTuOKKK4xpIRHpE8VFRC1YAhA0gPKsWRY4FVVeBtfca3EoL1+6KPuPBLicHVNZXskpQK884OAvuttqGimpHYHL3h/De25WjupeRdaSxwEoEoYNGkEJCIrnrYg8iOKydIqBiAKelJQUuFwuCIKA9PR0VFdXY9iwYQCApKQkNDbqrG5MRPILSXqGdGdjQ/h/mUdzEdETLOk5xuUEHIfQ5jgEFA2Rdmquq5H+qeWZuF1AVUXg/YVDgJ/L1HNxoq091WUEqE5V5fT2baSIxgbpvx6V3Z41V6IJ0r4+yoAyK6fju7R3t/wpOoJfsaEW7jXLcLCxAe70jIDvnpHTUExopliIKOApKChAVVUVAGDo0KF4/fXXMXz4cNhsNmzevBkFBQWGNpKoK3V1LozyQuIT5kVFz0VEs2/KYKmxIXCzPL8ASmyo7bhYa9lfLj0nK0e6OKslGrtc6iUTfvlJPaix2aVND4MtSY8bGkFZUyPQUNcRAPq/7142G6wL7pe2BFi1BPi1vP3zEKV/5XukgMgbULZ/lh5l3pRXeoZsZZzad9r7PZRCrwOB3z0Dp6GiXvlHFIGIAp5TTjkFBw8eBABMnToVixYtwqxZs6QT2my49dZbjWshURfr8oTKYBeOEBeVcIMzrb7JgqXGBvlUVXuysX8ApTqdpdQ+2gPHIenirFxd5GW1BQZXWnvxuJzG71cTDYtFvnGgnj2E1N43m+I96F8EoD0wWHA/gPZiqYqg1DuF5aP8vlgswOBh8pVtjkPwrFriO6/mc5W3OQ1FCS6igOd3v/ud7/+LioqwcuVK7NixA4IgYNSoURzhocTW1QmVyguJ8rEgwg7ONPvWPhrhcQMtipGTrJzAcwZ7TywWKYhx+u3P1dgAYdHDEO++EQEjHz16Am535+w03NkEi9Rf74qpSMtb9C+Sgh6/vYl8Ncy8K7GaFKkCat8N5Xepvbq8e+bF8uPKywITmkMENJyGokQX1caDXrm5uTjvvPOMOBVR7HXxX7LCtFnSXjRtrdKKpby+QHOTvouKRgCje+qqvW+ywElJz4XV3+BhSEpKku+QnZUDS7+BEO9/Gp7518unozKypIrsWq8fz9wu6V/EdbwEoPg3ASNzAbsUe8/vl+is9t3QH5SIvtE35Sif1S+HR9ZSTkNRgoso4Jk7dy7OOOMMjBs3Dunp6Ua3iSimuvovWfG51R0XtNYWIDkF1rseUj9WEcioJa4COqeu/PumVWXcZvcdE5BcPaAIqPhVyrWx2aSRmpxcWGfNR26fPqr7JwkZWUDfAfJpqYr9wDV/B37a3TXVy2NCCmzgqALq/Jbla9Ul0xpBS88InMbyfxWtoKR/ofZUYPtrCRlZsM1dbtoSBUQRBTwWiwX/+te/8Mwzz+B//ud/cMYZZ2DUqFExqaZOZLQu/0s2jCm0gARnlcTVYOfU7JtyusSrsNg38hDw2impHdNQbpe0espikQqGLn5Idf8ksaFWSsD119oC/G9p/AU7ySmRT7VZbR0rr5JTIMxdAUu/ge2Jw/KAR5XWCFqEo42WGxdq52gxF4e6iYgCnnvvvRcHDx7Ef//7X3zwwQf4+OOPkZOTg/Hjx2PChAnIz883up1EnSqmW92rTDNptkcZyKglrmqcE2jv56p7OoKO/oWw3LhQyhFRTsukpEKYNrvjtvK1lTv7tjRL/xyH4Fh6O3DLPb6HfK+rNcoQb8EOIAV0vfOB8rKwgx7r/25UvV82fSkCcFSpVpD3jcT55/Dk5GqONmp9X9T2zgHAXBzqlqKulu7xePDVV19h27Zt+Pzzz+FyufCb3/wGixcvNqqNhmG19MRiZOXmULq6srN/3zz1tQEXoIDlxe3t0dtOrQrnAc9vPwcA9RyaAUXSSEddDXCkXv9S8OxeUh6SNy9JFLtXCYni4arfRdX3v/147+cYyXda63sR7ve6u/8+SVRm7lvcVEsHpOmtE044ASeccAJ++OEHPPTQQ/jhhx+MaBtRSIYtIe+klVl6Ll6q00zK1/euqknPkKax/DYmVCNkZMEyc67vtT1rSqRjNTYUtMxbIe33ohyB2V8O3XWe/PmXjwgVJGkVv0xkZd+rfxe1vlf+FeT9p5y85Tr8PkvV75EyB8t7myUciHyiDniam5vx4YcfYtu2bdizZw+SkpIwbtw4I9pGFJpRv9A7aWVWxAGZsj3+e9oUDw+auOp7bf8ppPa9VwKSnNtfS8jIUt8AMJJgJxxJyVLuj9kCHkB9xZzWZo21hwPLbnjVOFRrlsm+R8ocLO9t7p1D5BNxwLNz5068++67+PTTT9HW1obi4mJce+21GDduHNLS0oxsI5E2g36hd9rKrAgDMll7lGUZQi0991ImB/9aLq3W8Zecop3oDABJSe0FKTtR3O+YHCGtJf/KSudHjwR/D9T291GO6ChzsNKk1bPcO4eoQ0QBz+zZs+FwOJCZmYlzzz0XZ5xxBvr3729024hCMuoXeqetzIowIPNvT0AeRoil50EpRxh6ZnYEScq2JqdEl3ejp+ZVZ+T1DCiS8ofSM4Bf9wXu4twVUlKlzQOX3ho4TahYWh6wKSAgvfc9M9srojsCAx7liE5OrnyEqH31F/fOIeoQUcBTWFiIq6++GieccAIsFovRbSLSLd5/oRsRkGmeI9TokXLvFatVvVyAxuvA5VJfVWWx6CuhEKvkSbdbvWp4V0rPkPZXUnv/Ghtkuxyr6jugo6RE6ZzA6a40+f5nHMkhCi3qVVqJhKu0Ekss+9bZy9SN6FuoFTiylVpB6mNp9SugdhMgTceIYtxPQ1kffxVA+3swZ3rX5wh590bS2o3aa0CRtGePMjDK6Q1r6ZMA2vuwYIb88+uEVYTe77xVo1p6ouPvysRk5CotBjwJzsxf9Gj7Fk3QEsky9XBeL1jf9J5Ha+m5an+UwYvN7qtkLo0GiAH78wCQX4gFi1QP656bYzNNFI7kFCCvAGg+Gt5y+mgIFmk5fvt+OZqVy+VPguX+p+G57Sr5XkQpqbCuesl3M5zPWknv96mrt2boavxdmZjialk6UbwKyHFZMENWhyjoBSOCZONQOTXKC497cWD5CLGhNnBFjl+7hWmzpKkSnRc+2Wv681v15VlTIt3nH9yU7wnYxVl67UfjP9gBpABn/96ufU3RAzQ1wrLg/vZtAebB8+CijqX9SckqOUui9P5nZst3YFZMWUUzdas714tL2MnkGPCQeSl/YfvtBBwywTeSZOMQFwzlhUe5G7HvGGWCql+7xWV3yIOhVUtkVbYDdthV7hJss0v/VVn1FeDXclhKn/QFVJr5MMkpUmV0/7wePQnLZtTSDM+qJbC2Bz1IToFvab9WgnbZ99JUoT+tkhOR0BvIcAk7mRwzjsm8gv3CDvHXq2XmPGl0I7ePb9fcsF9PeVvxmm61gp2h/qpWXjR/LZcumI5Dvs3uAL/gSpm7orbfTvvUVgCXE5450+EunQOxoS6wbRaL9N7c+09g4DHyxxIh2LEnBX88OSWy8+7fB3fpHGkaUauUhlJaevjfNx08B8qBw4qEZ42fC+933prfz9A2EMULjvBQwtPKUZCtXAmzYGIkUwghV8oo/oK25uQiYK2T8q9swSLP7QgIJBS3vUGJ5o6+iotfSqqvnao7LbucUiCltmnhwGMSN8cjPTNwib7NJk3j+abwZktTeOHW0nK7w18ZlpOrujN2tEnD4rI58u+PYAm6OzerpZOZMeChhKeVo+AftKglfRotVJCkDIhyF6xAVXNr0GOEabMhLru9I1hTXoSsNnlOTXvhUc0dfZXSM3wXVeuC+9X3jQGk+5QjHj+XwT3jQik3JdLRkFg5egSBu0gLss/Pc6BcGkELd4WXzRo8zym3D5CaBlRVSFOBSckQps02rkyKP+WIoICEX3kV00K/lNB0BzyzZ8+GIAi6T/zII49E1CCisOnIUdA7YtOZv0z92yAIAqxZOUBzRch2utMzAvN6vHr0lJY2KwuPah2vVFejWq1blXKlkyhK/7w5RolErTq7yyltAtheQV5cNid0vyyWjum8hlqg6WjwAMlmh7XkcSkfyvt+tjRLI0lRJA1rfm+TkuV9SErWfc541SmBIXULugOeESNGyAKenTt3oq6uDsOGDUNmZibq6+uxe/duZGdnY+TIkZ3SWNKvW/0VZGCyZWf8MlX9LDKz9Z9A2T9/Obn6C1TKCABE+ZSVzQb88pP+dsWr5BRY7v2n7PsuNtTCM+fa0KM1LidQvkd6P/TsAj14WMdUVEN9YIK41SoPFL3L/dWCmyi+x5qjnHNXSCOEba3SSNLcFbrPGbe4mowiFNYIj9f777+P3bt34+GHH0Zubsdqgurqatxzzz0YMWKEsa2ksHWnv4IM3WW2E36Zqn0WlrnLAQBifS3cIfZXscycF7jxnN/GgQGCBUiAtIIqW1GKQG9ybSJwu+ApuV32fnrWLAtvaurXcimpOdT+PRdeGVgry48w776O0Rv/76byM0rPkKbBvKvo+hdqfo/VAmit762l30DAbz8fU+BqMopQRKu0XnnlFVxyySWyYAcAevfujSlTpmDz5s2GNI6i0I3+CvJOA1lLHvfl7kRMZWWVb5VSpJTvfXmZ73xu7+Z0ilVWcqJqToh/cCQ21HasDHK5pOTb3D6QRnLkhEWrjF32HG9crsD3U/kZWG1S4BdMXoH8dpLKqq5H/qH9s+VyQnzuUdXvpnIVIAAp6HQ5pX82m+b32Bdg+fcx1ApBE4loBSURIkxaPnTokGZF9B49eqCqqiqqRinNnj0b1dXVAfefe+65uPbaaw19LdPgX0ER8Y0WeVfmeKd8ohkhU34WLifcq+8FHno2ZGDq24hQOTrhcvr2ewEg7ZLsHaVxHAIKh8Aybzk8t16laIwA8ZH2FVfJKXFfIiJqZT/Aff2kwGRvux1oCZJY3L8wMPFbLRG5rTX4iJpGMCRPqG+fbtPxPNXH6mpgmbei29TSivf6eRS/Igp4evfujf/+97844YQTAh575513DNsG2qukpAQev03NfvnlF9xzzz347W9/a+jrmAmLCUbG+8s0oBRDFCNklpnzAus5lZfh4PRJgRdVRWCquhGh7xx7fAnHvpIQfuf3rLoHgSuRRN8Oy77pE1MTA9+ClFRp3xvl+5qULN88UbkMX22VdlJyVNsfAFCfbmt/nur0lcofMwwCiEKLKOC58MILsWbNGsybNw/jxo1DVlYW6urq8OGHH2Lv3r3461//amgjMzIyZLdfeeUV9OnTRzNXyOl0ympmCYKA1NRUCIIQ1kqzRODtj7JfQma2L08kUWn1rUuoXVQibIeQmQ2PsnK5ywl35QHp/1NSfaUjrLPmy18nVKAVbBpMGQQpJUKJiM7Q/l4H7EnkbOsYCfItw29P7gaklV1JyUBbm3Rfcgos81bA4vez5jnws5Q/5E0SnnyltCKrPWCxzpofOFWl/Ixtdt/3wK2WizdrvjRC6H9Og35GYvoz18nYt8RkZJ8iLh66bds2vPjii6itrfXdl5WVhUsvvRRnnnmmYQ1UcrlcmDFjBi644AJcdNFFqsesW7cO69ev990uKipCaSn/+iH93HU1cCy9He4aB6w5uchdsEJaRh7ueWoPw3HvHWjbswvQKlxrsSKp+DcQIcLTUC97vUO3T0fbrq+DvoY1vx8saelw7t0ddvu6IyE1DUJ6BjyOQ2HvCG3N74eCJ9VzFN21h1Fx3WSIzU2y1/K/nTRiNPqseFL2POVn7H/MwemTOgLjEK9PRMFFVS1dFEUcPHgQR44cQc+ePVFQUNDpEeZHH32Ehx9+GKtXr0ZOjvoFSGuEx+FwmLJaen5+PiorK023M6oZ+uZadkf4u+4CQPFw2OYuh9hQ1/HXfEOd+lLplFRpdMLtjrq9piQIAAQgKxtobopuz6Di4bDPW6H6vVT9rC0W+TRZTm8pYdxvdAZA4IhN+yhQwDnbvxedxQw/c1rYt8Rkt9sDFkhFKqqdlgVBQL9+/QxpiF7vvvsujj/+eM1gB5DeILs9MD9BFEXTfRm82LfYCbrnUaS5P3U18NTXyM4rq6btJQiJt+lfV8rqBeuKp3w33fOuC1zen5UTmHujpr0Mh/e7GPC9VPuslRv/NTV2TKU5DsG9+l5pFZci/8Z7XrVcvK74WYj3n7losG+Jxcj+RFw89MCBA3jwwQdx/fXX4/LLL8fevXsBAC+//DJ27txpWAP9VVdX45tvvsFZZ53VKeen7sl/SXckS9BVlwl7Rbo6LitHSjr2O69qzo3JfrkZTrmtgPLzcLmkYCInT8rZsVikEbNMlc/NrwyHKuW5U1KBGxZK//WeN0WxujVEQGzolgtE3VxEAU95eTnmzZuH77//HiNGjJCtoGppacHWrVsNa6C/d999F5mZmaqrw4giFTRg8aMZGKksE/YeixqHVAA0GItF2jfHu3eOd2+RUEnHkdA15axxTEImRIq+naR9n0dKqrQPj/dxlxM4+LO0RN/jaS+V0RR4qhDBa8D+MEsfA155Vjqf1nm5XQRRl4loSuvf//43Bg0ahDvvvBM2mw0ff/yx77Hi4mJ88sknhjXQy+PxYNu2bRg/fjysVqvh56duTOcmjZq7V6vsmutZ8Ff5VIay6rm/wcOiX1Jss+lbdZWUHHrvnYxMKV9IKZFHk5RFQG0hfvW1tgADBgMV+6XbQXY+9lJdGq78LqWlS3v8cLsIoi4XUcCze/du3HjjjUhOTpaN7gBAZmYm6urqjGibzLfffguHw4EzzjjD8HNTN6cSsPgvJfbl5Ght36/Is4DLFZgPkt1LSlatcUiVul1O6a9+q1RZW2yog5CRJc8HsoQxAJvVS18eip6NBqPZVTpRuHQkeCcnw7pmQ3Svo/xuqdU+I6IuEVHAI4oibBp/IR09elQ1YThao0ePxrp16ww/L5mX3gKqqgGLnpGc9ukI5V/27nnXBTZGcaFzl87pyMsp3+N7jWB1mYLKypGCKDMnMNtsgNujPVIGBI6ktdcc877PHXSMVtVUd0yDNTVKozM5udLKqr59dTWZG4ASxY+IAp5Bgwbh008/xZgxYwIe++qrrzB48OCoG0aJLR6qtestoBoQsMyZLj+gxgEgjIuXMjASLBCmzZYfozWNFsmqruQUCNNmS1WxTas9f0g12BGA3Dxpc8hpswOKdUojZ3WBBVhlpxACp+yajvo+ewDSc2uqO8qC6Gk1d0AmihsRJS2ff/75+O9//4u1a9eivLwcAOBwOPDqq6/i3XffxXnnnWdkGykB6U0E7lSRFlBtalS97b14Wea176pbcrvqqi5h2ix5orLokS7C/rSKPUaSxNraIp0/LT385yYM9QKqAIDkZFjmrZA+m34DNVY1BXk+IL3vhUOkESGbXfp/rffTxIV4icwsohGeU045BZWVlXj55Zfx5ptvAgDuv/9+WK1WTJ06FWPHjjW0kZSAYlytXWyoDVmnSpOyzpLL6cuxAUKPHInPrQ4ciVD03zprPqxP3Ie2qkrZaJFq3S09ysvCOz7eZfWSVjSlpLW/d0GmoFpbQhZ3Va1X5a9XXsDzpemswKLFXFlFlJgi3njwoosuwvjx4/H111+jrq4OGRkZGD16tOGFQylBxbhae0DRzfZN47TIpuCUIzwuFzyrlkj/r1ztAwDlZVLejjdwUQvulP33nz7xnr+xQTpOWXfLK9hKr3ADpHjX2NCRe6PccNFb9NS/z6ECarXHk1OAHj2BnFzV74ZvClMth4eIEk5EAc+uXbswePBg9OrVK6BuVktLC/bu3atZ2JO6h5gnayovcCE2jQuZLPzrPu0pEZfTV4Hcs6YkMNiDIFuJBQDuNSVw+40S+TgOSdMpxcOBvbvlZQn86liansspfR7Kiu5Wm7SyTbnaTCOglgWySq0twIAizZEhrfwbMxZoJOoOIsrhWbx4MX799VfVxw4ePIjFixdH1ShKfOHuEBvtbsfK8wRc4NoviLo3D1RuvqdnGXP7eXwb0Pku1qK0EmvBjCCv5+fXcikPKClZ0Tl9TTAV5ciVxxMY7KSkBiaFew/3BrJaI2DMxyHqNiIuLaHF5XLBEs7+IUQwLsk58AInAIVDfCNMmq+jHCGwWYPf1pKV0zEyoDxnSzM8Dy5SD8j8uZwQS24PXFEkekLv2hyp5JTQm/HFjCAFjwOK1Kf0WpoDk8K9lO+zcsSI+ThE3Ybu33BNTU1oaurYFr2urg4Oh0N2TFtbG9577z1kZWUZ1kDqJoxKcg54ngjYbNrFPINtHuifR9O/SPqvt9yDclrFZpdyb1wuXz4P0jMUU1sA9pdDNlTjfd7+fYDbb8pMa4PAYHvQRMpiBVpbpXYJgrThoX/ldc29bLpKe/mH6krtQ7S+L8rpxf6FUmDHfXGIuh3dAc/rr7+O9evX+26vWLFC89jJkydH1yrqfoxKcg7In4H8Yqhz80CxoU5KJPYGOG63FORk5Wju9+JZUyJbvYXkFJUGKualsnJgXXA/3DdeKg94upLHL7gRRWnaqHCItONyYwPQ1ir1KykZsCcBzjZjXrdwiBR8lJfpS7pua9V+TOP7opZL1tn7QcXDHlREFEgQddZe//HHH7F7926Iooh///vf+P3vf4/c3FzZMXa7HQMHDozbhOXq6mo4neZazSIIAvr27YuKigro/CjjkthQF3BhsmRmh9031Q3miof7ghm11/G/GMkuVkFLNQjSxbrvACkQaqhr36QuzM+gcIj0X10jJ3GQtZycIk0teVcuhbuzs8UqldhQbgro/UzSM4D9e+UjTD4q/W8ffdITVIQTiAQ7NtTPnG8XbS+/71+8M8vvEzXsW2Ky2+2Grf7WPcIzdOhQDB06FADQ2tqKs846Czk5nP8mYxi1I62QkQXL0sc0V4iFeh39pR3ap1n27428sfYk6b+6p4ni4BdZa4sU7OTkwrLgfnhu/QvCaldSEqwlj7cHFOqBZ0DA4BM4FWi5caHu0ZNg+ycpAxzZ9F2QXbpVxXgPKiJSF1GW4iWXXGJ0O8ggZhxOF+tr4Q5jWiKq4Mmwi1Po0ZikIcPRVnnAoNfrQjXVQE21NJJWMBA4+LP+57a1SgGNMqBYtaQjtyY1TRpJcrZJ02jOtoCcooiKegYJRJTBUEByczjfixjvQUVE6iJa8vH000/j4YcfVn3s4YcfxrPP6qszQ8aLi5IOBhHra3Ho9ulSbauu6lOwi1NySuCFUEthcfBjbXbkLlgh1WtKVC3NQFKSvCRDqD1qPB7ps/TmRnmVl3V8xvv3SSNJHo/0Gm59idohtzbQKucBhA5owghafNsS5PYBioczMZooTkQU8Hz22WcYNWqU6mOjR4/GZ599FlWjKAoxGE43ag8dJfeaErTt+lpzZ2MjX8tLdrFSBiw9M2EpfRIBe/QoqSYrKxQWw5qVA/RI8PpXjQ2wLrgf1jUbYCl9Qn9AGCDIaJhyS4D+haqHhQr2gwYiyoCmf2HQoMVdexiuZXeofg/D3YOKiLpGRFNaNTU1yMvLU32sd+/eOHz4sOpj1AViMJyutyp52LSCNcXOxkYmhPpPh7mX3irPrzlSD8/SWxEyZ8UdYvl2cgpQ48Ch26cDGVnAYZV6TYkiK0eaRl11T3s9L535PP7Lw+tqgq/S6l+kbyl5iGA/2FRnuKu5HPfe0TnfeSLqNBEFPCkpKQF78Hg5HA7Y7ZH+lUfRiklJh84aVVIGbxHUUDI0p6m1RXt/HH/BdmW22nznaaup7igjoStROg5VVcKz4K/6V2uprKpSTVJOTgF6ZipWc0mfpafkdvXPMopgX2/el9hQC/eaZcDeH+UPMDGZKO5FFPAMGTIEr732Gk455RTY/HZndblceP311zFs2DDDGkjhMWq1U1g6aVRJraK4bK8bHa8VzuiTb6TCl18S6aqoIM9T7rXT2ADhhgUQ7/5bhK8VJkGQFy6NVkOt/mNTUmFZ+hgAURaUC9NmQ7zn7/JAtmcmrCWPy54e6rPsimBfcxUfE5OJ4l5EAc/FF1+MRYsW4dZbb8WZZ56JnJwcHD58GO+++y4cDgeuu+46o9tJcUzrQhPt6IqQkYU+K56U7S0R9kVN+Zf33t1wl85RbYtnzbKu30n4SD3Ee2/vuteL5R4d7QVcZSM6jkMQl90eWCFeLYCIYsrKMMo2WCzA4GFMTCZKABGP8Nxxxx148skn8fzzz/vu79OnD+644w4UFxcb1kCKf1oXmmhze8T6WhxaeSdclQelTe7S0qX9X4JtFqgMrJSjT+2rhFTbEotpCT1TZIkqJVU+1eUNYpTvs/eY4uHBA9l4WO6tLBcy8Ji4zd0x4xYVRNGIuFrg8ccfj1WrVqGiogINDQ3IyMhA3759jWwbJbooc3vca0rg9p8+aGmW9n9RBCvBAivfiNDe3VKw41Xj6Cji6b3AqpWloAgIsA8dDvefZsHz7KOBQYza+9xQJ+3AHERM8tMSWKctJiBKUFGXXu7bty+GDRvGYIcCBdv3RA+tAEl5f41ilVNNR0K9b/RpsCKv7OgR+RLmVUukC6i31AOFJgjAgMHy+5KSgcJieBrq4Xn2UQjTZkmfe10NPGtKIDbUSe9zSqr8eU2NIfdaiovl3o0NwW/HE+74TCSje4Rn165dGDx4MFJSUrBr166Qx8drPS3qOlH/Ra414tIeOPmG7GsU2yDUHoZ7zjXSpn7t02C48M/AI0ukApQiAqeSyvfAM2e69P/JKcGnmmz24MuoBUvnVDWPFxYLkJQMYe4KiGsfkj8mCED5Hkjr1A5AXHZHx5SV3yiDsvwHahzy6a94vTiHmFaLq2mkeJgCJIojuouHXnrppVi6dCmKi4tx6aWXhjz+pZdeirpxRmPx0PDE/Jf3kXppldahg8DRwBwe7ZpLKpT5JHqOd7nUAxurLXhlc6NXQsWTnN6wlj7pu+med538omqxyKcOlbdz+8hWX/m+Y8qK6XFacNNb6NTa2AB3ekbAz0Q8FQ4NVShXjZmLULJviSkmxUMXLVqE/v37+/6fzK8zcgDCCaL8V2l56mt8z/OsKZFGi2rU94JS1dYaXkNT04DMnMDN9ASLVB09WMCTnBJ+FfFYUwvSkpKl//q/d+15NrLPUfkc/77bk+SjZekZssMDlnn77dMTj4SMLNjmLte+uMTRNFJMtqggimO6Ax7/KSpOV5lH0ACkE355BwRRc6YHbESn63lrSqS8D72UF2IZlUKfdbVArcqO4aInSPAkSMFQTm/gcJVU9NJmDz/Y6mqCBcjuJc+Fstmlgp3+gV1yii8Q0QpUhGmzIT73qG8EBK2twSvKK79TWTm6NwA0evTRkHNyGokobkWdtEyJLWj9oWiTjtUoL3Aup/S6C2YEr4+lFnyl6ahDZbEAxcMhzF0hLXtWpTIEHG4OzoAi6TxuF3Dwl47il22tHbspW+L0x63/IKnEhT+XM3AUy+3WDoY9UtaO0DMDtrnLUfDkZtjmLgeaFcVRlUm+EX7HOqNIrhHnZOFQovile4Rn9erVuk8qCAJmzpwZUYOoiwUZxVEmHQvTZgUs5Y76L2Cvlmbpn2LqzFukUW0kAEDgCi0ljwiUl0Fc+xAsNy6URpSCJRxHwmIFDvys/Xg0mxkmp0gjLUa2WbBIwZc3oNm/TwrKQuU5uVwQG+qkzzzI/kaWucs77g8x4hFxYntnTB0ZcE5OIxHFL90Bz3fffSe73dTUhKamJlgsFvTs2RNHjhyBx+NBWloaevToYXhDqZMEuSApf3krd8iNJKfHd4FTJqn687vQyIo0+tu/D7hxEfDKM1JA4fIbjUhKBtraII3ciNLrlO+RXle5o68mlWkuLZ4gtbOi5XIZkwDtn6MjegBBUYG8sUHKrwmaeyTCs2AGLEsfgzBtlnwFlpciSAgV0EQcIHTG1BGno4hMTXfA8+ijj/r+v6ysDPfffz+mT5+OU045BRaLBR6PBx999BGee+453HzzzZ3RVuoEYf2FbeBfwGJDHTyrlkh1q1wuyIIL77Lz+lq07dFYhdXaAjzyD1hXvRRY1VzQCFZqHLAsuD90wCW9epg96yTBkqPDYQmRaO29uPtf8K02aXTJ/71oae6Y6lELjhRBQmeNeHTGJoTc2JDI3CLaafnZZ5/FH/7wB5x66qm++ywWC0499VTU1dXh6aefxpIlSwxrJHWesC5IBv4FLGRkATZbYNCRkgpc+GdpNClUUOJNBlbmhWjtoXP0CISMLFhmzpWKhO7fZ1xAEe/cilGo/oXS++83XSmufbijIn3/QghX/U3XKA6ALq8p1RmBFKejiMwtooBn7969mDJliupjAwcONHwPnpqaGjz33HP46quv0NbWhr59+2LmzJkYPHhw6CeTYQz/C1jtwtnSLG0QqGdZt3fZtN6SEK0tUiDlcnV9kdCYE6VgMi29I0G5rkaaxnK5IN5zizy4tNkgPrc6+CiO/3s+eBiDBSKKaxEFPKmpqfj2229x3HHHBTz27bffIjU1VeVZkWlsbMTChQsxcuRIzJ8/HxkZGTh06BDS0tIMew3Sx/C/gLUCFT3LuAWLtPIK7YHYrVfqe82y7ztGMbqblmZpZAeQ5WKp0jGKw+kfIkokEQU8p59+Ol599VW43W6ceuqpyMrKQl1dHT744AO88cYbmDhxomEN3Lx5M3r16oVZs2b57svLyzPs/N1ZLHdSFhtqpZEWmz0wh0e5Z443QPEfgejVG5Z+AwEg/DabaSfSYGUsBIuUe+2/07HevKu6GmnKy59iFIcjOkSUSCIKeC6//HLU19fjtddew2uvvSZ77LTTTsPll19uSOMA4LPPPsPo0aOxcuVK7Nq1Czk5OTj33HNx9tlnaz7H6XTKSkgIgoDU1FQIggBBEAxrWzzw9ketX2J9Ldx+f4VbZ82Xb4OvspmfzX9JcSdyr1mmSDRu3/wuIwvwuICDvwIAkgYPhWfmPLgeXSpfrZWVI+9zqPpX/mwhSkMkksxsoE5lg0SgPRBS7P2jNh0l057w7XJK/1JSpWkv7/en/T0P9t0K9p3UK9R3N5aM6F+8Yt8SU3fomyHn0ltLS83Bgwexc+dONDY2Ij09HSNHjkS/fv0MaxwA/OlPfwIAXHDBBfjtb3+LsrIyrF27Ftdffz3Gjx+v+px169Zh/fr1vttFRUUoLe1+f40eun062nZ97budNGI0+qzoqIN0cPokuCsP+G5b8/uh4MnNXdI25Wt72wdAtc3uuho4lt4Od40D1pxc5C5YAYgiHPfeAXeNA0hKhvuXffCNFGnVsxIEKeAxQU01+9AREGx22fulRkhJg9ge4NmLipFz8yLUPnKv9F5mZEGECE9DPaw5uXBVVcDjqPI919I7H/3WvhZwzlDfrWh19vmV3LWHfd8l7/fLymXpRKYS0QiPV0FBAQoKCoxqiyqPx4NjjjkGV1xxBQApeNm/fz/eeustzYBn8uTJsmk1b4TocDhMWTw0Pz8flZWVAXV9XFWVstttVZWoqKjw3XanZwA4ILvt/7iaaP7ylj1XubKqvX1K7hqHr2/i9FvhXlMCd1UlDi66KXjysVYcL4pdF+xYrNHt0ZOUrJ7P1J5L4/lre97M6nul97RnBnBwf8BIl+gdrQHg/HEXDq28G9aZ86T3ssYhfY53LIMnIwueG6bKnus5Uq/6nQj23Qr2ndQr1HfXaK5lHfs9uSsP4OCimzRHO43oX7xi3xKTmftmt9uRm5tryLkiDnicTie2bduG7777Do2NjZg+fTr69u2LHTt2YODAgejTp48hDczOzvYVLfXq378/PvnkE83n2O122O2BiamiKJruy+Cl2jeVZeT+x6itugr1/rjXlMimwdyr79WdyyF7LhCYf6Iy3eI+XA13ye0dbfVPto06+bh9+qazqptbBCDMChXS86SABjUO9Z2kg+TSiA118CyYEXyVW12N9ueYli5/rssJ19xrA3O8lN+t9IyOHbGzcuBe/FB0P28hvruGU9ljKtTrdbvfJybBviUWI/sTUXGfhoYGzJ07F0888QS+//57fPvtt2huln5J7tixA//5z38Ma+CwYcNw8OBB2X0HDx40rFy8mYWq6+NddWUteRzWOaX6RmqUFcrL96jXvlKjvKhk9wpon6/N3mDG2SaVLLj1ysAdl13R5uF4dx7upF8QVh1/TwgCpMDLj6V9F+SAz0MACocEXRElZGTBsvQx2fvqW5nllZWjvYlkjuIvKZdLtbaU8rsFQFaHyrH0ds026tHlNak6o24cEcWViEZ4nnvuOTQ1NaGkpASDBg3yTTcBwMiRI7F5s3F5IBdccAEWLlyIjRs34pRTTkFZWRneeecdXH/99Ya9hll1ykZqygrlLpf+EhPpGfK/2lXaJzbUSv+jayqofW8Zl8v4+ljRyO3TPkJRFTqRWi3Yai+oigFFUv/aWoGkZAhzV/hWpgWj/NzFhrqAkTzPmhLVTSRlo351NfL31S9ICig7Mu86WRvcNQ5lGBeWrt4EkLssE5lfRAHPF198gT/96U8YPHgwPB75mH2vXr1w+LDGqpEIFBcX47bbbsPzzz+PDRs2IC8vD3/5y19w2mmnGfYaFAbllAcA7N0Nd+mciJa1K5fGh70pYFurNDUWLwQBwg13Spv2RVvQcv++jv9vaYb43KNAkCBAa5sBteBB6wLvf6ysdhoQfNRDMQVlzcmNaDYvVrjLMpH5RRTwNDc3a04puVyugCAoWieeeCJOPPFEQ89JEcrJDcwr8auUHfSioUxUbmyQLtDR5OV4PAhMlBGAlBT1PBabHSgsBqorgfra8F5LD1GEWHK7/iXy4QgRQCnfy2Cfh54LfDijHspjcxesQFWzjg0kiYi6SEQBT15eHn788Ucce+yxAY+VlZV1+sotik40Gw5aZs7zK/qpmEYKNaKhnNJKzwh/FMRqDawLpZSTKxUJVUvezcqBdU6pNHoRTsBjsQCe9urroYQb7Njs0ghJeoZ0+5ef5JsFenmLqmp9fgYUd/UXzqiH/7GCIEhLups7b1UVEVG4IpoLOPXUU7F582bs2LHDl0EtCALKysrw5ptvcropzvlGAlSSUUMRMrJgXXA/rGs2dCSrejmq4F56q/4k5or9HRd5r/6FHcmqhUMA5Wo7t47Rw5xcKQBQnhsA6g7DfeOlUn5NSqoUyOgx8BggOVnfseEqLJYSxxfcD+uC+6UVWv5sNqmtNQ64S+dIhU/VPj8m3hIRaYpohGfSpEnYvXs37rvvPvTo0QMAsHTpUhw5cgTHH388zj//fEMbSQaLYiRANrqQnqHY4VgEyvdII0B+lbh9IxBaVc2Lh6uONgmCAMvKOxUb62mMsCSnAD0z5VMvarW6XC7pn3fkJyU1dKHS5BQpOIt0mspilQI3/+cnp8Ca3QvulDTA5ZKSftvbrpwe8uU1tTRL04nKab/2zy9REm89B8ohLpsTdjI2EVE0It5pWRRFfPTRR/jiiy9QX1+Pnj174sQTT8Qpp5wCi96/mrtYdXW1KTce7Nu3LyoqKnTvVxCQjFo4RD1A0fNcmz1wakt5X/FwWGbOhWfBXwODi9w+sJY8rtm3vNRkHJw+KUhQIkirmaxWKaDya7/nwM+h82lyegMNdcFXean1MRi1fX2U57BYkPSb49DW3ATs80vSLh4eMI3knnedPHBTniuMz68rhPpOum+8VP55pqTCuuqlLmxhdCL5mUsU7FtiMnPf7Ha7YdvQhD3C09bWhiVLluCSSy7BuHHjMG7cOEMaQl1HdQRBZ7JrRHkhdTXSqJBa0BJk2kWsr4Vj5f3a1dNTUmFZ+ljAhoS+EabyPaH36mmok441alm7zSYNQilrdSnP7/FII1caozUyypGq/oWyAEfP5xfLQrEBlJ+n1udLRGSgsAOepKQk/PLLL7BarZ3RHuoCofZQCRrUqF18ASmJ2f+2/9Lyxgb1JNz2SuliQ53qxde9pgRu5WaDXimpEOauUE/WVUuo1uItu6BWxsFmk0ZPtJbK5/QGah3y0ZygAVb7zs7BNDa050CJ8qnDwiEBI1heoT4/saFWProWKqjVYFjQlJQsD36TOik3iojIT0Q5PEOHDkVZWRlGjhxpdHsoFlS28deilieivOgFlDdoaQZaVKaVXE4p50fr4hss8PLfl0a5+iuS0RpvkrN/YGOxSgFMQ11gGQxAuj+c4WObNTAg6ttfWiLv9175kpD9l+sXD4e15PH2oEORp6O2+s2P6uhaBCN14Sx7D0aYuwListtlOTxmFVcja0TdXEQBz5///GesWLECWVlZOPnkk5GSkmJ0u6gLhZfsGniBV/uljvQMxUXW+zyVUY7yMvVRjSP1wRvuvWgrp48ikZoGlJfJ72trVYzsKNquK7ASpL2LMrKAn8sCH66qAHr0lL9XdTWBI2LtmzvKRpvag46QtKbJwmXQsndLv4FAAuXsRMOoIJGIohdRwHPnnXfC5XJh9erVWL16NZKTk30Vyb2efvppQxpInc87xeUNXDwlt2v+Nar2CxxA4H1qK6QAqE7puJzqoxqyRgrSiIt/cJOeIQUB/jsSh8Nvmkg6R4jRmkhyfYp/07Hvj9poUGtLez0tP1k5HVOEXu2bO8Km+JFVCzqUq+GUn0VKamQruMIYCaR2Bu+NRESRiyjgOfnkkwMCHIotsb5WqoAdxdC5rr9G9fwCr6uRyissuyP0km+vGkfwPXFEQFj4oDSNpZasG5TKqFJyCiw3LvS9R+4ZF4Y+Tf9C/flBNrs0XeVdch7sQpeWLp3bv9bV0lvV3zvllJhKhXllICJMmyV9Fn5TSJFMqyTKsve4wiCRKG5EFPDMnj3b6HZQlNzKlUqRDJ3rCWa0foEr7hOfWy2/YNvs0uiEVgDU1Chd9FVHhQBAhPjco/Jk6znXaHZF+dwArS3w3HaVFAvZk9STqpOSpV2dBUijS7/uA1x6ippCCor8c3OCyckN/KzUSngo+Y3UBAtEZJ+FjppcWlhvKnwMEoniR1gBT1tbGz799FM4HA5kZGRg7NixyMhQ2c2Wup4RQ+c6/hrV+gUeUI275Hb5EzMypVyV/eVQDUC8Vc+9y7QtlsBVU8o+NR1VP09Ob+DgL8H7CkhJyCK09+mRvb4yT0jHiis9wU7xcNWLoO993rdHO0cpPcM3UhM0EOG0SswwSCSKH7oDnpqaGixatAhVVVW++5599lnMmzcPQ4cO7ZTGURgMGDrX89eo1i/wgPuU7ak5LE1baamvk1+Ik5IREFQ0Nsh2JEaySoHQlmZ9wU7UItjcS7YrNWAfOgLiHaWqG4V15FXVdXwmjQ3y/ur9jDmtQkSkf6fl1atX48MPP8SkSZMwZMgQVFRUYNOmTcjOzsby5cs7u52GMPNOywd3fw/36nvjZvmr70JdXmbMpn7KZeHFw4GffghvWXgs2ewQ7nxAloNUsPghVDW36t4ZVRb8hPEZR/q8SJl511fA3P1j3xKTmfsWk52Wv/32W0yePBlTpkwBAIwZMwb5+fkoLS1FXV0dsrKyDGkQRcZ/5EVtrxb/C1xX7A3ibU9AWQTVg1VKMQQcA/mgSvmexAl2AKCwWFqOHUVF8UinRzitQkQURrX0uro6jBgxQnaf93Z9fYj9UqhLhaqGHk219LAFTJ8I7f/86AlclEnFoUpGxAubHSgcwmRVIqIY0x3weDweJCUlye7z3na7da5coa4RKkm1C5NYLTPntRe3tLdf/Ish3L1KSi4OJjlFep49Kfhx8ax4OKxrNsC64H7urktEFGNhrdI6ePCgrBK6p/2v7oMHDwYcO3jw4CibRhFTSVKVTWOpbUzXSYSMLPmGfeV7pDyWgJ2Y/bQXBbVkZkO8869wVx7otPZFpT2Aw/59ipVeAlD8m8Dl4YqpRPfih7q2vURE3VhYAc+jjz6qev+qVasC7nvppe6xdXw8UlttJasoDkgjLOkZYe8NorxoC9NmSfu8BMsHUhtRUtn917893nNYMjLjN+Dx9jO1hzzgUdtXB4EbOzqW3g7cck/nt5OIiPQHPDNnzuzMdpCBVJNUlUFHegasJY8DkIIYd+kcfUnO/kujHYfkuylrbXioMuIkC8q8xS79Rp48B8ohltyhvUdOPKiplv4pp+dyctWPV3wG7hqHMpuJiIg6ie6AZ8KECZ3YDDKSZjFPlYraYkMtPAv+GjRokY1MKIXaHBDaFda9r+EunRNYi+vX8vgKdmx27eX1KqUhVCkCP2tOLlT2dyYiok4QUWkJim+aBT4VAoIdr/agxRc47d2t/WL2JHlg4h2t8X+NUEvglUFSeZkxFdCN1L9QykXauztwxZjGFJaSMvDLXbACVc2tIZ9HRETRY8BjRnpWYTU2SIGIWuJwexKz5siOX74NWluB/XsDDvEFOv4bD+qd8nI5AyuIx1JKqq/QqGw0yvuYzhwo/1GtSPbhISKiyDHgMSOdBT7Vl6MLgMsFsaEu8HGLBRg8TDZK4553nfyY8j1w/+NmaYRGrcSD3zl9QVGNAwE7C1qsnTPKo2eTQwCw2oDsXoFJ1BrTc0REFN8Y8JiQ7gKfa0pUdkEWgfI90mPKwGnwsNCjM4DqiI/s+HZBc4OMHuGxWKT6XDfcBbzyTGCytLJOVdEQ1Wkq7lpMRJSYGPAkMLGhFu41y3CwsQHu9AzfaIPeAp+ywKiuRp6UW1cDy7wVIYuJWmbOg+fWK/U1WFkZXDmCZLNLAVFWjrSTcvkev8bbALcb+ot2CoBFkA4XPVLeTUsz8Mozqu+NWr0pIiIyDwY8Ccw7QiLtc31APT8mCM2VUoC0x46O0QwhI0vK6VHmAimnjpJTQo8OFRb71QNrD0B+/glwtoU/vZWSIo3g1NUALr8kY41dpZV9DbVUX4+uqFlGRET66C4tQXFIefHeuxvu0jlS/k2YLDPnSRXIc/sEjsSEIMxdIZWCkG4BAwYDt94rBUIWC5CSCmHefWG9ppCRBdvc5bCEuwt0++uhpVkKppRLyXWez4h6Y11as4yIiILiCE8iU46QeDy+C2u4eSZaozl6Riks/QYCj6wLPOmq4LtthxpBEutr4Tlcpav9PoOHSW31H3HymyrTHcgZUW+sC2uWERFRcAx4EpgvB2fvj4DHr4CrxoU1kimWgD19FsxQLQFhNN8eQco9bwCVTQAFaY+c/oXqydh+U2W6aa106+pzEBGRIeI+4Fm3bh3Wr18vuy8zMxOPP/54jFoUP3zTPivvRNuurzse0Liwelbd05EI7DgEz6olsNx4Z/AgSBk8tTT7pos8q5ZIgUYn5Kho7hEESLlBKanSDsc5uQGvq7VKLRzxcg4iIjJG3Ac8ADBgwAAsXLjQd9u/Ynt3J9bXSiuabHbpjvZRDlW/lgfcVtuVWTYaorbs3P98oTYV9LYz3NGlYNM/bpf0z+VSrVvlP1UmvW54++YYlWzMJexERPEjISIHi8WCrKws37+MjIzQT+om3GtK0Pbjd1Lg4XICNlt4F+cQeSayxGJlkcxQ5/ITdgKvcpQqJRXI6S2/z+UMei7ftFiYicNMNiYiMp+EGOGprKzEjBkzYLPZMGTIEFx++eXo06eP5vFOpxNOZ0eOhyAISE1NhSAIEOKpZIER6moVt2u0+9i/UL63jbc+lP8IzuFquOdOB3pmSbcbG4CsHFjnS6us3Kvv9Y18BOyVk5Wj/doaK8qss+arBmjWWfPhWX0vLI0N8KRnwDJrvvTaNdWq5xYEAWJ9Ldx+ozlwu1TrhIX8DqgEgUZ/b7znM933EebuG2Du/rFviak79M2Qc4minn32Y+fLL79Ea2srCgoKUFdXh40bN+LAgQNYuXIlevbsqfocZd5PUVERSkvNObVw6PbpsvydpBGj0WfFk6rHuutqUL34Zjj3lUnHFg1B9s13oerWqyE2NwV9HfvQERBsdrhrHLDm5CJ3wQq462ul57a2QkhORt79TyFp0DG62hmsve7aw3Dce4f0WhlZECHC01Dv+3/nvjJpb552QmoaLJnZ8NTXyvthT5IdF+r90WqrnucQEVF8i/uAR6mlpQU33ngjJk2ahIkTJ6oeozXC43A4ZPebwpF6WB5fgbaqQ0BWtuaIiZdr2R3yDQaLh0sjGlp5Ol7KlVEpqUDf/sA+vxGe4uGwzV2u+nSxoU4aoVFWG8/tA9uyJ6RjvCM0/gVHlQqHwPq3u+B++B/tOUQuaO6+rNJm673/1JHDUycbyQr1nkZCEATk5+ejsrISCfYjGJKZ+waYu3/sW2Iyc9/sdjtycwNzNSOREFNa/lJSUjBw4EBUVGhXmbbb7bDb7QH3i6Joui+D0DMTfVY8iYqKCl/fgvZRLWcnWGKylpZmYH95wLk0X7tnJqxzSlV3dPY+x72mRLu2ltev5UDPTGkqTiso8vJO2bX3UZg2SxbIaCYjt7fVX2d9b8z4nfQyc98Ac/ePfUtMZuybkf1JuIDH6XTiwIEDGD58eKybkpiUwU16RvsoiaJaeXIK0HeAL4cHrS3A/n2hzx1C0KXa4WzMp3VsSqrmPkGyYCvEqjIiIjKXuA94nnnmGYwdOxa5ubmor6/Hhg0b0NzcjPHjx8e6aQnDt8y6pho42iiNekCQRkAAeeKxzQ4UFgcGC0tvDTyx/whKe+Dknndd0NGToEu11UaaBItU/NP/NdWO1Wi3DHc+JiLqtuI+4KmpqcFDDz2EhoYGZGRkYMiQIVi6dCl69+4d+sndnC/Q0cqJ8QYr/rJy1AOSxgbFc+2w3LjQF1wYMXpimTkPnjnTZW215ObBk5kdMCKkNlIUMs+GOx8TEXVbcR/w3HzzzbFuQsKSbSqoRi1/RysIUKls7g0wxIZaKahSnjtMQkYWUFgsa7Otdx94brknYB43kk39uPMxEVH3FfcBD0UhVNDRftHXEwTIjlNMX8HliqgqudqOxsr25C5YgarmVr09Doo7HxMRdV8MeMwsIM/FJuW6KGpQ6QkC/I9TTl/5ylr4Xseua/REq6yF93UEQYA1Kwdo1l6RR0REpAcDHhOLKM8FOmpJKUeOlKM7ftNdQTGJmIiIuggDHpMJt/Cl2vERFxT1Wyml53WYRExERF0lIYqHkn7hFr5UPV5j5EVsqJWms2oc0n43yqr17Su81AIstdexzJwHFA6RAiWbHXC5IDbURdF7IiIidQx4zCbcaSKtnZf9td/2BS011dJOy0nJqsfpfR0hI6tjx2SXEyjfw8rkRETUKRjwmI0y6GhsgHvedXCXzlEfPVEJbiwz50k1tnL7AMXDO6aolEFLWrr6cXra5b3NPB4iIuoCzOExGVmicmODNBLT0qy5GaBWYrPqyi1lzk1Oru5l3prL35nHQ0REXYABj8nIlo/Pu04KdrxURk/C2Zsmmo37tF6HmwESEVFXYMBjZsrRk/bprXCWqPvrjI37uBkgERF1BebwmJgsFycl1Te1pWf1FhERkZlwhMfEwp3eIiIiMiuO8HQXWqukiIiIugEGPN2E5lJzIiKiboBTWt0Ek4OJiKg74wgPERERmR4DHiIiIjI9BjxERERkegx4iIiIyPQY8BAREZHpMeAhIiIi02PAQ0RERKbHgIeIiIhMjwEPERERmR4DHiIiIjI9BjxERERkegx4iIiIyPQY8BAREZHpMeAhIiIi02PAQ0RERKaXcAHPpk2bMHXqVKxduzbWTSEiIqIEkVABT1lZGd5++20MGjQo1k0hIiKiBJIwAU9LSwtWrVqFGTNmoEePHrFuDhERESUQW6wboNcTTzyBMWPGYNSoUdi4cWPQY51OJ5xOp++2IAhITU2FIAgQBKGzm9qlvP0xW78A9i1RmblvgLn7x74lpu7QNyMkRMDz4YcfYt++fSgpKdF1/KZNm7B+/Xrf7aKiIpSWliI3N7ezmhhz+fn5sW5Cp2HfEpOZ+waYu3/sW2Iyc9+MEPcBj8PhwNq1a7FgwQIkJSXpes7kyZMxceJE321vhOhwOGQjP2YgCALy8/NRWVkJURRj3RxDsW+Jycx9A8zdP/YtMZm5b3a73bDBirgPePbu3Yv6+nrMnTvXd5/H48H333+PLVu24Pnnn4fFIk9FstvtsNvtAecSRdF0XwYv9i0xsW+Jy8z9Y98Skxn7ZmR/4j7gOe6443DffffJ7luzZg0KCgowadKkgGCHiIiISCnuA57U1FQMHDhQdl9ycjJ69uwZcD8RERGRGg6PEBERkenF/QiPmrvvvjvWTSAiIqIEwhEeIiIiMj0GPERERGR6DHiIiIjI9BjwEBERkekx4CEiIiLTY8BDREREpseAh4iIiEyPAQ8RERGZHgMeIiIiMj0GPERERGR6DHiIiIjI9BjwEBERkekx4CEiIiLTY8BDREREpseAh4iIiEyPAQ8RERGZHgMeIiIiMj0GPERERGR6DHiIiIjI9BjwEBERkekx4CEiIiLTY8BDREREpseAh4iIiEyPAQ8RERGZHgMeIiIiMj0GPERERGR6DHiIiIjI9BjwEBERkekx4CEiIiLTY8BDREREpmeLdQNCeeutt/DWW2+huroaANC/f39MmTIFY8aMiXHLiIiIKFHEfcCTk5ODK664Avn5+QCA9957D8uXL8fy5csxYMCAGLeOiIiIEkHcBzxjx46V3b788svx1ltvYc+ePQx4iIiISJe4D3j8eTwefPzxx2htbcXQoUM1j3M6nXA6nb7bgiAgNTUVNltCdVcXQRAAAHa7HaIoxrg1xmLfEpOZ+waYu3/sW2Iyc9+MvG4LYgK8O7/88gsWLFgAp9OJlJQU/O1vf8MJJ5ygefy6deuwfv163+1x48bhpptu6oqmEhERkcGcTifsdntU50iIVVoFBQVYsWIFli5dinPPPRePPvoofv31V83jJ0+ejLVr1/r+TZs2DQ899BCam5u7sNVdo7m5GXPmzGHfEgz7lrjM3D/2LTGZvW8PPfSQbNYmUgkR8NhsNuTn5+OYY47BFVdcgcLCQrzxxhuax9vtdqSlpfn+paam4sMPPzTdUB8AiKKIffv2sW8Jhn1LXGbuH/uWmMzetw8//NCQcyVEwKMkiqIh0R4RERF1D3Ef8Dz//PP4/vvvUVVVhV9++QUvvPACvvvuO5x22mmxbhoREREliLhftlRfX49HHnkEtbW1SEtLw6BBg7BgwQKMGjVK9znsdjumTJkSdcJTPGLfEhP7lrjM3D/2LTGxb/okxCotIiIiomjE/ZQWERERUbQY8BAREZHpMeAhIiIi02PAQ0RERKYX96u0ovHWW2/hrbfeQnV1NQCgf//+mDJlCsaMGRPjlhlr06ZNeOGFF3D++efjqquuinVzoqYsDQIAmZmZePzxx2PUImPV1NTgueeew1dffYW2tjb07dsXM2fOxODBg2PdtKjMnj3b97Pm79xzz8W1114bgxYZx+124+WXX8YHH3yAuro6ZGdnY8KECbjoootgsST+343Nzc146aWX8Omnn6K+vh5FRUW46qqrUFxcHOumhWXXrl149dVXsW/fPtTW1uK2227DSSed5HtcFEW8/PLLeOedd9DY2IghQ4Zg+vTpCVOIOlT/PvnkE7z99tvYu3cvjhw5guXLl6OwsDB2DQ5DsL65XC68+OKL+PLLL1FVVYW0tDQcd9xxuOKKK5CTk6P7NUwd8OTk5OCKK65Afn4+AOC9997D8uXLsXz58oT5godSVlaGt99+G4MGDYp1Uww1YMAALFy40HfbDBcVAGhsbMTChQsxcuRIzJ8/HxkZGTh06BDS0tJi3bSolZSUwOPx+G7/8ssvuOeee/Db3/42hq0yxubNm7F161bMnj0b/fv3x969e7F69WqkpaXh/PPPj3Xzova///u/2L9/P2644Qbk5OTg/fffx5IlS/DAAw+EdUGJtdbWVhQWFuKMM87A/fffH/D45s2b8frrr2PWrFno27cvNm7ciHvuuQcPPvggUlNTY9Di8ITqX2trK4YNG4b/9//+Hx577LEYtDBywfrW1taGffv24eKLL0ZhYSEaGxvx9NNPY/ny5Vi2bJnu1zB1wDN27FjZ7csvvxxvvfUW9uzZY4qAp6WlBatWrcKMGTOwcePGWDfHUBaLBVlZWbFuhuE2b96MXr16YdasWb778vLyYtgi42RkZMhuv/LKK+jTpw9GjBgRoxYZ58cff8TYsWN9RYvz8vKwfft2/PTTTzFuWfTa2trwySef4I477vB9VlOnTsWOHTvw1ltv4bLLLotxC/UbM2aM5gi+KIp44403MHnyZJx88skApFHJ6667Dtu3b8c555zTlU2NSLD+AcDpp58OAKiqquqqJhkmWN/S0tJkfwADwNVXX4358+fD4XAgNzdX12uY489mHTweDz788EO0trZi6NChsW6OIZ544gmMGTMmrE0YE0VlZSVmzJiB2bNn48EHH8ShQ4di3SRDfPbZZxg8eDBWrlyJa6+9FnfccQfefvvtWDfLcC6XCx988AHOOOMMCIIQ6+ZE7Te/+Q127tyJgwcPAgDKy8uxe/duU0yPu91ueDyegI3dkpKS8MMPP8SoVcarqqpCXV0dRo8e7bvPbrdjxIgR2L17dwxbRpFoamqCIAhhjY6beoQHkIbVFyxYAKfTiZSUFNx2223o379/rJsVtQ8//BD79u1DSUlJrJtiuCFDhmD27NkoKChAXV0dNm7ciDvvvBMrV65Ez549Y928qFRVVWHr1q244IILMHnyZJSVleGpp56C3W7H+PHjY908w3z66ac4evQoJkyYEOumGGLSpEloamrC3//+d1gsFng8Hlx22WU49dRTY920qKWmpmLo0KHYsGED+vXrh6ysLGzfvh1lZWW+dAAzqKurAyDlA/rLzMyEw+GIQYsoUm1tbXj++ecxbtw4Bjz+CgoKsGLFChw9ehSffPIJHn30USxevDihgx6Hw4G1a9diwYIFSEpKinVzDOf/V/PAgQMxdOhQ3HjjjXjvvfcwceLEGLYseh6PB8cccwyuuOIKAEBRURH279+Pt956y1QBz7vvvovjjz8+ofI/gvnoo4/wwQcf4G9/+xsGDBiA8vJyrF271pe8nOhuuOEGrFmzBn/9619hsVhQVFSEcePGYd++fbFumuGUI44sNpBYXC4XHnzwQYiiGPZiCNMHPDabzfdXyjHHHIOffvoJb7zxBq6//voYtyxye/fuRX19PebOneu7z+Px4Pvvv8eWLVvw/PPPmybJFwBSUlIwcOBAVFRUxLopUcvOzg4Itvv3749PPvkkRi0yXnV1Nb755hvcdtttsW6KYZ577jlMmjQJ48aNAyAF4tXV1XjllVdMEfDk5+dj8eLFaGlpQXNzM7Kzs/HAAw+YJr8MgC8n0LvKzquhoSFg1Ifik8vlwgMPPIDq6mrcddddYS/2MH3AoySKIpxOZ6ybEZXjjjsO9913n+y+NWvWoKCgAJMmTTJVsAMATqcTBw4cwPDhw2PdlKgNGzbMlwfidfDgQfTu3TtGLTLeu+++i8zMTF+Crxm0trYG/FxZLBbTjQ6kpKQgJSUFjY2N+PrrrzFt2rRYN8kweXl5yMrKwjfffIOioiIA0gV0165d+NOf/hTj1lEo3mCnsrISixYtiii9wdQBz/PPP48xY8agV69eaGlpwYcffojvvvsOCxYsiHXTopKamoqBAwfK7ktOTkbPnj0D7k9EzzzzDMaOHYvc3FzU19djw4YNaG5uNsWUzwUXXICFCxdi48aNOOWUU1BWVoZ33nknoUcc/Xk8Hmzbtg3jx4+H1WqNdXMMc+KJJ2Ljxo3Izc1F//79UV5ejtdeew1nnHFGrJtmiK+++gqAlAJQWVmJZ599FgUFBQk3etXS0oLKykrf7aqqKpSXlyM9PR25ubk4//zzsWnTJvTt2xf5+fnYtGkTkpOTEyYXK1T/Ghsb4XA4UFNTAwC+P66ysrLiftVrsL5lZ2dj5cqV2LdvH+bMmQOPx+PLyUpPT4fNpi+UMXW19DVr1mDnzp2ora1FWloaBg0ahEmTJplyVdPdd9+NwsJCU2w8+OCDD+L7779HQ0MDMjIyMGTIEFx22WUJnXfl7/PPP8fzzz+PyspK5OXl4YILLsDZZ58d62YZ4uuvv8bSpUvx4IMPoqCgINbNMYxyY76cnByMGzcOU6ZM0f3LNp599NFHeOGFF3D48GGkp6fj5JNPxuWXX55w+0N99913WLx4ccD948ePx+zZs30bD7799ts4evQoiouLMX369IT5QzFU/7Zt24bVq1cHPD5lyhRMnTq1K5oYsWB9u+SSS3DDDTeoPm/RokUYOXKkrtcwdcBDREREBHSjfXiIiIio+2LAQ0RERKbHgIeIiIhMjwEPERERmR4DHiIiIjI9BjxERERkegx4iIiIyPQY8BAREZHpJf4WoURkCL07sYazs2kiePTRR7Fr1y48+uijsW4KEXUiBjxEBAC45557ZLc3bNiA7777DnfddZfsfrOU+CCi7oUBDxEBAIYOHSq7nZGRAUEQAu5Xam1tRXJycmc2jYgoagx4iEi3u+++G0eOHMH06dPx/PPPo7y8HGPHjsXNN9+MqVOnqhYpnD17NkaMGIHZs2f77qurq8O6devwxRdf+IpxTpgwARdddFHQKuvLly9HeXk5HnnkEVgs8hTE+fPnw+12o7S0FACwZcsWfPzxxzhw4ABaW1uRl5eH008/HRdccEHQgp9VVVW44YYbMGvWrIBq4Wp9rKiowLp16/Dtt9+iqakJffr0we9+9zv8/ve/9x3j8XiwadMmvP/++3A4HLDb7cjNzcWZZ56J888/X/sNJyLDMOAhorDU1tZi1apVmDRpEi6//HIIghDW8+vq6jBv3jxYLBZMmTIFffr0wY8//oiNGzeiuroas2bN0nzumWeeieXLl2Pnzp0YNWqU7/4DBw6grKwMV199te++Q4cOYdy4ccjLy4PNZsPPP/+MjRs34sCBA0FfIxy//vor7rzzTuTm5uLKK69EVlYWvvrqKzz11FM4cuQILrnkEgDAq6++ipdffhkXXXQRRowYAZfLhYMHD+Lo0aOGtIOIQmPAQ0RhaWxsxC233IJjjz02ouevW7cOR48excqVK5GbmwsAOO6445CUlIRnn30Wf/zjHzXzhMaMGYPMzExs27ZNFvC8++67sNlsOPXUU333/eUvf/H9v8fjwfDhw9GzZ0+sXr0aV155JdLT0yNqv7+nn34aqamp+Mc//oG0tDQAwKhRo+ByufDKK6/gvPPOQ3p6On744QcMHDhQNjJ0/PHHR/36RKQfl6UTUVh69OgRcbADAF988QVGjhyJ7OxsuN1u378xY8YAAHbt2qX5XKvVitNOOw2ffPIJmpqaAEjBzAcffICxY8eiZ8+evmP37duH0tJSXHPNNbjssstw+eWX45FHHoHH40FFRUXE7fdqa2vDzp078T//8z9ITk4O6IvT6cSePXsAAMXFxfj555/xxBNP4KuvvvK1nYi6Dkd4iCgs2dnZUT2/vr4en3/+OS6//HLVxxsaGoI+/8wzz8Rrr72GDz/8EOeccw6++uor1NbW4owzzvAd43A4cNddd6GgoABXXXUV8vLyYLfbUVZWhieffBJtbW1R9QGQRrrcbje2bNmCLVu2qB5z5MgRAMDkyZORkpKCDz74AFu3boXFYsHw4cPxpz/9Ccccc0zUbSGi0BjwEFFYtHJ27HY7XC5XwP3ei75Xz549MWjQIFx22WWq5wkVUPXv3x/FxcXYtm0bzjnnHGzbtg3Z2dkYPXq075hPP/0Ura2tuO2229C7d2/f/eXl5UHPDQBJSUkAAKfTGbQfPXr0gMViwemnn47f/e53qufKy8sDII1MTZw4ERMnTsTRo0fx7bff4oUXXsDSpUuxZs0arnIj6gIMeIjIEL1798bPP/8su2/nzp1oaWmR3XfCCSfgyy+/RJ8+fSLOo5kwYQKeeOIJ/PDDD/j8889xwQUXyFZteYMyu93uu08URbzzzjshz52ZmQm73R7Qlx07dshuJycnY+TIkdi3bx8GDRoUdOWXvx49euD//b//h5qaGqxduxbV1dXc24ioCzDgISJDnH766XjppZfw0ksvYcSIEfj111+xZcsWXzKv16WXXopvv/0WCxcuxHnnnYeCggK0tbWhuroaX375Ja677jr06tUr6GudeuqpeOaZZ/DQQw/B6XQGLB8fNWoUbDYbHnroIfzxj3+E0+nEW2+9pWtVlCAIOO200/Duu+8iPz8fgwYNQllZGbZv3x5w7NVXX42FCxfirrvuwrnnnovevXujubkZlZWV+Pzzz7Fo0SIAwLJlyzBw4EAMHjwYGRkZcDgceP3119G7d2/k5+eHbBMRRY8BDxEZ4o9//COampqwbds2/Oc//0FxcTH+/ve/Y8WKFbLjsrOzUVJSgg0bNuDVV1/F4cOHkZqairy8PBx//PHo0aNHyNdKS0vDSSedhO3bt2PYsGEoKCiQPd6vXz/ceuutePHFF3HfffehZ8+eOPXUUzFx4kTce++9Ic9/5ZVXAgA2b96MlpYWHHvssZg7d65sLyFAml4rLS3Fhg0b8OKLL6K+vh49evRA3759fUnYAHDsscfik08+wTvvvIPm5mZkZWVh1KhRuPjii3WPDBFRdARRFMVYN4KIiIioM3FZOhEREZkeAx4iIiIyPQY8REREZHoMeIiIiMj0GPAQERGR6THgISIiItNjwENERESmx4CHiIiITI8BDxEREZkeAx4iIiIyPQY8REREZHr/H+mIrl56015PAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (RF)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model r2_score 0.6948 with a standard deviation of 0.0331\n",
      "RF optimized model r2_score 0.6995 with a standard deviation of 0.0310\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"RF baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_rf.joblib']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf_reg, \"OUTPUT/rf_reg.joblib\")\n",
    "#joblib.dump(optimized_rf, \"OUTPUT/optimized_rf.joblib\") # fitted to whole training set with last random_state selected\n",
    "joblib.dump(optimizedCV_rf, \"OUTPUT/optimizedCV_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.695224     0.025102\n",
      "1                    TP       162.000000     9.729680\n",
      "2                    TN        88.000000     5.291503\n",
      "3                    FP        25.400000     4.765618\n",
      "4                    FN        21.700000     4.001389\n",
      "5              Accuracy         0.841468     0.024605\n",
      "6             Precision         0.863994     0.028421\n",
      "7           Sensitivity         0.881358     0.025407\n",
      "8           Specificity         0.776850     0.032157\n",
      "9              F1 score         0.872453     0.024433\n",
      "10  F1 score (weighted)         0.840940     0.024737\n",
      "11     F1 score (macro)         0.830864     0.023540\n",
      "12    Balanced Accuracy         0.829100     0.023812\n",
      "13                  MCC         0.662469     0.046904\n",
      "14                  NPV         0.802820     0.027822\n",
      "15              ROC_AUC         0.829100     0.023812\n",
      "CPU times: user 14.7 s, sys: 100 ms, total: 14.8 s\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_reg = lgbm.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=8,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_reg.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"rmse\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_reg.predict(X_test) \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "         # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 11:32:02,220] A new study created in memory with name: lgbmRegressor\n",
      "[I 2023-12-20 11:32:03,797] Trial 0 finished with value: 0.657906222195145 and parameters: {'n_estimators': 655, 'learning_rate': 0.16492323815512352, 'max_depth': 4, 'max_bin': 260, 'num_leaves': 213}. Best is trial 0 with value: 0.657906222195145.\n",
      "[I 2023-12-20 11:32:05,847] Trial 1 finished with value: 0.6604765646714678 and parameters: {'n_estimators': 549, 'learning_rate': 0.10659748520191774, 'max_depth': 5, 'max_bin': 281, 'num_leaves': 198}. Best is trial 1 with value: 0.6604765646714678.\n",
      "[I 2023-12-20 11:32:06,900] Trial 2 finished with value: 0.5544178391974761 and parameters: {'n_estimators': 161, 'learning_rate': 0.026833692786492377, 'max_depth': 4, 'max_bin': 286, 'num_leaves': 371}. Best is trial 1 with value: 0.6604765646714678.\n",
      "[I 2023-12-20 11:32:08,941] Trial 3 finished with value: 0.6697559205622158 and parameters: {'n_estimators': 897, 'learning_rate': 0.1544478360939618, 'max_depth': 10, 'max_bin': 183, 'num_leaves': 487}. Best is trial 3 with value: 0.6697559205622158.\n",
      "[I 2023-12-20 11:32:10,225] Trial 4 finished with value: 0.6474040944652588 and parameters: {'n_estimators': 155, 'learning_rate': 0.07381616882212311, 'max_depth': 6, 'max_bin': 158, 'num_leaves': 53}. Best is trial 3 with value: 0.6697559205622158.\n",
      "[I 2023-12-20 11:32:10,984] Trial 5 finished with value: 0.6086330607132042 and parameters: {'n_estimators': 83, 'learning_rate': 0.07961785029260403, 'max_depth': 5, 'max_bin': 221, 'num_leaves': 742}. Best is trial 3 with value: 0.6697559205622158.\n",
      "[I 2023-12-20 11:32:14,128] Trial 6 finished with value: 0.6799224077791927 and parameters: {'n_estimators': 798, 'learning_rate': 0.0881623915043523, 'max_depth': 11, 'max_bin': 259, 'num_leaves': 290}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:32:14,853] Trial 7 finished with value: 0.5237573892995455 and parameters: {'n_estimators': 92, 'learning_rate': 0.03465256918245775, 'max_depth': 4, 'max_bin': 260, 'num_leaves': 696}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:32:20,816] Trial 8 finished with value: 0.677236022031448 and parameters: {'n_estimators': 647, 'learning_rate': 0.022009238194204382, 'max_depth': 10, 'max_bin': 273, 'num_leaves': 337}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:32:23,530] Trial 9 finished with value: 0.6605947469217395 and parameters: {'n_estimators': 493, 'learning_rate': 0.050909790184742724, 'max_depth': 5, 'max_bin': 222, 'num_leaves': 538}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:32:25,570] Trial 10 finished with value: 0.6706059549429678 and parameters: {'n_estimators': 896, 'learning_rate': 0.197843034956916, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 74}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:32:35,165] Trial 11 finished with value: 0.6015972713064024 and parameters: {'n_estimators': 713, 'learning_rate': 0.003226284629635385, 'max_depth': 9, 'max_bin': 297, 'num_leaves': 324}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:32:44,966] Trial 12 finished with value: 0.6612308877920353 and parameters: {'n_estimators': 726, 'learning_rate': 0.006787748567576007, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 272}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:32:47,574] Trial 13 finished with value: 0.6786779942351273 and parameters: {'n_estimators': 414, 'learning_rate': 0.10856507285783668, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 470}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:32:49,905] Trial 14 finished with value: 0.6746264868872647 and parameters: {'n_estimators': 308, 'learning_rate': 0.11090922981723511, 'max_depth': 8, 'max_bin': 198, 'num_leaves': 524}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:32:52,570] Trial 15 finished with value: 0.6756060700363479 and parameters: {'n_estimators': 349, 'learning_rate': 0.12275132524953608, 'max_depth': 11, 'max_bin': 196, 'num_leaves': 447}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:32:55,295] Trial 16 finished with value: 0.6730550890049736 and parameters: {'n_estimators': 358, 'learning_rate': 0.08024087546346076, 'max_depth': 8, 'max_bin': 239, 'num_leaves': 617}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:32:57,728] Trial 17 finished with value: 0.6753227753844544 and parameters: {'n_estimators': 466, 'learning_rate': 0.130714342811971, 'max_depth': 10, 'max_bin': 154, 'num_leaves': 428}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:33:00,155] Trial 18 finished with value: 0.6789088947208437 and parameters: {'n_estimators': 250, 'learning_rate': 0.09341192908041378, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 146}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:33:02,241] Trial 19 finished with value: 0.6603977727389965 and parameters: {'n_estimators': 252, 'learning_rate': 0.058426933629655946, 'max_depth': 7, 'max_bin': 177, 'num_leaves': 125}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:33:04,742] Trial 20 finished with value: 0.6777521286991577 and parameters: {'n_estimators': 231, 'learning_rate': 0.08895111903155078, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 168}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:33:07,517] Trial 21 finished with value: 0.6760885565032401 and parameters: {'n_estimators': 402, 'learning_rate': 0.09780749407211366, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 272}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:33:10,062] Trial 22 finished with value: 0.675690224307147 and parameters: {'n_estimators': 557, 'learning_rate': 0.09756697184444517, 'max_depth': 9, 'max_bin': 242, 'num_leaves': 264}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:33:13,487] Trial 23 finished with value: 0.6762690856868545 and parameters: {'n_estimators': 430, 'learning_rate': 0.06342197696663249, 'max_depth': 12, 'max_bin': 179, 'num_leaves': 124}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:33:15,890] Trial 24 finished with value: 0.6765640005900337 and parameters: {'n_estimators': 794, 'learning_rate': 0.11788808643186934, 'max_depth': 9, 'max_bin': 231, 'num_leaves': 595}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:33:18,741] Trial 25 finished with value: 0.675064694556269 and parameters: {'n_estimators': 267, 'learning_rate': 0.09166621109688722, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 417}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:33:20,687] Trial 26 finished with value: 0.6711971915994578 and parameters: {'n_estimators': 188, 'learning_rate': 0.1374999333350548, 'max_depth': 10, 'max_bin': 168, 'num_leaves': 236}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:33:23,317] Trial 27 finished with value: 0.6771827673490003 and parameters: {'n_estimators': 557, 'learning_rate': 0.10780094702340182, 'max_depth': 12, 'max_bin': 190, 'num_leaves': 313}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:33:26,811] Trial 28 finished with value: 0.6762312697812611 and parameters: {'n_estimators': 807, 'learning_rate': 0.06944015104202284, 'max_depth': 9, 'max_bin': 212, 'num_leaves': 131}. Best is trial 6 with value: 0.6799224077791927.\n",
      "[I 2023-12-20 11:33:29,715] Trial 29 finished with value: 0.6811145309742541 and parameters: {'n_estimators': 623, 'learning_rate': 0.08839918661952019, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 372}. Best is trial 29 with value: 0.6811145309742541.\n",
      "[I 2023-12-20 11:33:33,734] Trial 30 finished with value: 0.6814001611339672 and parameters: {'n_estimators': 683, 'learning_rate': 0.05161338712162346, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 187}. Best is trial 30 with value: 0.6814001611339672.\n",
      "[I 2023-12-20 11:33:37,565] Trial 31 finished with value: 0.6809587131568235 and parameters: {'n_estimators': 650, 'learning_rate': 0.05063058617963079, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 205}. Best is trial 30 with value: 0.6814001611339672.\n",
      "[I 2023-12-20 11:33:41,487] Trial 32 finished with value: 0.6796981064454488 and parameters: {'n_estimators': 614, 'learning_rate': 0.048373076494651134, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 212}. Best is trial 30 with value: 0.6814001611339672.\n",
      "[I 2023-12-20 11:33:45,503] Trial 33 finished with value: 0.6823189404026337 and parameters: {'n_estimators': 730, 'learning_rate': 0.0437376663740463, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 181}. Best is trial 33 with value: 0.6823189404026337.\n",
      "[I 2023-12-20 11:33:49,860] Trial 34 finished with value: 0.6832845053769805 and parameters: {'n_estimators': 722, 'learning_rate': 0.043811540312463815, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 214}. Best is trial 34 with value: 0.6832845053769805.\n",
      "[I 2023-12-20 11:33:54,787] Trial 35 finished with value: 0.6834534789436021 and parameters: {'n_estimators': 699, 'learning_rate': 0.04012749765314866, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 371}. Best is trial 35 with value: 0.6834534789436021.\n",
      "[I 2023-12-20 11:34:00,212] Trial 36 finished with value: 0.6828505115576767 and parameters: {'n_estimators': 713, 'learning_rate': 0.035960296671514895, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 39}. Best is trial 35 with value: 0.6834534789436021.\n",
      "[I 2023-12-20 11:34:04,970] Trial 37 finished with value: 0.6810952915588258 and parameters: {'n_estimators': 760, 'learning_rate': 0.036679958797565415, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 41}. Best is trial 35 with value: 0.6834534789436021.\n",
      "[I 2023-12-20 11:34:12,437] Trial 38 finished with value: 0.6819324829049519 and parameters: {'n_estimators': 860, 'learning_rate': 0.01910927206665892, 'max_depth': 12, 'max_bin': 298, 'num_leaves': 84}. Best is trial 35 with value: 0.6834534789436021.\n",
      "[I 2023-12-20 11:34:16,796] Trial 39 finished with value: 0.6792113740906427 and parameters: {'n_estimators': 837, 'learning_rate': 0.038803800875997586, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 93}. Best is trial 35 with value: 0.6834534789436021.\n",
      "[I 2023-12-20 11:34:19,516] Trial 40 finished with value: 0.6250333995457803 and parameters: {'n_estimators': 752, 'learning_rate': 0.033833182314237806, 'max_depth': 3, 'max_bin': 293, 'num_leaves': 230}. Best is trial 35 with value: 0.6834534789436021.\n",
      "[I 2023-12-20 11:34:26,875] Trial 41 finished with value: 0.6808312447550301 and parameters: {'n_estimators': 845, 'learning_rate': 0.020135234734900316, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 83}. Best is trial 35 with value: 0.6834534789436021.\n",
      "[I 2023-12-20 11:34:34,788] Trial 42 finished with value: 0.6769474401575332 and parameters: {'n_estimators': 877, 'learning_rate': 0.015832635706269953, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 33}. Best is trial 35 with value: 0.6834534789436021.\n",
      "[I 2023-12-20 11:34:41,106] Trial 43 finished with value: 0.6818116414633617 and parameters: {'n_estimators': 698, 'learning_rate': 0.02711228894516414, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 104}. Best is trial 35 with value: 0.6834534789436021.\n",
      "[I 2023-12-20 11:34:50,438] Trial 44 finished with value: 0.6781589806431754 and parameters: {'n_estimators': 763, 'learning_rate': 0.011636032739481824, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 165}. Best is trial 35 with value: 0.6834534789436021.\n",
      "[I 2023-12-20 11:34:56,043] Trial 45 finished with value: 0.6766400934608993 and parameters: {'n_estimators': 594, 'learning_rate': 0.023721290133490867, 'max_depth': 10, 'max_bin': 298, 'num_leaves': 83}. Best is trial 35 with value: 0.6834534789436021.\n",
      "[I 2023-12-20 11:35:07,978] Trial 46 finished with value: 0.4977331401618629 and parameters: {'n_estimators': 686, 'learning_rate': 0.0014274539292141626, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 67}. Best is trial 35 with value: 0.6834534789436021.\n",
      "[I 2023-12-20 11:35:11,567] Trial 47 finished with value: 0.6686887786254363 and parameters: {'n_estimators': 855, 'learning_rate': 0.041901041369705384, 'max_depth': 6, 'max_bin': 261, 'num_leaves': 351}. Best is trial 35 with value: 0.6834534789436021.\n",
      "[I 2023-12-20 11:35:16,698] Trial 48 finished with value: 0.6781445949071772 and parameters: {'n_estimators': 783, 'learning_rate': 0.031365425306423646, 'max_depth': 10, 'max_bin': 277, 'num_leaves': 167}. Best is trial 35 with value: 0.6834534789436021.\n",
      "[I 2023-12-20 11:35:24,352] Trial 49 finished with value: 0.6820512083776562 and parameters: {'n_estimators': 724, 'learning_rate': 0.016430817975627024, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 62}. Best is trial 35 with value: 0.6834534789436021.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6835\n",
      "\tBest params:\n",
      "\t\tn_estimators: 699\n",
      "\t\tlearning_rate: 0.04012749765314866\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 282\n",
      "\t\tnum_leaves: 371\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.715728\n",
      "1                    TP  343.000000\n",
      "2                    TN  170.000000\n",
      "3                    FP   42.000000\n",
      "4                    FN   40.000000\n",
      "5              Accuracy    0.862185\n",
      "6             Precision    0.890909\n",
      "7           Sensitivity    0.895561\n",
      "8           Specificity    0.801900\n",
      "9              F1 score    0.893229\n",
      "10  F1 score (weighted)    0.862038\n",
      "11     F1 score (macro)    0.849458\n",
      "12    Balanced Accuracy    0.848724\n",
      "13                  MCC    0.698939\n",
      "14                  NPV    0.809500\n",
      "15              ROC_AUC    0.848724\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_lgbm_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_0_cat = np.where((y_pred_lgbm_0>= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 11:35:29,837] Trial 50 finished with value: 0.687274320115347 and parameters: {'n_estimators': 728, 'learning_rate': 0.04067600164839472, 'max_depth': 11, 'max_bin': 231, 'num_leaves': 302}. Best is trial 50 with value: 0.687274320115347.\n",
      "[I 2023-12-20 11:35:34,582] Trial 51 finished with value: 0.6878036907117739 and parameters: {'n_estimators': 727, 'learning_rate': 0.042153255681300655, 'max_depth': 11, 'max_bin': 231, 'num_leaves': 258}. Best is trial 51 with value: 0.6878036907117739.\n",
      "[I 2023-12-20 11:35:38,922] Trial 52 finished with value: 0.6866538052814001 and parameters: {'n_estimators': 671, 'learning_rate': 0.04322556501208784, 'max_depth': 10, 'max_bin': 229, 'num_leaves': 302}. Best is trial 51 with value: 0.6878036907117739.\n",
      "[I 2023-12-20 11:35:42,606] Trial 53 finished with value: 0.6871011371940992 and parameters: {'n_estimators': 506, 'learning_rate': 0.05894539062411565, 'max_depth': 10, 'max_bin': 229, 'num_leaves': 307}. Best is trial 51 with value: 0.6878036907117739.\n",
      "[I 2023-12-20 11:35:46,581] Trial 54 finished with value: 0.6871848074454181 and parameters: {'n_estimators': 511, 'learning_rate': 0.05902432555187207, 'max_depth': 10, 'max_bin': 228, 'num_leaves': 307}. Best is trial 51 with value: 0.6878036907117739.\n",
      "[I 2023-12-20 11:35:50,630] Trial 55 finished with value: 0.6855108479934644 and parameters: {'n_estimators': 507, 'learning_rate': 0.058512371975436725, 'max_depth': 10, 'max_bin': 229, 'num_leaves': 393}. Best is trial 51 with value: 0.6878036907117739.\n",
      "[I 2023-12-20 11:35:53,642] Trial 56 finished with value: 0.6825410530552728 and parameters: {'n_estimators': 513, 'learning_rate': 0.07214233890681573, 'max_depth': 8, 'max_bin': 229, 'num_leaves': 302}. Best is trial 51 with value: 0.6878036907117739.\n",
      "[I 2023-12-20 11:35:57,444] Trial 57 finished with value: 0.6837847892604859 and parameters: {'n_estimators': 517, 'learning_rate': 0.05821000873667108, 'max_depth': 9, 'max_bin': 220, 'num_leaves': 339}. Best is trial 51 with value: 0.6878036907117739.\n",
      "[I 2023-12-20 11:36:00,783] Trial 58 finished with value: 0.6860817136284372 and parameters: {'n_estimators': 459, 'learning_rate': 0.0807475953832224, 'max_depth': 10, 'max_bin': 236, 'num_leaves': 415}. Best is trial 51 with value: 0.6878036907117739.\n",
      "[I 2023-12-20 11:36:04,315] Trial 59 finished with value: 0.6891768204296794 and parameters: {'n_estimators': 465, 'learning_rate': 0.07843528416036004, 'max_depth': 10, 'max_bin': 234, 'num_leaves': 255}. Best is trial 59 with value: 0.6891768204296794.\n",
      "[I 2023-12-20 11:36:07,958] Trial 60 finished with value: 0.6860377447403772 and parameters: {'n_estimators': 578, 'learning_rate': 0.06702556144173419, 'max_depth': 9, 'max_bin': 219, 'num_leaves': 249}. Best is trial 59 with value: 0.6891768204296794.\n",
      "[I 2023-12-20 11:36:11,579] Trial 61 finished with value: 0.6896458725399434 and parameters: {'n_estimators': 437, 'learning_rate': 0.07586515019792811, 'max_depth': 10, 'max_bin': 237, 'num_leaves': 287}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:36:14,929] Trial 62 finished with value: 0.6882251698210112 and parameters: {'n_estimators': 390, 'learning_rate': 0.0746447736517943, 'max_depth': 10, 'max_bin': 235, 'num_leaves': 285}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:36:18,238] Trial 63 finished with value: 0.6867303064776198 and parameters: {'n_estimators': 357, 'learning_rate': 0.07787069872661144, 'max_depth': 10, 'max_bin': 237, 'num_leaves': 282}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:36:21,639] Trial 64 finished with value: 0.6858003349470658 and parameters: {'n_estimators': 382, 'learning_rate': 0.06456796495581792, 'max_depth': 9, 'max_bin': 244, 'num_leaves': 326}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:36:25,207] Trial 65 finished with value: 0.6883158218314561 and parameters: {'n_estimators': 439, 'learning_rate': 0.0747270786551576, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 256}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:36:28,463] Trial 66 finished with value: 0.6838407394795574 and parameters: {'n_estimators': 321, 'learning_rate': 0.07014776912884653, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 262}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:36:32,094] Trial 67 finished with value: 0.6872947016386448 and parameters: {'n_estimators': 454, 'learning_rate': 0.07703930752817513, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 240}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:36:35,546] Trial 68 finished with value: 0.6830486386487353 and parameters: {'n_estimators': 435, 'learning_rate': 0.08245767810786431, 'max_depth': 11, 'max_bin': 234, 'num_leaves': 246}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:36:39,451] Trial 69 finished with value: 0.6895509943095924 and parameters: {'n_estimators': 473, 'learning_rate': 0.07653624189372221, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 285}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:36:42,762] Trial 70 finished with value: 0.6886187874758198 and parameters: {'n_estimators': 466, 'learning_rate': 0.0852794660310886, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 279}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:36:46,331] Trial 71 finished with value: 0.6876637930378386 and parameters: {'n_estimators': 471, 'learning_rate': 0.07592542890199792, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 225}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:36:49,744] Trial 72 finished with value: 0.689273120013617 and parameters: {'n_estimators': 394, 'learning_rate': 0.08628852077501105, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 279}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:36:53,150] Trial 73 finished with value: 0.6862326189685614 and parameters: {'n_estimators': 392, 'learning_rate': 0.10142688779513616, 'max_depth': 11, 'max_bin': 206, 'num_leaves': 273}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:36:56,629] Trial 74 finished with value: 0.6866105186882517 and parameters: {'n_estimators': 423, 'learning_rate': 0.08553432907372399, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 347}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:36:59,879] Trial 75 finished with value: 0.6858585442083229 and parameters: {'n_estimators': 311, 'learning_rate': 0.09249827506292407, 'max_depth': 11, 'max_bin': 191, 'num_leaves': 287}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:37:02,787] Trial 76 finished with value: 0.6766967051308103 and parameters: {'n_estimators': 382, 'learning_rate': 0.07440350871656125, 'max_depth': 8, 'max_bin': 245, 'num_leaves': 191}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:37:05,991] Trial 77 finished with value: 0.6885051971337303 and parameters: {'n_estimators': 536, 'learning_rate': 0.0848446707037598, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 261}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:37:09,374] Trial 78 finished with value: 0.6878204466134159 and parameters: {'n_estimators': 538, 'learning_rate': 0.08461109536524376, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 358}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:37:13,179] Trial 79 finished with value: 0.6872234917121878 and parameters: {'n_estimators': 489, 'learning_rate': 0.08839641244290236, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 327}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:37:15,842] Trial 80 finished with value: 0.6824442827156451 and parameters: {'n_estimators': 541, 'learning_rate': 0.09648456441722421, 'max_depth': 7, 'max_bin': 224, 'num_leaves': 215}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:37:19,260] Trial 81 finished with value: 0.6867745167687812 and parameters: {'n_estimators': 452, 'learning_rate': 0.08502535412728776, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 365}. Best is trial 61 with value: 0.6896458725399434.\n",
      "[I 2023-12-20 11:37:22,682] Trial 82 finished with value: 0.6927201522444425 and parameters: {'n_estimators': 543, 'learning_rate': 0.08167653493955698, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 285}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:37:25,893] Trial 83 finished with value: 0.687021631410232 and parameters: {'n_estimators': 410, 'learning_rate': 0.07955662278824387, 'max_depth': 9, 'max_bin': 219, 'num_leaves': 283}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:37:29,371] Trial 84 finished with value: 0.6885788839016744 and parameters: {'n_estimators': 338, 'learning_rate': 0.07176788445889029, 'max_depth': 11, 'max_bin': 242, 'num_leaves': 395}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:37:32,655] Trial 85 finished with value: 0.6830151172815204 and parameters: {'n_estimators': 331, 'learning_rate': 0.0924361413618984, 'max_depth': 11, 'max_bin': 241, 'num_leaves': 397}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:37:36,604] Trial 86 finished with value: 0.6892429964823504 and parameters: {'n_estimators': 484, 'learning_rate': 0.0696008342417503, 'max_depth': 11, 'max_bin': 217, 'num_leaves': 464}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:37:39,863] Trial 87 finished with value: 0.689223432278916 and parameters: {'n_estimators': 285, 'learning_rate': 0.0686644522404352, 'max_depth': 11, 'max_bin': 196, 'num_leaves': 426}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:37:42,940] Trial 88 finished with value: 0.6818517320849515 and parameters: {'n_estimators': 283, 'learning_rate': 0.0685329840611955, 'max_depth': 10, 'max_bin': 189, 'num_leaves': 489}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:37:45,308] Trial 89 finished with value: 0.6721093938132234 and parameters: {'n_estimators': 178, 'learning_rate': 0.06390605852113061, 'max_depth': 11, 'max_bin': 197, 'num_leaves': 455}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:37:48,031] Trial 90 finished with value: 0.6806206207734249 and parameters: {'n_estimators': 230, 'learning_rate': 0.06930195081549999, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 496}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:37:51,811] Trial 91 finished with value: 0.6877414139414729 and parameters: {'n_estimators': 573, 'learning_rate': 0.08060523741198442, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 456}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:37:55,036] Trial 92 finished with value: 0.6860011455306181 and parameters: {'n_estimators': 476, 'learning_rate': 0.08777621608165999, 'max_depth': 11, 'max_bin': 186, 'num_leaves': 431}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:37:58,574] Trial 93 finished with value: 0.6885099448628232 and parameters: {'n_estimators': 485, 'learning_rate': 0.10148245461332533, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 379}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:37:59,750] Trial 94 finished with value: 0.6508441251229956 and parameters: {'n_estimators': 59, 'learning_rate': 0.1031265117059956, 'max_depth': 11, 'max_bin': 193, 'num_leaves': 544}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:38:02,631] Trial 95 finished with value: 0.6871783988995954 and parameters: {'n_estimators': 284, 'learning_rate': 0.09613827643506116, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 378}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:38:06,173] Trial 96 finished with value: 0.6869289738616383 and parameters: {'n_estimators': 486, 'learning_rate': 0.0705966158597244, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 507}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:38:07,875] Trial 97 finished with value: 0.655692400172868 and parameters: {'n_estimators': 339, 'learning_rate': 0.09088009121366926, 'max_depth': 4, 'max_bin': 212, 'num_leaves': 404}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:38:11,546] Trial 98 finished with value: 0.6872653352095799 and parameters: {'n_estimators': 368, 'learning_rate': 0.08014344724650595, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 438}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:38:13,572] Trial 99 finished with value: 0.6655853890795402 and parameters: {'n_estimators': 411, 'learning_rate': 0.09730726846994708, 'max_depth': 5, 'max_bin': 206, 'num_leaves': 467}. Best is trial 82 with value: 0.6927201522444425.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6927\n",
      "\tBest params:\n",
      "\t\tn_estimators: 543\n",
      "\t\tlearning_rate: 0.08167653493955698\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 219\n",
      "\t\tnum_leaves: 285\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.715728    0.717255\n",
      "1                    TP  343.000000  332.000000\n",
      "2                    TN  170.000000  174.000000\n",
      "3                    FP   42.000000   56.000000\n",
      "4                    FN   40.000000   33.000000\n",
      "5              Accuracy    0.862185    0.850420\n",
      "6             Precision    0.890909    0.855670\n",
      "7           Sensitivity    0.895561    0.909589\n",
      "8           Specificity    0.801900    0.756500\n",
      "9              F1 score    0.893229    0.881806\n",
      "10  F1 score (weighted)    0.862038    0.848768\n",
      "11     F1 score (macro)    0.849458    0.839072\n",
      "12    Balanced Accuracy    0.848724    0.833055\n",
      "13                  MCC    0.698939    0.681014\n",
      "14                  NPV    0.809500    0.840600\n",
      "15              ROC_AUC    0.848724    0.833055\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_lgbm_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    " \n",
    "y_pred_lgbm_1_cat = np.where((y_pred_lgbm_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 11:38:17,291] Trial 100 finished with value: 0.6846247749829466 and parameters: {'n_estimators': 444, 'learning_rate': 0.07384881562637256, 'max_depth': 9, 'max_bin': 194, 'num_leaves': 568}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:38:20,203] Trial 101 finished with value: 0.6903778909279967 and parameters: {'n_estimators': 541, 'learning_rate': 0.0860459890566046, 'max_depth': 11, 'max_bin': 239, 'num_leaves': 381}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:38:23,230] Trial 102 finished with value: 0.6919691204688563 and parameters: {'n_estimators': 527, 'learning_rate': 0.08218383926063097, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 382}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:38:26,724] Trial 103 finished with value: 0.6887820518320004 and parameters: {'n_estimators': 619, 'learning_rate': 0.08313840895308452, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 418}. Best is trial 82 with value: 0.6927201522444425.\n",
      "[I 2023-12-20 11:38:30,317] Trial 104 finished with value: 0.6983516070923239 and parameters: {'n_estimators': 631, 'learning_rate': 0.0795989958826334, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 665}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:38:33,976] Trial 105 finished with value: 0.6920636107572845 and parameters: {'n_estimators': 639, 'learning_rate': 0.07793261918866325, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 623}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:38:37,528] Trial 106 finished with value: 0.696798912011072 and parameters: {'n_estimators': 599, 'learning_rate': 0.07720219004333266, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 677}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:38:41,282] Trial 107 finished with value: 0.6952744111102137 and parameters: {'n_estimators': 642, 'learning_rate': 0.06559912045650541, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 674}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:38:44,547] Trial 108 finished with value: 0.6919693680909433 and parameters: {'n_estimators': 636, 'learning_rate': 0.08945513505177947, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 676}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:38:48,075] Trial 109 finished with value: 0.6958870366609041 and parameters: {'n_estimators': 642, 'learning_rate': 0.08955882907865823, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 680}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:38:51,348] Trial 110 finished with value: 0.6950379181656785 and parameters: {'n_estimators': 641, 'learning_rate': 0.09066177831915538, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 673}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:38:55,145] Trial 111 finished with value: 0.6968373040348191 and parameters: {'n_estimators': 631, 'learning_rate': 0.08948963329086851, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 686}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:38:58,628] Trial 112 finished with value: 0.6945266340652199 and parameters: {'n_estimators': 637, 'learning_rate': 0.09318971785908726, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 678}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:39:01,969] Trial 113 finished with value: 0.6952971216058051 and parameters: {'n_estimators': 636, 'learning_rate': 0.09132040136318595, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 690}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:39:05,412] Trial 114 finished with value: 0.6971754496325606 and parameters: {'n_estimators': 637, 'learning_rate': 0.09270938163718773, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 682}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:39:09,090] Trial 115 finished with value: 0.6969870635435635 and parameters: {'n_estimators': 639, 'learning_rate': 0.0925373260119976, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 676}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:39:12,519] Trial 116 finished with value: 0.6959788542074172 and parameters: {'n_estimators': 608, 'learning_rate': 0.09356964181577528, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 683}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:39:15,654] Trial 117 finished with value: 0.6949939109595404 and parameters: {'n_estimators': 661, 'learning_rate': 0.10609809250587444, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 718}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:39:18,833] Trial 118 finished with value: 0.694266782878518 and parameters: {'n_estimators': 670, 'learning_rate': 0.10376064680305973, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 729}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:39:22,193] Trial 119 finished with value: 0.6941728895636452 and parameters: {'n_estimators': 605, 'learning_rate': 0.09519413675398442, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 674}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:39:25,274] Trial 120 finished with value: 0.6940606200007036 and parameters: {'n_estimators': 668, 'learning_rate': 0.10798673984977394, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 642}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:39:28,384] Trial 121 finished with value: 0.6922521166196477 and parameters: {'n_estimators': 654, 'learning_rate': 0.10460770288117102, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 743}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:39:31,560] Trial 122 finished with value: 0.6939745790291136 and parameters: {'n_estimators': 671, 'learning_rate': 0.09891428243560027, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 713}. Best is trial 104 with value: 0.6983516070923239.\n",
      "[I 2023-12-20 11:39:34,974] Trial 123 finished with value: 0.6990131690414316 and parameters: {'n_estimators': 578, 'learning_rate': 0.09274509958766028, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 719}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:39:38,233] Trial 124 finished with value: 0.6908321209422239 and parameters: {'n_estimators': 588, 'learning_rate': 0.0930095219554914, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 692}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:39:41,180] Trial 125 finished with value: 0.6889863879248969 and parameters: {'n_estimators': 631, 'learning_rate': 0.11209466702660574, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 658}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:39:44,226] Trial 126 finished with value: 0.6901662976222156 and parameters: {'n_estimators': 606, 'learning_rate': 0.0988160433151192, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 711}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:39:47,533] Trial 127 finished with value: 0.6950171909803713 and parameters: {'n_estimators': 651, 'learning_rate': 0.09111507219438748, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 695}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:39:51,233] Trial 128 finished with value: 0.6959631358650352 and parameters: {'n_estimators': 571, 'learning_rate': 0.08998546478684502, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 654}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:39:54,382] Trial 129 finished with value: 0.6955475210060079 and parameters: {'n_estimators': 568, 'learning_rate': 0.09203478443563841, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 647}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:39:57,961] Trial 130 finished with value: 0.6957918168134787 and parameters: {'n_estimators': 563, 'learning_rate': 0.08886166384291745, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 644}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:01,283] Trial 131 finished with value: 0.6913626720838761 and parameters: {'n_estimators': 564, 'learning_rate': 0.09510677906097616, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 642}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:04,969] Trial 132 finished with value: 0.6966840087472532 and parameters: {'n_estimators': 589, 'learning_rate': 0.08948825326094668, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 656}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:08,423] Trial 133 finished with value: 0.6972322439206006 and parameters: {'n_estimators': 587, 'learning_rate': 0.08830617533271448, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 618}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:11,645] Trial 134 finished with value: 0.6938178149587774 and parameters: {'n_estimators': 598, 'learning_rate': 0.09997790552396578, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 651}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:15,042] Trial 135 finished with value: 0.6962568664103939 and parameters: {'n_estimators': 576, 'learning_rate': 0.08854777013924718, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 633}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:18,273] Trial 136 finished with value: 0.6922244324311219 and parameters: {'n_estimators': 581, 'learning_rate': 0.08821943715965233, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 608}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:21,738] Trial 137 finished with value: 0.696300183683945 and parameters: {'n_estimators': 565, 'learning_rate': 0.09690126227724535, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 633}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:24,828] Trial 138 finished with value: 0.6936597093925888 and parameters: {'n_estimators': 616, 'learning_rate': 0.09786959163903586, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 627}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:28,553] Trial 139 finished with value: 0.695184160805867 and parameters: {'n_estimators': 557, 'learning_rate': 0.08311723240718423, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 606}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:31,782] Trial 140 finished with value: 0.6954269224332332 and parameters: {'n_estimators': 597, 'learning_rate': 0.10134046564946139, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 579}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:34,860] Trial 141 finished with value: 0.6912049111636863 and parameters: {'n_estimators': 571, 'learning_rate': 0.08834036824094738, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 657}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:38,260] Trial 142 finished with value: 0.6933063786824646 and parameters: {'n_estimators': 579, 'learning_rate': 0.09513366460010778, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 638}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:41,502] Trial 143 finished with value: 0.6914328177759493 and parameters: {'n_estimators': 698, 'learning_rate': 0.08779899743904962, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 707}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:45,045] Trial 144 finished with value: 0.6941956195190973 and parameters: {'n_estimators': 620, 'learning_rate': 0.09348776841497655, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 654}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:48,553] Trial 145 finished with value: 0.6951403078851215 and parameters: {'n_estimators': 555, 'learning_rate': 0.08478600901848905, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 732}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:51,495] Trial 146 finished with value: 0.6930957225387352 and parameters: {'n_estimators': 595, 'learning_rate': 0.09751504017332785, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 590}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:55,200] Trial 147 finished with value: 0.692074180391431 and parameters: {'n_estimators': 610, 'learning_rate': 0.08049739618167696, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 661}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:40:58,466] Trial 148 finished with value: 0.6934123335625436 and parameters: {'n_estimators': 564, 'learning_rate': 0.08942752032937464, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 634}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:00,862] Trial 149 finished with value: 0.6811615596498024 and parameters: {'n_estimators': 587, 'learning_rate': 0.10053963826554516, 'max_depth': 6, 'max_bin': 256, 'num_leaves': 620}. Best is trial 123 with value: 0.6990131690414316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6990\n",
      "\tBest params:\n",
      "\t\tn_estimators: 578\n",
      "\t\tlearning_rate: 0.09274509958766028\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 256\n",
      "\t\tnum_leaves: 719\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.715728    0.717255    0.694202\n",
      "1                    TP  343.000000  332.000000  333.000000\n",
      "2                    TN  170.000000  174.000000  172.000000\n",
      "3                    FP   42.000000   56.000000   50.000000\n",
      "4                    FN   40.000000   33.000000   40.000000\n",
      "5              Accuracy    0.862185    0.850420    0.848739\n",
      "6             Precision    0.890909    0.855670    0.869452\n",
      "7           Sensitivity    0.895561    0.909589    0.892761\n",
      "8           Specificity    0.801900    0.756500    0.774800\n",
      "9              F1 score    0.893229    0.881806    0.880952\n",
      "10  F1 score (weighted)    0.862038    0.848768    0.847997\n",
      "11     F1 score (macro)    0.849458    0.839072    0.836790\n",
      "12    Balanced Accuracy    0.848724    0.833055    0.833768\n",
      "13                  MCC    0.698939    0.681014    0.674122\n",
      "14                  NPV    0.809500    0.840600    0.811300\n",
      "15              ROC_AUC    0.848724    0.833055    0.833768\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_lgbm_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_2_cat = np.where((y_pred_lgbm_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 11:41:03,978] Trial 150 finished with value: 0.663944518106398 and parameters: {'n_estimators': 526, 'learning_rate': 0.11155550034384552, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 685}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:06,701] Trial 151 finished with value: 0.6646687650291163 and parameters: {'n_estimators': 606, 'learning_rate': 0.10268513738013528, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 598}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:09,625] Trial 152 finished with value: 0.6696828855486764 and parameters: {'n_estimators': 592, 'learning_rate': 0.09494445381680433, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 701}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:12,552] Trial 153 finished with value: 0.667758554401529 and parameters: {'n_estimators': 552, 'learning_rate': 0.09129835421869129, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 663}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:15,494] Trial 154 finished with value: 0.6664245296080646 and parameters: {'n_estimators': 685, 'learning_rate': 0.0866431191495072, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 572}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:18,475] Trial 155 finished with value: 0.6698685490410823 and parameters: {'n_estimators': 616, 'learning_rate': 0.10055088360829861, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 725}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:21,240] Trial 156 finished with value: 0.6679546062327018 and parameters: {'n_estimators': 577, 'learning_rate': 0.1065721879437459, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 644}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:24,118] Trial 157 finished with value: 0.6623849316277216 and parameters: {'n_estimators': 625, 'learning_rate': 0.08323097999578918, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 749}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:27,054] Trial 158 finished with value: 0.670329550647404 and parameters: {'n_estimators': 598, 'learning_rate': 0.09350135824637261, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 665}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:30,446] Trial 159 finished with value: 0.6690583498858297 and parameters: {'n_estimators': 565, 'learning_rate': 0.07763405004071233, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 581}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:33,485] Trial 160 finished with value: 0.6647505735193404 and parameters: {'n_estimators': 657, 'learning_rate': 0.08821372464677292, 'max_depth': 12, 'max_bin': 249, 'num_leaves': 688}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:36,332] Trial 161 finished with value: 0.6647397759673257 and parameters: {'n_estimators': 628, 'learning_rate': 0.09108406283382747, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 702}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:38,984] Trial 162 finished with value: 0.66807320345793 and parameters: {'n_estimators': 593, 'learning_rate': 0.09700995013679077, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 615}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:42,314] Trial 163 finished with value: 0.6726873546048651 and parameters: {'n_estimators': 619, 'learning_rate': 0.09208707086724668, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 629}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:45,136] Trial 164 finished with value: 0.667841445470201 and parameters: {'n_estimators': 581, 'learning_rate': 0.08432877621916376, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 685}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:47,831] Trial 165 finished with value: 0.6665062557593499 and parameters: {'n_estimators': 645, 'learning_rate': 0.1037839551507696, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 673}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:51,102] Trial 166 finished with value: 0.6646732161247509 and parameters: {'n_estimators': 547, 'learning_rate': 0.08132158272002511, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 695}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:53,833] Trial 167 finished with value: 0.6648757158292448 and parameters: {'n_estimators': 609, 'learning_rate': 0.09833818716383926, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 652}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:56,645] Trial 168 finished with value: 0.6653437410251891 and parameters: {'n_estimators': 632, 'learning_rate': 0.08960018468748213, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 631}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:41:59,646] Trial 169 finished with value: 0.6686555525505626 and parameters: {'n_estimators': 673, 'learning_rate': 0.09500102395414373, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 667}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:02,501] Trial 170 finished with value: 0.6647559113268182 and parameters: {'n_estimators': 516, 'learning_rate': 0.08613314741220947, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 647}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:05,752] Trial 171 finished with value: 0.6668603592107513 and parameters: {'n_estimators': 652, 'learning_rate': 0.07886190396178744, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 684}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:08,602] Trial 172 finished with value: 0.6662050702344936 and parameters: {'n_estimators': 699, 'learning_rate': 0.09143175621428357, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 716}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:12,066] Trial 173 finished with value: 0.6659607957609299 and parameters: {'n_estimators': 637, 'learning_rate': 0.07388267453960277, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 672}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:14,999] Trial 174 finished with value: 0.6642698048086246 and parameters: {'n_estimators': 572, 'learning_rate': 0.10109018248750146, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 702}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:18,120] Trial 175 finished with value: 0.6647612018202049 and parameters: {'n_estimators': 606, 'learning_rate': 0.08544148285729387, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 685}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:21,017] Trial 176 finished with value: 0.6677396130316441 and parameters: {'n_estimators': 649, 'learning_rate': 0.09515283153676439, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 616}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:23,868] Trial 177 finished with value: 0.6680047216038212 and parameters: {'n_estimators': 625, 'learning_rate': 0.08882675489337431, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 648}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:26,864] Trial 178 finished with value: 0.6602443660648141 and parameters: {'n_estimators': 596, 'learning_rate': 0.08110049696576112, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 664}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:29,744] Trial 179 finished with value: 0.6707806241863747 and parameters: {'n_estimators': 559, 'learning_rate': 0.09816212195098106, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 636}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:32,596] Trial 180 finished with value: 0.6699868782862388 and parameters: {'n_estimators': 684, 'learning_rate': 0.10609720633473163, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 677}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:35,364] Trial 181 finished with value: 0.6630950737648188 and parameters: {'n_estimators': 547, 'learning_rate': 0.08150379599452995, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 594}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:38,278] Trial 182 finished with value: 0.6688894103737438 and parameters: {'n_estimators': 587, 'learning_rate': 0.08419715478051502, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 603}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:41,244] Trial 183 finished with value: 0.6614655437970827 and parameters: {'n_estimators': 529, 'learning_rate': 0.07565930542134172, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 660}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:44,301] Trial 184 finished with value: 0.6715690798305813 and parameters: {'n_estimators': 562, 'learning_rate': 0.09253888336912731, 'max_depth': 12, 'max_bin': 163, 'num_leaves': 612}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:46,164] Trial 185 finished with value: 0.635499208031305 and parameters: {'n_estimators': 572, 'learning_rate': 0.08665038814643336, 'max_depth': 3, 'max_bin': 259, 'num_leaves': 559}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:48,836] Trial 186 finished with value: 0.6647679417490497 and parameters: {'n_estimators': 614, 'learning_rate': 0.089218061659579, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 736}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:51,396] Trial 187 finished with value: 0.6666597804503288 and parameters: {'n_estimators': 592, 'learning_rate': 0.0946253867447465, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 631}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:53,556] Trial 188 finished with value: 0.657943923103581 and parameters: {'n_estimators': 639, 'learning_rate': 0.08399805772649543, 'max_depth': 7, 'max_bin': 267, 'num_leaves': 693}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:42:56,520] Trial 189 finished with value: 0.661328241513571 and parameters: {'n_estimators': 664, 'learning_rate': 0.07774679939311581, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 716}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:00,184] Trial 190 finished with value: 0.6676279802730163 and parameters: {'n_estimators': 626, 'learning_rate': 0.0657294470148918, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 648}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:03,152] Trial 191 finished with value: 0.6642015262886233 and parameters: {'n_estimators': 559, 'learning_rate': 0.08541297742978884, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 721}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:06,037] Trial 192 finished with value: 0.666927667279213 and parameters: {'n_estimators': 555, 'learning_rate': 0.08964850497641792, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 733}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:09,066] Trial 193 finished with value: 0.6649769967035575 and parameters: {'n_estimators': 503, 'learning_rate': 0.0818653568563237, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 707}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:12,137] Trial 194 finished with value: 0.6701540540984816 and parameters: {'n_estimators': 581, 'learning_rate': 0.09455350357028752, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 675}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:15,074] Trial 195 finished with value: 0.6681852793725134 and parameters: {'n_estimators': 531, 'learning_rate': 0.09880291880441279, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 696}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:18,163] Trial 196 finished with value: 0.6675067665923541 and parameters: {'n_estimators': 606, 'learning_rate': 0.09087338378240961, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 660}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:21,148] Trial 197 finished with value: 0.6659537319520753 and parameters: {'n_estimators': 544, 'learning_rate': 0.07416288003409148, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 623}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:24,115] Trial 198 finished with value: 0.6692702532391224 and parameters: {'n_estimators': 584, 'learning_rate': 0.08783103882701342, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 644}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:26,991] Trial 199 finished with value: 0.6672200561287422 and parameters: {'n_estimators': 604, 'learning_rate': 0.08374424927976214, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 680}. Best is trial 123 with value: 0.6990131690414316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6990\n",
      "\tBest params:\n",
      "\t\tn_estimators: 578\n",
      "\t\tlearning_rate: 0.09274509958766028\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 256\n",
      "\t\tnum_leaves: 719\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.715728    0.717255    0.694202    0.738094\n",
      "1                    TP  343.000000  332.000000  333.000000  332.000000\n",
      "2                    TN  170.000000  174.000000  172.000000  185.000000\n",
      "3                    FP   42.000000   56.000000   50.000000   41.000000\n",
      "4                    FN   40.000000   33.000000   40.000000   37.000000\n",
      "5              Accuracy    0.862185    0.850420    0.848739    0.868908\n",
      "6             Precision    0.890909    0.855670    0.869452    0.890080\n",
      "7           Sensitivity    0.895561    0.909589    0.892761    0.899729\n",
      "8           Specificity    0.801900    0.756500    0.774800    0.818600\n",
      "9              F1 score    0.893229    0.881806    0.880952    0.894879\n",
      "10  F1 score (weighted)    0.862038    0.848768    0.847997    0.868676\n",
      "11     F1 score (macro)    0.849458    0.839072    0.836790    0.860386\n",
      "12    Balanced Accuracy    0.848724    0.833055    0.833768    0.859157\n",
      "13                  MCC    0.698939    0.681014    0.674122    0.720859\n",
      "14                  NPV    0.809500    0.840600    0.811300    0.833300\n",
      "15              ROC_AUC    0.848724    0.833055    0.833768    0.859157\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_lgbm_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "\n",
    "y_pred_lgbm_3_cat = np.where((y_pred_lgbm_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 11:43:30,971] Trial 200 finished with value: 0.691072073111395 and parameters: {'n_estimators': 564, 'learning_rate': 0.10144047923973257, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 606}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:34,747] Trial 201 finished with value: 0.6890588875215512 and parameters: {'n_estimators': 645, 'learning_rate': 0.09154924041975339, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 673}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:38,392] Trial 202 finished with value: 0.6911006016140896 and parameters: {'n_estimators': 633, 'learning_rate': 0.09419905538998007, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 667}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:42,126] Trial 203 finished with value: 0.6897836491984068 and parameters: {'n_estimators': 619, 'learning_rate': 0.08653936381600637, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 691}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:46,168] Trial 204 finished with value: 0.6924301657545417 and parameters: {'n_estimators': 658, 'learning_rate': 0.07975836518803343, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 653}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:49,445] Trial 205 finished with value: 0.6901326445951892 and parameters: {'n_estimators': 597, 'learning_rate': 0.097399698507294, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 639}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:52,880] Trial 206 finished with value: 0.6894410758010132 and parameters: {'n_estimators': 640, 'learning_rate': 0.09153475726610862, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 706}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:56,466] Trial 207 finished with value: 0.6900921685837123 and parameters: {'n_estimators': 577, 'learning_rate': 0.08878333069419461, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 731}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:43:59,832] Trial 208 finished with value: 0.6849169748207813 and parameters: {'n_estimators': 617, 'learning_rate': 0.08300331265986281, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 666}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:04,236] Trial 209 finished with value: 0.6915426895969244 and parameters: {'n_estimators': 595, 'learning_rate': 0.06113700729896038, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 684}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:07,416] Trial 210 finished with value: 0.6892661987558257 and parameters: {'n_estimators': 552, 'learning_rate': 0.09422102915395521, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 619}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:10,838] Trial 211 finished with value: 0.6889069024910516 and parameters: {'n_estimators': 649, 'learning_rate': 0.09134462138958158, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 695}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:13,961] Trial 212 finished with value: 0.6820630913829818 and parameters: {'n_estimators': 670, 'learning_rate': 0.08861763254778661, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 700}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:15,838] Trial 213 finished with value: 0.6751234812322925 and parameters: {'n_estimators': 112, 'learning_rate': 0.09743048371087919, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 681}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:19,286] Trial 214 finished with value: 0.6876162755752386 and parameters: {'n_estimators': 626, 'learning_rate': 0.08520589934403583, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 715}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:22,473] Trial 215 finished with value: 0.6876090689331606 and parameters: {'n_estimators': 651, 'learning_rate': 0.103520742979846, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 749}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:25,438] Trial 216 finished with value: 0.6845974672341981 and parameters: {'n_estimators': 572, 'learning_rate': 0.09137712141392043, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 653}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:28,948] Trial 217 finished with value: 0.6884910280971488 and parameters: {'n_estimators': 687, 'learning_rate': 0.07837857733158286, 'max_depth': 12, 'max_bin': 151, 'num_leaves': 633}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:32,605] Trial 218 finished with value: 0.6937006649034594 and parameters: {'n_estimators': 606, 'learning_rate': 0.10008333658914305, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 667}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:35,077] Trial 219 finished with value: 0.6839890775231752 and parameters: {'n_estimators': 630, 'learning_rate': 0.18385941884582524, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 584}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:39,022] Trial 220 finished with value: 0.6873177902926626 and parameters: {'n_estimators': 585, 'learning_rate': 0.08669372498869006, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 692}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:42,555] Trial 221 finished with value: 0.6890034996396479 and parameters: {'n_estimators': 652, 'learning_rate': 0.09541166315964048, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 724}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:45,639] Trial 222 finished with value: 0.6855296688587045 and parameters: {'n_estimators': 672, 'learning_rate': 0.10672810802923093, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 710}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:49,909] Trial 223 finished with value: 0.6906217041261328 and parameters: {'n_estimators': 662, 'learning_rate': 0.05407330345400929, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 677}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:53,117] Trial 224 finished with value: 0.6880286534063508 and parameters: {'n_estimators': 640, 'learning_rate': 0.09181256491142356, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 650}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:44:56,094] Trial 225 finished with value: 0.6882187893780769 and parameters: {'n_estimators': 614, 'learning_rate': 0.11106900216451773, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 719}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:00,150] Trial 226 finished with value: 0.6903572838817762 and parameters: {'n_estimators': 599, 'learning_rate': 0.0725897129980212, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 695}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:04,179] Trial 227 finished with value: 0.6905611358351245 and parameters: {'n_estimators': 711, 'learning_rate': 0.08294833981877757, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 662}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:07,578] Trial 228 finished with value: 0.6877706252237727 and parameters: {'n_estimators': 550, 'learning_rate': 0.10339829039320221, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 632}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:10,977] Trial 229 finished with value: 0.6871170746142753 and parameters: {'n_estimators': 571, 'learning_rate': 0.09667622855994604, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 683}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:14,670] Trial 230 finished with value: 0.6898448394877418 and parameters: {'n_estimators': 657, 'learning_rate': 0.08950598782518675, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 735}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:18,027] Trial 231 finished with value: 0.6930249931555166 and parameters: {'n_estimators': 634, 'learning_rate': 0.09447845847311259, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 672}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:21,589] Trial 232 finished with value: 0.6898673615990646 and parameters: {'n_estimators': 621, 'learning_rate': 0.09169059952634023, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 699}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:25,138] Trial 233 finished with value: 0.6849639139288016 and parameters: {'n_estimators': 641, 'learning_rate': 0.08659167002815711, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 657}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:28,418] Trial 234 finished with value: 0.6918184922124773 and parameters: {'n_estimators': 683, 'learning_rate': 0.10014957000989763, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 679}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:32,196] Trial 235 finished with value: 0.6927326027603853 and parameters: {'n_estimators': 586, 'learning_rate': 0.09406581357212598, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 642}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:35,806] Trial 236 finished with value: 0.6872035486623305 and parameters: {'n_estimators': 610, 'learning_rate': 0.08852787801272392, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 708}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:39,705] Trial 237 finished with value: 0.6936890805618241 and parameters: {'n_estimators': 660, 'learning_rate': 0.08197689823961364, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 604}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:42,869] Trial 238 finished with value: 0.6885674545791062 and parameters: {'n_estimators': 631, 'learning_rate': 0.09850897834174023, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 671}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:45,937] Trial 239 finished with value: 0.6875277437966153 and parameters: {'n_estimators': 560, 'learning_rate': 0.1087199204002833, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 688}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:47,993] Trial 240 finished with value: 0.6636950582426377 and parameters: {'n_estimators': 530, 'learning_rate': 0.09200205405837207, 'max_depth': 4, 'max_bin': 260, 'num_leaves': 623}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:51,037] Trial 241 finished with value: 0.6895407962888491 and parameters: {'n_estimators': 669, 'learning_rate': 0.11402797744994972, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 723}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:53,938] Trial 242 finished with value: 0.6864759723779514 and parameters: {'n_estimators': 682, 'learning_rate': 0.10475355372335389, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 738}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:45:57,728] Trial 243 finished with value: 0.6876898837690364 and parameters: {'n_estimators': 643, 'learning_rate': 0.0859958485140025, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 729}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:01,085] Trial 244 finished with value: 0.6898934827089261 and parameters: {'n_estimators': 605, 'learning_rate': 0.10301399838650561, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 707}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:04,742] Trial 245 finished with value: 0.6896463901360479 and parameters: {'n_estimators': 586, 'learning_rate': 0.09511989522308463, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 657}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:08,752] Trial 246 finished with value: 0.6897008769869063 and parameters: {'n_estimators': 659, 'learning_rate': 0.06309241039439306, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 690}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:11,880] Trial 247 finished with value: 0.6861602660803979 and parameters: {'n_estimators': 625, 'learning_rate': 0.1177061738228646, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 641}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:15,535] Trial 248 finished with value: 0.6877920588343562 and parameters: {'n_estimators': 571, 'learning_rate': 0.08925904259753203, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 670}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:19,184] Trial 249 finished with value: 0.6898930745844346 and parameters: {'n_estimators': 641, 'learning_rate': 0.09951728629884644, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 712}. Best is trial 123 with value: 0.6990131690414316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6990\n",
      "\tBest params:\n",
      "\t\tn_estimators: 578\n",
      "\t\tlearning_rate: 0.09274509958766028\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 256\n",
      "\t\tnum_leaves: 719\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.715728    0.717255    0.694202    0.738094   \n",
      "1                    TP  343.000000  332.000000  333.000000  332.000000   \n",
      "2                    TN  170.000000  174.000000  172.000000  185.000000   \n",
      "3                    FP   42.000000   56.000000   50.000000   41.000000   \n",
      "4                    FN   40.000000   33.000000   40.000000   37.000000   \n",
      "5              Accuracy    0.862185    0.850420    0.848739    0.868908   \n",
      "6             Precision    0.890909    0.855670    0.869452    0.890080   \n",
      "7           Sensitivity    0.895561    0.909589    0.892761    0.899729   \n",
      "8           Specificity    0.801900    0.756500    0.774800    0.818600   \n",
      "9              F1 score    0.893229    0.881806    0.880952    0.894879   \n",
      "10  F1 score (weighted)    0.862038    0.848768    0.847997    0.868676   \n",
      "11     F1 score (macro)    0.849458    0.839072    0.836790    0.860386   \n",
      "12    Balanced Accuracy    0.848724    0.833055    0.833768    0.859157   \n",
      "13                  MCC    0.698939    0.681014    0.674122    0.720859   \n",
      "14                  NPV    0.809500    0.840600    0.811300    0.833300   \n",
      "15              ROC_AUC    0.848724    0.833055    0.833768    0.859157   \n",
      "\n",
      "          Set4  \n",
      "0     0.681368  \n",
      "1   333.000000  \n",
      "2   169.000000  \n",
      "3    62.000000  \n",
      "4    31.000000  \n",
      "5     0.843697  \n",
      "6     0.843038  \n",
      "7     0.914835  \n",
      "8     0.731600  \n",
      "9     0.877470  \n",
      "10    0.841268  \n",
      "11    0.830847  \n",
      "12    0.823218  \n",
      "13    0.666913  \n",
      "14    0.845000  \n",
      "15    0.823218  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_lgbm_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    " \n",
    "y_pred_lgbm_4_cat = np.where((y_pred_lgbm_4 >= 6.6) , 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 11:46:23,491] Trial 250 finished with value: 0.6822268252339306 and parameters: {'n_estimators': 598, 'learning_rate': 0.078878118178478, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 680}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:26,583] Trial 251 finished with value: 0.6809138456717088 and parameters: {'n_estimators': 618, 'learning_rate': 0.0841280068692746, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 654}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:29,877] Trial 252 finished with value: 0.6829395628025832 and parameters: {'n_estimators': 671, 'learning_rate': 0.0919171046334233, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 750}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:32,914] Trial 253 finished with value: 0.6821002405869397 and parameters: {'n_estimators': 547, 'learning_rate': 0.09581198068805412, 'max_depth': 12, 'max_bin': 176, 'num_leaves': 699}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:36,309] Trial 254 finished with value: 0.680558802080991 and parameters: {'n_estimators': 698, 'learning_rate': 0.08853252093093474, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 614}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:39,985] Trial 255 finished with value: 0.6785130819163777 and parameters: {'n_estimators': 650, 'learning_rate': 0.06899191695637333, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 723}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:42,974] Trial 256 finished with value: 0.6783179681198697 and parameters: {'n_estimators': 580, 'learning_rate': 0.10900473699148813, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 665}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:45,823] Trial 257 finished with value: 0.6845187183691197 and parameters: {'n_estimators': 626, 'learning_rate': 0.13379547509068052, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 636}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:48,670] Trial 258 finished with value: 0.6838956850966492 and parameters: {'n_estimators': 564, 'learning_rate': 0.1494543081861205, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 687}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:51,890] Trial 259 finished with value: 0.6853337362961236 and parameters: {'n_estimators': 591, 'learning_rate': 0.10254071353043666, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 654}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:55,612] Trial 260 finished with value: 0.679530472028316 and parameters: {'n_estimators': 609, 'learning_rate': 0.07640314550532654, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 677}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:46:58,674] Trial 261 finished with value: 0.6776239079647619 and parameters: {'n_estimators': 642, 'learning_rate': 0.08291804591824728, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 699}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:01,854] Trial 262 finished with value: 0.687116760827118 and parameters: {'n_estimators': 538, 'learning_rate': 0.09293613049998398, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 644}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:04,932] Trial 263 finished with value: 0.6822698374228657 and parameters: {'n_estimators': 660, 'learning_rate': 0.0973537430548497, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 667}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:07,932] Trial 264 finished with value: 0.6789203822642461 and parameters: {'n_estimators': 609, 'learning_rate': 0.08694416875640179, 'max_depth': 8, 'max_bin': 260, 'num_leaves': 720}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:11,262] Trial 265 finished with value: 0.6827444571562322 and parameters: {'n_estimators': 631, 'learning_rate': 0.09045802368411669, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 630}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:14,015] Trial 266 finished with value: 0.6783388255086037 and parameters: {'n_estimators': 591, 'learning_rate': 0.0976467070603037, 'max_depth': 11, 'max_bin': 263, 'num_leaves': 685}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:16,893] Trial 267 finished with value: 0.684882588455411 and parameters: {'n_estimators': 566, 'learning_rate': 0.10609019185860762, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 600}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:20,532] Trial 268 finished with value: 0.6838174502690525 and parameters: {'n_estimators': 674, 'learning_rate': 0.07991345568088919, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 709}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:23,780] Trial 269 finished with value: 0.6811944900127984 and parameters: {'n_estimators': 624, 'learning_rate': 0.08629733420474461, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 622}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:27,027] Trial 270 finished with value: 0.681474694229248 and parameters: {'n_estimators': 578, 'learning_rate': 0.09246825759186693, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 664}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:30,299] Trial 271 finished with value: 0.6797226607828266 and parameters: {'n_estimators': 651, 'learning_rate': 0.1003605317609695, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 697}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:32,983] Trial 272 finished with value: 0.6729363784436335 and parameters: {'n_estimators': 602, 'learning_rate': 0.08317083297714384, 'max_depth': 6, 'max_bin': 264, 'num_leaves': 730}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:38,529] Trial 273 finished with value: 0.6776727703288181 and parameters: {'n_estimators': 548, 'learning_rate': 0.030995862412370345, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 652}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:42,636] Trial 274 finished with value: 0.68220824474055 and parameters: {'n_estimators': 519, 'learning_rate': 0.0725188350656437, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 677}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:46,727] Trial 275 finished with value: 0.6803477757075916 and parameters: {'n_estimators': 691, 'learning_rate': 0.06633996478386134, 'max_depth': 11, 'max_bin': 262, 'num_leaves': 641}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:49,913] Trial 276 finished with value: 0.6845196347269693 and parameters: {'n_estimators': 641, 'learning_rate': 0.09497404473342712, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 686}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:53,256] Trial 277 finished with value: 0.6821632466684557 and parameters: {'n_estimators': 613, 'learning_rate': 0.08979199555912415, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 706}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:47:56,844] Trial 278 finished with value: 0.6780286798575129 and parameters: {'n_estimators': 563, 'learning_rate': 0.08759796906835449, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 736}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:06,227] Trial 279 finished with value: 0.6508045836852969 and parameters: {'n_estimators': 582, 'learning_rate': 0.006457329144772317, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 667}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:09,752] Trial 280 finished with value: 0.6800316848095166 and parameters: {'n_estimators': 631, 'learning_rate': 0.07764801010435758, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 613}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:14,343] Trial 281 finished with value: 0.6841162639500874 and parameters: {'n_estimators': 657, 'learning_rate': 0.05054524390254558, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 693}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:17,847] Trial 282 finished with value: 0.6845290244137623 and parameters: {'n_estimators': 595, 'learning_rate': 0.10228541028547285, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 651}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:21,685] Trial 283 finished with value: 0.683354590308081 and parameters: {'n_estimators': 617, 'learning_rate': 0.09405046176818478, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 674}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:25,776] Trial 284 finished with value: 0.6842105243099403 and parameters: {'n_estimators': 666, 'learning_rate': 0.05793067206691594, 'max_depth': 11, 'max_bin': 270, 'num_leaves': 715}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:29,512] Trial 285 finished with value: 0.6850762905570964 and parameters: {'n_estimators': 555, 'learning_rate': 0.08438381991883444, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 586}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:32,886] Trial 286 finished with value: 0.6848450484325129 and parameters: {'n_estimators': 642, 'learning_rate': 0.09826232263695156, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 626}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:36,204] Trial 287 finished with value: 0.681207061179348 and parameters: {'n_estimators': 572, 'learning_rate': 0.09021675947164846, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 659}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:39,163] Trial 288 finished with value: 0.6804370954513479 and parameters: {'n_estimators': 599, 'learning_rate': 0.10686260947624787, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 641}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:42,462] Trial 289 finished with value: 0.6829554650874564 and parameters: {'n_estimators': 679, 'learning_rate': 0.08106143305411714, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 518}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:45,768] Trial 290 finished with value: 0.6846041519513297 and parameters: {'n_estimators': 619, 'learning_rate': 0.0928445979358179, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 683}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:48,207] Trial 291 finished with value: 0.6824037878269595 and parameters: {'n_estimators': 581, 'learning_rate': 0.11500908973036475, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 700}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:51,440] Trial 292 finished with value: 0.6809439356346629 and parameters: {'n_estimators': 539, 'learning_rate': 0.08607208646399545, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 554}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:54,201] Trial 293 finished with value: 0.6798300071222783 and parameters: {'n_estimators': 640, 'learning_rate': 0.09689127326048871, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 722}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:48:57,263] Trial 294 finished with value: 0.6821600693467218 and parameters: {'n_estimators': 709, 'learning_rate': 0.08937778629126973, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 673}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:00,952] Trial 295 finished with value: 0.6791029693683183 and parameters: {'n_estimators': 599, 'learning_rate': 0.07255500325927244, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 738}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:04,018] Trial 296 finished with value: 0.6821557154244384 and parameters: {'n_estimators': 654, 'learning_rate': 0.1046104249942219, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 660}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:11,491] Trial 297 finished with value: 0.6721077633470245 and parameters: {'n_estimators': 620, 'learning_rate': 0.01415812070881918, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 633}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:14,204] Trial 298 finished with value: 0.6797007840767526 and parameters: {'n_estimators': 560, 'learning_rate': 0.12082427761487761, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 692}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:17,139] Trial 299 finished with value: 0.6842695174367899 and parameters: {'n_estimators': 631, 'learning_rate': 0.11005056190838615, 'max_depth': 11, 'max_bin': 260, 'num_leaves': 708}. Best is trial 123 with value: 0.6990131690414316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6990\n",
      "\tBest params:\n",
      "\t\tn_estimators: 578\n",
      "\t\tlearning_rate: 0.09274509958766028\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 256\n",
      "\t\tnum_leaves: 719\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.715728    0.717255    0.694202    0.738094   \n",
      "1                    TP  343.000000  332.000000  333.000000  332.000000   \n",
      "2                    TN  170.000000  174.000000  172.000000  185.000000   \n",
      "3                    FP   42.000000   56.000000   50.000000   41.000000   \n",
      "4                    FN   40.000000   33.000000   40.000000   37.000000   \n",
      "5              Accuracy    0.862185    0.850420    0.848739    0.868908   \n",
      "6             Precision    0.890909    0.855670    0.869452    0.890080   \n",
      "7           Sensitivity    0.895561    0.909589    0.892761    0.899729   \n",
      "8           Specificity    0.801900    0.756500    0.774800    0.818600   \n",
      "9              F1 score    0.893229    0.881806    0.880952    0.894879   \n",
      "10  F1 score (weighted)    0.862038    0.848768    0.847997    0.868676   \n",
      "11     F1 score (macro)    0.849458    0.839072    0.836790    0.860386   \n",
      "12    Balanced Accuracy    0.848724    0.833055    0.833768    0.859157   \n",
      "13                  MCC    0.698939    0.681014    0.674122    0.720859   \n",
      "14                  NPV    0.809500    0.840600    0.811300    0.833300   \n",
      "15              ROC_AUC    0.848724    0.833055    0.833768    0.859157   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.681368    0.706702  \n",
      "1   333.000000  338.000000  \n",
      "2   169.000000  178.000000  \n",
      "3    62.000000   40.000000  \n",
      "4    31.000000   39.000000  \n",
      "5     0.843697    0.867227  \n",
      "6     0.843038    0.894180  \n",
      "7     0.914835    0.896552  \n",
      "8     0.731600    0.816500  \n",
      "9     0.877470    0.895364  \n",
      "10    0.841268    0.867162  \n",
      "11    0.830847    0.856878  \n",
      "12    0.823218    0.856533  \n",
      "13    0.666913    0.713761  \n",
      "14    0.845000    0.820300  \n",
      "15    0.823218    0.856533  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_lgbm_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_5_cat = np.where((y_pred_lgbm_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 11:49:20,879] Trial 300 finished with value: 0.6883296401023598 and parameters: {'n_estimators': 502, 'learning_rate': 0.08054682673102434, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 650}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:24,676] Trial 301 finished with value: 0.6916068490943303 and parameters: {'n_estimators': 587, 'learning_rate': 0.0935064448046914, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 613}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:28,231] Trial 302 finished with value: 0.6886740247520062 and parameters: {'n_estimators': 607, 'learning_rate': 0.10087273878678085, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 679}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:30,544] Trial 303 finished with value: 0.6703305291309063 and parameters: {'n_estimators': 576, 'learning_rate': 0.08675880299396951, 'max_depth': 5, 'max_bin': 271, 'num_leaves': 662}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:34,305] Trial 304 finished with value: 0.6869387623556134 and parameters: {'n_estimators': 739, 'learning_rate': 0.07629236932033637, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 691}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:37,851] Trial 305 finished with value: 0.6891515570955804 and parameters: {'n_estimators': 648, 'learning_rate': 0.0836181595455532, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 718}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:41,091] Trial 306 finished with value: 0.6892291680028182 and parameters: {'n_estimators': 669, 'learning_rate': 0.09557125217913472, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 673}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:44,057] Trial 307 finished with value: 0.6867607580319308 and parameters: {'n_estimators': 524, 'learning_rate': 0.09112143139690894, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 750}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:47,513] Trial 308 finished with value: 0.692629710817143 and parameters: {'n_estimators': 551, 'learning_rate': 0.09899540628770301, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 599}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:50,837] Trial 309 finished with value: 0.6790483015375814 and parameters: {'n_estimators': 632, 'learning_rate': 0.06705634040368419, 'max_depth': 7, 'max_bin': 254, 'num_leaves': 643}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:53,832] Trial 310 finished with value: 0.6871854318711919 and parameters: {'n_estimators': 596, 'learning_rate': 0.12395333109101911, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 626}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:49:57,292] Trial 311 finished with value: 0.687412669667697 and parameters: {'n_estimators': 621, 'learning_rate': 0.08836931452281328, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 698}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:00,846] Trial 312 finished with value: 0.6910224560484195 and parameters: {'n_estimators': 683, 'learning_rate': 0.08242904791265991, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 662}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:04,199] Trial 313 finished with value: 0.6877195331078821 and parameters: {'n_estimators': 564, 'learning_rate': 0.09393852397611811, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 730}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:07,253] Trial 314 finished with value: 0.6847543616978534 and parameters: {'n_estimators': 655, 'learning_rate': 0.10186821391071442, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 680}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:10,767] Trial 315 finished with value: 0.6888614140928102 and parameters: {'n_estimators': 608, 'learning_rate': 0.08618810744070857, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 646}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:14,717] Trial 316 finished with value: 0.6901600208234797 and parameters: {'n_estimators': 582, 'learning_rate': 0.09092482034318379, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 702}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:18,610] Trial 317 finished with value: 0.6878595188022947 and parameters: {'n_estimators': 538, 'learning_rate': 0.07821930321460273, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 571}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:22,025] Trial 318 finished with value: 0.6890996917970863 and parameters: {'n_estimators': 637, 'learning_rate': 0.09557179020030783, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 712}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:25,242] Trial 319 finished with value: 0.6848798583881183 and parameters: {'n_estimators': 670, 'learning_rate': 0.10761742894154902, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 684}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:29,590] Trial 320 finished with value: 0.6937496430020971 and parameters: {'n_estimators': 613, 'learning_rate': 0.06319576429243387, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 631}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:33,214] Trial 321 finished with value: 0.6888786163527467 and parameters: {'n_estimators': 572, 'learning_rate': 0.0845967032310008, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 667}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:36,793] Trial 322 finished with value: 0.6883114947710227 and parameters: {'n_estimators': 596, 'learning_rate': 0.08986021208439704, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 651}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:41,249] Trial 323 finished with value: 0.6875361470444593 and parameters: {'n_estimators': 659, 'learning_rate': 0.05589718663155536, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 618}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:44,529] Trial 324 finished with value: 0.6857981142790953 and parameters: {'n_estimators': 694, 'learning_rate': 0.09834400356165407, 'max_depth': 11, 'max_bin': 262, 'num_leaves': 689}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:49,786] Trial 325 finished with value: 0.6819526508041367 and parameters: {'n_estimators': 634, 'learning_rate': 0.036383572841170976, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 728}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:51,867] Trial 326 finished with value: 0.6790185957487819 and parameters: {'n_estimators': 556, 'learning_rate': 0.18844557775024484, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 671}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:50:55,606] Trial 327 finished with value: 0.6896549024531696 and parameters: {'n_estimators': 615, 'learning_rate': 0.07260472784189508, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 708}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:02,626] Trial 328 finished with value: 0.6783789199405702 and parameters: {'n_estimators': 577, 'learning_rate': 0.02025522781157675, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 639}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:06,361] Trial 329 finished with value: 0.6898056756038169 and parameters: {'n_estimators': 642, 'learning_rate': 0.08091911998025854, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 659}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:09,430] Trial 330 finished with value: 0.68400376301709 and parameters: {'n_estimators': 595, 'learning_rate': 0.0934398022180019, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 599}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:13,651] Trial 331 finished with value: 0.6871178999646969 and parameters: {'n_estimators': 629, 'learning_rate': 0.051632587690327685, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 681}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:17,238] Trial 332 finished with value: 0.6852653559002613 and parameters: {'n_estimators': 663, 'learning_rate': 0.08788115090106863, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 698}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:20,346] Trial 333 finished with value: 0.6886348605559396 and parameters: {'n_estimators': 590, 'learning_rate': 0.10303963075732124, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 719}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:24,310] Trial 334 finished with value: 0.6906697758358733 and parameters: {'n_estimators': 544, 'learning_rate': 0.07572694088256413, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 654}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:27,597] Trial 335 finished with value: 0.6878926109700224 and parameters: {'n_estimators': 613, 'learning_rate': 0.09652184443627063, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 671}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:31,566] Trial 336 finished with value: 0.6896876638840947 and parameters: {'n_estimators': 647, 'learning_rate': 0.09183245557867444, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 627}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:35,110] Trial 337 finished with value: 0.6915278708090773 and parameters: {'n_estimators': 567, 'learning_rate': 0.08423375610014826, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 690}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:38,254] Trial 338 finished with value: 0.6913724320822293 and parameters: {'n_estimators': 623, 'learning_rate': 0.09998957085289015, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 649}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:42,631] Trial 339 finished with value: 0.6917720196410339 and parameters: {'n_estimators': 682, 'learning_rate': 0.06229277681509365, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 732}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:45,975] Trial 340 finished with value: 0.6862321084341044 and parameters: {'n_estimators': 528, 'learning_rate': 0.08849736436940735, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 608}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:49,873] Trial 341 finished with value: 0.6880591529599989 and parameters: {'n_estimators': 604, 'learning_rate': 0.06872326482045077, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 675}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:52,660] Trial 342 finished with value: 0.6817740380932814 and parameters: {'n_estimators': 586, 'learning_rate': 0.11044516939359818, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 706}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:51:56,751] Trial 343 finished with value: 0.6941819914615999 and parameters: {'n_estimators': 644, 'learning_rate': 0.0790037106328685, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 640}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:52:00,208] Trial 344 finished with value: 0.6843953497692745 and parameters: {'n_estimators': 553, 'learning_rate': 0.10501942810265283, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 664}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:52:03,664] Trial 345 finished with value: 0.6865577067844748 and parameters: {'n_estimators': 661, 'learning_rate': 0.09241061436308756, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 588}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:52:07,218] Trial 346 finished with value: 0.6918716728313139 and parameters: {'n_estimators': 629, 'learning_rate': 0.08597876924566157, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 740}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:52:10,705] Trial 347 finished with value: 0.6887882808727764 and parameters: {'n_estimators': 572, 'learning_rate': 0.09641835394995416, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 481}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:52:13,147] Trial 348 finished with value: 0.6320853794923036 and parameters: {'n_estimators': 607, 'learning_rate': 0.04242115533682901, 'max_depth': 3, 'max_bin': 272, 'num_leaves': 686}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:52:18,103] Trial 349 finished with value: 0.6883045780960776 and parameters: {'n_estimators': 673, 'learning_rate': 0.05920166966417506, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 715}. Best is trial 123 with value: 0.6990131690414316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.699013\n",
      "\tBest params:\n",
      "\t\tn_estimators: 578\n",
      "\t\tlearning_rate: 0.09274509958766028\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 256\n",
      "\t\tnum_leaves: 719\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.715728    0.717255    0.694202    0.738094   \n",
      "1                    TP  343.000000  332.000000  333.000000  332.000000   \n",
      "2                    TN  170.000000  174.000000  172.000000  185.000000   \n",
      "3                    FP   42.000000   56.000000   50.000000   41.000000   \n",
      "4                    FN   40.000000   33.000000   40.000000   37.000000   \n",
      "5              Accuracy    0.862185    0.850420    0.848739    0.868908   \n",
      "6             Precision    0.890909    0.855670    0.869452    0.890080   \n",
      "7           Sensitivity    0.895561    0.909589    0.892761    0.899729   \n",
      "8           Specificity    0.801900    0.756500    0.774800    0.818600   \n",
      "9              F1 score    0.893229    0.881806    0.880952    0.894879   \n",
      "10  F1 score (weighted)    0.862038    0.848768    0.847997    0.868676   \n",
      "11     F1 score (macro)    0.849458    0.839072    0.836790    0.860386   \n",
      "12    Balanced Accuracy    0.848724    0.833055    0.833768    0.859157   \n",
      "13                  MCC    0.698939    0.681014    0.674122    0.720859   \n",
      "14                  NPV    0.809500    0.840600    0.811300    0.833300   \n",
      "15              ROC_AUC    0.848724    0.833055    0.833768    0.859157   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.681368    0.706702    0.694797  \n",
      "1   333.000000  338.000000  327.000000  \n",
      "2   169.000000  178.000000  173.000000  \n",
      "3    62.000000   40.000000   55.000000  \n",
      "4    31.000000   39.000000   40.000000  \n",
      "5     0.843697    0.867227    0.840336  \n",
      "6     0.843038    0.894180    0.856021  \n",
      "7     0.914835    0.896552    0.891008  \n",
      "8     0.731600    0.816500    0.758800  \n",
      "9     0.877470    0.895364    0.873164  \n",
      "10    0.841268    0.867162    0.839220  \n",
      "11    0.830847    0.856878    0.828872  \n",
      "12    0.823218    0.856533    0.824890  \n",
      "13    0.666913    0.713761    0.658939  \n",
      "14    0.845000    0.820300    0.812200  \n",
      "15    0.823218    0.856533    0.824890  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_lgbm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_6_cat = np.where((y_pred_lgbm_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 11:52:22,537] Trial 350 finished with value: 0.6929187288633326 and parameters: {'n_estimators': 708, 'learning_rate': 0.0897032325173672, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 697}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:52:25,133] Trial 351 finished with value: 0.6902423176981625 and parameters: {'n_estimators': 589, 'learning_rate': 0.15122229354451677, 'max_depth': 11, 'max_bin': 264, 'num_leaves': 661}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:52:28,790] Trial 352 finished with value: 0.6898110095708481 and parameters: {'n_estimators': 625, 'learning_rate': 0.08233050086896285, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 621}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:52:33,874] Trial 353 finished with value: 0.6914141020091783 and parameters: {'n_estimators': 651, 'learning_rate': 0.045958962699042885, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 678}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:52:37,403] Trial 354 finished with value: 0.6953109069929992 and parameters: {'n_estimators': 604, 'learning_rate': 0.10003770487723802, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 636}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:52:41,239] Trial 355 finished with value: 0.6949535793347323 and parameters: {'n_estimators': 605, 'learning_rate': 0.09529718837652941, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 642}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:52:45,507] Trial 356 finished with value: 0.6983222409006289 and parameters: {'n_estimators': 594, 'learning_rate': 0.09982850297948397, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 633}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:52:48,963] Trial 357 finished with value: 0.6916941076353146 and parameters: {'n_estimators': 576, 'learning_rate': 0.10054439181183097, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 609}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:52:57,725] Trial 358 finished with value: 0.5368574639693773 and parameters: {'n_estimators': 563, 'learning_rate': 0.0023074119298192725, 'max_depth': 9, 'max_bin': 249, 'num_leaves': 630}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:01,207] Trial 359 finished with value: 0.6943321489054032 and parameters: {'n_estimators': 589, 'learning_rate': 0.09979759669839208, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 624}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:04,938] Trial 360 finished with value: 0.6946138157057524 and parameters: {'n_estimators': 537, 'learning_rate': 0.10524571853262835, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 637}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:09,301] Trial 361 finished with value: 0.6896988579974725 and parameters: {'n_estimators': 600, 'learning_rate': 0.08528852143402804, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 650}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:13,367] Trial 362 finished with value: 0.6932044795207508 and parameters: {'n_estimators': 556, 'learning_rate': 0.07598424023602038, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 602}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:16,644] Trial 363 finished with value: 0.6843010406098569 and parameters: {'n_estimators': 581, 'learning_rate': 0.09268022361858609, 'max_depth': 11, 'max_bin': 239, 'num_leaves': 613}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:20,471] Trial 364 finished with value: 0.694503248872325 and parameters: {'n_estimators': 610, 'learning_rate': 0.09693524929537997, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 637}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:24,146] Trial 365 finished with value: 0.6906302046978219 and parameters: {'n_estimators': 598, 'learning_rate': 0.08934919591109339, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 659}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:28,428] Trial 366 finished with value: 0.6938224482643933 and parameters: {'n_estimators': 570, 'learning_rate': 0.07965363024397404, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 622}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:31,639] Trial 367 finished with value: 0.6920964255409005 and parameters: {'n_estimators': 516, 'learning_rate': 0.11470813195649673, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 651}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:34,343] Trial 368 finished with value: 0.6909971415680977 and parameters: {'n_estimators': 620, 'learning_rate': 0.16673117828000406, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 667}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:38,300] Trial 369 finished with value: 0.6928858000282834 and parameters: {'n_estimators': 581, 'learning_rate': 0.08746033319192394, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 590}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:42,462] Trial 370 finished with value: 0.6901892567757977 and parameters: {'n_estimators': 557, 'learning_rate': 0.07022884575134715, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 641}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:45,314] Trial 371 finished with value: 0.6865991485038001 and parameters: {'n_estimators': 614, 'learning_rate': 0.09372141866174735, 'max_depth': 8, 'max_bin': 250, 'num_leaves': 633}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:49,135] Trial 372 finished with value: 0.6897019552949997 and parameters: {'n_estimators': 592, 'learning_rate': 0.08236231454626457, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 655}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:52,593] Trial 373 finished with value: 0.689731941458805 and parameters: {'n_estimators': 630, 'learning_rate': 0.10062162600242149, 'max_depth': 11, 'max_bin': 262, 'num_leaves': 674}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:53:56,620] Trial 374 finished with value: 0.6947145776975312 and parameters: {'n_estimators': 541, 'learning_rate': 0.09034279615349008, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 614}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:00,169] Trial 375 finished with value: 0.6914559963546679 and parameters: {'n_estimators': 571, 'learning_rate': 0.09707497821044274, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 684}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:03,508] Trial 376 finished with value: 0.6887504750176683 and parameters: {'n_estimators': 603, 'learning_rate': 0.10891073180381444, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 693}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:07,080] Trial 377 finished with value: 0.6914229304531964 and parameters: {'n_estimators': 638, 'learning_rate': 0.08509532934796203, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 530}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:09,251] Trial 378 finished with value: 0.6805053058683358 and parameters: {'n_estimators': 620, 'learning_rate': 0.10306860809419469, 'max_depth': 6, 'max_bin': 269, 'num_leaves': 663}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:13,414] Trial 379 finished with value: 0.6936534308217462 and parameters: {'n_estimators': 588, 'learning_rate': 0.07508269383202841, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 647}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:17,572] Trial 380 finished with value: 0.6909063802972445 and parameters: {'n_estimators': 554, 'learning_rate': 0.092600299926397, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 701}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:21,356] Trial 381 finished with value: 0.6901220846633656 and parameters: {'n_estimators': 645, 'learning_rate': 0.08785466615959663, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 577}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:24,782] Trial 382 finished with value: 0.6942384278219597 and parameters: {'n_estimators': 609, 'learning_rate': 0.09706921352996101, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 629}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:28,768] Trial 383 finished with value: 0.691819545532207 and parameters: {'n_estimators': 568, 'learning_rate': 0.08122282028441691, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 671}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:31,521] Trial 384 finished with value: 0.6741571086238676 and parameters: {'n_estimators': 208, 'learning_rate': 0.0492227851943277, 'max_depth': 11, 'max_bin': 250, 'num_leaves': 654}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:35,351] Trial 385 finished with value: 0.6959636308063437 and parameters: {'n_estimators': 586, 'learning_rate': 0.09109435812908773, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 686}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:38,877] Trial 386 finished with value: 0.6913518106716476 and parameters: {'n_estimators': 585, 'learning_rate': 0.09090544354013769, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 675}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:42,371] Trial 387 finished with value: 0.6900739972573666 and parameters: {'n_estimators': 541, 'learning_rate': 0.08573563261544279, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 689}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:44,663] Trial 388 finished with value: 0.6758014988801022 and parameters: {'n_estimators': 593, 'learning_rate': 0.09507547497174522, 'max_depth': 5, 'max_bin': 246, 'num_leaves': 640}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:48,545] Trial 389 finished with value: 0.6944003212208041 and parameters: {'n_estimators': 567, 'learning_rate': 0.08913046925275042, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 661}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:52,373] Trial 390 finished with value: 0.6921899850074934 and parameters: {'n_estimators': 602, 'learning_rate': 0.08315501854349822, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 616}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:54,053] Trial 391 finished with value: 0.675278769377177 and parameters: {'n_estimators': 124, 'learning_rate': 0.09205335359270318, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 135}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:54:58,276] Trial 392 finished with value: 0.693951587365864 and parameters: {'n_estimators': 579, 'learning_rate': 0.07774873579236855, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 680}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:02,921] Trial 393 finished with value: 0.693673480168459 and parameters: {'n_estimators': 625, 'learning_rate': 0.054869004139942834, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 598}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:06,945] Trial 394 finished with value: 0.6872417117913281 and parameters: {'n_estimators': 812, 'learning_rate': 0.07219474567978793, 'max_depth': 11, 'max_bin': 262, 'num_leaves': 651}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:10,688] Trial 395 finished with value: 0.6897660574887059 and parameters: {'n_estimators': 504, 'learning_rate': 0.08696122916572403, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 630}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:14,260] Trial 396 finished with value: 0.6927498922692394 and parameters: {'n_estimators': 610, 'learning_rate': 0.09504501814090294, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 667}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:17,081] Trial 397 finished with value: 0.6829017389580277 and parameters: {'n_estimators': 549, 'learning_rate': 0.14161867339744003, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 692}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:21,148] Trial 398 finished with value: 0.6896812321568897 and parameters: {'n_estimators': 585, 'learning_rate': 0.06591402342056342, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 709}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:24,807] Trial 399 finished with value: 0.6905712356206481 and parameters: {'n_estimators': 532, 'learning_rate': 0.0911642386327296, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 681}. Best is trial 123 with value: 0.6990131690414316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6990132\n",
      "\tBest params:\n",
      "\t\tn_estimators: 578\n",
      "\t\tlearning_rate: 0.09274509958766028\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 256\n",
      "\t\tnum_leaves: 719\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.715728    0.717255    0.694202    0.738094   \n",
      "1                    TP  343.000000  332.000000  333.000000  332.000000   \n",
      "2                    TN  170.000000  174.000000  172.000000  185.000000   \n",
      "3                    FP   42.000000   56.000000   50.000000   41.000000   \n",
      "4                    FN   40.000000   33.000000   40.000000   37.000000   \n",
      "5              Accuracy    0.862185    0.850420    0.848739    0.868908   \n",
      "6             Precision    0.890909    0.855670    0.869452    0.890080   \n",
      "7           Sensitivity    0.895561    0.909589    0.892761    0.899729   \n",
      "8           Specificity    0.801900    0.756500    0.774800    0.818600   \n",
      "9              F1 score    0.893229    0.881806    0.880952    0.894879   \n",
      "10  F1 score (weighted)    0.862038    0.848768    0.847997    0.868676   \n",
      "11     F1 score (macro)    0.849458    0.839072    0.836790    0.860386   \n",
      "12    Balanced Accuracy    0.848724    0.833055    0.833768    0.859157   \n",
      "13                  MCC    0.698939    0.681014    0.674122    0.720859   \n",
      "14                  NPV    0.809500    0.840600    0.811300    0.833300   \n",
      "15              ROC_AUC    0.848724    0.833055    0.833768    0.859157   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.681368    0.706702    0.694797    0.684628  \n",
      "1   333.000000  338.000000  327.000000  323.000000  \n",
      "2   169.000000  178.000000  173.000000  168.000000  \n",
      "3    62.000000   40.000000   55.000000   52.000000  \n",
      "4    31.000000   39.000000   40.000000   52.000000  \n",
      "5     0.843697    0.867227    0.840336    0.825210  \n",
      "6     0.843038    0.894180    0.856021    0.861333  \n",
      "7     0.914835    0.896552    0.891008    0.861333  \n",
      "8     0.731600    0.816500    0.758800    0.763600  \n",
      "9     0.877470    0.895364    0.873164    0.861333  \n",
      "10    0.841268    0.867162    0.839220    0.825210  \n",
      "11    0.830847    0.856878    0.828872    0.812485  \n",
      "12    0.823218    0.856533    0.824890    0.812485  \n",
      "13    0.666913    0.713761    0.658939    0.624970  \n",
      "14    0.845000    0.820300    0.812200    0.763600  \n",
      "15    0.823218    0.856533    0.824890    0.812485  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_lgbm_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_7_cat = np.where((y_pred_lgbm_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 11:55:28,075] Trial 400 finished with value: 0.6829870475815489 and parameters: {'n_estimators': 560, 'learning_rate': 0.12741545816453353, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 644}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:31,141] Trial 401 finished with value: 0.6773965521943243 and parameters: {'n_estimators': 598, 'learning_rate': 0.09888971064044327, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 661}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:34,419] Trial 402 finished with value: 0.6770899674214316 and parameters: {'n_estimators': 620, 'learning_rate': 0.08208045817158592, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 623}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:37,829] Trial 403 finished with value: 0.67990838423781 and parameters: {'n_estimators': 574, 'learning_rate': 0.0876506339883867, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 698}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:41,741] Trial 404 finished with value: 0.6768320320833761 and parameters: {'n_estimators': 632, 'learning_rate': 0.0596703652358021, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 673}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:44,163] Trial 405 finished with value: 0.6733434645628616 and parameters: {'n_estimators': 593, 'learning_rate': 0.1981428314299318, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 654}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:47,014] Trial 406 finished with value: 0.6755471042020293 and parameters: {'n_estimators': 616, 'learning_rate': 0.0941383610426451, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 635}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:50,602] Trial 407 finished with value: 0.6842497086155929 and parameters: {'n_estimators': 518, 'learning_rate': 0.0791856282658669, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 608}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:53,886] Trial 408 finished with value: 0.6842964011514449 and parameters: {'n_estimators': 568, 'learning_rate': 0.0993359003983412, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 693}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:55:57,409] Trial 409 finished with value: 0.6819098054046216 and parameters: {'n_estimators': 636, 'learning_rate': 0.0841566050478196, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 666}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:56:00,989] Trial 410 finished with value: 0.6809930493488657 and parameters: {'n_estimators': 554, 'learning_rate': 0.09007293743970315, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 684}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:56:07,374] Trial 411 finished with value: 0.6770102594875385 and parameters: {'n_estimators': 604, 'learning_rate': 0.0260668385867536, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 705}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:56:12,283] Trial 412 finished with value: 0.678810547299514 and parameters: {'n_estimators': 584, 'learning_rate': 0.03757245172720531, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 650}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:56:15,053] Trial 413 finished with value: 0.6737336607051188 and parameters: {'n_estimators': 650, 'learning_rate': 0.09430552300562584, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 623}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:56:17,912] Trial 414 finished with value: 0.6784940630677163 and parameters: {'n_estimators': 616, 'learning_rate': 0.1029146909642969, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 671}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:56:21,090] Trial 415 finished with value: 0.6815573614155019 and parameters: {'n_estimators': 574, 'learning_rate': 0.08437620833926873, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 644}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:56:24,766] Trial 416 finished with value: 0.6764497689923867 and parameters: {'n_estimators': 895, 'learning_rate': 0.07486433939361957, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 714}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:56:27,803] Trial 417 finished with value: 0.6818270272196072 and parameters: {'n_estimators': 603, 'learning_rate': 0.09721750095590002, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 689}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:56:31,186] Trial 418 finished with value: 0.6833520459266159 and parameters: {'n_estimators': 632, 'learning_rate': 0.08998407272804732, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 660}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:56:34,249] Trial 419 finished with value: 0.6766699257085272 and parameters: {'n_estimators': 546, 'learning_rate': 0.08665518190929576, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 599}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:56:38,091] Trial 420 finished with value: 0.6799749306005515 and parameters: {'n_estimators': 594, 'learning_rate': 0.06297057371772417, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 677}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:56:41,845] Trial 421 finished with value: 0.6762025033704123 and parameters: {'n_estimators': 654, 'learning_rate': 0.08072091765673778, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 633}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:56:46,442] Trial 422 finished with value: 0.6806448456457097 and parameters: {'n_estimators': 562, 'learning_rate': 0.04747823078161105, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 702}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:56:54,822] Trial 423 finished with value: 0.6725460840108557 and parameters: {'n_estimators': 621, 'learning_rate': 0.012413638933290075, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 613}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:56:57,919] Trial 424 finished with value: 0.6763313208967767 and parameters: {'n_estimators': 583, 'learning_rate': 0.09281733567740531, 'max_depth': 11, 'max_bin': 259, 'num_leaves': 645}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:01,540] Trial 425 finished with value: 0.6766574479346156 and parameters: {'n_estimators': 641, 'learning_rate': 0.07024735894264002, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 720}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:04,416] Trial 426 finished with value: 0.6723074494984175 and parameters: {'n_estimators': 605, 'learning_rate': 0.1007228906617174, 'max_depth': 7, 'max_bin': 265, 'num_leaves': 669}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:07,758] Trial 427 finished with value: 0.6785927088251854 and parameters: {'n_estimators': 527, 'learning_rate': 0.08730293089693025, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 555}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:10,965] Trial 428 finished with value: 0.679498724607605 and parameters: {'n_estimators': 577, 'learning_rate': 0.09611735162396544, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 684}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:14,339] Trial 429 finished with value: 0.6815852330459057 and parameters: {'n_estimators': 628, 'learning_rate': 0.09098519796418572, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 658}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:17,879] Trial 430 finished with value: 0.6775971602833306 and parameters: {'n_estimators': 553, 'learning_rate': 0.07767065496190909, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 636}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:21,421] Trial 431 finished with value: 0.6800374435728785 and parameters: {'n_estimators': 661, 'learning_rate': 0.08410451536492095, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 700}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:24,011] Trial 432 finished with value: 0.6778220511187779 and parameters: {'n_estimators': 593, 'learning_rate': 0.10629282970186485, 'max_depth': 12, 'max_bin': 173, 'num_leaves': 585}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:27,042] Trial 433 finished with value: 0.6779514870490309 and parameters: {'n_estimators': 607, 'learning_rate': 0.09807636020622248, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 675}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:37,402] Trial 434 finished with value: 0.617233747958269 and parameters: {'n_estimators': 618, 'learning_rate': 0.0036227074349421823, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 627}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:40,478] Trial 435 finished with value: 0.6815412941264481 and parameters: {'n_estimators': 570, 'learning_rate': 0.09315336654575143, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 657}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:44,996] Trial 436 finished with value: 0.6756215037042583 and parameters: {'n_estimators': 645, 'learning_rate': 0.04267661052202833, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 687}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:48,166] Trial 437 finished with value: 0.6790489991963884 and parameters: {'n_estimators': 536, 'learning_rate': 0.08948843349632077, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 723}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:51,253] Trial 438 finished with value: 0.6735774250478515 and parameters: {'n_estimators': 591, 'learning_rate': 0.08048707077158307, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 646}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:55,338] Trial 439 finished with value: 0.6829808359666323 and parameters: {'n_estimators': 620, 'learning_rate': 0.06624987379027336, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 608}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:57:59,281] Trial 440 finished with value: 0.6777842374169494 and parameters: {'n_estimators': 561, 'learning_rate': 0.05242981012486147, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 665}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:01,840] Trial 441 finished with value: 0.6751774460074891 and parameters: {'n_estimators': 675, 'learning_rate': 0.11390613046977831, 'max_depth': 9, 'max_bin': 250, 'num_leaves': 703}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:04,745] Trial 442 finished with value: 0.6777885664905077 and parameters: {'n_estimators': 585, 'learning_rate': 0.08581669034988436, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 684}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:06,970] Trial 443 finished with value: 0.6752261444336269 and parameters: {'n_estimators': 631, 'learning_rate': 0.16198990561075866, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 103}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:09,847] Trial 444 finished with value: 0.678261749390091 and parameters: {'n_estimators': 650, 'learning_rate': 0.11800914826254147, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 618}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:12,784] Trial 445 finished with value: 0.6742349087745565 and parameters: {'n_estimators': 605, 'learning_rate': 0.09551769994980638, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 637}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:15,722] Trial 446 finished with value: 0.6761487340610591 and parameters: {'n_estimators': 548, 'learning_rate': 0.10272288492974146, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 671}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:18,959] Trial 447 finished with value: 0.6756093406186098 and parameters: {'n_estimators': 575, 'learning_rate': 0.073834957495865, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 744}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:21,687] Trial 448 finished with value: 0.6781132222061896 and parameters: {'n_estimators': 603, 'learning_rate': 0.13111373836589185, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 652}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:24,425] Trial 449 finished with value: 0.6769001888622417 and parameters: {'n_estimators': 633, 'learning_rate': 0.0884302431911746, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 693}. Best is trial 123 with value: 0.6990131690414316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.69901317\n",
      "\tBest params:\n",
      "\t\tn_estimators: 578\n",
      "\t\tlearning_rate: 0.09274509958766028\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 256\n",
      "\t\tnum_leaves: 719\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.715728    0.717255    0.694202    0.738094   \n",
      "1                    TP  343.000000  332.000000  333.000000  332.000000   \n",
      "2                    TN  170.000000  174.000000  172.000000  185.000000   \n",
      "3                    FP   42.000000   56.000000   50.000000   41.000000   \n",
      "4                    FN   40.000000   33.000000   40.000000   37.000000   \n",
      "5              Accuracy    0.862185    0.850420    0.848739    0.868908   \n",
      "6             Precision    0.890909    0.855670    0.869452    0.890080   \n",
      "7           Sensitivity    0.895561    0.909589    0.892761    0.899729   \n",
      "8           Specificity    0.801900    0.756500    0.774800    0.818600   \n",
      "9              F1 score    0.893229    0.881806    0.880952    0.894879   \n",
      "10  F1 score (weighted)    0.862038    0.848768    0.847997    0.868676   \n",
      "11     F1 score (macro)    0.849458    0.839072    0.836790    0.860386   \n",
      "12    Balanced Accuracy    0.848724    0.833055    0.833768    0.859157   \n",
      "13                  MCC    0.698939    0.681014    0.674122    0.720859   \n",
      "14                  NPV    0.809500    0.840600    0.811300    0.833300   \n",
      "15              ROC_AUC    0.848724    0.833055    0.833768    0.859157   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.681368    0.706702    0.694797    0.684628    0.724183  \n",
      "1   333.000000  338.000000  327.000000  323.000000  316.000000  \n",
      "2   169.000000  178.000000  173.000000  168.000000  185.000000  \n",
      "3    62.000000   40.000000   55.000000   52.000000   57.000000  \n",
      "4    31.000000   39.000000   40.000000   52.000000   37.000000  \n",
      "5     0.843697    0.867227    0.840336    0.825210    0.842017  \n",
      "6     0.843038    0.894180    0.856021    0.861333    0.847185  \n",
      "7     0.914835    0.896552    0.891008    0.861333    0.895184  \n",
      "8     0.731600    0.816500    0.758800    0.763600    0.764500  \n",
      "9     0.877470    0.895364    0.873164    0.861333    0.870523  \n",
      "10    0.841268    0.867162    0.839220    0.825210    0.840788  \n",
      "11    0.830847    0.856878    0.828872    0.812485    0.833969  \n",
      "12    0.823218    0.856533    0.824890    0.812485    0.829823  \n",
      "13    0.666913    0.713761    0.658939    0.624970    0.670001  \n",
      "14    0.845000    0.820300    0.812200    0.763600    0.833300  \n",
      "15    0.823218    0.856533    0.824890    0.812485    0.829823  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_lgbm_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_8_cat = np.where((y_pred_lgbm_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 11:58:28,469] Trial 450 finished with value: 0.6863229436858493 and parameters: {'n_estimators': 660, 'learning_rate': 0.0924603637111206, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 719}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:31,476] Trial 451 finished with value: 0.6829481843220995 and parameters: {'n_estimators': 584, 'learning_rate': 0.09900271172022318, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 675}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:33,936] Trial 452 finished with value: 0.6817946188386657 and parameters: {'n_estimators': 563, 'learning_rate': 0.13881517633965998, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 629}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:37,440] Trial 453 finished with value: 0.6863402917246907 and parameters: {'n_estimators': 613, 'learning_rate': 0.08391185626610925, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 439}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:39,682] Trial 454 finished with value: 0.6566844269693606 and parameters: {'n_estimators': 646, 'learning_rate': 0.09017360706523492, 'max_depth': 4, 'max_bin': 275, 'num_leaves': 707}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:43,473] Trial 455 finished with value: 0.6832902893535694 and parameters: {'n_estimators': 624, 'learning_rate': 0.07827355562967274, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 660}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:46,358] Trial 456 finished with value: 0.6794439011577526 and parameters: {'n_estimators': 591, 'learning_rate': 0.09546082166499133, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 179}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:49,873] Trial 457 finished with value: 0.6848606568813922 and parameters: {'n_estimators': 529, 'learning_rate': 0.0865753205068972, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 593}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:53,242] Trial 458 finished with value: 0.6841273594872288 and parameters: {'n_estimators': 573, 'learning_rate': 0.10274315519922837, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 642}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:58:55,584] Trial 459 finished with value: 0.6757131731633998 and parameters: {'n_estimators': 551, 'learning_rate': 0.1760891909970852, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 683}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:59:05,100] Trial 460 finished with value: 0.6651643447780087 and parameters: {'n_estimators': 687, 'learning_rate': 0.009094603783008387, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 660}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:59:08,581] Trial 461 finished with value: 0.6835694441912252 and parameters: {'n_estimators': 608, 'learning_rate': 0.0818477131830027, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 694}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:59:12,836] Trial 462 finished with value: 0.6826182871676723 and parameters: {'n_estimators': 637, 'learning_rate': 0.05980484862649879, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 671}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:59:15,432] Trial 463 finished with value: 0.6744467247566377 and parameters: {'n_estimators': 592, 'learning_rate': 0.19907188604251297, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 619}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:59:19,010] Trial 464 finished with value: 0.6862999449702978 and parameters: {'n_estimators': 664, 'learning_rate': 0.09337917733138576, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 651}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:59:22,586] Trial 465 finished with value: 0.6848760810590285 and parameters: {'n_estimators': 502, 'learning_rate': 0.09823294929562959, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 709}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:59:25,709] Trial 466 finished with value: 0.6857052105217729 and parameters: {'n_estimators': 620, 'learning_rate': 0.10708552821986413, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 728}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:59:29,643] Trial 467 finished with value: 0.681098618012924 and parameters: {'n_estimators': 566, 'learning_rate': 0.07153012713942244, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 634}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:59:35,950] Trial 468 finished with value: 0.6826011842741112 and parameters: {'n_estimators': 598, 'learning_rate': 0.02752048908800238, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 685}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:59:40,806] Trial 469 finished with value: 0.6665853743800958 and parameters: {'n_estimators': 644, 'learning_rate': 0.023562228146695285, 'max_depth': 8, 'max_bin': 272, 'num_leaves': 605}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:59:43,803] Trial 470 finished with value: 0.6817977450110166 and parameters: {'n_estimators': 539, 'learning_rate': 0.08881850942308477, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 646}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:59:50,409] Trial 471 finished with value: 0.6761969330905362 and parameters: {'n_estimators': 579, 'learning_rate': 0.021564223913105723, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 667}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:59:53,716] Trial 472 finished with value: 0.6827805979757959 and parameters: {'n_estimators': 626, 'learning_rate': 0.0915678098237459, 'max_depth': 11, 'max_bin': 242, 'num_leaves': 697}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 11:59:57,357] Trial 473 finished with value: 0.6826333360170097 and parameters: {'n_estimators': 604, 'learning_rate': 0.08509852171222407, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 678}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:00,955] Trial 474 finished with value: 0.6836176991888829 and parameters: {'n_estimators': 560, 'learning_rate': 0.07653821849037462, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 623}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:04,419] Trial 475 finished with value: 0.6840099114601712 and parameters: {'n_estimators': 582, 'learning_rate': 0.10039941249811637, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 572}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:07,674] Trial 476 finished with value: 0.6823812444356802 and parameters: {'n_estimators': 662, 'learning_rate': 0.09431147160760484, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 658}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:11,032] Trial 477 finished with value: 0.6824229071237525 and parameters: {'n_estimators': 616, 'learning_rate': 0.08296759354337897, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 709}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:13,497] Trial 478 finished with value: 0.6747094908191583 and parameters: {'n_estimators': 641, 'learning_rate': 0.08814448526931862, 'max_depth': 7, 'max_bin': 268, 'num_leaves': 641}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:16,544] Trial 479 finished with value: 0.6821093757145016 and parameters: {'n_estimators': 598, 'learning_rate': 0.0969017027100751, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 691}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:19,423] Trial 480 finished with value: 0.6815762920394962 and parameters: {'n_estimators': 548, 'learning_rate': 0.1099388723447425, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 736}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:24,347] Trial 481 finished with value: 0.6828057024759712 and parameters: {'n_estimators': 521, 'learning_rate': 0.03535257421973555, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 670}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:27,277] Trial 482 finished with value: 0.686095741054503 and parameters: {'n_estimators': 485, 'learning_rate': 0.12039216608248912, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 318}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:32,002] Trial 483 finished with value: 0.6830302388827174 and parameters: {'n_estimators': 576, 'learning_rate': 0.04672382036177421, 'max_depth': 11, 'max_bin': 260, 'num_leaves': 657}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:34,590] Trial 484 finished with value: 0.6788171058835709 and parameters: {'n_estimators': 631, 'learning_rate': 0.14654000617087962, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 626}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:37,560] Trial 485 finished with value: 0.6798720126910143 and parameters: {'n_estimators': 671, 'learning_rate': 0.08036643169755303, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 54}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:49,174] Trial 486 finished with value: 0.44601663068300895 and parameters: {'n_estimators': 607, 'learning_rate': 0.0012649251597937616, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 510}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:52,766] Trial 487 finished with value: 0.6809457902733672 and parameters: {'n_estimators': 570, 'learning_rate': 0.06668818390161564, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 680}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:56,206] Trial 488 finished with value: 0.6867860983344387 and parameters: {'n_estimators': 653, 'learning_rate': 0.09220353829303879, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 605}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:00:59,265] Trial 489 finished with value: 0.6809018290379727 and parameters: {'n_estimators': 590, 'learning_rate': 0.1045937787121892, 'max_depth': 12, 'max_bin': 184, 'num_leaves': 719}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:01:02,591] Trial 490 finished with value: 0.6849586825577922 and parameters: {'n_estimators': 618, 'learning_rate': 0.08739164427898057, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 642}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:01:05,792] Trial 491 finished with value: 0.6812294012204274 and parameters: {'n_estimators': 548, 'learning_rate': 0.09664485642136919, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 699}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:01:08,782] Trial 492 finished with value: 0.6800138494104578 and parameters: {'n_estimators': 640, 'learning_rate': 0.09162586463899244, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 666}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:01:11,690] Trial 493 finished with value: 0.6796591976948985 and parameters: {'n_estimators': 595, 'learning_rate': 0.14259196521056863, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 652}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:01:15,293] Trial 494 finished with value: 0.6866910136745242 and parameters: {'n_estimators': 567, 'learning_rate': 0.08165350594344033, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 681}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:01:18,765] Trial 495 finished with value: 0.6835892774117323 and parameters: {'n_estimators': 680, 'learning_rate': 0.1003659682488496, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 616}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:01:22,180] Trial 496 finished with value: 0.6812382465211927 and parameters: {'n_estimators': 614, 'learning_rate': 0.07538448151138599, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 586}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:01:25,047] Trial 497 finished with value: 0.6820220645197718 and parameters: {'n_estimators': 585, 'learning_rate': 0.129571798281465, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 632}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:01:27,201] Trial 498 finished with value: 0.6721018270262547 and parameters: {'n_estimators': 631, 'learning_rate': 0.18800736216135538, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 688}. Best is trial 123 with value: 0.6990131690414316.\n",
      "[I 2023-12-20 12:01:30,860] Trial 499 finished with value: 0.6831693770110111 and parameters: {'n_estimators': 654, 'learning_rate': 0.08559185208428606, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 710}. Best is trial 123 with value: 0.6990131690414316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.699013169\n",
      "\tBest params:\n",
      "\t\tn_estimators: 578\n",
      "\t\tlearning_rate: 0.09274509958766028\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 256\n",
      "\t\tnum_leaves: 719\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.715728    0.717255    0.694202    0.738094   \n",
      "1                    TP  343.000000  332.000000  333.000000  332.000000   \n",
      "2                    TN  170.000000  174.000000  172.000000  185.000000   \n",
      "3                    FP   42.000000   56.000000   50.000000   41.000000   \n",
      "4                    FN   40.000000   33.000000   40.000000   37.000000   \n",
      "5              Accuracy    0.862185    0.850420    0.848739    0.868908   \n",
      "6             Precision    0.890909    0.855670    0.869452    0.890080   \n",
      "7           Sensitivity    0.895561    0.909589    0.892761    0.899729   \n",
      "8           Specificity    0.801900    0.756500    0.774800    0.818600   \n",
      "9              F1 score    0.893229    0.881806    0.880952    0.894879   \n",
      "10  F1 score (weighted)    0.862038    0.848768    0.847997    0.868676   \n",
      "11     F1 score (macro)    0.849458    0.839072    0.836790    0.860386   \n",
      "12    Balanced Accuracy    0.848724    0.833055    0.833768    0.859157   \n",
      "13                  MCC    0.698939    0.681014    0.674122    0.720859   \n",
      "14                  NPV    0.809500    0.840600    0.811300    0.833300   \n",
      "15              ROC_AUC    0.848724    0.833055    0.833768    0.859157   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.681368    0.706702    0.694797    0.684628    0.724183    0.724018  \n",
      "1   333.000000  338.000000  327.000000  323.000000  316.000000  335.000000  \n",
      "2   169.000000  178.000000  173.000000  168.000000  185.000000  184.000000  \n",
      "3    62.000000   40.000000   55.000000   52.000000   57.000000   47.000000  \n",
      "4    31.000000   39.000000   40.000000   52.000000   37.000000   29.000000  \n",
      "5     0.843697    0.867227    0.840336    0.825210    0.842017    0.872269  \n",
      "6     0.843038    0.894180    0.856021    0.861333    0.847185    0.876963  \n",
      "7     0.914835    0.896552    0.891008    0.861333    0.895184    0.920330  \n",
      "8     0.731600    0.816500    0.758800    0.763600    0.764500    0.796500  \n",
      "9     0.877470    0.895364    0.873164    0.861333    0.870523    0.898123  \n",
      "10    0.841268    0.867162    0.839220    0.825210    0.840788    0.871221  \n",
      "11    0.830847    0.856878    0.828872    0.812485    0.833969    0.863476  \n",
      "12    0.823218    0.856533    0.824890    0.812485    0.829823    0.858433  \n",
      "13    0.666913    0.713761    0.658939    0.624970    0.670001    0.728741  \n",
      "14    0.845000    0.820300    0.812200    0.763600    0.833300    0.863800  \n",
      "15    0.823218    0.856533    0.824890    0.812485    0.829823    0.858433  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_lgbm_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_lgbm_9_cat = np.where((y_pred_lgbm_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.708027     0.027724\n",
      "1                    TP       162.900000    10.407796\n",
      "2                    TN        88.900000     6.773314\n",
      "3                    FP        24.500000     3.064129\n",
      "4                    FN        20.800000     3.994441\n",
      "5              Accuracy         0.847527     0.021114\n",
      "6             Precision         0.868773     0.019823\n",
      "7           Sensitivity         0.886087     0.025478\n",
      "8           Specificity         0.783740     0.024688\n",
      "9              F1 score         0.877282     0.021327\n",
      "10  F1 score (weighted)         0.846994     0.020969\n",
      "11     F1 score (macro)         0.837162     0.020238\n",
      "12    Balanced Accuracy         0.834914     0.019525\n",
      "13                  MCC         0.674857     0.040769\n",
      "14                  NPV         0.811170     0.027923\n",
      "15              ROC_AUC         0.834914     0.019525\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.715728</td>\n",
       "      <td>0.717255</td>\n",
       "      <td>0.694202</td>\n",
       "      <td>0.738094</td>\n",
       "      <td>0.681368</td>\n",
       "      <td>0.706702</td>\n",
       "      <td>0.694797</td>\n",
       "      <td>0.684628</td>\n",
       "      <td>0.724183</td>\n",
       "      <td>0.724018</td>\n",
       "      <td>0.708098</td>\n",
       "      <td>0.018830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>338.000000</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>323.000000</td>\n",
       "      <td>316.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>331.200000</td>\n",
       "      <td>7.627436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>175.800000</td>\n",
       "      <td>6.729702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>50.200000</td>\n",
       "      <td>7.539525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>37.800000</td>\n",
       "      <td>6.373556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.862185</td>\n",
       "      <td>0.850420</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>0.868908</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.867227</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>0.825210</td>\n",
       "      <td>0.842017</td>\n",
       "      <td>0.872269</td>\n",
       "      <td>0.852101</td>\n",
       "      <td>0.015157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.855670</td>\n",
       "      <td>0.869452</td>\n",
       "      <td>0.890080</td>\n",
       "      <td>0.843038</td>\n",
       "      <td>0.894180</td>\n",
       "      <td>0.856021</td>\n",
       "      <td>0.861333</td>\n",
       "      <td>0.847185</td>\n",
       "      <td>0.876963</td>\n",
       "      <td>0.868483</td>\n",
       "      <td>0.018780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.895561</td>\n",
       "      <td>0.909589</td>\n",
       "      <td>0.892761</td>\n",
       "      <td>0.899729</td>\n",
       "      <td>0.914835</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.891008</td>\n",
       "      <td>0.861333</td>\n",
       "      <td>0.895184</td>\n",
       "      <td>0.920330</td>\n",
       "      <td>0.897688</td>\n",
       "      <td>0.016176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.801900</td>\n",
       "      <td>0.756500</td>\n",
       "      <td>0.774800</td>\n",
       "      <td>0.818600</td>\n",
       "      <td>0.731600</td>\n",
       "      <td>0.816500</td>\n",
       "      <td>0.758800</td>\n",
       "      <td>0.763600</td>\n",
       "      <td>0.764500</td>\n",
       "      <td>0.796500</td>\n",
       "      <td>0.778330</td>\n",
       "      <td>0.028726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.893229</td>\n",
       "      <td>0.881806</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.894879</td>\n",
       "      <td>0.877470</td>\n",
       "      <td>0.895364</td>\n",
       "      <td>0.873164</td>\n",
       "      <td>0.861333</td>\n",
       "      <td>0.870523</td>\n",
       "      <td>0.898123</td>\n",
       "      <td>0.882685</td>\n",
       "      <td>0.012402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.862038</td>\n",
       "      <td>0.848768</td>\n",
       "      <td>0.847997</td>\n",
       "      <td>0.868676</td>\n",
       "      <td>0.841268</td>\n",
       "      <td>0.867162</td>\n",
       "      <td>0.839220</td>\n",
       "      <td>0.825210</td>\n",
       "      <td>0.840788</td>\n",
       "      <td>0.871221</td>\n",
       "      <td>0.851235</td>\n",
       "      <td>0.015350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.849458</td>\n",
       "      <td>0.839072</td>\n",
       "      <td>0.836790</td>\n",
       "      <td>0.860386</td>\n",
       "      <td>0.830847</td>\n",
       "      <td>0.856878</td>\n",
       "      <td>0.828872</td>\n",
       "      <td>0.812485</td>\n",
       "      <td>0.833969</td>\n",
       "      <td>0.863476</td>\n",
       "      <td>0.841223</td>\n",
       "      <td>0.016119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.848724</td>\n",
       "      <td>0.833055</td>\n",
       "      <td>0.833768</td>\n",
       "      <td>0.859157</td>\n",
       "      <td>0.823218</td>\n",
       "      <td>0.856533</td>\n",
       "      <td>0.824890</td>\n",
       "      <td>0.812485</td>\n",
       "      <td>0.829823</td>\n",
       "      <td>0.858433</td>\n",
       "      <td>0.838009</td>\n",
       "      <td>0.016580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.698939</td>\n",
       "      <td>0.681014</td>\n",
       "      <td>0.674122</td>\n",
       "      <td>0.720859</td>\n",
       "      <td>0.666913</td>\n",
       "      <td>0.713761</td>\n",
       "      <td>0.658939</td>\n",
       "      <td>0.624970</td>\n",
       "      <td>0.670001</td>\n",
       "      <td>0.728741</td>\n",
       "      <td>0.683826</td>\n",
       "      <td>0.031930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.809500</td>\n",
       "      <td>0.840600</td>\n",
       "      <td>0.811300</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.820300</td>\n",
       "      <td>0.812200</td>\n",
       "      <td>0.763600</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.863800</td>\n",
       "      <td>0.823290</td>\n",
       "      <td>0.027164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.848724</td>\n",
       "      <td>0.833055</td>\n",
       "      <td>0.833768</td>\n",
       "      <td>0.859157</td>\n",
       "      <td>0.823218</td>\n",
       "      <td>0.856533</td>\n",
       "      <td>0.824890</td>\n",
       "      <td>0.812485</td>\n",
       "      <td>0.829823</td>\n",
       "      <td>0.858433</td>\n",
       "      <td>0.838009</td>\n",
       "      <td>0.016580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.715728    0.717255    0.694202    0.738094   \n",
       "1                    TP  343.000000  332.000000  333.000000  332.000000   \n",
       "2                    TN  170.000000  174.000000  172.000000  185.000000   \n",
       "3                    FP   42.000000   56.000000   50.000000   41.000000   \n",
       "4                    FN   40.000000   33.000000   40.000000   37.000000   \n",
       "5              Accuracy    0.862185    0.850420    0.848739    0.868908   \n",
       "6             Precision    0.890909    0.855670    0.869452    0.890080   \n",
       "7           Sensitivity    0.895561    0.909589    0.892761    0.899729   \n",
       "8           Specificity    0.801900    0.756500    0.774800    0.818600   \n",
       "9              F1 score    0.893229    0.881806    0.880952    0.894879   \n",
       "10  F1 score (weighted)    0.862038    0.848768    0.847997    0.868676   \n",
       "11     F1 score (macro)    0.849458    0.839072    0.836790    0.860386   \n",
       "12    Balanced Accuracy    0.848724    0.833055    0.833768    0.859157   \n",
       "13                  MCC    0.698939    0.681014    0.674122    0.720859   \n",
       "14                  NPV    0.809500    0.840600    0.811300    0.833300   \n",
       "15              ROC_AUC    0.848724    0.833055    0.833768    0.859157   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.681368    0.706702    0.694797    0.684628    0.724183    0.724018   \n",
       "1   333.000000  338.000000  327.000000  323.000000  316.000000  335.000000   \n",
       "2   169.000000  178.000000  173.000000  168.000000  185.000000  184.000000   \n",
       "3    62.000000   40.000000   55.000000   52.000000   57.000000   47.000000   \n",
       "4    31.000000   39.000000   40.000000   52.000000   37.000000   29.000000   \n",
       "5     0.843697    0.867227    0.840336    0.825210    0.842017    0.872269   \n",
       "6     0.843038    0.894180    0.856021    0.861333    0.847185    0.876963   \n",
       "7     0.914835    0.896552    0.891008    0.861333    0.895184    0.920330   \n",
       "8     0.731600    0.816500    0.758800    0.763600    0.764500    0.796500   \n",
       "9     0.877470    0.895364    0.873164    0.861333    0.870523    0.898123   \n",
       "10    0.841268    0.867162    0.839220    0.825210    0.840788    0.871221   \n",
       "11    0.830847    0.856878    0.828872    0.812485    0.833969    0.863476   \n",
       "12    0.823218    0.856533    0.824890    0.812485    0.829823    0.858433   \n",
       "13    0.666913    0.713761    0.658939    0.624970    0.670001    0.728741   \n",
       "14    0.845000    0.820300    0.812200    0.763600    0.833300    0.863800   \n",
       "15    0.823218    0.856533    0.824890    0.812485    0.829823    0.858433   \n",
       "\n",
       "           ave       std  \n",
       "0     0.708098  0.018830  \n",
       "1   331.200000  7.627436  \n",
       "2   175.800000  6.729702  \n",
       "3    50.200000  7.539525  \n",
       "4    37.800000  6.373556  \n",
       "5     0.852101  0.015157  \n",
       "6     0.868483  0.018780  \n",
       "7     0.897688  0.016176  \n",
       "8     0.778330  0.028726  \n",
       "9     0.882685  0.012402  \n",
       "10    0.851235  0.015350  \n",
       "11    0.841223  0.016119  \n",
       "12    0.838009  0.016580  \n",
       "13    0.683826  0.031930  \n",
       "14    0.823290  0.027164  \n",
       "15    0.838009  0.016580  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test['std'] = mat_met_lgbm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_lgbm0</th>\n",
       "      <th>y_pred_lgbm1</th>\n",
       "      <th>y_pred_lgbm2</th>\n",
       "      <th>y_pred_lgbm3</th>\n",
       "      <th>y_pred_lgbm4</th>\n",
       "      <th>y_pred_lgbm_ave</th>\n",
       "      <th>y_pred_lgbm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4084049</td>\n",
       "      <td>0</td>\n",
       "      <td>6.26</td>\n",
       "      <td>8.878516</td>\n",
       "      <td>8.937248</td>\n",
       "      <td>9.085690</td>\n",
       "      <td>8.857421</td>\n",
       "      <td>9.108667</td>\n",
       "      <td>8.521257</td>\n",
       "      <td>1.015760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL2178343</td>\n",
       "      <td>1</td>\n",
       "      <td>5.96</td>\n",
       "      <td>6.114580</td>\n",
       "      <td>6.157364</td>\n",
       "      <td>5.939508</td>\n",
       "      <td>6.246302</td>\n",
       "      <td>6.325022</td>\n",
       "      <td>6.123796</td>\n",
       "      <td>0.139942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL454672</td>\n",
       "      <td>2</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.905282</td>\n",
       "      <td>6.537709</td>\n",
       "      <td>6.838597</td>\n",
       "      <td>6.742520</td>\n",
       "      <td>6.731499</td>\n",
       "      <td>6.759268</td>\n",
       "      <td>0.115045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4299417</td>\n",
       "      <td>3</td>\n",
       "      <td>8.14</td>\n",
       "      <td>7.512262</td>\n",
       "      <td>7.470262</td>\n",
       "      <td>7.548025</td>\n",
       "      <td>7.595083</td>\n",
       "      <td>7.436492</td>\n",
       "      <td>7.617021</td>\n",
       "      <td>0.239392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3692580</td>\n",
       "      <td>4</td>\n",
       "      <td>5.24</td>\n",
       "      <td>5.677999</td>\n",
       "      <td>5.905695</td>\n",
       "      <td>5.745454</td>\n",
       "      <td>5.773960</td>\n",
       "      <td>5.884241</td>\n",
       "      <td>5.704558</td>\n",
       "      <td>0.222018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL3775269</td>\n",
       "      <td>2966</td>\n",
       "      <td>7.40</td>\n",
       "      <td>6.966037</td>\n",
       "      <td>7.028249</td>\n",
       "      <td>6.599467</td>\n",
       "      <td>7.110020</td>\n",
       "      <td>7.099422</td>\n",
       "      <td>7.033866</td>\n",
       "      <td>0.237166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL3339019</td>\n",
       "      <td>2967</td>\n",
       "      <td>8.07</td>\n",
       "      <td>8.090380</td>\n",
       "      <td>7.919378</td>\n",
       "      <td>8.125828</td>\n",
       "      <td>7.922273</td>\n",
       "      <td>8.077443</td>\n",
       "      <td>8.034217</td>\n",
       "      <td>0.082072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL3771312</td>\n",
       "      <td>2968</td>\n",
       "      <td>6.52</td>\n",
       "      <td>6.400063</td>\n",
       "      <td>6.133403</td>\n",
       "      <td>6.105581</td>\n",
       "      <td>6.509356</td>\n",
       "      <td>5.929801</td>\n",
       "      <td>6.266367</td>\n",
       "      <td>0.222881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL3589701</td>\n",
       "      <td>2969</td>\n",
       "      <td>6.49</td>\n",
       "      <td>7.110973</td>\n",
       "      <td>7.029569</td>\n",
       "      <td>7.064424</td>\n",
       "      <td>7.360439</td>\n",
       "      <td>6.844358</td>\n",
       "      <td>6.983294</td>\n",
       "      <td>0.267677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL4092997</td>\n",
       "      <td>2970</td>\n",
       "      <td>7.47</td>\n",
       "      <td>7.213737</td>\n",
       "      <td>7.239544</td>\n",
       "      <td>7.200743</td>\n",
       "      <td>7.338344</td>\n",
       "      <td>7.471919</td>\n",
       "      <td>7.322381</td>\n",
       "      <td>0.113922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_lgbm0  y_pred_lgbm1  \\\n",
       "0         CHEMBL4084049            0     6.26      8.878516      8.937248   \n",
       "1         CHEMBL2178343            1     5.96      6.114580      6.157364   \n",
       "2          CHEMBL454672            2     6.80      6.905282      6.537709   \n",
       "3         CHEMBL4299417            3     8.14      7.512262      7.470262   \n",
       "4         CHEMBL3692580            4     5.24      5.677999      5.905695   \n",
       "...                 ...          ...      ...           ...           ...   \n",
       "2966      CHEMBL3775269         2966     7.40      6.966037      7.028249   \n",
       "2967      CHEMBL3339019         2967     8.07      8.090380      7.919378   \n",
       "2968      CHEMBL3771312         2968     6.52      6.400063      6.133403   \n",
       "2969      CHEMBL3589701         2969     6.49      7.110973      7.029569   \n",
       "2970      CHEMBL4092997         2970     7.47      7.213737      7.239544   \n",
       "\n",
       "      y_pred_lgbm2  y_pred_lgbm3  y_pred_lgbm4  y_pred_lgbm_ave  \\\n",
       "0         9.085690      8.857421      9.108667         8.521257   \n",
       "1         5.939508      6.246302      6.325022         6.123796   \n",
       "2         6.838597      6.742520      6.731499         6.759268   \n",
       "3         7.548025      7.595083      7.436492         7.617021   \n",
       "4         5.745454      5.773960      5.884241         5.704558   \n",
       "...            ...           ...           ...              ...   \n",
       "2966      6.599467      7.110020      7.099422         7.033866   \n",
       "2967      8.125828      7.922273      8.077443         8.034217   \n",
       "2968      6.105581      6.509356      5.929801         6.266367   \n",
       "2969      7.064424      7.360439      6.844358         6.983294   \n",
       "2970      7.200743      7.338344      7.471919         7.322381   \n",
       "\n",
       "      y_pred_lgbm_std  \n",
       "0            1.015760  \n",
       "1            0.139942  \n",
       "2            0.115045  \n",
       "3            0.239392  \n",
       "4            0.222018  \n",
       "...               ...  \n",
       "2966         0.237166  \n",
       "2967         0.082072  \n",
       "2968         0.222881  \n",
       "2969         0.267677  \n",
       "2970         0.113922  \n",
       "\n",
       "[2971 rows x 10 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_lgbm_cat = np.where((y_pred_optimized_lgbm >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_lgbm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_lgbm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "lgbm_run0 = data_lgbm[['y_test_idx0', 'y_test0', 'y_pred_lgbm0']]\n",
    "lgbm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "lgbm_run0.reset_index(inplace=True, drop=True)\n",
    "lgbm_run1 = data_lgbm[['y_test_idx1', 'y_test1', 'y_pred_lgbm1']]\n",
    "lgbm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "lgbm_run1.reset_index(inplace=True, drop=True)\n",
    "lgbm_run2 = data_lgbm[['y_test_idx2', 'y_test2', 'y_pred_lgbm2']]\n",
    "lgbm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "lgbm_run2.reset_index(inplace=True, drop=True)\n",
    "lgbm_run3 = data_lgbm[['y_test_idx3', 'y_test3', 'y_pred_lgbm3']]\n",
    "lgbm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "lgbm_run3.reset_index(inplace=True, drop=True)\n",
    "lgbm_run4 = data_lgbm[['y_test_idx4', 'y_test4', 'y_pred_lgbm4']]\n",
    "lgbm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "lgbm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "lgbm_5preds = pd.concat([chembl_id, lgbm_run0, lgbm_run1, lgbm_run2, lgbm_run3, lgbm_run4], axis=1)\n",
    "lgbm_5preds = lgbm_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_lgbm0', 'y_pred_lgbm1', 'y_pred_lgbm2', 'y_pred_lgbm3', 'y_pred_lgbm4']]\n",
    "lgbm_5preds['y_pred_lgbm_ave'] = lgbm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "lgbm_5preds['y_pred_lgbm_std'] = lgbm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "lgbm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db4ac315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model r2_score 0.6875 with a standard deviation of 0.0260\n",
      "LightGBM optimized model r2_score 0.6995 with a standard deviation of 0.0278\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#r2_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"r2\")\n",
    "r2_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"r2\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)r2_score %0.4f with a standard deviation of %0.4f\" % (r2_cv_lgbm_opt_testSet.mean(), r2_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(r2_cv_lgbm_opt), np.std(r2_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_lgbm.joblib']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm_reg, \"OUTPUT/lgbm_reg.joblib\")\n",
    "#joblib.dump(optimized_lgbm, \"OUTPUT/optimized_lgbm.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"OUTPUT/optimizedCV_lgbm.joblib\") \n",
    "#loaded_rf = joblib.load(\"OUTPUT/optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.673113     0.016691\n",
      "1                    TP       164.500000     8.592633\n",
      "2                    TN        86.400000     4.325634\n",
      "3                    FP        27.000000     4.714045\n",
      "4                    FN        19.200000     4.049691\n",
      "5              Accuracy         0.844502     0.023055\n",
      "6             Precision         0.858615     0.027174\n",
      "7           Sensitivity         0.895286     0.022657\n",
      "8           Specificity         0.763010     0.028324\n",
      "9              F1 score         0.876393     0.021587\n",
      "10  F1 score (weighted)         0.843407     0.023202\n",
      "11     F1 score (macro)         0.832953     0.022281\n",
      "12    Balanced Accuracy         0.829141     0.021464\n",
      "13                  MCC         0.667786     0.044512\n",
      "14                  NPV         0.818870     0.034463\n",
      "15              ROC_AUC         0.829141     0.021464\n",
      "CPU times: user 44min 52s, sys: 1.3 s, total: 44min 54s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_reg.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores),np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred>=6.6), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 12:03:50,281] A new study created in memory with name: XGBRegressor\n",
      "[I 2023-12-20 12:04:02,362] Trial 0 finished with value: 0.3844853360150289 and parameters: {'n_estimators': 606, 'eta': 0.004933388081264989, 'max_depth': 12, 'alpha': 0.47250000000000003, 'lambda': 31.848924799307763, 'max_bin': 268}. Best is trial 0 with value: 0.3844853360150289.\n",
      "[I 2023-12-20 12:04:15,925] Trial 1 finished with value: 0.667978127895575 and parameters: {'n_estimators': 466, 'eta': 0.015475368338753353, 'max_depth': 11, 'alpha': 0.0238, 'lambda': 10.317526140220128, 'max_bin': 391}. Best is trial 1 with value: 0.667978127895575.\n",
      "[I 2023-12-20 12:04:25,745] Trial 2 finished with value: 0.6812226496276657 and parameters: {'n_estimators': 697, 'eta': 0.08036768841580752, 'max_depth': 9, 'alpha': 0.9136000000000001, 'lambda': 14.837415534196865, 'max_bin': 434}. Best is trial 2 with value: 0.6812226496276657.\n",
      "[I 2023-12-20 12:04:38,451] Trial 3 finished with value: 0.6813655894817886 and parameters: {'n_estimators': 585, 'eta': 0.05627182338665728, 'max_depth': 9, 'alpha': 0.5145000000000001, 'lambda': 22.545879259666435, 'max_bin': 315}. Best is trial 3 with value: 0.6813655894817886.\n",
      "[I 2023-12-20 12:04:42,999] Trial 4 finished with value: 0.6759460168077369 and parameters: {'n_estimators': 232, 'eta': 0.08123863033524158, 'max_depth': 7, 'alpha': 0.5531, 'lambda': 20.51548251657203, 'max_bin': 410}. Best is trial 3 with value: 0.6813655894817886.\n",
      "[I 2023-12-20 12:04:50,110] Trial 5 finished with value: 0.6866494215202839 and parameters: {'n_estimators': 543, 'eta': 0.09232483212453962, 'max_depth': 5, 'alpha': 0.9425, 'lambda': 17.27122072706171, 'max_bin': 330}. Best is trial 5 with value: 0.6866494215202839.\n",
      "[I 2023-12-20 12:04:54,217] Trial 6 finished with value: 0.6629422888438952 and parameters: {'n_estimators': 213, 'eta': 0.055498745847812676, 'max_depth': 7, 'alpha': 0.7033, 'lambda': 14.819585728409775, 'max_bin': 500}. Best is trial 5 with value: 0.6866494215202839.\n",
      "[I 2023-12-20 12:05:03,063] Trial 7 finished with value: 0.6627934364551739 and parameters: {'n_estimators': 360, 'eta': 0.01581578260833838, 'max_depth': 9, 'alpha': 0.0146, 'lambda': 1.5837252212021498, 'max_bin': 342}. Best is trial 5 with value: 0.6866494215202839.\n",
      "[I 2023-12-20 12:05:15,151] Trial 8 finished with value: 0.6789094976184983 and parameters: {'n_estimators': 370, 'eta': 0.051998125508175494, 'max_depth': 12, 'alpha': 0.24750000000000003, 'lambda': 10.913267021393565, 'max_bin': 479}. Best is trial 5 with value: 0.6866494215202839.\n",
      "[I 2023-12-20 12:05:37,693] Trial 9 finished with value: 0.6789848844939929 and parameters: {'n_estimators': 886, 'eta': 0.026742402735086605, 'max_depth': 12, 'alpha': 0.2546, 'lambda': 7.7262540704118035, 'max_bin': 299}. Best is trial 5 with value: 0.6866494215202839.\n",
      "[I 2023-12-20 12:05:38,689] Trial 10 finished with value: 0.5664745446131513 and parameters: {'n_estimators': 61, 'eta': 0.08966552268936372, 'max_depth': 5, 'alpha': 0.9953000000000001, 'lambda': 37.26396358594006, 'max_bin': 351}. Best is trial 5 with value: 0.6866494215202839.\n",
      "[I 2023-12-20 12:05:45,413] Trial 11 finished with value: 0.6839384566197867 and parameters: {'n_estimators': 637, 'eta': 0.09621693648505003, 'max_depth': 5, 'alpha': 0.7562, 'lambda': 23.97706685319139, 'max_bin': 312}. Best is trial 5 with value: 0.6866494215202839.\n",
      "[I 2023-12-20 12:05:52,757] Trial 12 finished with value: 0.6854654641349838 and parameters: {'n_estimators': 791, 'eta': 0.09779372143178008, 'max_depth': 5, 'alpha': 0.8180000000000001, 'lambda': 26.287990632058065, 'max_bin': 270}. Best is trial 5 with value: 0.6866494215202839.\n",
      "[I 2023-12-20 12:06:01,155] Trial 13 finished with value: 0.6826927249602435 and parameters: {'n_estimators': 828, 'eta': 0.0986820379471708, 'max_depth': 6, 'alpha': 0.8345, 'lambda': 27.27293016571473, 'max_bin': 250}. Best is trial 5 with value: 0.6866494215202839.\n",
      "[I 2023-12-20 12:06:12,934] Trial 14 finished with value: 0.6880014516467357 and parameters: {'n_estimators': 748, 'eta': 0.06995155049214333, 'max_depth': 7, 'alpha': 0.7006, 'lambda': 28.564736955539086, 'max_bin': 282}. Best is trial 14 with value: 0.6880014516467357.\n",
      "[I 2023-12-20 12:06:25,276] Trial 15 finished with value: 0.6897303586015451 and parameters: {'n_estimators': 728, 'eta': 0.07047965876576312, 'max_depth': 7, 'alpha': 0.6266, 'lambda': 33.147206722706315, 'max_bin': 352}. Best is trial 15 with value: 0.6897303586015451.\n",
      "[I 2023-12-20 12:06:37,538] Trial 16 finished with value: 0.6861855858455919 and parameters: {'n_estimators': 715, 'eta': 0.0652132385762978, 'max_depth': 7, 'alpha': 0.6089, 'lambda': 39.954210258138204, 'max_bin': 367}. Best is trial 15 with value: 0.6897303586015451.\n",
      "[I 2023-12-20 12:06:49,861] Trial 17 finished with value: 0.6878235407074383 and parameters: {'n_estimators': 745, 'eta': 0.06966805588716414, 'max_depth': 8, 'alpha': 0.36920000000000003, 'lambda': 31.680768941094293, 'max_bin': 292}. Best is trial 15 with value: 0.6897303586015451.\n",
      "[I 2023-12-20 12:07:06,908] Trial 18 finished with value: 0.6909837714801064 and parameters: {'n_estimators': 871, 'eta': 0.043660304597448726, 'max_depth': 8, 'alpha': 0.6636000000000001, 'lambda': 31.66114386215811, 'max_bin': 435}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:07:27,229] Trial 19 finished with value: 0.6874893900048276 and parameters: {'n_estimators': 855, 'eta': 0.04081036581401076, 'max_depth': 10, 'alpha': 0.4062, 'lambda': 33.51307092637655, 'max_bin': 449}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:07:45,255] Trial 20 finished with value: 0.6861151238518007 and parameters: {'n_estimators': 897, 'eta': 0.03706599157761524, 'max_depth': 8, 'alpha': 0.6155, 'lambda': 35.50692277009998, 'max_bin': 401}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:07:55,648] Trial 21 finished with value: 0.6868111957660059 and parameters: {'n_estimators': 777, 'eta': 0.06826698122210213, 'max_depth': 6, 'alpha': 0.6825, 'lambda': 29.25999237060606, 'max_bin': 436}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:08:10,434] Trial 22 finished with value: 0.6865726218864674 and parameters: {'n_estimators': 675, 'eta': 0.04445546097140295, 'max_depth': 8, 'alpha': 0.6769000000000001, 'lambda': 28.593195755880537, 'max_bin': 370}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:08:21,827] Trial 23 finished with value: 0.6823868550255276 and parameters: {'n_estimators': 796, 'eta': 0.05973780674645515, 'max_depth': 6, 'alpha': 0.7918000000000001, 'lambda': 31.388404888626553, 'max_bin': 414}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:08:32,058] Trial 24 finished with value: 0.682407663969874 and parameters: {'n_estimators': 519, 'eta': 0.04858858692525005, 'max_depth': 7, 'alpha': 0.5640000000000001, 'lambda': 35.04218191029277, 'max_bin': 473}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:08:43,458] Trial 25 finished with value: 0.6841921837155651 and parameters: {'n_estimators': 753, 'eta': 0.07684060487051943, 'max_depth': 10, 'alpha': 0.8713000000000001, 'lambda': 24.971723138634434, 'max_bin': 358}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:08:56,340] Trial 26 finished with value: 0.6851006666061432 and parameters: {'n_estimators': 684, 'eta': 0.059227941557507664, 'max_depth': 8, 'alpha': 0.7461, 'lambda': 29.292088697806328, 'max_bin': 380}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:09:07,051] Trial 27 finished with value: 0.6836500861945554 and parameters: {'n_estimators': 827, 'eta': 0.07171459670797088, 'max_depth': 6, 'alpha': 0.4539, 'lambda': 38.423006454930956, 'max_bin': 329}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:09:19,688] Trial 28 finished with value: 0.6851937424295462 and parameters: {'n_estimators': 657, 'eta': 0.06431297936264993, 'max_depth': 7, 'alpha': 0.6294000000000001, 'lambda': 35.90759302029791, 'max_bin': 283}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:09:30,999] Trial 29 finished with value: 0.68311248879275 and parameters: {'n_estimators': 598, 'eta': 0.08612746353551701, 'max_depth': 10, 'alpha': 0.3098, 'lambda': 32.52216408938865, 'max_bin': 262}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:09:40,642] Trial 30 finished with value: 0.6845853106614805 and parameters: {'n_estimators': 440, 'eta': 0.07179512118886974, 'max_depth': 8, 'alpha': 0.48760000000000003, 'lambda': 30.711451129209184, 'max_bin': 423}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:09:52,710] Trial 31 finished with value: 0.6844962243743314 and parameters: {'n_estimators': 738, 'eta': 0.073568853710931, 'max_depth': 8, 'alpha': 0.44210000000000005, 'lambda': 33.160050223847406, 'max_bin': 304}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:10:04,143] Trial 32 finished with value: 0.6835343416891356 and parameters: {'n_estimators': 731, 'eta': 0.06311849513420127, 'max_depth': 7, 'alpha': 0.39540000000000003, 'lambda': 30.94846759565884, 'max_bin': 296}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:10:16,219] Trial 33 finished with value: 0.6833738983436721 and parameters: {'n_estimators': 849, 'eta': 0.07675342684011482, 'max_depth': 9, 'alpha': 0.2886, 'lambda': 27.20046957691833, 'max_bin': 287}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:10:26,486] Trial 34 finished with value: 0.6864988495323499 and parameters: {'n_estimators': 603, 'eta': 0.08490177416645336, 'max_depth': 8, 'alpha': 0.1282, 'lambda': 33.94299075994537, 'max_bin': 275}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:10:40,168] Trial 35 finished with value: 0.686596925882879 and parameters: {'n_estimators': 773, 'eta': 0.06800935368427458, 'max_depth': 9, 'alpha': 0.552, 'lambda': 31.406149373838222, 'max_bin': 388}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:10:51,008] Trial 36 finished with value: 0.6883609951586674 and parameters: {'n_estimators': 900, 'eta': 0.08050691561907831, 'max_depth': 7, 'alpha': 0.3421, 'lambda': 22.93143038477781, 'max_bin': 324}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:10:59,981] Trial 37 finished with value: 0.6886416049959937 and parameters: {'n_estimators': 892, 'eta': 0.08083756092459259, 'max_depth': 6, 'alpha': 0.1434, 'lambda': 21.97941705055628, 'max_bin': 331}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:11:09,089] Trial 38 finished with value: 0.686855764270638 and parameters: {'n_estimators': 882, 'eta': 0.08041637079912414, 'max_depth': 6, 'alpha': 0.11800000000000001, 'lambda': 22.034680446808228, 'max_bin': 331}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:11:17,593] Trial 39 finished with value: 0.6890134071222084 and parameters: {'n_estimators': 828, 'eta': 0.08455112168691319, 'max_depth': 6, 'alpha': 0.1638, 'lambda': 19.69637475793999, 'max_bin': 340}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:11:25,869] Trial 40 finished with value: 0.6835170258379609 and parameters: {'n_estimators': 819, 'eta': 0.09206386993287496, 'max_depth': 6, 'alpha': 0.09860000000000001, 'lambda': 20.363491881512807, 'max_bin': 349}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:11:34,642] Trial 41 finished with value: 0.6870804146953017 and parameters: {'n_estimators': 891, 'eta': 0.08400023992271177, 'max_depth': 6, 'alpha': 0.19410000000000002, 'lambda': 20.098282801126725, 'max_bin': 324}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:11:44,277] Trial 42 finished with value: 0.6832316766685957 and parameters: {'n_estimators': 860, 'eta': 0.07976024126619584, 'max_depth': 7, 'alpha': 0.1739, 'lambda': 17.864399448601063, 'max_bin': 317}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:11:53,138] Trial 43 finished with value: 0.6872047939987865 and parameters: {'n_estimators': 899, 'eta': 0.08891602404903229, 'max_depth': 7, 'alpha': 0.2044, 'lambda': 24.79789495113917, 'max_bin': 341}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:12:02,223] Trial 44 finished with value: 0.6862329962347399 and parameters: {'n_estimators': 817, 'eta': 0.07687029927261227, 'max_depth': 6, 'alpha': 0.0644, 'lambda': 22.669686717024977, 'max_bin': 363}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:12:08,753] Trial 45 finished with value: 0.6798407204021888 and parameters: {'n_estimators': 847, 'eta': 0.09420710529060361, 'max_depth': 5, 'alpha': 0.33480000000000004, 'lambda': 26.19505400093046, 'max_bin': 339}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:12:17,072] Trial 46 finished with value: 0.6820567454140167 and parameters: {'n_estimators': 794, 'eta': 0.08807931150455016, 'max_depth': 7, 'alpha': 0.0665, 'lambda': 17.881479244923497, 'max_bin': 308}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:12:21,201] Trial 47 finished with value: 0.6704036742286695 and parameters: {'n_estimators': 299, 'eta': 0.08462503129519672, 'max_depth': 5, 'alpha': 0.2313, 'lambda': 23.795477618846753, 'max_bin': 380}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:12:22,582] Trial 48 finished with value: 0.624228969976066 and parameters: {'n_estimators': 75, 'eta': 0.09369933324379681, 'max_depth': 6, 'alpha': 0.1529, 'lambda': 14.519388286338938, 'max_bin': 352}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:12:30,555] Trial 49 finished with value: 0.6846593948300506 and parameters: {'n_estimators': 699, 'eta': 0.099624716502835, 'max_depth': 7, 'alpha': 0.015300000000000001, 'lambda': 21.731820371522236, 'max_bin': 322}. Best is trial 18 with value: 0.6909837714801064.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6910\n",
      "\tBest params:\n",
      "\t\tn_estimators: 871\n",
      "\t\teta: 0.043660304597448726\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.6636000000000001\n",
      "\t\tlambda: 31.66114386215811\n",
      "\t\tmax_bin: 435\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBRegressor\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.733450\n",
      "1                    TP  349.000000\n",
      "2                    TN  168.000000\n",
      "3                    FP   44.000000\n",
      "4                    FN   34.000000\n",
      "5              Accuracy    0.868908\n",
      "6             Precision    0.888041\n",
      "7           Sensitivity    0.911227\n",
      "8           Specificity    0.792500\n",
      "9              F1 score    0.899485\n",
      "10  F1 score (weighted)    0.868169\n",
      "11     F1 score (macro)    0.855539\n",
      "12    Balanced Accuracy    0.851840\n",
      "13                  MCC    0.711657\n",
      "14                  NPV    0.831700\n",
      "15              ROC_AUC    0.851840\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_xgb_0)\n",
    "y_pred_xgb_0_cat = np.where((y_pred_xgb_0 >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 12:12:43,683] Trial 50 finished with value: 0.6909616445158642 and parameters: {'n_estimators': 866, 'eta': 0.08040596090614452, 'max_depth': 9, 'alpha': 0.5263, 'lambda': 26.767296873056623, 'max_bin': 401}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:12:55,900] Trial 51 finished with value: 0.6904513011882236 and parameters: {'n_estimators': 857, 'eta': 0.07915783476240157, 'max_depth': 9, 'alpha': 0.5319, 'lambda': 23.86750263752462, 'max_bin': 437}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:13:09,304] Trial 52 finished with value: 0.6896130897614058 and parameters: {'n_estimators': 862, 'eta': 0.0757211799006327, 'max_depth': 9, 'alpha': 0.5214, 'lambda': 26.568913383649775, 'max_bin': 464}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:13:20,895] Trial 53 finished with value: 0.6891827481941876 and parameters: {'n_estimators': 809, 'eta': 0.07487033479174779, 'max_depth': 9, 'alpha': 0.5268, 'lambda': 25.937795346802627, 'max_bin': 452}. Best is trial 18 with value: 0.6909837714801064.\n",
      "[I 2023-12-20 12:13:32,338] Trial 54 finished with value: 0.6916158083459727 and parameters: {'n_estimators': 853, 'eta': 0.07610767060023371, 'max_depth': 9, 'alpha': 0.5171, 'lambda': 26.222756972337205, 'max_bin': 453}. Best is trial 54 with value: 0.6916158083459727.\n",
      "[I 2023-12-20 12:13:45,453] Trial 55 finished with value: 0.6935182160518626 and parameters: {'n_estimators': 859, 'eta': 0.07305544942841566, 'max_depth': 9, 'alpha': 0.5818, 'lambda': 27.750471751587405, 'max_bin': 463}. Best is trial 55 with value: 0.6935182160518626.\n",
      "[I 2023-12-20 12:14:00,521] Trial 56 finished with value: 0.6909965876071151 and parameters: {'n_estimators': 775, 'eta': 0.05397793515016557, 'max_depth': 10, 'alpha': 0.5949, 'lambda': 28.123407787822035, 'max_bin': 485}. Best is trial 55 with value: 0.6935182160518626.\n",
      "[I 2023-12-20 12:14:16,561] Trial 57 finished with value: 0.6880540079901547 and parameters: {'n_estimators': 859, 'eta': 0.05302148677983868, 'max_depth': 11, 'alpha': 0.5888, 'lambda': 28.225680321393526, 'max_bin': 498}. Best is trial 55 with value: 0.6935182160518626.\n",
      "[I 2023-12-20 12:14:32,595] Trial 58 finished with value: 0.6906402645118139 and parameters: {'n_estimators': 773, 'eta': 0.05821863955991683, 'max_depth': 10, 'alpha': 0.7297, 'lambda': 29.9234390907177, 'max_bin': 436}. Best is trial 55 with value: 0.6935182160518626.\n",
      "[I 2023-12-20 12:14:47,365] Trial 59 finished with value: 0.691776155616543 and parameters: {'n_estimators': 643, 'eta': 0.057665391571655095, 'max_depth': 10, 'alpha': 0.6505000000000001, 'lambda': 30.045728039106617, 'max_bin': 489}. Best is trial 55 with value: 0.6935182160518626.\n",
      "[I 2023-12-20 12:15:02,887] Trial 60 finished with value: 0.6901443101916793 and parameters: {'n_estimators': 548, 'eta': 0.052239187543307995, 'max_depth': 11, 'alpha': 0.6554, 'lambda': 27.99898080840625, 'max_bin': 489}. Best is trial 55 with value: 0.6935182160518626.\n",
      "[I 2023-12-20 12:15:18,404] Trial 61 finished with value: 0.6917638732245509 and parameters: {'n_estimators': 754, 'eta': 0.05872889728976197, 'max_depth': 10, 'alpha': 0.7286, 'lambda': 27.3807553647431, 'max_bin': 461}. Best is trial 55 with value: 0.6935182160518626.\n",
      "[I 2023-12-20 12:15:34,448] Trial 62 finished with value: 0.6919794419853018 and parameters: {'n_estimators': 768, 'eta': 0.05360984644585245, 'max_depth': 10, 'alpha': 0.7838, 'lambda': 29.58830249613252, 'max_bin': 467}. Best is trial 55 with value: 0.6935182160518626.\n",
      "[I 2023-12-20 12:15:49,925] Trial 63 finished with value: 0.6897468808114379 and parameters: {'n_estimators': 626, 'eta': 0.055817269367842065, 'max_depth': 11, 'alpha': 0.7776000000000001, 'lambda': 29.614892020681953, 'max_bin': 468}. Best is trial 55 with value: 0.6935182160518626.\n",
      "[I 2023-12-20 12:16:07,011] Trial 64 finished with value: 0.693438847736128 and parameters: {'n_estimators': 697, 'eta': 0.048392735801415114, 'max_depth': 10, 'alpha': 0.65, 'lambda': 28.26620195375046, 'max_bin': 485}. Best is trial 55 with value: 0.6935182160518626.\n",
      "[I 2023-12-20 12:16:23,274] Trial 65 finished with value: 0.6926152922706523 and parameters: {'n_estimators': 654, 'eta': 0.04794722494972272, 'max_depth': 10, 'alpha': 0.7139, 'lambda': 28.187569277398005, 'max_bin': 481}. Best is trial 55 with value: 0.6935182160518626.\n",
      "[I 2023-12-20 12:16:40,134] Trial 66 finished with value: 0.6943768679583077 and parameters: {'n_estimators': 650, 'eta': 0.04842160259498763, 'max_depth': 10, 'alpha': 0.7108, 'lambda': 29.833720305355676, 'max_bin': 460}. Best is trial 66 with value: 0.6943768679583077.\n",
      "[I 2023-12-20 12:16:55,842] Trial 67 finished with value: 0.6928861439851117 and parameters: {'n_estimators': 661, 'eta': 0.04921526155480187, 'max_depth': 10, 'alpha': 0.7138, 'lambda': 30.5036279741777, 'max_bin': 461}. Best is trial 66 with value: 0.6943768679583077.\n",
      "[I 2023-12-20 12:17:10,874] Trial 68 finished with value: 0.6934975488009603 and parameters: {'n_estimators': 567, 'eta': 0.048487216217591854, 'max_depth': 10, 'alpha': 0.8151, 'lambda': 32.304392232695704, 'max_bin': 480}. Best is trial 66 with value: 0.6943768679583077.\n",
      "[I 2023-12-20 12:17:27,762] Trial 69 finished with value: 0.6921686787077801 and parameters: {'n_estimators': 571, 'eta': 0.048860853517175735, 'max_depth': 11, 'alpha': 0.8601000000000001, 'lambda': 32.780485825028876, 'max_bin': 478}. Best is trial 66 with value: 0.6943768679583077.\n",
      "[I 2023-12-20 12:17:45,025] Trial 70 finished with value: 0.6889651163943602 and parameters: {'n_estimators': 560, 'eta': 0.04876281156418788, 'max_depth': 12, 'alpha': 0.8480000000000001, 'lambda': 34.5800216669638, 'max_bin': 477}. Best is trial 66 with value: 0.6943768679583077.\n",
      "[I 2023-12-20 12:18:00,277] Trial 71 finished with value: 0.6866652388414263 and parameters: {'n_estimators': 512, 'eta': 0.0460085155698188, 'max_depth': 11, 'alpha': 0.9161, 'lambda': 32.35809836715642, 'max_bin': 500}. Best is trial 66 with value: 0.6943768679583077.\n",
      "[I 2023-12-20 12:18:16,084] Trial 72 finished with value: 0.6923257290189441 and parameters: {'n_estimators': 660, 'eta': 0.05024934316877879, 'max_depth': 10, 'alpha': 0.9924000000000001, 'lambda': 30.594935081745593, 'max_bin': 445}. Best is trial 66 with value: 0.6943768679583077.\n",
      "[I 2023-12-20 12:18:32,976] Trial 73 finished with value: 0.6901174264270531 and parameters: {'n_estimators': 570, 'eta': 0.04063441997056085, 'max_depth': 11, 'alpha': 0.9609000000000001, 'lambda': 31.731937575502464, 'max_bin': 445}. Best is trial 66 with value: 0.6943768679583077.\n",
      "[I 2023-12-20 12:18:46,030] Trial 74 finished with value: 0.6895338729177098 and parameters: {'n_estimators': 466, 'eta': 0.04909458283625117, 'max_depth': 10, 'alpha': 0.8781, 'lambda': 33.0099785715539, 'max_bin': 480}. Best is trial 66 with value: 0.6943768679583077.\n",
      "[I 2023-12-20 12:19:01,581] Trial 75 finished with value: 0.6946402540980345 and parameters: {'n_estimators': 664, 'eta': 0.05007155101920096, 'max_depth': 10, 'alpha': 0.9883000000000001, 'lambda': 30.273199992214998, 'max_bin': 459}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:19:18,426] Trial 76 finished with value: 0.6938360953899985 and parameters: {'n_estimators': 662, 'eta': 0.04572510076135466, 'max_depth': 10, 'alpha': 0.9992000000000001, 'lambda': 30.651093252555487, 'max_bin': 456}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:19:35,070] Trial 77 finished with value: 0.6922245318466006 and parameters: {'n_estimators': 622, 'eta': 0.045646932575012386, 'max_depth': 10, 'alpha': 0.8069000000000001, 'lambda': 29.16421718889081, 'max_bin': 455}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:19:54,367] Trial 78 finished with value: 0.6898924725085662 and parameters: {'n_estimators': 697, 'eta': 0.03641579530635391, 'max_depth': 10, 'alpha': 0.7117, 'lambda': 30.57256964636991, 'max_bin': 460}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:20:12,347] Trial 79 finished with value: 0.6904751486179898 and parameters: {'n_estimators': 672, 'eta': 0.04294979821839015, 'max_depth': 10, 'alpha': 0.9004000000000001, 'lambda': 34.35852693195934, 'max_bin': 421}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:20:28,142] Trial 80 finished with value: 0.692778078841745 and parameters: {'n_estimators': 517, 'eta': 0.05072261688692886, 'max_depth': 11, 'alpha': 0.9460000000000001, 'lambda': 28.839916095084824, 'max_bin': 472}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:20:43,995] Trial 81 finished with value: 0.6903440632881491 and parameters: {'n_estimators': 513, 'eta': 0.04688347809166568, 'max_depth': 11, 'alpha': 0.9540000000000001, 'lambda': 28.75920163761266, 'max_bin': 472}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:20:56,585] Trial 82 finished with value: 0.6908321914342881 and parameters: {'n_estimators': 439, 'eta': 0.05048128273011273, 'max_depth': 10, 'alpha': 0.9209, 'lambda': 31.717111967515912, 'max_bin': 493}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:21:11,354] Trial 83 finished with value: 0.6859724617923605 and parameters: {'n_estimators': 718, 'eta': 0.06161665865641221, 'max_depth': 12, 'alpha': 0.6879000000000001, 'lambda': 27.785548864782484, 'max_bin': 484}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:21:28,074] Trial 84 finished with value: 0.6914957875658077 and parameters: {'n_estimators': 596, 'eta': 0.051866264679740454, 'max_depth': 11, 'alpha': 0.9929, 'lambda': 31.171696668364493, 'max_bin': 473}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:21:44,345] Trial 85 finished with value: 0.6921701084083944 and parameters: {'n_estimators': 644, 'eta': 0.05508962210468638, 'max_depth': 10, 'alpha': 0.8278000000000001, 'lambda': 33.56281346280873, 'max_bin': 461}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:21:56,714] Trial 86 finished with value: 0.6869659176311315 and parameters: {'n_estimators': 490, 'eta': 0.04288590488534855, 'max_depth': 9, 'alpha': 0.7621, 'lambda': 29.10909703743806, 'max_bin': 425}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:22:13,547] Trial 87 finished with value: 0.6881669281352214 and parameters: {'n_estimators': 583, 'eta': 0.04718175413941991, 'max_depth': 11, 'alpha': 0.9663, 'lambda': 30.5081221507115, 'max_bin': 446}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:22:29,634] Trial 88 finished with value: 0.6868675702079036 and parameters: {'n_estimators': 621, 'eta': 0.03934506624451734, 'max_depth': 10, 'alpha': 0.9398000000000001, 'lambda': 25.618732555619363, 'max_bin': 471}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:22:43,629] Trial 89 finished with value: 0.6900544707407138 and parameters: {'n_estimators': 536, 'eta': 0.05602717343169664, 'max_depth': 10, 'alpha': 0.5710000000000001, 'lambda': 27.45849501784204, 'max_bin': 494}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:23:00,357] Trial 90 finished with value: 0.6912007812702872 and parameters: {'n_estimators': 685, 'eta': 0.04382348221938894, 'max_depth': 9, 'alpha': 0.6294000000000001, 'lambda': 32.70981063770555, 'max_bin': 457}. Best is trial 75 with value: 0.6946402540980345.\n",
      "[I 2023-12-20 12:23:15,819] Trial 91 finished with value: 0.6959783393233525 and parameters: {'n_estimators': 655, 'eta': 0.0506166923482371, 'max_depth': 10, 'alpha': 0.9786, 'lambda': 30.234544740700578, 'max_bin': 442}. Best is trial 91 with value: 0.6959783393233525.\n",
      "[I 2023-12-20 12:23:32,855] Trial 92 finished with value: 0.6913696299174094 and parameters: {'n_estimators': 653, 'eta': 0.05179738166274366, 'max_depth': 10, 'alpha': 0.8851, 'lambda': 28.863876882343313, 'max_bin': 482}. Best is trial 91 with value: 0.6959783393233525.\n",
      "[I 2023-12-20 12:23:49,431] Trial 93 finished with value: 0.6918265163282648 and parameters: {'n_estimators': 709, 'eta': 0.046984473656650254, 'max_depth': 10, 'alpha': 0.9767, 'lambda': 32.10481355075757, 'max_bin': 441}. Best is trial 91 with value: 0.6959783393233525.\n",
      "[I 2023-12-20 12:24:01,519] Trial 94 finished with value: 0.6909197601964744 and parameters: {'n_estimators': 434, 'eta': 0.06124632131431297, 'max_depth': 10, 'alpha': 0.9438000000000001, 'lambda': 30.1149968712262, 'max_bin': 429}. Best is trial 91 with value: 0.6959783393233525.\n",
      "[I 2023-12-20 12:24:17,455] Trial 95 finished with value: 0.6947984879123487 and parameters: {'n_estimators': 673, 'eta': 0.05065408301541243, 'max_depth': 9, 'alpha': 0.672, 'lambda': 28.40537519499354, 'max_bin': 464}. Best is trial 91 with value: 0.6959783393233525.\n",
      "[I 2023-12-20 12:24:31,144] Trial 96 finished with value: 0.6892963287451466 and parameters: {'n_estimators': 609, 'eta': 0.05657384384136576, 'max_depth': 9, 'alpha': 0.6444000000000001, 'lambda': 27.009129975159812, 'max_bin': 467}. Best is trial 91 with value: 0.6959783393233525.\n",
      "[I 2023-12-20 12:24:46,682] Trial 97 finished with value: 0.694083593577705 and parameters: {'n_estimators': 677, 'eta': 0.05069238809271743, 'max_depth': 9, 'alpha': 0.6891, 'lambda': 31.273430083846964, 'max_bin': 450}. Best is trial 91 with value: 0.6959783393233525.\n",
      "[I 2023-12-20 12:25:03,104] Trial 98 finished with value: 0.6910152333518096 and parameters: {'n_estimators': 676, 'eta': 0.045266531356806, 'max_depth': 9, 'alpha': 0.6824, 'lambda': 31.092431103892288, 'max_bin': 449}. Best is trial 91 with value: 0.6959783393233525.\n",
      "[I 2023-12-20 12:25:19,945] Trial 99 finished with value: 0.6947861206438082 and parameters: {'n_estimators': 728, 'eta': 0.053331340250776155, 'max_depth': 9, 'alpha': 0.7443000000000001, 'lambda': 33.85673273865346, 'max_bin': 457}. Best is trial 91 with value: 0.6959783393233525.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6960\n",
      "\tBest params:\n",
      "\t\tn_estimators: 655\n",
      "\t\teta: 0.0506166923482371\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.9786\n",
      "\t\tlambda: 30.234544740700578\n",
      "\t\tmax_bin: 442\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.733450    0.709249\n",
      "1                    TP  349.000000  326.000000\n",
      "2                    TN  168.000000  175.000000\n",
      "3                    FP   44.000000   55.000000\n",
      "4                    FN   34.000000   39.000000\n",
      "5              Accuracy    0.868908    0.842017\n",
      "6             Precision    0.888041    0.855643\n",
      "7           Sensitivity    0.911227    0.893151\n",
      "8           Specificity    0.792500    0.760900\n",
      "9              F1 score    0.899485    0.873995\n",
      "10  F1 score (weighted)    0.868169    0.840864\n",
      "11     F1 score (macro)    0.855539    0.831141\n",
      "12    Balanced Accuracy    0.851840    0.827010\n",
      "13                  MCC    0.711657    0.663639\n",
      "14                  NPV    0.831700    0.817800\n",
      "15              ROC_AUC    0.851840    0.827010\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_xgb_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_1_cat = np.where((y_pred_xgb_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 12:25:38,198] Trial 100 finished with value: 0.7044423270777538 and parameters: {'n_estimators': 729, 'eta': 0.05377098800571908, 'max_depth': 9, 'alpha': 0.7555000000000001, 'lambda': 35.21390932887055, 'max_bin': 455}. Best is trial 100 with value: 0.7044423270777538.\n",
      "[I 2023-12-20 12:25:53,792] Trial 101 finished with value: 0.7037075612093825 and parameters: {'n_estimators': 717, 'eta': 0.05415057247927819, 'max_depth': 9, 'alpha': 0.7416, 'lambda': 33.58077749893111, 'max_bin': 431}. Best is trial 100 with value: 0.7044423270777538.\n",
      "[I 2023-12-20 12:26:09,912] Trial 102 finished with value: 0.705214520901297 and parameters: {'n_estimators': 722, 'eta': 0.054399433036234734, 'max_depth': 9, 'alpha': 0.7556, 'lambda': 36.23218912466738, 'max_bin': 441}. Best is trial 102 with value: 0.705214520901297.\n",
      "[I 2023-12-20 12:26:26,057] Trial 103 finished with value: 0.7070320620717057 and parameters: {'n_estimators': 743, 'eta': 0.053554575310967655, 'max_depth': 9, 'alpha': 0.7589, 'lambda': 35.59370622566603, 'max_bin': 429}. Best is trial 103 with value: 0.7070320620717057.\n",
      "[I 2023-12-20 12:26:40,835] Trial 104 finished with value: 0.7057585265347426 and parameters: {'n_estimators': 745, 'eta': 0.053956940652181234, 'max_depth': 8, 'alpha': 0.7417, 'lambda': 35.65634912152552, 'max_bin': 410}. Best is trial 103 with value: 0.7070320620717057.\n",
      "[I 2023-12-20 12:26:56,025] Trial 105 finished with value: 0.7032414716876146 and parameters: {'n_estimators': 730, 'eta': 0.05407894578097976, 'max_depth': 8, 'alpha': 0.7606, 'lambda': 35.95033526066751, 'max_bin': 441}. Best is trial 103 with value: 0.7070320620717057.\n",
      "[I 2023-12-20 12:27:10,998] Trial 106 finished with value: 0.7030124343836206 and parameters: {'n_estimators': 728, 'eta': 0.05371740825764176, 'max_depth': 8, 'alpha': 0.7488, 'lambda': 36.26642521045007, 'max_bin': 412}. Best is trial 103 with value: 0.7070320620717057.\n",
      "[I 2023-12-20 12:27:26,726] Trial 107 finished with value: 0.7074441570677151 and parameters: {'n_estimators': 749, 'eta': 0.054845717051706634, 'max_depth': 8, 'alpha': 0.7489, 'lambda': 36.305982687842885, 'max_bin': 416}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:27:41,966] Trial 108 finished with value: 0.703982239558486 and parameters: {'n_estimators': 733, 'eta': 0.05445203445356409, 'max_depth': 8, 'alpha': 0.7991, 'lambda': 36.3826878737956, 'max_bin': 406}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:27:56,731] Trial 109 finished with value: 0.7027335074020173 and parameters: {'n_estimators': 750, 'eta': 0.05744556498822279, 'max_depth': 8, 'alpha': 0.7664000000000001, 'lambda': 36.307213758441065, 'max_bin': 410}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:28:11,292] Trial 110 finished with value: 0.7017617128008982 and parameters: {'n_estimators': 742, 'eta': 0.05974234774431502, 'max_depth': 8, 'alpha': 0.76, 'lambda': 36.29266932081945, 'max_bin': 411}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:28:26,149] Trial 111 finished with value: 0.7011252646010964 and parameters: {'n_estimators': 750, 'eta': 0.05743153022861404, 'max_depth': 8, 'alpha': 0.7665000000000001, 'lambda': 36.344990721748275, 'max_bin': 412}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:28:41,681] Trial 112 finished with value: 0.7023086462382628 and parameters: {'n_estimators': 751, 'eta': 0.05984245852300078, 'max_depth': 8, 'alpha': 0.7651, 'lambda': 36.70646179427502, 'max_bin': 408}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:28:56,372] Trial 113 finished with value: 0.7019983678940268 and parameters: {'n_estimators': 799, 'eta': 0.05963151998453031, 'max_depth': 8, 'alpha': 0.7984, 'lambda': 36.794405570877885, 'max_bin': 400}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:29:11,343] Trial 114 finished with value: 0.7055680093272886 and parameters: {'n_estimators': 798, 'eta': 0.05467225353126603, 'max_depth': 8, 'alpha': 0.7917000000000001, 'lambda': 37.54512982978868, 'max_bin': 403}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:29:26,696] Trial 115 finished with value: 0.7051096710191795 and parameters: {'n_estimators': 759, 'eta': 0.05514156670088466, 'max_depth': 8, 'alpha': 0.7345, 'lambda': 38.16286274003611, 'max_bin': 405}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:29:42,812] Trial 116 finished with value: 0.706027604251742 and parameters: {'n_estimators': 786, 'eta': 0.054765408111961664, 'max_depth': 8, 'alpha': 0.7382000000000001, 'lambda': 38.02161273826493, 'max_bin': 417}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:29:59,132] Trial 117 finished with value: 0.7065250544314564 and parameters: {'n_estimators': 791, 'eta': 0.05461861287275898, 'max_depth': 8, 'alpha': 0.7334, 'lambda': 37.573833203399744, 'max_bin': 418}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:30:15,442] Trial 118 finished with value: 0.7068955119808591 and parameters: {'n_estimators': 780, 'eta': 0.05494435295567921, 'max_depth': 8, 'alpha': 0.7326, 'lambda': 38.33487278072752, 'max_bin': 418}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:30:31,876] Trial 119 finished with value: 0.7053120919630674 and parameters: {'n_estimators': 836, 'eta': 0.055422630945506375, 'max_depth': 8, 'alpha': 0.8475, 'lambda': 38.068681057122745, 'max_bin': 419}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:30:47,272] Trial 120 finished with value: 0.705023427072128 and parameters: {'n_estimators': 838, 'eta': 0.06393209920653037, 'max_depth': 8, 'alpha': 0.8383, 'lambda': 37.86858224148802, 'max_bin': 419}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:31:01,447] Trial 121 finished with value: 0.7057565952353818 and parameters: {'n_estimators': 786, 'eta': 0.06283360607846572, 'max_depth': 8, 'alpha': 0.8483, 'lambda': 37.66730663040998, 'max_bin': 420}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:31:16,357] Trial 122 finished with value: 0.7055676333035634 and parameters: {'n_estimators': 834, 'eta': 0.06501735549662119, 'max_depth': 8, 'alpha': 0.8320000000000001, 'lambda': 37.86242790189786, 'max_bin': 419}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:31:30,808] Trial 123 finished with value: 0.7029874614560276 and parameters: {'n_estimators': 826, 'eta': 0.06364712573589869, 'max_depth': 8, 'alpha': 0.8442000000000001, 'lambda': 38.63581194285698, 'max_bin': 417}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:31:46,094] Trial 124 finished with value: 0.704961921203633 and parameters: {'n_estimators': 789, 'eta': 0.06526462568997181, 'max_depth': 8, 'alpha': 0.8252, 'lambda': 37.72183841245017, 'max_bin': 418}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:32:02,679] Trial 125 finished with value: 0.7071396231139955 and parameters: {'n_estimators': 805, 'eta': 0.05634628197893672, 'max_depth': 8, 'alpha': 0.7845000000000001, 'lambda': 38.99668291917717, 'max_bin': 396}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:32:17,806] Trial 126 finished with value: 0.7071742584129689 and parameters: {'n_estimators': 806, 'eta': 0.05589172643834978, 'max_depth': 8, 'alpha': 0.7286, 'lambda': 39.38577440892971, 'max_bin': 393}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:32:33,661] Trial 127 finished with value: 0.7034542308699214 and parameters: {'n_estimators': 806, 'eta': 0.061648482997522125, 'max_depth': 8, 'alpha': 0.8654000000000001, 'lambda': 39.32865932633308, 'max_bin': 392}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:32:49,167] Trial 128 finished with value: 0.7057414579922147 and parameters: {'n_estimators': 790, 'eta': 0.05742694089501445, 'max_depth': 8, 'alpha': 0.7906000000000001, 'lambda': 39.51385135133031, 'max_bin': 394}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:33:03,219] Trial 129 finished with value: 0.7022013952162146 and parameters: {'n_estimators': 786, 'eta': 0.056821153672006956, 'max_depth': 7, 'alpha': 0.7837000000000001, 'lambda': 39.779971704378575, 'max_bin': 394}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:33:18,301] Trial 130 finished with value: 0.7051310912166187 and parameters: {'n_estimators': 836, 'eta': 0.06620536416083302, 'max_depth': 8, 'alpha': 0.8108000000000001, 'lambda': 37.355675702245684, 'max_bin': 385}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:33:32,823] Trial 131 finished with value: 0.7044373727187878 and parameters: {'n_estimators': 811, 'eta': 0.05826941516993347, 'max_depth': 8, 'alpha': 0.7916000000000001, 'lambda': 38.91230109985901, 'max_bin': 397}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:33:47,987] Trial 132 finished with value: 0.7047877917641922 and parameters: {'n_estimators': 778, 'eta': 0.06141758591652854, 'max_depth': 8, 'alpha': 0.727, 'lambda': 38.45967929194669, 'max_bin': 426}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:34:04,421] Trial 133 finished with value: 0.704563075390914 and parameters: {'n_estimators': 798, 'eta': 0.056535031147322355, 'max_depth': 8, 'alpha': 0.8224, 'lambda': 37.56498571789647, 'max_bin': 376}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:34:19,483] Trial 134 finished with value: 0.7014205150614583 and parameters: {'n_estimators': 875, 'eta': 0.05915644439428116, 'max_depth': 8, 'alpha': 0.6993, 'lambda': 39.93608565055949, 'max_bin': 386}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:34:34,302] Trial 135 finished with value: 0.7040913338189576 and parameters: {'n_estimators': 765, 'eta': 0.05572048180300998, 'max_depth': 8, 'alpha': 0.8508, 'lambda': 37.260337744497086, 'max_bin': 404}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:34:48,736] Trial 136 finished with value: 0.7025924673538416 and parameters: {'n_estimators': 814, 'eta': 0.061824874613350914, 'max_depth': 8, 'alpha': 0.8915000000000001, 'lambda': 39.04244375410689, 'max_bin': 432}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:35:02,227] Trial 137 finished with value: 0.7032078492564444 and parameters: {'n_estimators': 781, 'eta': 0.0665876556009188, 'max_depth': 7, 'alpha': 0.781, 'lambda': 35.23097095625998, 'max_bin': 398}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:35:18,412] Trial 138 finished with value: 0.7040629084824206 and parameters: {'n_estimators': 832, 'eta': 0.05838006947438748, 'max_depth': 8, 'alpha': 0.8103, 'lambda': 38.33869709652566, 'max_bin': 423}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:35:34,059] Trial 139 finished with value: 0.7020545959118621 and parameters: {'n_estimators': 818, 'eta': 0.05233343598315073, 'max_depth': 8, 'alpha': 0.7247, 'lambda': 37.26148427516, 'max_bin': 415}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:35:48,760] Trial 140 finished with value: 0.7028124391356402 and parameters: {'n_estimators': 768, 'eta': 0.06394759059440043, 'max_depth': 8, 'alpha': 0.7846000000000001, 'lambda': 38.91478039351418, 'max_bin': 428}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:36:04,338] Trial 141 finished with value: 0.7008917464190715 and parameters: {'n_estimators': 839, 'eta': 0.06711270367543531, 'max_depth': 8, 'alpha': 0.8157000000000001, 'lambda': 37.29008834612502, 'max_bin': 384}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:36:19,946] Trial 142 finished with value: 0.7020371190441402 and parameters: {'n_estimators': 795, 'eta': 0.060186118270526515, 'max_depth': 8, 'alpha': 0.8320000000000001, 'lambda': 39.95943331554716, 'max_bin': 373}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:36:35,690] Trial 143 finished with value: 0.7016835341334812 and parameters: {'n_estimators': 842, 'eta': 0.055638691426899686, 'max_depth': 8, 'alpha': 0.7015, 'lambda': 37.90953026333419, 'max_bin': 390}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:36:50,974] Trial 144 finished with value: 0.7040933143385677 and parameters: {'n_estimators': 820, 'eta': 0.06316275438423555, 'max_depth': 8, 'alpha': 0.7973, 'lambda': 35.04759864851357, 'max_bin': 382}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:37:06,312] Trial 145 finished with value: 0.705675028956931 and parameters: {'n_estimators': 783, 'eta': 0.06562845560630448, 'max_depth': 8, 'alpha': 0.8635, 'lambda': 35.63310260901948, 'max_bin': 401}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:37:19,089] Trial 146 finished with value: 0.6980360745308283 and parameters: {'n_estimators': 787, 'eta': 0.06906991623122771, 'max_depth': 7, 'alpha': 0.8562000000000001, 'lambda': 35.69474548545123, 'max_bin': 403}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:37:34,832] Trial 147 finished with value: 0.705240784159683 and parameters: {'n_estimators': 766, 'eta': 0.052481668811264606, 'max_depth': 8, 'alpha': 0.8655, 'lambda': 38.975083852519504, 'max_bin': 421}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:37:38,031] Trial 148 finished with value: 0.6511083453938771 and parameters: {'n_estimators': 141, 'eta': 0.0528016745031687, 'max_depth': 8, 'alpha': 0.9034000000000001, 'lambda': 39.428901397726904, 'max_bin': 421}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:37:53,787] Trial 149 finished with value: 0.7070740265027815 and parameters: {'n_estimators': 768, 'eta': 0.05790167298966994, 'max_depth': 8, 'alpha': 0.8621000000000001, 'lambda': 38.4342205679171, 'max_bin': 415}. Best is trial 107 with value: 0.7074441570677151.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7074\n",
      "\tBest params:\n",
      "\t\tn_estimators: 749\n",
      "\t\teta: 0.054845717051706634\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.7489\n",
      "\t\tlambda: 36.305982687842885\n",
      "\t\tmax_bin: 416\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.733450    0.709249    0.685721\n",
      "1                    TP  349.000000  326.000000  335.000000\n",
      "2                    TN  168.000000  175.000000  172.000000\n",
      "3                    FP   44.000000   55.000000   50.000000\n",
      "4                    FN   34.000000   39.000000   38.000000\n",
      "5              Accuracy    0.868908    0.842017    0.852101\n",
      "6             Precision    0.888041    0.855643    0.870130\n",
      "7           Sensitivity    0.911227    0.893151    0.898123\n",
      "8           Specificity    0.792500    0.760900    0.774800\n",
      "9              F1 score    0.899485    0.873995    0.883905\n",
      "10  F1 score (weighted)    0.868169    0.840864    0.851217\n",
      "11     F1 score (macro)    0.855539    0.831141    0.840101\n",
      "12    Balanced Accuracy    0.851840    0.827010    0.836449\n",
      "13                  MCC    0.711657    0.663639    0.680989\n",
      "14                  NPV    0.831700    0.817800    0.819000\n",
      "15              ROC_AUC    0.851840    0.827010    0.836449\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_xgb_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_2_cat = np.where((y_pred_xgb_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 12:38:08,453] Trial 150 finished with value: 0.6788500057852017 and parameters: {'n_estimators': 801, 'eta': 0.05821854394354962, 'max_depth': 7, 'alpha': 0.8753000000000001, 'lambda': 38.14801208019432, 'max_bin': 414}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:38:22,394] Trial 151 finished with value: 0.6815103438334329 and parameters: {'n_estimators': 766, 'eta': 0.05667108633970791, 'max_depth': 8, 'alpha': 0.8646, 'lambda': 39.06942703303547, 'max_bin': 395}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:38:34,099] Trial 152 finished with value: 0.6769258075107494 and parameters: {'n_estimators': 775, 'eta': 0.062068980747741184, 'max_depth': 8, 'alpha': 0.9144, 'lambda': 38.446517979516166, 'max_bin': 409}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:38:48,140] Trial 153 finished with value: 0.6776949917340512 and parameters: {'n_estimators': 758, 'eta': 0.05184536112013584, 'max_depth': 8, 'alpha': 0.8833000000000001, 'lambda': 39.29706259150819, 'max_bin': 425}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:39:01,012] Trial 154 finished with value: 0.6784424569998573 and parameters: {'n_estimators': 802, 'eta': 0.06029660048718846, 'max_depth': 8, 'alpha': 0.8488, 'lambda': 37.25767596937719, 'max_bin': 420}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:39:15,048] Trial 155 finished with value: 0.6795156048909885 and parameters: {'n_estimators': 877, 'eta': 0.055612508187281015, 'max_depth': 8, 'alpha': 0.8325, 'lambda': 34.46863254054447, 'max_bin': 416}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:39:28,493] Trial 156 finished with value: 0.6797422609528867 and parameters: {'n_estimators': 780, 'eta': 0.057710054009568086, 'max_depth': 8, 'alpha': 0.7313000000000001, 'lambda': 35.62252875893283, 'max_bin': 433}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:39:39,829] Trial 157 finished with value: 0.6778600383277295 and parameters: {'n_estimators': 746, 'eta': 0.06498550045226024, 'max_depth': 8, 'alpha': 0.779, 'lambda': 38.090116715937874, 'max_bin': 400}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:39:54,542] Trial 158 finished with value: 0.6780542183378272 and parameters: {'n_estimators': 821, 'eta': 0.051967914133865076, 'max_depth': 8, 'alpha': 0.8059000000000001, 'lambda': 36.78300514948202, 'max_bin': 407}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:40:06,349] Trial 159 finished with value: 0.6730362994018847 and parameters: {'n_estimators': 850, 'eta': 0.060824133634683905, 'max_depth': 8, 'alpha': 0.8393, 'lambda': 39.016883954060056, 'max_bin': 416}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:40:20,696] Trial 160 finished with value: 0.6800246594769301 and parameters: {'n_estimators': 806, 'eta': 0.05395640584782697, 'max_depth': 8, 'alpha': 0.8718, 'lambda': 37.04235360407229, 'max_bin': 427}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:40:34,566] Trial 161 finished with value: 0.6796174346187037 and parameters: {'n_estimators': 706, 'eta': 0.055240188523439114, 'max_depth': 9, 'alpha': 0.7362000000000001, 'lambda': 35.65693115979555, 'max_bin': 435}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:40:47,100] Trial 162 finished with value: 0.6782663480264043 and parameters: {'n_estimators': 787, 'eta': 0.05866842701028364, 'max_depth': 8, 'alpha': 0.7484000000000001, 'lambda': 39.9561457019893, 'max_bin': 423}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:40:54,811] Trial 163 finished with value: 0.6721976723438947 and parameters: {'n_estimators': 336, 'eta': 0.054642193817243515, 'max_depth': 8, 'alpha': 0.772, 'lambda': 37.89860668285156, 'max_bin': 403}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:41:08,145] Trial 164 finished with value: 0.6787950006575503 and parameters: {'n_estimators': 769, 'eta': 0.05222031522092738, 'max_depth': 8, 'alpha': 0.7203, 'lambda': 38.62004622906095, 'max_bin': 411}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:41:20,627] Trial 165 finished with value: 0.6782880884338278 and parameters: {'n_estimators': 751, 'eta': 0.05670431956499958, 'max_depth': 8, 'alpha': 0.7946000000000001, 'lambda': 36.81526007475969, 'max_bin': 419}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:41:32,938] Trial 166 finished with value: 0.676046002192575 and parameters: {'n_estimators': 736, 'eta': 0.06326975649039099, 'max_depth': 9, 'alpha': 0.8193, 'lambda': 34.81686782910615, 'max_bin': 438}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:41:44,664] Trial 167 finished with value: 0.6810679905555744 and parameters: {'n_estimators': 787, 'eta': 0.07031817775392238, 'max_depth': 8, 'alpha': 0.7528, 'lambda': 37.73342508672096, 'max_bin': 391}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:41:59,139] Trial 168 finished with value: 0.6810525683588766 and parameters: {'n_estimators': 824, 'eta': 0.04995455112146406, 'max_depth': 8, 'alpha': 0.6722, 'lambda': 35.8520418117865, 'max_bin': 407}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:42:12,920] Trial 169 finished with value: 0.6761688095586372 and parameters: {'n_estimators': 766, 'eta': 0.053759004458451914, 'max_depth': 8, 'alpha': 0.7112, 'lambda': 38.66482487042858, 'max_bin': 430}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:42:25,414] Trial 170 finished with value: 0.6785604933799986 and parameters: {'n_estimators': 806, 'eta': 0.06016560280120997, 'max_depth': 9, 'alpha': 0.9271, 'lambda': 39.2633513301681, 'max_bin': 396}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:42:36,543] Trial 171 finished with value: 0.6764477434576653 and parameters: {'n_estimators': 840, 'eta': 0.0664070311983152, 'max_depth': 8, 'alpha': 0.8142, 'lambda': 37.35544973982077, 'max_bin': 388}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:42:46,872] Trial 172 finished with value: 0.6781061191391724 and parameters: {'n_estimators': 860, 'eta': 0.0685790819047591, 'max_depth': 8, 'alpha': 0.7791, 'lambda': 36.759348711304355, 'max_bin': 413}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:42:59,681] Trial 173 finished with value: 0.6793915176166541 and parameters: {'n_estimators': 831, 'eta': 0.057847534275173446, 'max_depth': 8, 'alpha': 0.8599, 'lambda': 37.591235368369894, 'max_bin': 403}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:43:09,950] Trial 174 finished with value: 0.6750046907154396 and parameters: {'n_estimators': 792, 'eta': 0.06473619167028906, 'max_depth': 8, 'alpha': 0.7923, 'lambda': 36.25854388457297, 'max_bin': 422}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:43:23,012] Trial 175 finished with value: 0.6800475119175395 and parameters: {'n_estimators': 812, 'eta': 0.05612379229200177, 'max_depth': 7, 'alpha': 0.8284, 'lambda': 38.2551641234229, 'max_bin': 377}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:43:36,363] Trial 176 finished with value: 0.6793371478887549 and parameters: {'n_estimators': 718, 'eta': 0.05236173209308274, 'max_depth': 8, 'alpha': 0.7591, 'lambda': 39.93761574177392, 'max_bin': 399}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:43:47,810] Trial 177 finished with value: 0.6769222616739144 and parameters: {'n_estimators': 758, 'eta': 0.06173505748115312, 'max_depth': 8, 'alpha': 0.8051, 'lambda': 34.35180342455535, 'max_bin': 415}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:43:58,877] Trial 178 finished with value: 0.67803429196174 and parameters: {'n_estimators': 782, 'eta': 0.06648256586214753, 'max_depth': 8, 'alpha': 0.9005000000000001, 'lambda': 37.00383219568073, 'max_bin': 428}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:44:11,107] Trial 179 finished with value: 0.6790888273157165 and parameters: {'n_estimators': 835, 'eta': 0.058954466149240936, 'max_depth': 8, 'alpha': 0.7443000000000001, 'lambda': 35.69790320630672, 'max_bin': 364}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:44:25,196] Trial 180 finished with value: 0.6782323434092629 and parameters: {'n_estimators': 738, 'eta': 0.05606512162060362, 'max_depth': 9, 'alpha': 0.8456, 'lambda': 38.707993179187014, 'max_bin': 407}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:44:38,088] Trial 181 finished with value: 0.6807351110859667 and parameters: {'n_estimators': 762, 'eta': 0.054980706640853666, 'max_depth': 8, 'alpha': 0.7235, 'lambda': 38.1141291937274, 'max_bin': 393}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:44:52,388] Trial 182 finished with value: 0.6807835999946542 and parameters: {'n_estimators': 797, 'eta': 0.053714629735943, 'max_depth': 8, 'alpha': 0.7364, 'lambda': 37.75975226009006, 'max_bin': 407}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:45:05,360] Trial 183 finished with value: 0.6806550395223516 and parameters: {'n_estimators': 749, 'eta': 0.057881024587545016, 'max_depth': 8, 'alpha': 0.7048, 'lambda': 39.05221178534838, 'max_bin': 402}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:45:19,136] Trial 184 finished with value: 0.6777080156333467 and parameters: {'n_estimators': 771, 'eta': 0.050630254899902666, 'max_depth': 8, 'alpha': 0.7788, 'lambda': 36.51895923404791, 'max_bin': 386}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:45:32,779] Trial 185 finished with value: 0.6761260353920397 and parameters: {'n_estimators': 816, 'eta': 0.052791661713143645, 'max_depth': 8, 'alpha': 0.7453000000000001, 'lambda': 35.089654217591345, 'max_bin': 419}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:45:45,972] Trial 186 finished with value: 0.6804009547166481 and parameters: {'n_estimators': 780, 'eta': 0.05469807718208179, 'max_depth': 8, 'alpha': 0.8063, 'lambda': 37.539456384229226, 'max_bin': 412}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:45:57,522] Trial 187 finished with value: 0.676249661505611 and parameters: {'n_estimators': 757, 'eta': 0.06230158419322129, 'max_depth': 8, 'alpha': 0.6948000000000001, 'lambda': 38.39623865271228, 'max_bin': 425}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:46:09,707] Trial 188 finished with value: 0.6784665326650661 and parameters: {'n_estimators': 716, 'eta': 0.05975490797932751, 'max_depth': 8, 'alpha': 0.7725000000000001, 'lambda': 39.34199838343165, 'max_bin': 396}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:46:23,433] Trial 189 finished with value: 0.6793670464550197 and parameters: {'n_estimators': 797, 'eta': 0.05588789519630857, 'max_depth': 8, 'alpha': 0.8875000000000001, 'lambda': 37.0246999051634, 'max_bin': 410}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:46:37,251] Trial 190 finished with value: 0.6806888831095712 and parameters: {'n_estimators': 846, 'eta': 0.05741471981354208, 'max_depth': 9, 'alpha': 0.8296, 'lambda': 36.00818030197808, 'max_bin': 418}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:46:48,300] Trial 191 finished with value: 0.675429999868887 and parameters: {'n_estimators': 825, 'eta': 0.06489465845451757, 'max_depth': 8, 'alpha': 0.8449000000000001, 'lambda': 37.89093729238027, 'max_bin': 420}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:46:59,802] Trial 192 finished with value: 0.6775054940621251 and parameters: {'n_estimators': 840, 'eta': 0.06835442112396856, 'max_depth': 8, 'alpha': 0.873, 'lambda': 38.272458341812424, 'max_bin': 415}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:47:11,154] Trial 193 finished with value: 0.678062406913263 and parameters: {'n_estimators': 874, 'eta': 0.07108687008131871, 'max_depth': 8, 'alpha': 0.8332, 'lambda': 39.3401569139685, 'max_bin': 433}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:47:22,320] Trial 194 finished with value: 0.6751811942216625 and parameters: {'n_estimators': 809, 'eta': 0.0635121034594641, 'max_depth': 8, 'alpha': 0.7644000000000001, 'lambda': 37.414375552618225, 'max_bin': 403}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:47:36,189] Trial 195 finished with value: 0.6790028667324377 and parameters: {'n_estimators': 779, 'eta': 0.05124525038941933, 'max_depth': 8, 'alpha': 0.7958000000000001, 'lambda': 36.49623414889257, 'max_bin': 423}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:47:41,922] Trial 196 finished with value: 0.6665033355923999 and parameters: {'n_estimators': 250, 'eta': 0.06051363691501319, 'max_depth': 8, 'alpha': 0.8602000000000001, 'lambda': 38.50484442191251, 'max_bin': 410}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:47:54,463] Trial 197 finished with value: 0.6763728175515621 and parameters: {'n_estimators': 856, 'eta': 0.05456666910786565, 'max_depth': 8, 'alpha': 0.7341000000000001, 'lambda': 39.91647142450348, 'max_bin': 380}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:48:08,554] Trial 198 finished with value: 0.6790221268543217 and parameters: {'n_estimators': 748, 'eta': 0.04916451041113465, 'max_depth': 8, 'alpha': 0.8094, 'lambda': 37.68630797781794, 'max_bin': 428}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:48:20,610] Trial 199 finished with value: 0.6800728893238068 and parameters: {'n_estimators': 791, 'eta': 0.06614394543723333, 'max_depth': 8, 'alpha': 0.7566, 'lambda': 35.18085675168248, 'max_bin': 418}. Best is trial 107 with value: 0.7074441570677151.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7074\n",
      "\tBest params:\n",
      "\t\tn_estimators: 749\n",
      "\t\teta: 0.054845717051706634\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.7489\n",
      "\t\tlambda: 36.305982687842885\n",
      "\t\tmax_bin: 416\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.733450    0.709249    0.685721    0.744080\n",
      "1                    TP  349.000000  326.000000  335.000000  333.000000\n",
      "2                    TN  168.000000  175.000000  172.000000  183.000000\n",
      "3                    FP   44.000000   55.000000   50.000000   43.000000\n",
      "4                    FN   34.000000   39.000000   38.000000   36.000000\n",
      "5              Accuracy    0.868908    0.842017    0.852101    0.867227\n",
      "6             Precision    0.888041    0.855643    0.870130    0.885638\n",
      "7           Sensitivity    0.911227    0.893151    0.898123    0.902439\n",
      "8           Specificity    0.792500    0.760900    0.774800    0.809700\n",
      "9              F1 score    0.899485    0.873995    0.883905    0.893960\n",
      "10  F1 score (weighted)    0.868169    0.840864    0.851217    0.866806\n",
      "11     F1 score (macro)    0.855539    0.831141    0.840101    0.858216\n",
      "12    Balanced Accuracy    0.851840    0.827010    0.836449    0.856087\n",
      "13                  MCC    0.711657    0.663639    0.680989    0.716700\n",
      "14                  NPV    0.831700    0.817800    0.819000    0.835600\n",
      "15              ROC_AUC    0.851840    0.827010    0.836449    0.856087\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_xgb_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_3_cat = np.where((y_pred_xgb_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 12:48:35,483] Trial 200 finished with value: 0.6959443378119495 and parameters: {'n_estimators': 831, 'eta': 0.058515432922029896, 'max_depth': 7, 'alpha': 0.7883, 'lambda': 36.95105983377507, 'max_bin': 400}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:48:49,386] Trial 201 finished with value: 0.6978809652552751 and parameters: {'n_estimators': 795, 'eta': 0.06454431660049155, 'max_depth': 8, 'alpha': 0.8229000000000001, 'lambda': 37.84724149204441, 'max_bin': 417}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:49:03,249] Trial 202 finished with value: 0.7028297548028009 and parameters: {'n_estimators': 774, 'eta': 0.06821887443761854, 'max_depth': 8, 'alpha': 0.8458, 'lambda': 38.891284864876575, 'max_bin': 422}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:49:17,094] Trial 203 finished with value: 0.6997189959026023 and parameters: {'n_estimators': 811, 'eta': 0.06307015469789969, 'max_depth': 8, 'alpha': 0.8184, 'lambda': 38.12358698818207, 'max_bin': 406}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:49:30,725] Trial 204 finished with value: 0.6974532388338631 and parameters: {'n_estimators': 764, 'eta': 0.06561306007962733, 'max_depth': 8, 'alpha': 0.8684000000000001, 'lambda': 36.20137150346726, 'max_bin': 413}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:49:45,310] Trial 205 finished with value: 0.6973099846634188 and parameters: {'n_estimators': 743, 'eta': 0.053001327878175415, 'max_depth': 8, 'alpha': 0.4747, 'lambda': 34.07679346062125, 'max_bin': 425}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:50:01,351] Trial 206 finished with value: 0.6944453376081223 and parameters: {'n_estimators': 788, 'eta': 0.05506903902046069, 'max_depth': 8, 'alpha': 0.7148, 'lambda': 37.189036092693705, 'max_bin': 393}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:50:17,614] Trial 207 finished with value: 0.697771106036912 and parameters: {'n_estimators': 819, 'eta': 0.05682551935636581, 'max_depth': 8, 'alpha': 0.8926000000000001, 'lambda': 39.21577313436004, 'max_bin': 438}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:50:32,109] Trial 208 finished with value: 0.6988872214732135 and parameters: {'n_estimators': 800, 'eta': 0.06093378710324866, 'max_depth': 8, 'alpha': 0.8367, 'lambda': 38.43546419964665, 'max_bin': 389}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:50:44,705] Trial 209 finished with value: 0.6981103464226763 and parameters: {'n_estimators': 735, 'eta': 0.06959320317176873, 'max_depth': 9, 'alpha': 0.7755000000000001, 'lambda': 39.9837053236371, 'max_bin': 417}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:50:59,407] Trial 210 finished with value: 0.6967168033052658 and parameters: {'n_estimators': 767, 'eta': 0.05195763260063973, 'max_depth': 8, 'alpha': 0.7478, 'lambda': 35.61435681015041, 'max_bin': 431}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:51:13,743] Trial 211 finished with value: 0.6999858388002884 and parameters: {'n_estimators': 775, 'eta': 0.06174928075251923, 'max_depth': 8, 'alpha': 0.7267, 'lambda': 37.67379556456119, 'max_bin': 426}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:51:28,566] Trial 212 finished with value: 0.6983902935364387 and parameters: {'n_estimators': 780, 'eta': 0.05905109647217946, 'max_depth': 8, 'alpha': 0.6953, 'lambda': 38.69038563095563, 'max_bin': 421}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:51:42,260] Trial 213 finished with value: 0.7019027129521879 and parameters: {'n_estimators': 756, 'eta': 0.06268254581127239, 'max_depth': 8, 'alpha': 0.7248, 'lambda': 36.85152228654163, 'max_bin': 412}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:51:55,128] Trial 214 finished with value: 0.6976082827243226 and parameters: {'n_estimators': 799, 'eta': 0.06744591844295117, 'max_depth': 8, 'alpha': 0.7656000000000001, 'lambda': 38.14186831947702, 'max_bin': 406}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:52:10,992] Trial 215 finished with value: 0.699391349983076 and parameters: {'n_estimators': 819, 'eta': 0.05651842246827939, 'max_depth': 8, 'alpha': 0.7942, 'lambda': 39.198291386049675, 'max_bin': 398}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:52:26,112] Trial 216 finished with value: 0.701701830059079 and parameters: {'n_estimators': 785, 'eta': 0.0644681062204006, 'max_depth': 8, 'alpha': 0.8539, 'lambda': 37.341289929990154, 'max_bin': 416}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:52:39,966] Trial 217 finished with value: 0.6951139831457586 and parameters: {'n_estimators': 728, 'eta': 0.05400316176822109, 'max_depth': 8, 'alpha': 0.7403000000000001, 'lambda': 36.26824680189418, 'max_bin': 428}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:52:55,906] Trial 218 finished with value: 0.5867460676920697 and parameters: {'n_estimators': 829, 'eta': 0.005374784975145666, 'max_depth': 8, 'alpha': 0.8138000000000001, 'lambda': 38.57568498282896, 'max_bin': 421}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:53:10,171] Trial 219 finished with value: 0.7009143585059281 and parameters: {'n_estimators': 851, 'eta': 0.07271973009433186, 'max_depth': 8, 'alpha': 0.7774000000000001, 'lambda': 37.794556560351694, 'max_bin': 410}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:53:24,522] Trial 220 finished with value: 0.6951059324330804 and parameters: {'n_estimators': 769, 'eta': 0.06013583674364931, 'max_depth': 8, 'alpha': 0.7544000000000001, 'lambda': 39.33552279394225, 'max_bin': 434}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:53:39,551] Trial 221 finished with value: 0.6955010388133251 and parameters: {'n_estimators': 807, 'eta': 0.05666705791620283, 'max_depth': 8, 'alpha': 0.8243, 'lambda': 37.42198509102145, 'max_bin': 374}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:53:48,485] Trial 222 finished with value: 0.6897461834127464 and parameters: {'n_estimators': 400, 'eta': 0.05562517934908833, 'max_depth': 8, 'alpha': 0.8353, 'lambda': 38.257979686888255, 'max_bin': 377}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:54:04,381] Trial 223 finished with value: 0.7015626522707531 and parameters: {'n_estimators': 797, 'eta': 0.05787247156433088, 'max_depth': 8, 'alpha': 0.866, 'lambda': 36.798153148933174, 'max_bin': 415}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:54:20,244] Trial 224 finished with value: 0.6964533655283559 and parameters: {'n_estimators': 791, 'eta': 0.05316885241626701, 'max_depth': 8, 'alpha': 0.4091, 'lambda': 34.845214290175704, 'max_bin': 384}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:54:36,033] Trial 225 finished with value: 0.699353516363674 and parameters: {'n_estimators': 749, 'eta': 0.05561780387968716, 'max_depth': 8, 'alpha': 0.8033, 'lambda': 37.56390037671552, 'max_bin': 403}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:54:52,499] Trial 226 finished with value: 0.6977106250527599 and parameters: {'n_estimators': 810, 'eta': 0.051161975900359424, 'max_depth': 8, 'alpha': 0.7144, 'lambda': 38.714421537477506, 'max_bin': 424}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:55:07,362] Trial 227 finished with value: 0.7002832733010039 and parameters: {'n_estimators': 764, 'eta': 0.058639228202087175, 'max_depth': 8, 'alpha': 0.8206, 'lambda': 35.942087167646456, 'max_bin': 392}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:55:22,191] Trial 228 finished with value: 0.6961745944430806 and parameters: {'n_estimators': 840, 'eta': 0.06160184042974938, 'max_depth': 9, 'alpha': 0.8849, 'lambda': 37.00188505061206, 'max_bin': 407}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:55:35,460] Trial 229 finished with value: 0.697621372177821 and parameters: {'n_estimators': 782, 'eta': 0.06517260305010482, 'max_depth': 8, 'alpha': 0.8514, 'lambda': 39.380062480448444, 'max_bin': 418}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:55:50,401] Trial 230 finished with value: 0.6972724432808854 and parameters: {'n_estimators': 828, 'eta': 0.05379553867749853, 'max_depth': 8, 'alpha': 0.6718000000000001, 'lambda': 33.45738767999964, 'max_bin': 398}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:56:05,614] Trial 231 finished with value: 0.6924994211789401 and parameters: {'n_estimators': 718, 'eta': 0.054243013615892524, 'max_depth': 9, 'alpha': 0.7586, 'lambda': 35.3184606478367, 'max_bin': 411}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:56:21,126] Trial 232 finished with value: 0.698266595391332 and parameters: {'n_estimators': 738, 'eta': 0.05693932304981365, 'max_depth': 9, 'alpha': 0.7362000000000001, 'lambda': 36.34853929161871, 'max_bin': 421}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:56:37,559] Trial 233 finished with value: 0.6987884395101538 and parameters: {'n_estimators': 753, 'eta': 0.053365974879364814, 'max_depth': 9, 'alpha': 0.7731, 'lambda': 38.29102925309949, 'max_bin': 366}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:56:53,500] Trial 234 finished with value: 0.6958321114380732 and parameters: {'n_estimators': 696, 'eta': 0.05056227051223089, 'max_depth': 9, 'alpha': 0.7888000000000001, 'lambda': 34.50355795164478, 'max_bin': 425}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:57:07,959] Trial 235 finished with value: 0.6950295468319705 and parameters: {'n_estimators': 783, 'eta': 0.055945050181426946, 'max_depth': 8, 'alpha': 0.7472000000000001, 'lambda': 35.32049861325369, 'max_bin': 414}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:57:23,579] Trial 236 finished with value: 0.7007884509355439 and parameters: {'n_estimators': 807, 'eta': 0.05919545259233019, 'max_depth': 8, 'alpha': 0.729, 'lambda': 37.735750073849566, 'max_bin': 369}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:57:38,905] Trial 237 finished with value: 0.6964881212983373 and parameters: {'n_estimators': 732, 'eta': 0.052483493582728726, 'max_depth': 8, 'alpha': 0.7985, 'lambda': 39.99177690489147, 'max_bin': 443}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:57:53,874] Trial 238 finished with value: 0.696061148612398 and parameters: {'n_estimators': 766, 'eta': 0.05488114200654408, 'max_depth': 8, 'alpha': 0.8351000000000001, 'lambda': 36.63862586559381, 'max_bin': 386}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:58:07,923] Trial 239 finished with value: 0.7011657534725872 and parameters: {'n_estimators': 794, 'eta': 0.06318554662486131, 'max_depth': 8, 'alpha': 0.7613000000000001, 'lambda': 38.66592692910864, 'max_bin': 431}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:58:20,002] Trial 240 finished with value: 0.6934353474071974 and parameters: {'n_estimators': 867, 'eta': 0.06664960319303216, 'max_depth': 8, 'alpha': 0.6956, 'lambda': 24.106068368730135, 'max_bin': 403}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:58:34,969] Trial 241 finished with value: 0.6932596299158482 and parameters: {'n_estimators': 820, 'eta': 0.05730438098859137, 'max_depth': 8, 'alpha': 0.7865000000000001, 'lambda': 38.86502081423668, 'max_bin': 397}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:58:50,466] Trial 242 finished with value: 0.6978077414862133 and parameters: {'n_estimators': 806, 'eta': 0.05821037951287857, 'max_depth': 8, 'alpha': 0.8168000000000001, 'lambda': 37.82660414066999, 'max_bin': 251}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:59:03,142] Trial 243 finished with value: 0.6944871319856954 and parameters: {'n_estimators': 773, 'eta': 0.05506995019141923, 'max_depth': 8, 'alpha': 0.7897000000000001, 'lambda': 13.191927179990067, 'max_bin': 394}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:59:16,340] Trial 244 finished with value: 0.694270639983075 and parameters: {'n_estimators': 786, 'eta': 0.06060100418238501, 'max_depth': 8, 'alpha': 0.8513000000000001, 'lambda': 18.497338920116768, 'max_bin': 388}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:59:32,454] Trial 245 finished with value: 0.6983372188795495 and parameters: {'n_estimators': 842, 'eta': 0.0570083099125196, 'max_depth': 8, 'alpha': 0.7524000000000001, 'lambda': 37.249021475714805, 'max_bin': 418}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 12:59:47,457] Trial 246 finished with value: 0.6953101544319412 and parameters: {'n_estimators': 755, 'eta': 0.05226936268228527, 'max_depth': 8, 'alpha': 0.8092, 'lambda': 39.07117515310917, 'max_bin': 408}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:00:05,165] Trial 247 finished with value: 0.6992062212925079 and parameters: {'n_estimators': 816, 'eta': 0.04933221030767334, 'max_depth': 9, 'alpha': 0.7749, 'lambda': 38.233758308637405, 'max_bin': 400}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:00:15,967] Trial 248 finished with value: 0.6984912890889616 and parameters: {'n_estimators': 799, 'eta': 0.09690389160600019, 'max_depth': 8, 'alpha': 0.8331000000000001, 'lambda': 36.06434214093494, 'max_bin': 412}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:00:30,217] Trial 249 finished with value: 0.6993141538632838 and parameters: {'n_estimators': 777, 'eta': 0.059458490513746784, 'max_depth': 8, 'alpha': 0.7159, 'lambda': 39.16908229831024, 'max_bin': 426}. Best is trial 107 with value: 0.7074441570677151.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7074\n",
      "\tBest params:\n",
      "\t\tn_estimators: 749\n",
      "\t\teta: 0.054845717051706634\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.7489\n",
      "\t\tlambda: 36.305982687842885\n",
      "\t\tmax_bin: 416\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.733450    0.709249    0.685721    0.744080   \n",
      "1                    TP  349.000000  326.000000  335.000000  333.000000   \n",
      "2                    TN  168.000000  175.000000  172.000000  183.000000   \n",
      "3                    FP   44.000000   55.000000   50.000000   43.000000   \n",
      "4                    FN   34.000000   39.000000   38.000000   36.000000   \n",
      "5              Accuracy    0.868908    0.842017    0.852101    0.867227   \n",
      "6             Precision    0.888041    0.855643    0.870130    0.885638   \n",
      "7           Sensitivity    0.911227    0.893151    0.898123    0.902439   \n",
      "8           Specificity    0.792500    0.760900    0.774800    0.809700   \n",
      "9              F1 score    0.899485    0.873995    0.883905    0.893960   \n",
      "10  F1 score (weighted)    0.868169    0.840864    0.851217    0.866806   \n",
      "11     F1 score (macro)    0.855539    0.831141    0.840101    0.858216   \n",
      "12    Balanced Accuracy    0.851840    0.827010    0.836449    0.856087   \n",
      "13                  MCC    0.711657    0.663639    0.680989    0.716700   \n",
      "14                  NPV    0.831700    0.817800    0.819000    0.835600   \n",
      "15              ROC_AUC    0.851840    0.827010    0.836449    0.856087   \n",
      "\n",
      "          Set4  \n",
      "0     0.680602  \n",
      "1   334.000000  \n",
      "2   173.000000  \n",
      "3    58.000000  \n",
      "4    30.000000  \n",
      "5     0.852101  \n",
      "6     0.852041  \n",
      "7     0.917582  \n",
      "8     0.748900  \n",
      "9     0.883598  \n",
      "10    0.850069  \n",
      "11    0.840416  \n",
      "12    0.833250  \n",
      "13    0.685119  \n",
      "14    0.852200  \n",
      "15    0.833250  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_xgb_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_4_cat = np.where((y_pred_xgb_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c6c1fb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_xgb_4_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 13:00:42,476] Trial 250 finished with value: 0.689080597736644 and parameters: {'n_estimators': 739, 'eta': 0.05412068604544054, 'max_depth': 8, 'alpha': 0.8751, 'lambda': 4.714440963790171, 'max_bin': 420}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:00:56,287] Trial 251 finished with value: 0.6909412731462352 and parameters: {'n_estimators': 832, 'eta': 0.061814227287679144, 'max_depth': 8, 'alpha': 0.736, 'lambda': 37.308079686829885, 'max_bin': 405}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:01:09,005] Trial 252 finished with value: 0.688961700163102 and parameters: {'n_estimators': 758, 'eta': 0.06478168248042654, 'max_depth': 8, 'alpha': 0.8005, 'lambda': 38.14631683536538, 'max_bin': 379}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:01:23,896] Trial 253 finished with value: 0.6887055257014367 and parameters: {'n_estimators': 809, 'eta': 0.057025982479271695, 'max_depth': 9, 'alpha': 0.766, 'lambda': 36.6390072092107, 'max_bin': 449}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:01:38,324] Trial 254 finished with value: 0.6902173182741603 and parameters: {'n_estimators': 715, 'eta': 0.055066764578886686, 'max_depth': 8, 'alpha': 0.8524, 'lambda': 35.615812693703226, 'max_bin': 415}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:01:54,017] Trial 255 finished with value: 0.6929793302800201 and parameters: {'n_estimators': 790, 'eta': 0.051692743654291363, 'max_depth': 8, 'alpha': 0.8232, 'lambda': 39.457826422548315, 'max_bin': 395}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:02:06,440] Trial 256 finished with value: 0.6879663564250487 and parameters: {'n_estimators': 770, 'eta': 0.058563896809660385, 'max_depth': 8, 'alpha': 0.7418, 'lambda': 37.58758099841871, 'max_bin': 423}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:02:17,667] Trial 257 finished with value: 0.6863334883553389 and parameters: {'n_estimators': 826, 'eta': 0.06787619101937949, 'max_depth': 8, 'alpha': 0.9076000000000001, 'lambda': 22.13146684573926, 'max_bin': 437}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:02:31,180] Trial 258 finished with value: 0.6877692999487521 and parameters: {'n_estimators': 852, 'eta': 0.06346936977164092, 'max_depth': 8, 'alpha': 0.7826000000000001, 'lambda': 39.98569822858894, 'max_bin': 390}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:02:45,981] Trial 259 finished with value: 0.6891864263182625 and parameters: {'n_estimators': 793, 'eta': 0.05272960792890214, 'max_depth': 8, 'alpha': 0.7108, 'lambda': 25.84876172594028, 'max_bin': 409}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:02:56,472] Trial 260 finished with value: 0.69076952594774 and parameters: {'n_estimators': 746, 'eta': 0.0703635658623288, 'max_depth': 8, 'alpha': 0.7589, 'lambda': 21.26373946859517, 'max_bin': 428}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:03:12,155] Trial 261 finished with value: 0.6904075842636543 and parameters: {'n_estimators': 811, 'eta': 0.05607461774958752, 'max_depth': 9, 'alpha': 0.8369000000000001, 'lambda': 38.543923655579135, 'max_bin': 415}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:03:26,343] Trial 262 finished with value: 0.68702315946504 and parameters: {'n_estimators': 775, 'eta': 0.06054051765737471, 'max_depth': 8, 'alpha': 0.8709, 'lambda': 34.40740050243698, 'max_bin': 399}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:03:40,054] Trial 263 finished with value: 0.6934586918147083 and parameters: {'n_estimators': 760, 'eta': 0.0661567600464, 'max_depth': 8, 'alpha': 0.8077000000000001, 'lambda': 36.887619760705896, 'max_bin': 383}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:03:54,725] Trial 264 finished with value: 0.6878445904546745 and parameters: {'n_estimators': 889, 'eta': 0.047518973366331894, 'max_depth': 8, 'alpha': 0.7792, 'lambda': 37.9181568427738, 'max_bin': 421}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:04:08,042] Trial 265 finished with value: 0.6883667178314659 and parameters: {'n_estimators': 727, 'eta': 0.05399807326891512, 'max_depth': 8, 'alpha': 0.7363000000000001, 'lambda': 38.59886813540346, 'max_bin': 418}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:04:19,964] Trial 266 finished with value: 0.6864946975030127 and parameters: {'n_estimators': 700, 'eta': 0.058857961499249746, 'max_depth': 7, 'alpha': 0.7251000000000001, 'lambda': 36.1992173172826, 'max_bin': 405}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:04:34,708] Trial 267 finished with value: 0.6876787791229793 and parameters: {'n_estimators': 797, 'eta': 0.05067135132633002, 'max_depth': 8, 'alpha': 0.798, 'lambda': 37.22869705395742, 'max_bin': 412}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:04:48,719] Trial 268 finished with value: 0.6880566671527428 and parameters: {'n_estimators': 829, 'eta': 0.05685813602455942, 'max_depth': 8, 'alpha': 0.8592000000000001, 'lambda': 33.25241925785506, 'max_bin': 432}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:05:03,047] Trial 269 finished with value: 0.6882920483653928 and parameters: {'n_estimators': 784, 'eta': 0.0625253315171113, 'max_depth': 9, 'alpha': 0.8263, 'lambda': 39.25123113688929, 'max_bin': 401}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:05:17,941] Trial 270 finished with value: 0.6888480235923466 and parameters: {'n_estimators': 812, 'eta': 0.05499990736480996, 'max_depth': 8, 'alpha': 0.759, 'lambda': 35.23412026636374, 'max_bin': 426}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:05:32,372] Trial 271 finished with value: 0.6891726418517419 and parameters: {'n_estimators': 759, 'eta': 0.053029272440524355, 'max_depth': 8, 'alpha': 0.8439000000000001, 'lambda': 37.937239487964455, 'max_bin': 394}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:05:51,143] Trial 272 finished with value: 0.6893085050191663 and parameters: {'n_estimators': 842, 'eta': 0.03344414780572509, 'max_depth': 9, 'alpha': 0.8912, 'lambda': 32.666444947915245, 'max_bin': 410}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:06:03,084] Trial 273 finished with value: 0.6886406540598884 and parameters: {'n_estimators': 778, 'eta': 0.08649524636506997, 'max_depth': 8, 'alpha': 0.7041000000000001, 'lambda': 38.80340165937763, 'max_bin': 373}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:06:17,306] Trial 274 finished with value: 0.6898863594106793 and parameters: {'n_estimators': 742, 'eta': 0.05796808976788927, 'max_depth': 8, 'alpha': 0.6823, 'lambda': 36.58536018993506, 'max_bin': 419}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:06:31,672] Trial 275 finished with value: 0.6902024231689947 and parameters: {'n_estimators': 802, 'eta': 0.05032877462417134, 'max_depth': 8, 'alpha': 0.7751, 'lambda': 34.086250695572815, 'max_bin': 414}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:06:44,140] Trial 276 finished with value: 0.6881221321077803 and parameters: {'n_estimators': 771, 'eta': 0.06070493733592244, 'max_depth': 8, 'alpha': 0.8139000000000001, 'lambda': 37.551074385234074, 'max_bin': 405}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:06:58,799] Trial 277 finished with value: 0.6886335079485381 and parameters: {'n_estimators': 791, 'eta': 0.05569992241408472, 'max_depth': 8, 'alpha': 0.7432000000000001, 'lambda': 35.57297468064042, 'max_bin': 390}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:07:11,085] Trial 278 finished with value: 0.6884069310643763 and parameters: {'n_estimators': 726, 'eta': 0.0743964181396561, 'max_depth': 8, 'alpha': 0.7933, 'lambda': 38.45253488546474, 'max_bin': 422}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:07:23,828] Trial 279 finished with value: 0.688452352079336 and parameters: {'n_estimators': 824, 'eta': 0.06416622643412478, 'max_depth': 8, 'alpha': 0.7218, 'lambda': 19.971621983169662, 'max_bin': 430}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:07:37,260] Trial 280 finished with value: 0.6876303291465499 and parameters: {'n_estimators': 863, 'eta': 0.05301977716281108, 'max_depth': 8, 'alpha': 0.275, 'lambda': 39.49153319434597, 'max_bin': 398}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:07:48,106] Trial 281 finished with value: 0.6922358833949822 and parameters: {'n_estimators': 756, 'eta': 0.06724199295447232, 'max_depth': 7, 'alpha': 0.8738, 'lambda': 15.55851247246115, 'max_bin': 381}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:08:02,846] Trial 282 finished with value: 0.6912074754978275 and parameters: {'n_estimators': 809, 'eta': 0.057733414821627534, 'max_depth': 9, 'alpha': 0.7596, 'lambda': 36.998766591679434, 'max_bin': 416}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:08:18,024] Trial 283 finished with value: 0.6919301497208121 and parameters: {'n_estimators': 840, 'eta': 0.04856202798112035, 'max_depth': 8, 'alpha': 0.8239000000000001, 'lambda': 36.06001954749924, 'max_bin': 409}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:08:32,687] Trial 284 finished with value: 0.6891508713509403 and parameters: {'n_estimators': 785, 'eta': 0.0597122887388261, 'max_depth': 8, 'alpha': 0.8469, 'lambda': 39.9681360635266, 'max_bin': 424}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:08:35,501] Trial 285 finished with value: 0.6400060676917333 and parameters: {'n_estimators': 122, 'eta': 0.05488634522984894, 'max_depth': 8, 'alpha': 0.789, 'lambda': 37.80936615551383, 'max_bin': 438}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:08:47,814] Trial 286 finished with value: 0.687112979192104 and parameters: {'n_estimators': 744, 'eta': 0.06223569094374623, 'max_depth': 8, 'alpha': 0.7446, 'lambda': 34.821767569928966, 'max_bin': 403}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:08:58,049] Trial 287 finished with value: 0.6894371861491478 and parameters: {'n_estimators': 767, 'eta': 0.0895648129340788, 'max_depth': 8, 'alpha': 0.81, 'lambda': 38.899185688461266, 'max_bin': 360}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:09:11,418] Trial 288 finished with value: 0.6888194642748925 and parameters: {'n_estimators': 819, 'eta': 0.06917169601934917, 'max_depth': 9, 'alpha': 0.7704000000000001, 'lambda': 36.82260021462333, 'max_bin': 416}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:09:26,076] Trial 289 finished with value: 0.6881575790920147 and parameters: {'n_estimators': 792, 'eta': 0.05638860830246194, 'max_depth': 8, 'alpha': 0.8591000000000001, 'lambda': 38.14485992996027, 'max_bin': 389}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:09:40,893] Trial 290 finished with value: 0.6891393049300489 and parameters: {'n_estimators': 805, 'eta': 0.051963144062351085, 'max_depth': 8, 'alpha': 0.7248, 'lambda': 37.56168527625267, 'max_bin': 428}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:09:54,524] Trial 291 finished with value: 0.6904209507813406 and parameters: {'n_estimators': 775, 'eta': 0.06524197305430893, 'max_depth': 8, 'alpha': 0.8382000000000001, 'lambda': 38.86184938284724, 'max_bin': 410}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:10:07,254] Trial 292 finished with value: 0.6912087451027531 and parameters: {'n_estimators': 853, 'eta': 0.07220019987595509, 'max_depth': 9, 'alpha': 0.7529, 'lambda': 36.00460437661434, 'max_bin': 422}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:10:20,190] Trial 293 finished with value: 0.6922616062302354 and parameters: {'n_estimators': 737, 'eta': 0.05382665544864703, 'max_depth': 8, 'alpha': 0.5472, 'lambda': 27.5633251970001, 'max_bin': 445}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:10:32,721] Trial 294 finished with value: 0.6890324430444982 and parameters: {'n_estimators': 711, 'eta': 0.058920148536026534, 'max_depth': 8, 'alpha': 0.7027, 'lambda': 32.218089456071596, 'max_bin': 397}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:10:43,657] Trial 295 finished with value: 0.68567063761142 and parameters: {'n_estimators': 755, 'eta': 0.06062279303814927, 'max_depth': 8, 'alpha': 0.6085, 'lambda': 22.728513351423263, 'max_bin': 406}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:10:55,633] Trial 296 finished with value: 0.686721662984625 and parameters: {'n_estimators': 832, 'eta': 0.05671476049812677, 'max_depth': 8, 'alpha': 0.9277000000000001, 'lambda': 25.18630964264661, 'max_bin': 414}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:11:13,540] Trial 297 finished with value: 0.6704947723655387 and parameters: {'n_estimators': 800, 'eta': 0.01564433846854211, 'max_depth': 8, 'alpha': 0.8944000000000001, 'lambda': 38.2961059330188, 'max_bin': 419}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:11:25,169] Trial 298 finished with value: 0.6882505497499066 and parameters: {'n_estimators': 779, 'eta': 0.05065032237534388, 'max_depth': 7, 'alpha': 0.7972, 'lambda': 20.983717918204757, 'max_bin': 384}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:11:40,216] Trial 299 finished with value: 0.6907485955250626 and parameters: {'n_estimators': 817, 'eta': 0.05447462118669573, 'max_depth': 8, 'alpha': 0.8215, 'lambda': 37.06729019190036, 'max_bin': 394}. Best is trial 107 with value: 0.7074441570677151.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7074\n",
      "\tBest params:\n",
      "\t\tn_estimators: 749\n",
      "\t\teta: 0.054845717051706634\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.7489\n",
      "\t\tlambda: 36.305982687842885\n",
      "\t\tmax_bin: 416\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.733450    0.709249    0.685721    0.744080   \n",
      "1                    TP  349.000000  326.000000  335.000000  333.000000   \n",
      "2                    TN  168.000000  175.000000  172.000000  183.000000   \n",
      "3                    FP   44.000000   55.000000   50.000000   43.000000   \n",
      "4                    FN   34.000000   39.000000   38.000000   36.000000   \n",
      "5              Accuracy    0.868908    0.842017    0.852101    0.867227   \n",
      "6             Precision    0.888041    0.855643    0.870130    0.885638   \n",
      "7           Sensitivity    0.911227    0.893151    0.898123    0.902439   \n",
      "8           Specificity    0.792500    0.760900    0.774800    0.809700   \n",
      "9              F1 score    0.899485    0.873995    0.883905    0.893960   \n",
      "10  F1 score (weighted)    0.868169    0.840864    0.851217    0.866806   \n",
      "11     F1 score (macro)    0.855539    0.831141    0.840101    0.858216   \n",
      "12    Balanced Accuracy    0.851840    0.827010    0.836449    0.856087   \n",
      "13                  MCC    0.711657    0.663639    0.680989    0.716700   \n",
      "14                  NPV    0.831700    0.817800    0.819000    0.835600   \n",
      "15              ROC_AUC    0.851840    0.827010    0.836449    0.856087   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.680602    0.715920  \n",
      "1   334.000000  345.000000  \n",
      "2   173.000000  176.000000  \n",
      "3    58.000000   42.000000  \n",
      "4    30.000000   32.000000  \n",
      "5     0.852101    0.875630  \n",
      "6     0.852041    0.891473  \n",
      "7     0.917582    0.915119  \n",
      "8     0.748900    0.807300  \n",
      "9     0.883598    0.903141  \n",
      "10    0.850069    0.874984  \n",
      "11    0.840416    0.864716  \n",
      "12    0.833250    0.861229  \n",
      "13    0.685119    0.730003  \n",
      "14    0.852200    0.846200  \n",
      "15    0.833250    0.861229  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_xgb_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "\n",
    "y_pred_xgb_5_cat = np.where((y_pred_xgb_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 13:11:56,681] Trial 300 finished with value: 0.6913896774027146 and parameters: {'n_estimators': 872, 'eta': 0.06364882036096932, 'max_depth': 8, 'alpha': 0.7676000000000001, 'lambda': 39.38883542930747, 'max_bin': 433}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:12:09,607] Trial 301 finished with value: 0.685607258503265 and parameters: {'n_estimators': 765, 'eta': 0.05805191248057508, 'max_depth': 8, 'alpha': 0.7831, 'lambda': 29.567578371530114, 'max_bin': 424}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:12:25,434] Trial 302 finished with value: 0.6889583570629182 and parameters: {'n_estimators': 793, 'eta': 0.0520834576101595, 'max_depth': 9, 'alpha': 0.8789, 'lambda': 33.709423382905086, 'max_bin': 400}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:12:37,691] Trial 303 finished with value: 0.6891684731976995 and parameters: {'n_estimators': 746, 'eta': 0.06691233693383554, 'max_depth': 8, 'alpha': 0.7362000000000001, 'lambda': 36.40289585906671, 'max_bin': 411}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:12:51,688] Trial 304 finished with value: 0.6880422677470556 and parameters: {'n_estimators': 841, 'eta': 0.06169602171929095, 'max_depth': 8, 'alpha': 0.8376, 'lambda': 37.843911177566234, 'max_bin': 420}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:13:06,248] Trial 305 finished with value: 0.6890901950470294 and parameters: {'n_estimators': 817, 'eta': 0.054860939048674985, 'max_depth': 8, 'alpha': 0.8083, 'lambda': 34.94046718890173, 'max_bin': 405}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:13:21,175] Trial 306 finished with value: 0.6901453515198288 and parameters: {'n_estimators': 691, 'eta': 0.0564401600475964, 'max_depth': 8, 'alpha': 0.8603000000000001, 'lambda': 38.59060675029593, 'max_bin': 451}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:13:37,233] Trial 307 finished with value: 0.6882631801490134 and parameters: {'n_estimators': 782, 'eta': 0.04929826566799532, 'max_depth': 8, 'alpha': 0.75, 'lambda': 37.33717674888785, 'max_bin': 415}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:13:52,719] Trial 308 finished with value: 0.6887315317481674 and parameters: {'n_estimators': 728, 'eta': 0.059535865257268124, 'max_depth': 9, 'alpha': 0.6896, 'lambda': 35.44222457429418, 'max_bin': 429}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:14:08,340] Trial 309 finished with value: 0.6887240011295195 and parameters: {'n_estimators': 766, 'eta': 0.05319800553771283, 'max_depth': 8, 'alpha': 0.7163, 'lambda': 39.31287203544791, 'max_bin': 409}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:14:19,294] Trial 310 finished with value: 0.6877637701624221 and parameters: {'n_estimators': 794, 'eta': 0.06366620860400189, 'max_depth': 8, 'alpha': 0.7755000000000001, 'lambda': 10.925803234411028, 'max_bin': 387}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:14:32,789] Trial 311 finished with value: 0.686577376180793 and parameters: {'n_estimators': 810, 'eta': 0.06534753976261912, 'max_depth': 8, 'alpha': 0.7939, 'lambda': 36.6439154000012, 'max_bin': 419}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:14:43,508] Trial 312 finished with value: 0.6869309523173766 and parameters: {'n_estimators': 489, 'eta': 0.05606194388707695, 'max_depth': 8, 'alpha': 0.6542, 'lambda': 38.249831856293106, 'max_bin': 441}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:15:04,292] Trial 313 finished with value: 0.6860530778428652 and parameters: {'n_estimators': 831, 'eta': 0.024492647583583514, 'max_depth': 9, 'alpha': 0.047900000000000005, 'lambda': 39.94404324683986, 'max_bin': 402}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:15:19,192] Trial 314 finished with value: 0.6907235174728481 and parameters: {'n_estimators': 753, 'eta': 0.05805717073402351, 'max_depth': 8, 'alpha': 0.8235, 'lambda': 37.47023289817621, 'max_bin': 392}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:15:33,464] Trial 315 finished with value: 0.6868148147578308 and parameters: {'n_estimators': 778, 'eta': 0.04605669156107594, 'max_depth': 8, 'alpha': 0.7517, 'lambda': 27.099571282601676, 'max_bin': 426}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:15:49,969] Trial 316 finished with value: 0.6892878340354947 and parameters: {'n_estimators': 856, 'eta': 0.05116624900786396, 'max_depth': 8, 'alpha': 0.8515, 'lambda': 35.82229513659058, 'max_bin': 375}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:16:02,455] Trial 317 finished with value: 0.6868442414707949 and parameters: {'n_estimators': 802, 'eta': 0.06902602719795557, 'max_depth': 7, 'alpha': 0.7299, 'lambda': 38.841493229184486, 'max_bin': 415}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:16:16,239] Trial 318 finished with value: 0.6895910728510823 and parameters: {'n_estimators': 723, 'eta': 0.060841030394081355, 'max_depth': 8, 'alpha': 0.7721, 'lambda': 36.63000405062281, 'max_bin': 436}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:16:25,929] Trial 319 finished with value: 0.6781189167093444 and parameters: {'n_estimators': 766, 'eta': 0.05424813022695341, 'max_depth': 5, 'alpha': 0.8133, 'lambda': 31.33723314326124, 'max_bin': 396}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:16:37,817] Trial 320 finished with value: 0.6899017461003092 and parameters: {'n_estimators': 825, 'eta': 0.07773025809399252, 'max_depth': 8, 'alpha': 0.9072, 'lambda': 24.143616748495752, 'max_bin': 423}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:16:51,737] Trial 321 finished with value: 0.6878145980142609 and parameters: {'n_estimators': 790, 'eta': 0.05725196691686003, 'max_depth': 9, 'alpha': 0.8731, 'lambda': 18.366230994925303, 'max_bin': 412}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:17:06,469] Trial 322 finished with value: 0.6872897445331887 and parameters: {'n_estimators': 747, 'eta': 0.052974789363469006, 'max_depth': 8, 'alpha': 0.7959, 'lambda': 38.10537859119805, 'max_bin': 406}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:17:22,007] Trial 323 finished with value: 0.6900039405288692 and parameters: {'n_estimators': 805, 'eta': 0.04820896037839845, 'max_depth': 8, 'alpha': 0.8389000000000001, 'lambda': 34.416888637208515, 'max_bin': 418}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:17:35,180] Trial 324 finished with value: 0.6866349446068281 and parameters: {'n_estimators': 776, 'eta': 0.05525313762399639, 'max_depth': 8, 'alpha': 0.7617, 'lambda': 28.64356495667556, 'max_bin': 430}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:17:48,756] Trial 325 finished with value: 0.6881329313049978 and parameters: {'n_estimators': 709, 'eta': 0.06250246254979006, 'max_depth': 8, 'alpha': 0.7157, 'lambda': 38.8865389939159, 'max_bin': 398}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:18:04,450] Trial 326 finished with value: 0.6919046773872694 and parameters: {'n_estimators': 845, 'eta': 0.05857125195147048, 'max_depth': 8, 'alpha': 0.7423000000000001, 'lambda': 37.39638167914195, 'max_bin': 381}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:18:18,271] Trial 327 finished with value: 0.6878574511033445 and parameters: {'n_estimators': 760, 'eta': 0.07142220836061897, 'max_depth': 9, 'alpha': 0.7842, 'lambda': 39.240872251071714, 'max_bin': 455}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:18:32,475] Trial 328 finished with value: 0.6893429548400243 and parameters: {'n_estimators': 820, 'eta': 0.06635120943585687, 'max_depth': 8, 'alpha': 0.8149000000000001, 'lambda': 36.09081863152979, 'max_bin': 410}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:18:46,384] Trial 329 finished with value: 0.6880482826736838 and parameters: {'n_estimators': 740, 'eta': 0.0601325901388405, 'max_depth': 8, 'alpha': 0.6983, 'lambda': 37.94166034089089, 'max_bin': 424}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:19:01,969] Trial 330 finished with value: 0.6888999696538884 and parameters: {'n_estimators': 792, 'eta': 0.05124436448060503, 'max_depth': 8, 'alpha': 0.8523000000000001, 'lambda': 39.99274842350252, 'max_bin': 402}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:19:16,476] Trial 331 finished with value: 0.6889106269947807 and parameters: {'n_estimators': 884, 'eta': 0.05602799963235687, 'max_depth': 8, 'alpha': 0.733, 'lambda': 26.69279152076767, 'max_bin': 418}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:19:30,446] Trial 332 finished with value: 0.6874840525573461 and parameters: {'n_estimators': 781, 'eta': 0.05406763889788757, 'max_depth': 8, 'alpha': 0.8356, 'lambda': 36.92875019699992, 'max_bin': 407}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:19:38,675] Trial 333 finished with value: 0.683382805688964 and parameters: {'n_estimators': 330, 'eta': 0.06381840103295644, 'max_depth': 9, 'alpha': 0.8807, 'lambda': 19.199552670928785, 'max_bin': 392}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:19:52,775] Trial 334 finished with value: 0.6887000243585183 and parameters: {'n_estimators': 802, 'eta': 0.06788383980333235, 'max_depth': 8, 'alpha': 0.759, 'lambda': 38.43516381968893, 'max_bin': 415}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:20:07,722] Trial 335 finished with value: 0.6880243098863073 and parameters: {'n_estimators': 835, 'eta': 0.0583340972432847, 'max_depth': 8, 'alpha': 0.8, 'lambda': 37.390276155710744, 'max_bin': 426}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:20:22,239] Trial 336 finished with value: 0.688154962022147 and parameters: {'n_estimators': 764, 'eta': 0.052729879845887784, 'max_depth': 8, 'alpha': 0.7774000000000001, 'lambda': 35.276610599012535, 'max_bin': 386}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:20:33,877] Trial 337 finished with value: 0.6877671112949579 and parameters: {'n_estimators': 734, 'eta': 0.06174301328768449, 'max_depth': 8, 'alpha': 0.6714, 'lambda': 25.903276791131, 'max_bin': 422}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:20:45,348] Trial 338 finished with value: 0.6868548944410564 and parameters: {'n_estimators': 816, 'eta': 0.05625542081122066, 'max_depth': 8, 'alpha': 0.3416, 'lambda': 17.49091182793922, 'max_bin': 434}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:20:55,515] Trial 339 finished with value: 0.6814978390712267 and parameters: {'n_estimators': 785, 'eta': 0.09195733522546179, 'max_depth': 8, 'alpha': 0.8307, 'lambda': 16.69619763706241, 'max_bin': 412}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:21:10,487] Trial 340 finished with value: 0.6868929492516662 and parameters: {'n_estimators': 861, 'eta': 0.05049633112567173, 'max_depth': 7, 'alpha': 0.7123, 'lambda': 38.98504618293728, 'max_bin': 370}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:21:25,613] Trial 341 finished with value: 0.6911521278676462 and parameters: {'n_estimators': 752, 'eta': 0.06531598424907764, 'max_depth': 9, 'alpha': 0.8627, 'lambda': 32.69816344954573, 'max_bin': 401}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:21:40,751] Trial 342 finished with value: 0.5611611294364514 and parameters: {'n_estimators': 805, 'eta': 0.005058104884614258, 'max_depth': 8, 'alpha': 0.7454000000000001, 'lambda': 33.72484146995559, 'max_bin': 419}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:21:51,915] Trial 343 finished with value: 0.6871368443841115 and parameters: {'n_estimators': 772, 'eta': 0.08477280650789502, 'max_depth': 8, 'alpha': 0.501, 'lambda': 36.205600671332974, 'max_bin': 396}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:22:05,963] Trial 344 finished with value: 0.6896360810019877 and parameters: {'n_estimators': 829, 'eta': 0.059561847663342354, 'max_depth': 8, 'alpha': 0.8168000000000001, 'lambda': 37.9610693961791, 'max_bin': 413}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:22:21,263] Trial 345 finished with value: 0.6879704928939486 and parameters: {'n_estimators': 784, 'eta': 0.054767332902687135, 'max_depth': 8, 'alpha': 0.7875000000000001, 'lambda': 36.927600708913346, 'max_bin': 429}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:22:35,302] Trial 346 finished with value: 0.6880084005458731 and parameters: {'n_estimators': 724, 'eta': 0.057173191447055065, 'max_depth': 8, 'alpha': 0.7656000000000001, 'lambda': 34.663995156869476, 'max_bin': 407}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:22:50,128] Trial 347 finished with value: 0.6896735063304898 and parameters: {'n_estimators': 758, 'eta': 0.0526350588716226, 'max_depth': 8, 'alpha': 0.7298, 'lambda': 31.9691237441617, 'max_bin': 391}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:23:05,669] Trial 348 finished with value: 0.683448925337063 and parameters: {'n_estimators': 813, 'eta': 0.06991795013662298, 'max_depth': 12, 'alpha': 0.8963000000000001, 'lambda': 23.460331031468073, 'max_bin': 420}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:23:18,939] Trial 349 finished with value: 0.6853472543954033 and parameters: {'n_estimators': 796, 'eta': 0.06147639047114109, 'max_depth': 9, 'alpha': 0.8006000000000001, 'lambda': 20.162347059577524, 'max_bin': 403}. Best is trial 107 with value: 0.7074441570677151.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7074\n",
      "\tBest params:\n",
      "\t\tn_estimators: 749\n",
      "\t\teta: 0.054845717051706634\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.7489\n",
      "\t\tlambda: 36.305982687842885\n",
      "\t\tmax_bin: 416\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.733450    0.709249    0.685721    0.744080   \n",
      "1                    TP  349.000000  326.000000  335.000000  333.000000   \n",
      "2                    TN  168.000000  175.000000  172.000000  183.000000   \n",
      "3                    FP   44.000000   55.000000   50.000000   43.000000   \n",
      "4                    FN   34.000000   39.000000   38.000000   36.000000   \n",
      "5              Accuracy    0.868908    0.842017    0.852101    0.867227   \n",
      "6             Precision    0.888041    0.855643    0.870130    0.885638   \n",
      "7           Sensitivity    0.911227    0.893151    0.898123    0.902439   \n",
      "8           Specificity    0.792500    0.760900    0.774800    0.809700   \n",
      "9              F1 score    0.899485    0.873995    0.883905    0.893960   \n",
      "10  F1 score (weighted)    0.868169    0.840864    0.851217    0.866806   \n",
      "11     F1 score (macro)    0.855539    0.831141    0.840101    0.858216   \n",
      "12    Balanced Accuracy    0.851840    0.827010    0.836449    0.856087   \n",
      "13                  MCC    0.711657    0.663639    0.680989    0.716700   \n",
      "14                  NPV    0.831700    0.817800    0.819000    0.835600   \n",
      "15              ROC_AUC    0.851840    0.827010    0.836449    0.856087   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.680602    0.715920    0.704537  \n",
      "1   334.000000  345.000000  329.000000  \n",
      "2   173.000000  176.000000  175.000000  \n",
      "3    58.000000   42.000000   53.000000  \n",
      "4    30.000000   32.000000   38.000000  \n",
      "5     0.852101    0.875630    0.847059  \n",
      "6     0.852041    0.891473    0.861257  \n",
      "7     0.917582    0.915119    0.896458  \n",
      "8     0.748900    0.807300    0.767500  \n",
      "9     0.883598    0.903141    0.878505  \n",
      "10    0.850069    0.874984    0.845989  \n",
      "11    0.840416    0.864716    0.836078  \n",
      "12    0.833250    0.861229    0.832001  \n",
      "13    0.685119    0.730003    0.673361  \n",
      "14    0.852200    0.846200    0.821600  \n",
      "15    0.833250    0.861229    0.832001  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_xgb_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_6_cat = np.where((y_pred_xgb_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 13:23:37,960] Trial 350 finished with value: 0.6934676707667495 and parameters: {'n_estimators': 849, 'eta': 0.04943098334358285, 'max_depth': 8, 'alpha': 0.8497, 'lambda': 38.3738657129233, 'max_bin': 426}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:23:53,379] Trial 351 finished with value: 0.6950348320086982 and parameters: {'n_estimators': 774, 'eta': 0.0552309154010373, 'max_depth': 8, 'alpha': 0.8246, 'lambda': 39.40626459929836, 'max_bin': 409}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:24:03,940] Trial 352 finished with value: 0.6933655612378089 and parameters: {'n_estimators': 748, 'eta': 0.078878523651149, 'max_depth': 8, 'alpha': 0.7523000000000001, 'lambda': 25.08362173065336, 'max_bin': 447}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:24:08,845] Trial 353 finished with value: 0.687024509916516 and parameters: {'n_estimators': 247, 'eta': 0.09463411017445233, 'max_depth': 8, 'alpha': 0.4274, 'lambda': 2.2883078109789388, 'max_bin': 414}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:24:25,130] Trial 354 finished with value: 0.6939933265172252 and parameters: {'n_estimators': 820, 'eta': 0.05856593964083768, 'max_depth': 8, 'alpha': 0.777, 'lambda': 35.49527132796125, 'max_bin': 419}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:24:34,660] Trial 355 finished with value: -0.530935254260788 and parameters: {'n_estimators': 791, 'eta': 0.002392376339104517, 'max_depth': 9, 'alpha': 0.6892, 'lambda': 37.46751910043679, 'max_bin': 386}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:24:48,937] Trial 356 finished with value: 0.6954590659206309 and parameters: {'n_estimators': 767, 'eta': 0.06376463787700569, 'max_depth': 8, 'alpha': 0.8617, 'lambda': 36.48321661978717, 'max_bin': 290}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:25:03,300] Trial 357 finished with value: 0.6913542326448283 and parameters: {'n_estimators': 839, 'eta': 0.07598605781754708, 'max_depth': 8, 'alpha': 0.7256, 'lambda': 38.29302957207294, 'max_bin': 400}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:25:16,602] Trial 358 finished with value: 0.6903669704119126 and parameters: {'n_estimators': 704, 'eta': 0.052610200027335755, 'max_depth': 7, 'alpha': 0.8028000000000001, 'lambda': 39.304882113814884, 'max_bin': 377}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:25:31,862] Trial 359 finished with value: 0.6930003928281236 and parameters: {'n_estimators': 736, 'eta': 0.05723565428436258, 'max_depth': 8, 'alpha': 0.7663000000000001, 'lambda': 37.43636082657052, 'max_bin': 433}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:25:41,807] Trial 360 finished with value: 0.6913899452072211 and parameters: {'n_estimators': 802, 'eta': 0.0666141288220059, 'max_depth': 8, 'alpha': 0.8331000000000001, 'lambda': 7.490858039706776, 'max_bin': 439}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:25:58,369] Trial 361 finished with value: 0.6928286432334387 and parameters: {'n_estimators': 781, 'eta': 0.04654355670249863, 'max_depth': 8, 'alpha': 0.7429, 'lambda': 38.767875536886436, 'max_bin': 422}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:26:17,339] Trial 362 finished with value: 0.6507214424206075 and parameters: {'n_estimators': 873, 'eta': 0.007935763360768698, 'max_depth': 8, 'alpha': 0.8756, 'lambda': 21.29849434129126, 'max_bin': 415}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:26:37,393] Trial 363 finished with value: 0.6806969109698309 and parameters: {'n_estimators': 820, 'eta': 0.01917520905923419, 'max_depth': 9, 'alpha': 0.7854, 'lambda': 39.96735024128609, 'max_bin': 395}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:26:50,542] Trial 364 finished with value: 0.692143802671809 and parameters: {'n_estimators': 760, 'eta': 0.05413364358792884, 'max_depth': 8, 'alpha': 0.22660000000000002, 'lambda': 22.279787055479485, 'max_bin': 408}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:27:05,363] Trial 365 finished with value: 0.6982961966411505 and parameters: {'n_estimators': 803, 'eta': 0.06065270633206408, 'max_depth': 8, 'alpha': 0.7075, 'lambda': 35.81592260378322, 'max_bin': 425}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:27:19,997] Trial 366 finished with value: 0.6938805618914984 and parameters: {'n_estimators': 784, 'eta': 0.05580145653527437, 'max_depth': 8, 'alpha': 0.8152, 'lambda': 36.62950364110704, 'max_bin': 404}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:27:35,623] Trial 367 finished with value: 0.6930855725169469 and parameters: {'n_estimators': 720, 'eta': 0.04439234872617419, 'max_depth': 8, 'alpha': 0.9208000000000001, 'lambda': 37.86491203721947, 'max_bin': 417}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:27:52,407] Trial 368 finished with value: 0.6948468589860296 and parameters: {'n_estimators': 745, 'eta': 0.05212489234545489, 'max_depth': 9, 'alpha': 0.8453, 'lambda': 38.90522337432999, 'max_bin': 430}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:28:06,727] Trial 369 finished with value: 0.6960702620923132 and parameters: {'n_estimators': 833, 'eta': 0.06304202785785741, 'max_depth': 8, 'alpha': 0.7577, 'lambda': 27.777324784387456, 'max_bin': 410}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:28:20,197] Trial 370 finished with value: 0.6904273499167809 and parameters: {'n_estimators': 769, 'eta': 0.05990651081226278, 'max_depth': 8, 'alpha': 0.7985, 'lambda': 35.159170450743574, 'max_bin': 382}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:28:36,506] Trial 371 finished with value: 0.693485761999387 and parameters: {'n_estimators': 809, 'eta': 0.050121641611456574, 'max_depth': 8, 'alpha': 0.7246, 'lambda': 30.91396996340712, 'max_bin': 397}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:28:51,263] Trial 372 finished with value: 0.6919205432027263 and parameters: {'n_estimators': 854, 'eta': 0.05723265233204175, 'max_depth': 8, 'alpha': 0.8883000000000001, 'lambda': 37.09531349732103, 'max_bin': 391}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:29:07,666] Trial 373 finished with value: 0.6907474767021035 and parameters: {'n_estimators': 790, 'eta': 0.03903183254412608, 'max_depth': 8, 'alpha': 0.7713, 'lambda': 28.972821397492723, 'max_bin': 420}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:29:25,453] Trial 374 finished with value: 0.6914721637571699 and parameters: {'n_estimators': 752, 'eta': 0.04251590049034062, 'max_depth': 9, 'alpha': 0.8304, 'lambda': 32.9997174387026, 'max_bin': 414}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:29:41,109] Trial 375 finished with value: 0.6944460195008796 and parameters: {'n_estimators': 822, 'eta': 0.05492565352993983, 'max_depth': 8, 'alpha': 0.7494000000000001, 'lambda': 38.08157389765809, 'max_bin': 424}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:29:45,125] Trial 376 finished with value: 0.661897757221263 and parameters: {'n_estimators': 170, 'eta': 0.065174940678974, 'max_depth': 8, 'alpha': 0.8523000000000001, 'lambda': 33.82181302900079, 'max_bin': 411}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:30:00,765] Trial 377 finished with value: 0.6909438533433728 and parameters: {'n_estimators': 775, 'eta': 0.053565627650486024, 'max_depth': 8, 'alpha': 0.8144, 'lambda': 36.50907220028455, 'max_bin': 428}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:30:15,342] Trial 378 finished with value: 0.6936607690604909 and parameters: {'n_estimators': 801, 'eta': 0.05839006802410167, 'max_depth': 7, 'alpha': 0.365, 'lambda': 38.553615120976694, 'max_bin': 275}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:30:30,144] Trial 379 finished with value: 0.6899162895192269 and parameters: {'n_estimators': 734, 'eta': 0.048393473165384136, 'max_depth': 8, 'alpha': 0.7901, 'lambda': 37.61233247836622, 'max_bin': 404}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:30:45,025] Trial 380 finished with value: 0.692267742296667 and parameters: {'n_estimators': 766, 'eta': 0.06720510257415946, 'max_depth': 8, 'alpha': 0.7074, 'lambda': 39.37545085345118, 'max_bin': 389}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:31:05,262] Trial 381 finished with value: 0.6936919164792275 and parameters: {'n_estimators': 836, 'eta': 0.03643042575909141, 'max_depth': 9, 'alpha': 0.7347, 'lambda': 35.78448899250692, 'max_bin': 399}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:31:14,634] Trial 382 finished with value: 0.6875660946098486 and parameters: {'n_estimators': 422, 'eta': 0.06212287845094207, 'max_depth': 8, 'alpha': 0.8678, 'lambda': 37.12805856275508, 'max_bin': 434}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:31:25,619] Trial 383 finished with value: 0.6938814069378204 and parameters: {'n_estimators': 789, 'eta': 0.07368707189897651, 'max_depth': 8, 'alpha': 0.7791, 'lambda': 13.502811762439014, 'max_bin': 418}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:31:43,124] Trial 384 finished with value: 0.6931217128519988 and parameters: {'n_estimators': 818, 'eta': 0.03453946342879595, 'max_depth': 8, 'alpha': 0.8067000000000001, 'lambda': 34.567397070706846, 'max_bin': 466}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:31:58,120] Trial 385 finished with value: 0.6944970618385468 and parameters: {'n_estimators': 802, 'eta': 0.05134107063292271, 'max_depth': 8, 'alpha': 0.6823, 'lambda': 22.644188787001482, 'max_bin': 406}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:32:12,801] Trial 386 finished with value: 0.6969272314229265 and parameters: {'n_estimators': 754, 'eta': 0.05617757214615806, 'max_depth': 8, 'alpha': 0.8289000000000001, 'lambda': 38.358856472376246, 'max_bin': 422}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:32:25,877] Trial 387 finished with value: 0.6958951762324391 and parameters: {'n_estimators': 779, 'eta': 0.05364480950474213, 'max_depth': 8, 'alpha': 0.7348, 'lambda': 16.597595545563017, 'max_bin': 414}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:32:39,897] Trial 388 finished with value: 0.6949092659202358 and parameters: {'n_estimators': 866, 'eta': 0.06923516548671, 'max_depth': 9, 'alpha': 0.7607, 'lambda': 19.59422666956181, 'max_bin': 394}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:32:54,478] Trial 389 finished with value: 0.692136660559618 and parameters: {'n_estimators': 719, 'eta': 0.05963322416630191, 'max_depth': 8, 'alpha': 0.8536, 'lambda': 39.17585034482191, 'max_bin': 379}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:33:09,865] Trial 390 finished with value: 0.6960062422157317 and parameters: {'n_estimators': 843, 'eta': 0.057377203058367784, 'max_depth': 8, 'alpha': 0.9051, 'lambda': 26.83206321934015, 'max_bin': 409}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:33:25,668] Trial 391 finished with value: 0.6876115897771032 and parameters: {'n_estimators': 734, 'eta': 0.03042125419973917, 'max_depth': 8, 'alpha': 0.7763, 'lambda': 39.99685095813592, 'max_bin': 441}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:33:38,393] Trial 392 finished with value: 0.6939913208845321 and parameters: {'n_estimators': 764, 'eta': 0.08292310717951885, 'max_depth': 8, 'alpha': 0.7177, 'lambda': 37.80672510771127, 'max_bin': 427}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:33:49,610] Trial 393 finished with value: 0.6931615114661704 and parameters: {'n_estimators': 540, 'eta': 0.055237703849971675, 'max_depth': 8, 'alpha': 0.8389000000000001, 'lambda': 24.774327691931425, 'max_bin': 417}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:34:03,450] Trial 394 finished with value: 0.6957378544397521 and parameters: {'n_estimators': 794, 'eta': 0.0647942105208432, 'max_depth': 8, 'alpha': 0.7485, 'lambda': 36.2459013776821, 'max_bin': 401}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:34:18,010] Trial 395 finished with value: 0.6952503312096785 and parameters: {'n_estimators': 814, 'eta': 0.051733360863084145, 'max_depth': 7, 'alpha': 0.8023, 'lambda': 37.03186700046123, 'max_bin': 385}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:34:33,086] Trial 396 finished with value: 0.6954408358926232 and parameters: {'n_estimators': 781, 'eta': 0.06189522024731773, 'max_depth': 8, 'alpha': 0.8242, 'lambda': 38.63979822240773, 'max_bin': 370}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:34:49,058] Trial 397 finished with value: 0.6973806086195047 and parameters: {'n_estimators': 745, 'eta': 0.059556784621433545, 'max_depth': 9, 'alpha': 0.8806, 'lambda': 30.11014504737136, 'max_bin': 302}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:35:05,113] Trial 398 finished with value: 0.6916367004813987 and parameters: {'n_estimators': 827, 'eta': 0.05368326047523378, 'max_depth': 8, 'alpha': 0.7887000000000001, 'lambda': 35.029513062820406, 'max_bin': 423}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:35:16,737] Trial 399 finished with value: 0.6946682807289625 and parameters: {'n_estimators': 688, 'eta': 0.08772601779605152, 'max_depth': 8, 'alpha': 0.6348, 'lambda': 37.75675505587395, 'max_bin': 455}. Best is trial 107 with value: 0.7074441570677151.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7074\n",
      "\tBest params:\n",
      "\t\tn_estimators: 749\n",
      "\t\teta: 0.054845717051706634\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.7489\n",
      "\t\tlambda: 36.305982687842885\n",
      "\t\tmax_bin: 416\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.733450    0.709249    0.685721    0.744080   \n",
      "1                    TP  349.000000  326.000000  335.000000  333.000000   \n",
      "2                    TN  168.000000  175.000000  172.000000  183.000000   \n",
      "3                    FP   44.000000   55.000000   50.000000   43.000000   \n",
      "4                    FN   34.000000   39.000000   38.000000   36.000000   \n",
      "5              Accuracy    0.868908    0.842017    0.852101    0.867227   \n",
      "6             Precision    0.888041    0.855643    0.870130    0.885638   \n",
      "7           Sensitivity    0.911227    0.893151    0.898123    0.902439   \n",
      "8           Specificity    0.792500    0.760900    0.774800    0.809700   \n",
      "9              F1 score    0.899485    0.873995    0.883905    0.893960   \n",
      "10  F1 score (weighted)    0.868169    0.840864    0.851217    0.866806   \n",
      "11     F1 score (macro)    0.855539    0.831141    0.840101    0.858216   \n",
      "12    Balanced Accuracy    0.851840    0.827010    0.836449    0.856087   \n",
      "13                  MCC    0.711657    0.663639    0.680989    0.716700   \n",
      "14                  NPV    0.831700    0.817800    0.819000    0.835600   \n",
      "15              ROC_AUC    0.851840    0.827010    0.836449    0.856087   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.680602    0.715920    0.704537    0.695594  \n",
      "1   334.000000  345.000000  329.000000  330.000000  \n",
      "2   173.000000  176.000000  175.000000  165.000000  \n",
      "3    58.000000   42.000000   53.000000   55.000000  \n",
      "4    30.000000   32.000000   38.000000   45.000000  \n",
      "5     0.852101    0.875630    0.847059    0.831933  \n",
      "6     0.852041    0.891473    0.861257    0.857143  \n",
      "7     0.917582    0.915119    0.896458    0.880000  \n",
      "8     0.748900    0.807300    0.767500    0.750000  \n",
      "9     0.883598    0.903141    0.878505    0.868421  \n",
      "10    0.850069    0.874984    0.845989    0.831084  \n",
      "11    0.840416    0.864716    0.836078    0.817931  \n",
      "12    0.833250    0.861229    0.832001    0.815000  \n",
      "13    0.685119    0.730003    0.673361    0.636396  \n",
      "14    0.852200    0.846200    0.821600    0.785700  \n",
      "15    0.833250    0.861229    0.832001    0.815000  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_xgb_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "y_pred_xgb_7_cat = np.where((y_pred_xgb_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 13:35:32,342] Trial 400 finished with value: 0.6916356267882969 and parameters: {'n_estimators': 806, 'eta': 0.0564468033471987, 'max_depth': 8, 'alpha': 0.7568, 'lambda': 26.341831769324212, 'max_bin': 412}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:35:51,208] Trial 401 finished with value: 0.6610708532777757 and parameters: {'n_estimators': 770, 'eta': 0.010319586747103926, 'max_depth': 9, 'alpha': 0.8657, 'lambda': 21.89501032752604, 'max_bin': 417}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:36:03,247] Trial 402 finished with value: 0.6867060913390506 and parameters: {'n_estimators': 893, 'eta': 0.07088505445113691, 'max_depth': 8, 'alpha': 0.7364, 'lambda': 36.60892019223973, 'max_bin': 404}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:36:17,809] Trial 403 finished with value: 0.6877362569347538 and parameters: {'n_estimators': 792, 'eta': 0.058073196276801964, 'max_depth': 8, 'alpha': 0.8130000000000001, 'lambda': 39.002322366609754, 'max_bin': 432}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:36:30,770] Trial 404 finished with value: 0.6857923474905551 and parameters: {'n_estimators': 755, 'eta': 0.06782176721682778, 'max_depth': 8, 'alpha': 0.6989000000000001, 'lambda': 35.792511242367866, 'max_bin': 396}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:36:48,367] Trial 405 finished with value: 0.6923302443656782 and parameters: {'n_estimators': 852, 'eta': 0.05015283480756276, 'max_depth': 8, 'alpha': 0.7691, 'lambda': 37.95618844061912, 'max_bin': 423}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:37:02,100] Trial 406 finished with value: 0.6876757840453271 and parameters: {'n_estimators': 826, 'eta': 0.06308352214460083, 'max_depth': 8, 'alpha': 0.8455, 'lambda': 37.00116585802782, 'max_bin': 390}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:37:16,637] Trial 407 finished with value: 0.6900180870884277 and parameters: {'n_estimators': 711, 'eta': 0.055046471375698874, 'max_depth': 8, 'alpha': 0.7871, 'lambda': 39.37526919583034, 'max_bin': 409}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:37:32,326] Trial 408 finished with value: 0.6893523639718627 and parameters: {'n_estimators': 778, 'eta': 0.052883658452073816, 'max_depth': 8, 'alpha': 0.7221000000000001, 'lambda': 38.350648751541534, 'max_bin': 418}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:37:47,027] Trial 409 finished with value: 0.6874540675639241 and parameters: {'n_estimators': 800, 'eta': 0.06057313162995739, 'max_depth': 9, 'alpha': 0.7501, 'lambda': 37.517383259594354, 'max_bin': 428}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:38:00,066] Trial 410 finished with value: 0.6893565371984668 and parameters: {'n_estimators': 740, 'eta': 0.06538443511566476, 'max_depth': 8, 'alpha': 0.8309000000000001, 'lambda': 36.13909896597983, 'max_bin': 415}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:38:15,532] Trial 411 finished with value: 0.6886267932242655 and parameters: {'n_estimators': 762, 'eta': 0.04830503215205539, 'max_depth': 8, 'alpha': 0.8021, 'lambda': 33.9313767393173, 'max_bin': 401}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:38:30,612] Trial 412 finished with value: 0.6882907505588782 and parameters: {'n_estimators': 813, 'eta': 0.056833040644388495, 'max_depth': 8, 'alpha': 0.7667, 'lambda': 38.5982533086019, 'max_bin': 445}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:38:43,897] Trial 413 finished with value: 0.6879732004375347 and parameters: {'n_estimators': 779, 'eta': 0.05476246022977007, 'max_depth': 8, 'alpha': 0.8913000000000001, 'lambda': 31.876483792053318, 'max_bin': 408}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:38:58,950] Trial 414 finished with value: 0.6901278824959036 and parameters: {'n_estimators': 798, 'eta': 0.051436461513426644, 'max_depth': 8, 'alpha': 0.8621000000000001, 'lambda': 35.01025729691585, 'max_bin': 436}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:39:14,602] Trial 415 finished with value: 0.6870700471230469 and parameters: {'n_estimators': 836, 'eta': 0.05790953240464082, 'max_depth': 9, 'alpha': 0.6656000000000001, 'lambda': 20.675308157118902, 'max_bin': 421}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:39:26,343] Trial 416 finished with value: 0.6836030975739857 and parameters: {'n_estimators': 753, 'eta': 0.06203320490650409, 'max_depth': 8, 'alpha': 0.7091000000000001, 'lambda': 39.397679780311364, 'max_bin': 393}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:39:36,368] Trial 417 finished with value: 0.6838339030540956 and parameters: {'n_estimators': 725, 'eta': 0.09112561227578694, 'max_depth': 8, 'alpha': 0.8131, 'lambda': 36.94270117415093, 'max_bin': 310}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:39:49,088] Trial 418 finished with value: 0.6892175276250676 and parameters: {'n_estimators': 813, 'eta': 0.05936134056539838, 'max_depth': 8, 'alpha': 0.7849, 'lambda': 24.446763015557487, 'max_bin': 413}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:40:03,710] Trial 419 finished with value: 0.691555685670187 and parameters: {'n_estimators': 789, 'eta': 0.05564465948956148, 'max_depth': 8, 'alpha': 0.7351000000000001, 'lambda': 37.79786598228078, 'max_bin': 384}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:40:17,221] Trial 420 finished with value: 0.6907470687860087 and parameters: {'n_estimators': 853, 'eta': 0.06406055855110654, 'max_depth': 8, 'alpha': 0.8434, 'lambda': 38.54483508435446, 'max_bin': 462}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:40:27,573] Trial 421 finished with value: 0.6866674379620921 and parameters: {'n_estimators': 764, 'eta': 0.08222968972382867, 'max_depth': 7, 'alpha': 0.3049, 'lambda': 37.2848956754426, 'max_bin': 427}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:40:41,996] Trial 422 finished with value: 0.6887926214702277 and parameters: {'n_estimators': 831, 'eta': 0.053544999612344504, 'max_depth': 8, 'alpha': 0.7643000000000001, 'lambda': 35.850804390765624, 'max_bin': 348}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:40:53,213] Trial 423 finished with value: 0.6843864086049193 and parameters: {'n_estimators': 777, 'eta': 0.06659081703585723, 'max_depth': 9, 'alpha': 0.8296, 'lambda': 12.138765804693502, 'max_bin': 405}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:41:01,322] Trial 424 finished with value: 0.6829672762343529 and parameters: {'n_estimators': 368, 'eta': 0.04624627841742252, 'max_depth': 8, 'alpha': 0.7947000000000001, 'lambda': 15.76626586410718, 'max_bin': 420}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:41:16,615] Trial 425 finished with value: 0.6891867333156326 and parameters: {'n_estimators': 739, 'eta': 0.05011444129347218, 'max_depth': 8, 'alpha': 0.876, 'lambda': 39.27932218286795, 'max_bin': 401}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:41:26,456] Trial 426 finished with value: 0.68702014053435 and parameters: {'n_estimators': 808, 'eta': 0.09582498794168244, 'max_depth': 8, 'alpha': 0.0988, 'lambda': 36.637905574476356, 'max_bin': 412}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:41:41,739] Trial 427 finished with value: 0.6899640124747679 and parameters: {'n_estimators': 871, 'eta': 0.05681575499695747, 'max_depth': 8, 'alpha': 0.7448, 'lambda': 38.17272829456224, 'max_bin': 398}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:41:49,430] Trial 428 finished with value: 0.6834903489433998 and parameters: {'n_estimators': 787, 'eta': 0.09270920404667399, 'max_depth': 8, 'alpha': 0.6932, 'lambda': 14.687891374548439, 'max_bin': 388}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:42:02,230] Trial 429 finished with value: 0.6862667446760523 and parameters: {'n_estimators': 768, 'eta': 0.06095911929445293, 'max_depth': 7, 'alpha': 0.8088000000000001, 'lambda': 39.9316798992588, 'max_bin': 424}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:42:16,670] Trial 430 finished with value: 0.6865685448629537 and parameters: {'n_estimators': 701, 'eta': 0.052884660659203435, 'max_depth': 9, 'alpha': 0.9381, 'lambda': 34.43119909785844, 'max_bin': 377}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:42:31,229] Trial 431 finished with value: 0.6899461881631407 and parameters: {'n_estimators': 821, 'eta': 0.05883462868366387, 'max_depth': 8, 'alpha': 0.7167, 'lambda': 29.337650265748366, 'max_bin': 432}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:42:40,992] Trial 432 finished with value: 0.6854346520176093 and parameters: {'n_estimators': 457, 'eta': 0.07221418058139215, 'max_depth': 8, 'alpha': 0.7742, 'lambda': 32.702456822268076, 'max_bin': 415}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:42:58,378] Trial 433 finished with value: 0.6822998116927182 and parameters: {'n_estimators': 795, 'eta': 0.02735989508910076, 'max_depth': 8, 'alpha': 0.8464, 'lambda': 37.64356288557896, 'max_bin': 407}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:43:08,761] Trial 434 finished with value: 0.6883826705184741 and parameters: {'n_estimators': 750, 'eta': 0.06821060548885033, 'max_depth': 8, 'alpha': 0.7358, 'lambda': 23.832929098861886, 'max_bin': 419}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:43:25,643] Trial 435 finished with value: 0.6906241959579037 and parameters: {'n_estimators': 844, 'eta': 0.05539118387745324, 'max_depth': 9, 'alpha': 0.8223, 'lambda': 35.41881022810012, 'max_bin': 451}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:43:39,866] Trial 436 finished with value: 0.688342726821676 and parameters: {'n_estimators': 729, 'eta': 0.05148900080700861, 'max_depth': 8, 'alpha': 0.7553000000000001, 'lambda': 38.616393546788494, 'max_bin': 412}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:43:55,848] Trial 437 finished with value: 0.6890741354883386 and parameters: {'n_estimators': 771, 'eta': 0.0425801658371029, 'max_depth': 8, 'alpha': 0.8594, 'lambda': 36.5397148501358, 'max_bin': 425}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:44:10,069] Trial 438 finished with value: 0.6881533064641288 and parameters: {'n_estimators': 810, 'eta': 0.06359311733456731, 'max_depth': 8, 'alpha': 0.781, 'lambda': 39.94153034305071, 'max_bin': 393}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:44:19,675] Trial 439 finished with value: 0.6855527370780905 and parameters: {'n_estimators': 790, 'eta': 0.0860213952946286, 'max_depth': 8, 'alpha': 0.7948000000000001, 'lambda': 19.02547172449585, 'max_bin': 404}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:44:37,839] Trial 440 finished with value: 0.6738283732618959 and parameters: {'n_estimators': 830, 'eta': 0.01313291438263297, 'max_depth': 8, 'alpha': 0.8869, 'lambda': 9.963055475126612, 'max_bin': 419}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:44:50,580] Trial 441 finished with value: 0.6885766081156792 and parameters: {'n_estimators': 759, 'eta': 0.054170357715893555, 'max_depth': 7, 'alpha': 0.7544000000000001, 'lambda': 37.25308829952928, 'max_bin': 438}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:45:03,152] Trial 442 finished with value: 0.6880003960772958 and parameters: {'n_estimators': 780, 'eta': 0.07587335262919784, 'max_depth': 9, 'alpha': 0.9131, 'lambda': 38.791021292091926, 'max_bin': 397}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:45:16,003] Trial 443 finished with value: 0.6885453940582154 and parameters: {'n_estimators': 801, 'eta': 0.057564305993894734, 'max_depth': 8, 'alpha': 0.7277, 'lambda': 21.28145309405464, 'max_bin': 373}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:45:28,660] Trial 444 finished with value: 0.6869386175856804 and parameters: {'n_estimators': 588, 'eta': 0.047857055805905574, 'max_depth': 8, 'alpha': 0.8385, 'lambda': 37.92112003211752, 'max_bin': 428}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:45:41,088] Trial 445 finished with value: 0.687030021273806 and parameters: {'n_estimators': 753, 'eta': 0.0605047975256799, 'max_depth': 8, 'alpha': 0.8153, 'lambda': 33.34205507475381, 'max_bin': 410}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:45:51,341] Trial 446 finished with value: 0.6856051898467351 and parameters: {'n_estimators': 818, 'eta': 0.06602847338462967, 'max_depth': 8, 'alpha': 0.7673, 'lambda': 18.735323705010142, 'max_bin': 416}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:46:04,619] Trial 447 finished with value: 0.6888847521418555 and parameters: {'n_estimators': 847, 'eta': 0.05596755814424724, 'max_depth': 8, 'alpha': 0.7957000000000001, 'lambda': 36.03229350138902, 'max_bin': 421}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:46:18,687] Trial 448 finished with value: 0.6898176414340167 and parameters: {'n_estimators': 720, 'eta': 0.052773647158884766, 'max_depth': 8, 'alpha': 0.8645, 'lambda': 29.849165860006227, 'max_bin': 406}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:46:32,736] Trial 449 finished with value: 0.6866704832163639 and parameters: {'n_estimators': 737, 'eta': 0.058051376090054785, 'max_depth': 8, 'alpha': 0.7431, 'lambda': 38.91463881058024, 'max_bin': 400}. Best is trial 107 with value: 0.7074441570677151.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7074\n",
      "\tBest params:\n",
      "\t\tn_estimators: 749\n",
      "\t\teta: 0.054845717051706634\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.7489\n",
      "\t\tlambda: 36.305982687842885\n",
      "\t\tmax_bin: 416\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.733450    0.709249    0.685721    0.744080   \n",
      "1                    TP  349.000000  326.000000  335.000000  333.000000   \n",
      "2                    TN  168.000000  175.000000  172.000000  183.000000   \n",
      "3                    FP   44.000000   55.000000   50.000000   43.000000   \n",
      "4                    FN   34.000000   39.000000   38.000000   36.000000   \n",
      "5              Accuracy    0.868908    0.842017    0.852101    0.867227   \n",
      "6             Precision    0.888041    0.855643    0.870130    0.885638   \n",
      "7           Sensitivity    0.911227    0.893151    0.898123    0.902439   \n",
      "8           Specificity    0.792500    0.760900    0.774800    0.809700   \n",
      "9              F1 score    0.899485    0.873995    0.883905    0.893960   \n",
      "10  F1 score (weighted)    0.868169    0.840864    0.851217    0.866806   \n",
      "11     F1 score (macro)    0.855539    0.831141    0.840101    0.858216   \n",
      "12    Balanced Accuracy    0.851840    0.827010    0.836449    0.856087   \n",
      "13                  MCC    0.711657    0.663639    0.680989    0.716700   \n",
      "14                  NPV    0.831700    0.817800    0.819000    0.835600   \n",
      "15              ROC_AUC    0.851840    0.827010    0.836449    0.856087   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.680602    0.715920    0.704537    0.695594    0.717151  \n",
      "1   334.000000  345.000000  329.000000  330.000000  319.000000  \n",
      "2   173.000000  176.000000  175.000000  165.000000  183.000000  \n",
      "3    58.000000   42.000000   53.000000   55.000000   59.000000  \n",
      "4    30.000000   32.000000   38.000000   45.000000   34.000000  \n",
      "5     0.852101    0.875630    0.847059    0.831933    0.843697  \n",
      "6     0.852041    0.891473    0.861257    0.857143    0.843915  \n",
      "7     0.917582    0.915119    0.896458    0.880000    0.903683  \n",
      "8     0.748900    0.807300    0.767500    0.750000    0.756200  \n",
      "9     0.883598    0.903141    0.878505    0.868421    0.872777  \n",
      "10    0.850069    0.874984    0.845989    0.831084    0.842114  \n",
      "11    0.840416    0.864716    0.836078    0.817931    0.835081  \n",
      "12    0.833250    0.861229    0.832001    0.815000    0.829941  \n",
      "13    0.685119    0.730003    0.673361    0.636396    0.673418  \n",
      "14    0.852200    0.846200    0.821600    0.785700    0.843300  \n",
      "15    0.833250    0.861229    0.832001    0.815000    0.829941  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_xgb_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "y_pred_xgb_8_cat = np.where((y_pred_xgb_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 13:46:47,004] Trial 450 finished with value: 0.6903393161430701 and parameters: {'n_estimators': 786, 'eta': 0.08116069110803548, 'max_depth': 9, 'alpha': 0.7076, 'lambda': 37.06469013644204, 'max_bin': 382}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:46:59,053] Trial 451 finished with value: 0.6921006733079308 and parameters: {'n_estimators': 769, 'eta': 0.06195644635004216, 'max_depth': 8, 'alpha': 0.6806, 'lambda': 23.40644623807046, 'max_bin': 472}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:47:14,467] Trial 452 finished with value: 0.6920817015044461 and parameters: {'n_estimators': 832, 'eta': 0.05452104580570812, 'max_depth': 8, 'alpha': 0.8195, 'lambda': 25.68015429339902, 'max_bin': 413}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:47:30,377] Trial 453 finished with value: 0.693210262707719 and parameters: {'n_estimators': 804, 'eta': 0.049361575887303555, 'max_depth': 8, 'alpha': 0.7804, 'lambda': 28.606706805803782, 'max_bin': 431}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:47:44,910] Trial 454 finished with value: 0.6899500251545334 and parameters: {'n_estimators': 866, 'eta': 0.05922417769722568, 'max_depth': 8, 'alpha': 0.8393, 'lambda': 27.676889743024773, 'max_bin': 422}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:48:02,408] Trial 455 finished with value: 0.6839602917343023 and parameters: {'n_estimators': 791, 'eta': 0.023845777369948386, 'max_depth': 8, 'alpha': 0.7219, 'lambda': 30.460802196823188, 'max_bin': 443}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:48:17,270] Trial 456 finished with value: 0.6925104183750743 and parameters: {'n_estimators': 812, 'eta': 0.06982145034240712, 'max_depth': 9, 'alpha': 0.7629, 'lambda': 38.03858391091018, 'max_bin': 388}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:48:29,320] Trial 457 finished with value: 0.6907430076683637 and parameters: {'n_estimators': 745, 'eta': 0.06453418794785332, 'max_depth': 8, 'alpha': 0.7986000000000001, 'lambda': 18.159529140732285, 'max_bin': 395}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:48:35,711] Trial 458 finished with value: 0.6748231160715241 and parameters: {'n_estimators': 277, 'eta': 0.05114275179859351, 'max_depth': 8, 'alpha': 0.875, 'lambda': 31.653390281015604, 'max_bin': 417}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:48:40,346] Trial 459 finished with value: 0.6637535693390272 and parameters: {'n_estimators': 198, 'eta': 0.056702240259507805, 'max_depth': 8, 'alpha': 0.8237, 'lambda': 35.6464954254205, 'max_bin': 408}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:48:55,987] Trial 460 finished with value: 0.692733444104084 and parameters: {'n_estimators': 773, 'eta': 0.05265668205047473, 'max_depth': 8, 'alpha': 0.7443000000000001, 'lambda': 39.22435485867644, 'max_bin': 425}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:49:09,768] Trial 461 finished with value: 0.6907150193064417 and parameters: {'n_estimators': 759, 'eta': 0.06252972434807746, 'max_depth': 8, 'alpha': 0.8535, 'lambda': 34.44985006800777, 'max_bin': 401}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:49:22,162] Trial 462 finished with value: 0.6913934374426419 and parameters: {'n_estimators': 679, 'eta': 0.0741058789175676, 'max_depth': 8, 'alpha': 0.7764000000000001, 'lambda': 36.5991354511239, 'max_bin': 364}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:49:34,831] Trial 463 finished with value: 0.6902712757138622 and parameters: {'n_estimators': 823, 'eta': 0.05415952342824731, 'max_depth': 6, 'alpha': 0.8997, 'lambda': 38.293512999625364, 'max_bin': 412}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:49:47,748] Trial 464 finished with value: 0.6895971651438797 and parameters: {'n_estimators': 796, 'eta': 0.05985454190552255, 'max_depth': 9, 'alpha': 0.45, 'lambda': 17.953266820460055, 'max_bin': 429}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:50:01,070] Trial 465 finished with value: 0.692802213657296 and parameters: {'n_estimators': 631, 'eta': 0.055871756870619044, 'max_depth': 8, 'alpha': 0.7016, 'lambda': 37.43483554992106, 'max_bin': 418}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:50:17,364] Trial 466 finished with value: 0.6920693540738635 and parameters: {'n_estimators': 777, 'eta': 0.04529918760304705, 'max_depth': 8, 'alpha': 0.8098000000000001, 'lambda': 35.13011904742782, 'max_bin': 436}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:50:30,786] Trial 467 finished with value: 0.6929626256325603 and parameters: {'n_estimators': 712, 'eta': 0.06708201138587078, 'max_depth': 8, 'alpha': 0.7291000000000001, 'lambda': 39.45674517074941, 'max_bin': 391}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:50:45,526] Trial 468 finished with value: 0.6937345186577834 and parameters: {'n_estimators': 748, 'eta': 0.057839502447350546, 'max_depth': 8, 'alpha': 0.8435, 'lambda': 36.38402956748796, 'max_bin': 405}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:50:56,186] Trial 469 finished with value: 0.6898060399883452 and parameters: {'n_estimators': 846, 'eta': 0.07860934680751061, 'max_depth': 9, 'alpha': 0.7564000000000001, 'lambda': 23.244222228601902, 'max_bin': 415}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:51:03,189] Trial 470 finished with value: -3.5634308851018375 and parameters: {'n_estimators': 807, 'eta': 0.001342270054430869, 'max_depth': 8, 'alpha': 0.774, 'lambda': 19.40381934430424, 'max_bin': 423}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:51:17,740] Trial 471 finished with value: 0.6918835726435832 and parameters: {'n_estimators': 782, 'eta': 0.05433265080631201, 'max_depth': 8, 'alpha': 0.7959, 'lambda': 24.46459650878892, 'max_bin': 458}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:51:28,655] Trial 472 finished with value: 0.692290614382909 and parameters: {'n_estimators': 880, 'eta': 0.09094614246095048, 'max_depth': 8, 'alpha': 0.8701000000000001, 'lambda': 38.462092676430146, 'max_bin': 398}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:51:41,539] Trial 473 finished with value: 0.6924110158558576 and parameters: {'n_estimators': 762, 'eta': 0.06502035108373282, 'max_depth': 8, 'alpha': 0.8286, 'lambda': 37.570065088239, 'max_bin': 409}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:51:57,814] Trial 474 finished with value: 0.6725304757576792 and parameters: {'n_estimators': 730, 'eta': 0.01788210009261782, 'max_depth': 8, 'alpha': 0.7428, 'lambda': 38.72787910362148, 'max_bin': 316}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:52:13,988] Trial 475 finished with value: 0.6939470435287367 and parameters: {'n_estimators': 836, 'eta': 0.05144414683202197, 'max_depth': 8, 'alpha': 0.7142000000000001, 'lambda': 33.30064878961942, 'max_bin': 381}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:52:25,194] Trial 476 finished with value: 0.6898787226309884 and parameters: {'n_estimators': 798, 'eta': 0.09424005913996344, 'max_depth': 9, 'alpha': 0.8076, 'lambda': 25.3449561553049, 'max_bin': 421}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:52:35,899] Trial 477 finished with value: 0.6919558023768977 and parameters: {'n_estimators': 820, 'eta': 0.08384509974517887, 'max_depth': 8, 'alpha': 0.8553000000000001, 'lambda': 39.95843706692661, 'max_bin': 427}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:52:51,022] Trial 478 finished with value: 0.6936574965973081 and parameters: {'n_estimators': 778, 'eta': 0.06082420121268311, 'max_depth': 8, 'alpha': 0.7629, 'lambda': 37.05357652722024, 'max_bin': 413}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:53:04,930] Trial 479 finished with value: 0.6918290482330318 and parameters: {'n_estimators': 860, 'eta': 0.05630041321012505, 'max_depth': 7, 'alpha': 0.7877000000000001, 'lambda': 38.03451817259821, 'max_bin': 403}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:53:17,782] Trial 480 finished with value: 0.691841762975605 and parameters: {'n_estimators': 793, 'eta': 0.06894378306512934, 'max_depth': 8, 'alpha': 0.7312000000000001, 'lambda': 30.938083768898792, 'max_bin': 433}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:53:32,802] Trial 481 finished with value: 0.693564775982748 and parameters: {'n_estimators': 743, 'eta': 0.0589708512121857, 'max_depth': 8, 'alpha': 0.8247, 'lambda': 35.80923661322378, 'max_bin': 387}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:53:49,557] Trial 482 finished with value: 0.6901580853866865 and parameters: {'n_estimators': 763, 'eta': 0.039653853742183325, 'max_depth': 8, 'alpha': 0.8401000000000001, 'lambda': 39.163870205505354, 'max_bin': 395}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:54:02,222] Trial 483 finished with value: 0.6917482707973839 and parameters: {'n_estimators': 812, 'eta': 0.06292918569156328, 'max_depth': 8, 'alpha': 0.8868, 'lambda': 17.46187601690437, 'max_bin': 417}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:54:21,396] Trial 484 finished with value: 0.6632697088201064 and parameters: {'n_estimators': 831, 'eta': 0.0077515926375448865, 'max_depth': 9, 'alpha': 0.6894, 'lambda': 1.7504962044185106, 'max_bin': 410}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:54:36,880] Trial 485 finished with value: 0.6868601458177583 and parameters: {'n_estimators': 786, 'eta': 0.03261620442451364, 'max_depth': 8, 'alpha': 0.7537, 'lambda': 8.233536373755886, 'max_bin': 422}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:54:50,593] Trial 486 finished with value: 0.6915047790350128 and parameters: {'n_estimators': 769, 'eta': 0.054240268291526415, 'max_depth': 8, 'alpha': 0.7783, 'lambda': 37.48573246860808, 'max_bin': 405}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:55:04,408] Trial 487 finished with value: 0.6906593798418371 and parameters: {'n_estimators': 706, 'eta': 0.04960771304053273, 'max_depth': 8, 'alpha': 0.7994, 'lambda': 36.393942348331805, 'max_bin': 262}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:55:20,443] Trial 488 finished with value: 0.6952310030623141 and parameters: {'n_estimators': 805, 'eta': 0.05226770321164658, 'max_depth': 8, 'alpha': 0.7252000000000001, 'lambda': 38.23368729116702, 'max_bin': 447}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:55:34,504] Trial 489 finished with value: 0.6927052065424809 and parameters: {'n_estimators': 738, 'eta': 0.05730614277336924, 'max_depth': 8, 'alpha': 0.8607, 'lambda': 34.22264038121283, 'max_bin': 416}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:55:50,080] Trial 490 finished with value: 0.694856882016908 and parameters: {'n_estimators': 755, 'eta': 0.05510396065478045, 'max_depth': 9, 'alpha': 0.8166, 'lambda': 35.02457310749482, 'max_bin': 391}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:56:01,703] Trial 491 finished with value: 0.6919036399915729 and parameters: {'n_estimators': 820, 'eta': 0.08002253628480419, 'max_depth': 8, 'alpha': 0.7688, 'lambda': 37.06906816888749, 'max_bin': 426}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:56:13,164] Trial 492 finished with value: 0.6898394308552958 and parameters: {'n_estimators': 844, 'eta': 0.07144925701171781, 'max_depth': 8, 'alpha': 0.7421, 'lambda': 39.218210869578506, 'max_bin': 412}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:56:20,334] Trial 493 finished with value: 0.6839004715687542 and parameters: {'n_estimators': 794, 'eta': 0.09835501859698784, 'max_depth': 5, 'alpha': 0.6564, 'lambda': 26.521705078047248, 'max_bin': 420}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:56:34,487] Trial 494 finished with value: 0.6926964599789118 and parameters: {'n_estimators': 771, 'eta': 0.060919534570292276, 'max_depth': 8, 'alpha': 0.7060000000000001, 'lambda': 37.79469594974974, 'max_bin': 400}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:56:50,255] Trial 495 finished with value: 0.6928366794446467 and parameters: {'n_estimators': 783, 'eta': 0.047624091986050235, 'max_depth': 8, 'alpha': 0.8401000000000001, 'lambda': 38.60269707178139, 'max_bin': 375}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:56:52,117] Trial 496 finished with value: 0.6396599249268217 and parameters: {'n_estimators': 72, 'eta': 0.0898328343866272, 'max_depth': 9, 'alpha': 0.791, 'lambda': 36.132664349224946, 'max_bin': 431}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:57:03,652] Trial 497 finished with value: 0.691143567925898 and parameters: {'n_estimators': 801, 'eta': 0.06413471483811196, 'max_depth': 8, 'alpha': 0.8775000000000001, 'lambda': 16.057645418080504, 'max_bin': 407}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:57:16,019] Trial 498 finished with value: 0.6892725620092537 and parameters: {'n_estimators': 753, 'eta': 0.05871328099792687, 'max_depth': 8, 'alpha': 0.5681, 'lambda': 20.01893805360468, 'max_bin': 385}. Best is trial 107 with value: 0.7074441570677151.\n",
      "[I 2023-12-20 13:57:30,108] Trial 499 finished with value: 0.6912297436339025 and parameters: {'n_estimators': 820, 'eta': 0.06626691755378769, 'max_depth': 8, 'alpha': 0.9055000000000001, 'lambda': 27.63265159549118, 'max_bin': 441}. Best is trial 107 with value: 0.7074441570677151.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7074\n",
      "\tBest params:\n",
      "\t\tn_estimators: 749\n",
      "\t\teta: 0.054845717051706634\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.7489\n",
      "\t\tlambda: 36.305982687842885\n",
      "\t\tmax_bin: 416\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.733450    0.709249    0.685721    0.744080   \n",
      "1                    TP  349.000000  326.000000  335.000000  333.000000   \n",
      "2                    TN  168.000000  175.000000  172.000000  183.000000   \n",
      "3                    FP   44.000000   55.000000   50.000000   43.000000   \n",
      "4                    FN   34.000000   39.000000   38.000000   36.000000   \n",
      "5              Accuracy    0.868908    0.842017    0.852101    0.867227   \n",
      "6             Precision    0.888041    0.855643    0.870130    0.885638   \n",
      "7           Sensitivity    0.911227    0.893151    0.898123    0.902439   \n",
      "8           Specificity    0.792500    0.760900    0.774800    0.809700   \n",
      "9              F1 score    0.899485    0.873995    0.883905    0.893960   \n",
      "10  F1 score (weighted)    0.868169    0.840864    0.851217    0.866806   \n",
      "11     F1 score (macro)    0.855539    0.831141    0.840101    0.858216   \n",
      "12    Balanced Accuracy    0.851840    0.827010    0.836449    0.856087   \n",
      "13                  MCC    0.711657    0.663639    0.680989    0.716700   \n",
      "14                  NPV    0.831700    0.817800    0.819000    0.835600   \n",
      "15              ROC_AUC    0.851840    0.827010    0.836449    0.856087   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.680602    0.715920    0.704537    0.695594    0.717151    0.715255  \n",
      "1   334.000000  345.000000  329.000000  330.000000  319.000000  333.000000  \n",
      "2   173.000000  176.000000  175.000000  165.000000  183.000000  182.000000  \n",
      "3    58.000000   42.000000   53.000000   55.000000   59.000000   49.000000  \n",
      "4    30.000000   32.000000   38.000000   45.000000   34.000000   31.000000  \n",
      "5     0.852101    0.875630    0.847059    0.831933    0.843697    0.865546  \n",
      "6     0.852041    0.891473    0.861257    0.857143    0.843915    0.871728  \n",
      "7     0.917582    0.915119    0.896458    0.880000    0.903683    0.914835  \n",
      "8     0.748900    0.807300    0.767500    0.750000    0.756200    0.787900  \n",
      "9     0.883598    0.903141    0.878505    0.868421    0.872777    0.892761  \n",
      "10    0.850069    0.874984    0.845989    0.831084    0.842114    0.864443  \n",
      "11    0.840416    0.864716    0.836078    0.817931    0.835081    0.856291  \n",
      "12    0.833250    0.861229    0.832001    0.815000    0.829941    0.851357  \n",
      "13    0.685119    0.730003    0.673361    0.636396    0.673418    0.714354  \n",
      "14    0.852200    0.846200    0.821600    0.785700    0.843300    0.854500  \n",
      "15    0.833250    0.861229    0.832001    0.815000    0.829941    0.851357  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_xgb_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_xgb_9_cat = np.where((y_pred_xgb_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvUAAAHJCAYAAAAFEsbGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABVwElEQVR4nO3dd3wUdf7H8fdsNoGQHkqCQoDQVLqgIgQpKqjHSa8qoAIqnj+xg41yKIrlPLGBBTgVkd5OBBVQmjQVBFTA0HsJpEBI2fn9EbOXsgmbZLObgdfz8eBBMjM789lPdjfvnXz3O4ZpmqYAAAAAWJbN1wUAAAAAKBlCPQAAAGBxhHoAAADA4gj1AAAAgMUR6gEAAACLI9QDAAAAFkeoBwAAACyOUA8AAABYHKEeAAAAsDhCPeAD7dq1k2EYpXqMQYMGyTAM7d27t1SP466pU6fKMAxNnTrV16V4xKV2f0qTNx7vAHC5I9TjsrJp0ybde++9io2NVWBgoEJDQ9WoUSM99dRTOnTokMeOU9YCtTesXLlShmFo9OjRvi7FbdnBfNCgQQVuk32/2rVr59Fjjx49WoZhaOXKlR7drzdkP75z/gsKClKjRo307LPP6syZM6Vy3NL4OQDApcLu6wIAbzBNUyNGjNCECRNkt9t16623qlevXkpLS9PatWv1+uuv67333tO0adPUs2fPUq/nP//5j86dO1eqxxg/frxGjBihK6+8slSP465u3bqpZcuWqlq1qq9L8YhL7f4UR5cuXdS0aVNJ0tGjR7Vo0SKNHz9es2fP1oYNGxQeHu7T+gDgckKox2Vh7NixmjBhgmrWrKnFixerQYMGudbPmTNHd999t/r27atly5apQ4cOpVpPTExMqe5fkqpWrVqmAmdYWJjCwsJ8XYbHXGr3pzi6du2a668cr7/+um644Qbt2LFDEydO1AsvvOC74gDgMsPwG1zy9uzZo3Hjxsnf318LFy7MF+glqUePHvrXv/6lzMxMPfTQQ3I4HM51OcdOL168WK1atVJQUJAiIiLUs2dP7dq1K9e+DMPQtGnTJEm1atVyDk+oWbOmcxtXY4xzDl/ZtGmTbrvtNoWHhys8PFw9evTQgQMHJEm7du1S7969VblyZQUGBqp9+/baunVrvvvkaghQzZo18w2byPkvZ0DbuXOnRowYoRYtWqhy5coqV66catSooSFDhmj//v35jtW+fXtJ0pgxY3LtM3t4SWFj0Ddt2qTu3burSpUqzuM89NBDOnz4cKH3a9KkSWrUqJHKly+vqKgoDRkypNSGfuRV0P35+eef1adPH9WoUUPlypVTxYoV1bhxYz366KNKT0+XlPVzGDNmjCSpffv2ufqV0+HDhzVs2DDVrFlTAQEBqly5srp166aNGzcWWs9///tf3XTTTQoNDZVhGEpISFCFChVUu3Ztmabp8v507txZhmFo8+bNxe5JcHCwBg4cKElav379Rbd3OBx67733dN111yk4OFhBQUFq0aKF3nvvPZfPQUn6/vvvc/XLSsO9AKA0caYel7wpU6YoIyNDvXr1UqNGjQrcbvDgwRo7dqx27typ77//3hlSs82dO1dLlixRt27d1K5dO/3yyy+aM2eOVqxYobVr16p+/fqSpFGjRmn+/PnasmWLHn30UecQBHeHImzcuFGvvvqq2rZtq8GDB+vXX3/V3LlztW3bNs2bN09xcXG65pprNGDAAO3fv19z5szRLbfcovj4eAUHBxe67+HDh7sMvYsWLdJPP/2kChUq5Lq/H3zwgdq3b69WrVopICBA27Zt08cff6yFCxdq8+bNqlatmqSsM7aSNG3aNLVt2zbXuOecb2ZcWbBggXr16iXDMNSzZ0/FxMRo06ZN+uCDD7RgwQKtXr1asbGx+W739NNPa+nSpfr73/+ujh07asWKFfroo4+cPz9f+OWXX3TjjTfKZrPpzjvvVK1atZSYmKjdu3fr/fff10svvSR/f38NHz5c8+fP1/fff6+BAwe67FF8fLzi4uJ05MgR3XzzzerXr58OHDigWbNm6b///a9mzZqlLl265LvdrFmz9PXXX+uOO+7Qgw8+qD179igiIkJ9+/bVlClT9O233+rWW2/NdZsDBw5oyZIlat68uZo3b16iHhT0psGV/v3768svv1RMTIwGDx4swzA0b948Pfzww/rhhx80Y8YMSVLTpk01atQojRkzRjVq1Mj15pMx9gDwFxO4xLVv396UZE6ePPmi2/br18+UZP7zn/90LpsyZYopyZRkLlq0KNf2b731linJ7NChQ67lAwcONCWZe/bscXmctm3bmnmffitWrHAe57PPPsu17r777jMlmWFhYea4ceNyrXvppZdMSeZbb71VpBqyLVu2zLTb7WadOnXMEydOOJcfPHjQTE1Nzbf9V199ZdpsNvOBBx5wWf+oUaNcHie7j1OmTHEuS0pKMiMjI00/Pz9zzZo1ubZ/+eWXTUnmLbfc4vJ+xcTEmPv27XMuT09PN9u0aWNKMn/88cdC73Pempo0aWKOGjXK5b/s47Vt2/ai9+exxx4zJZnz5s3Ld6zTp0+bmZmZzu9HjRplSjJXrFjhsrZbb73VlGS+8soruZavWrXKtNlsZkREhJmYmJivHsMwzCVLluTb36ZNm0xJZo8ePfKte+GFF9x+jpjm/34GOe+7aZpmSkqK2aBBA1OSOWbMGOdyV4/3zz//3JRktmjRwkxOTnYuT05ONq+99lqXzwNXPwcAQBbO1OOSd/ToUUlS9erVL7pt9jauhn106NBBnTt3zrXsH//4hyZOnKjly5dr3759qlGjRonrbdOmje66665cywYOHKhPPvlEERERGjFiRK51d999t5577jn98ssvRT7Wtm3b1LNnT4WFhemrr75SpUqVnOsK+oDt7bffrmuuuUbLli0r8vHymj9/vk6fPq277rpLrVq1yrXuySef1KRJk/Ttt9+67O2LL76Y67MJdrtd9957r1atWqWNGzfqhhtucLuOLVu2aMuWLSW7M5JziEjOv3hki4iIcHs/Bw8e1DfffKMaNWroiSeeyLUuLi5Offv21fTp0zVv3jwNGDAg1/o777xTt912W759Nm/eXNddd50WLlyoY8eOKSoqSpKUmZmpjz/+WCEhIerfv7/bNUpZP7/s4V3Hjh3TokWLdOjQIdWuXVuPPPJIobf95JNPJGV9oDsoKMi5PCgoSK+88oo6duyojz/+ON9zAQDgGmPqcckz/xoO4M482dnbuNq2bdu2+Zb5+fkpLi5OUtZYak9wNfzhiiuukJQ1DMHPz8/luoMHDxbpOEeOHNHf/vY3XbhwQfPmzVPdunVzrTdNU5999pluueUWVa5cWXa73TmOedu2bR6ZAjS7Z3mHOkmSv7+/s+euetuiRYt8y7LflCUkJBSpjoEDB8o0TZf/VqxY4fZ++vbtKz8/P3Xt2lUDBw7Uf/7zH/35559FqkX63/1t06aN7Pb8515uueUWSdJPP/2Ub11hb2aGDRum9PR0Z6CWsoZeHT58WHfffXeucO2OBQsWaMyYMRozZoymTZum0NBQPfXUU9qwYcNF38T8/PPPstlsLp9X7du3l5+fn8v7BwBwjVCPS172DDDZHzQtTHYwdjVrTPaZzbyio6MlSWfPni1uibm4mlElO9gVti77Q5juSElJUefOnXXgwAFNmTJFbdq0ybfN448/rnvuuUc7duxQp06d9MQTT2jUqFEaNWqUatSoobS0NLePV5DsnmX3MK/sn4Or3hbWi8zMzBLXVhzXXXedVq1apQ4dOmjWrFkaOHCg6tSpo6uvvlpffvml2/spSV8Kuo0k9enTR5GRkfroo4+cb3YnTZokSXrwwQfdri/blClTnG9+zp07px07dmjChAmKjIy86G3Pnj2ryMhI+fv751tnt9tVqVIlJSYmFrkmALhcMfwGl7y4uDitWLFC3377rQYPHlzgdpmZmc6zsq1bt863/tixYy5vlz28xyrTGzocDvXr108//fSTXnrpJfXr1y/fNsePH9fbb7+thg0bau3atQoJCcm1/osvvvBILdk9y+5hXkeOHMm1nRXceOONWrx4sS5cuKDNmzfr66+/1sSJE9WvXz9VrlzZrelSS9KXwv4iFRgYqEGDBunNN9/UN998o3r16mnZsmVq2bKlGjdu7M7d85iwsDCdPn1a6enp+YJ9RkaGTp48qdDQUK/WBABWxpl6XPIGDRokPz8/zZ07Vzt27Chwu08++USHDx9W/fr1XQ4JcDWjSmZmplavXi1JatasmXN59hAZX50xLszw4cO1aNEi3XfffXr22WddbhMfHy+Hw6GOHTvmC/QHDx5UfHx8vtsU5z5n98zVVVUzMjKcvb322mvd3mdZUa5cObVq1Upjx47V22+/LdM0NX/+fOf6wvqV3ZfVq1crIyMj3/rsN5/F6ctDDz0kwzA0adIkffjhh3I4HHrggQeKvJ+SatasmRwOh3744Yd863744QdlZmbmu382m61MPqcAoCwg1OOSFxsbq2effVbp6en6+9//7jLYz58/X48++qj8/Pz03nvvyWbL/9RYvny5Fi9enGvZO++8oz///FPt27fP9UHOihUrSnJvyI83vfXWW5o4caJuvvlmffDBBwVulz3F4urVq3OFqOTkZA0ZMsRl0CzOfe7atasiIyP1xRdf6Mcff8xXa3x8vG655RavXKzLE1atWuVySEz2X3nKly/vXFZYv6pVq6Zbb71Ve/fu1VtvvZVr3fr16zV9+nRFRESoW7duRa6xTp06uvXWW7Vw4UJNnjxZ4eHh6tOnT5H3U1L33XefJGnkyJG5rq587tw554fB77///ly3qVixYpl7TgFAWcHwG1wWRo8erZSUFL355ptq0qSJOnXqpAYNGig9PV1r167V+vXrFRgYqC+++KLA4RF33nmnunXrpm7duqlOnTrasmWLvvrqK0VGRuq9997Lte3NN9+s1157TUOGDFGPHj0UHBys8PBw/eMf//DG3XXp6NGjeuKJJ2QYhho1aqSXXnop3zZNmzZV165dFR0drb59+2rGjBlq2rSpOnbsqLNnz+qbb75R+fLl1bRp03yz7dSvX19XXnmlZsyYIX9/f8XExMgwDN1zzz0FzgoUHBysTz75RL169VLbtm3Vq1cvxcTEaPPmzVq2bJmio6OdY76t4I033tCyZcvUrl07xcbGKjg4WNu3b9eSJUsUHh6uoUOHOrdt3769bDabRo4cqV9//dX5wdLnn39ekvTBBx+odevWeuqpp7Rs2TK1aNHCOU+9zWbTlClT8v0VxV0PPfSQli1bppMnT+r//u//FBgYWPI7X0T9+/fXggULNHPmTDVo0EBdu3aVYRiaP3++9uzZo969e+eb+ebmm2/WjBkz1KVLFzVr1kx2u1033XSTbrrpJq/XDwBljm9m0gR8Y/369eaAAQPMmjVrmuXLlzeDgoLMBg0amE888YR54MABl7fJOR/54sWLzZYtW5oVKlQww8LCzO7du5t//PGHy9u98cYb5lVXXWUGBASYkswaNWo41xU2T72red737NljSjIHDhzo8lhyMX933nnqs/dR2L+c+09JSTGfffZZs3bt2ma5cuXMatWqmcOGDTNPnjzpsn7TNM0NGzaYHTp0MENDQ03DMHLNw+5qXvect+vatatZqVIl09/f36xevbr54IMPmocOHcq3bWHz719srvy8smsqqK859+nOPPVLly41Bw0aZF599dVmaGioWaFCBbNevXrmI488Yu7duzffvj/99FOzSZMmZvny5Z0/g5wOHjxoPvjgg2ZMTIzp7+9vVqxY0ezSpYu5YcOGAu+Lq/7mlZGRYVaqVMmUZG7fvv2i2+dV0Dz1BSno8ZKZmWm+++67ZvPmzc3AwEAzMDDQvPbaa8133nkn15z+2Y4dO2b269fPrFKlimmz2Yr0swaAS51hmkW4/B9wGZo6daruvfdeTZkyJdeVLAGr+vPPP1W3bl3FxcW5HNMOALAextQDwGXmtddek2maPh0OBgDwLMbUA8BlYN++ffr000+1a9cuffrpp2rWrJl69uzp67IAAB5CqAeAy8CePXv0wgsvKCgoSJ06ddL777/vcpYnAIA1MaYeAAAAsDhO0wAAAAAWR6gHAAAALI5QDwAAAFgcoR4AAACwuMt29puEhARlZGR4dJ+VK1fWiRMnPLpPuEavvYM+ewd99h567R2l0We73a6IiAiP7hO4lFy2oT4jI0Pp6eke259hGM79MqFQ6aLX3kGfvYM+ew+99g76DPgGw28AAAAAiyPUAwAAABZHqAcAAAAsjlAPAAAAWNxl+0FZAACAojp69KjOnTvn6zJwiTMMQ4ZhKCoqSoGBgW7dhlAPAADghjNnzig1NVUhISG+LgWXAYfDoUOHDunKK690K9gz/AYAAMANCQkJqlChgq/LwGXCZrMpJCREx44dc2/7Uq4HAADgkmCapnMefsAbbDab29d7INQDAAAAZZS7oZ4x9bAk0zTlcDjc3vZiZ1bcPftSlH0ZhuH82upnd4rTn4Luu6tt3Nmn1XsIAEBpItT7WPa7r+zQYpqmbLbi/wEl7/7cCUEOh8NjodfdGnP+OSlv+C3oeClpmXrr+4P6+vcEuRfnfc/2110I8DMUVt6um2qHaeiNVygowM+3hbkhJS1Toxds09fbjijD4ZCfYahNbKiGtKyq4HL2XNtNXndEq/ckKi0zU+fTsn46DtNUuiPrvgcH2BQeaFfSBYfSHQ6dT8t6zAX622S3Ze03Z1+y9/lD/FmdPZ/u3E9oOb8Ce1icx2jOx1z29+48Hwo7Xs43LBfbJnu77DeohZ2NyfvakPfNTt774Wp53tu6Wu/qDVdB22T36mLHy1lTYfcvZ18uVnthCtsm72vMxc6Auep73hoLewNf2HFc9TfvsovVWNDPKOex89ab8/uLPd7dOb67ZxFR9jVv3lxDhw7VAw88UKJtSmrGjBl6/vnntXv37lI7hieUtToJ9aWgsF+ANptNyRcy9O7qQ1q284xS0x1y9XJYKyJA/+paR5WDAwo8RvZ+U9Iy8+3PkBTgpwKD5MmUdD02f7fiT1/wxF0uMUNy2Qerc/x1p1IzTKUmp2v2lpPauD9Jk3rVzRWMpeIHTE/+JUL635unJb8n5PuZzN56SrO3nnJrP9lSM0ylZmTq5LnMvFXpXLrD7f1m7SejWDV4kzuP5fJ+UuUQfx0+m67MIj7wQwJsKudvU/KFTF3I+CugFrMOV/wMKcBuU6Dd0Ll0h9IyTLfeRFvnOfyzpKLXG+xvKMNhKjXPw9hmSKYpGTnewGe/iT2bmqnE1AxdyPvQz77tX//n7K/x1z4dpnv1udpHWRAUsFW31g/Xw62vtMRJjEvdoUOH9Nprr+m7777T6dOnFRUVpdtvv11PPPGEIiMji7SvpUuXevTDwq7eJHTp0kU333yzx46R16JFizRkyBBt2rRJ1apVy7e+VatWateunV5++eVSq6E0EOo9JCUtU8/N+1VzNh9QakbJf7XtSUhT1yk7in17U9KFTOl4StkPQVIxw4Bpqlxmmqw2IOPoiQvq8t5mX5chSTIKOcNW3ot1WFGJHnfp0qnUVJUrxk0z06Vzygpz7s1cXAxpUupfxyi9x4E13gJkc6Rn9cOdKHPuvHTubNbXfm7epqiMMt6+VEeA5v96Sj8fTNZHfeoT7F3w1pDCvXv36o477lDt2rU1adIkxcTE6I8//tCYMWP03XffacmSJYqIiHB7f5UqVSrFarMEBga6PTd7cdx2222KjIzUl19+qSeeeCLXuvXr12v37t2aPHlyqR2/tBDqPeBEcpru/uw3JaWVtXMlJWSassnMOg2l/4UY469fxrl/qeTeJmu9mWtdrn2Y+beXTBmmZDcz5O9wfWqrQnqqGp6KV2DGBfk7MuVXwHYAAN9ZHtNcR4IqaV/CBU1ed1iPta3u65LKhJS0TL2/+qB++DNBGQ5Tdpuhm2pH6KG4aqX2xmfEiBEKCAjQzJkznUG5WrVqatiwoW644Qa9/PLLeu2115zbJycn68EHH9TXX3+tkJAQPfrooxo8eLBzfd4z64mJiRozZoyWLFmi1NRUNW3aVGPHjlXDhg2dt/n666/1xhtv6Pfff1dQUJBatmypqVOnqmvXrjpw4IBeeOEFvfDCC5Kk48eP5xrWsnv3brVq1Upr1qxR3bp1nft8//339dFHH2nTpk0yDEN//PGHRo8erXXr1qlChQpq166d/vnPf6pixYr5euLv76+ePXtqxowZevzxx3O9ufriiy/UpEkTNWzYUO+//75mzJihffv2KTw8XB07dtSLL76o4OBgl71+5JFHdPbsWf3nP/9xLnv++ee1bds2zZ8/X1LWm7l33nlH06ZN0/HjxxUbG6snnnhCf//7393+mRaEUF9CKWmZuvvz3IHePzNDdkeGrkg5qQrpqVl/TpUpwzRl/PW/7a+vy2ekyd+RIel/YVlSrpNYhovALDN/UHZvWxe3+2u9Tab8HA75mZmy//V/YWdyywqTD0/iEmFa7u9O1sRrRunL+VheHZ+ox9r6sJgyIiUtU/dN3669p1JzDZea9csxbdx/Vp/0b+DxYJ+QkKAVK1bo2WefzXfmOyoqSj169NCCBQs0YcIEZ7B99913NXz4cD311FNasWKFXnjhBdWpU0ft2rXLt3/TNNW/f39FRERo+vTpCg0N1bRp09SzZ0+tW7dOERER+uabb3Tvvfdq+PDhevfdd5WWlqZvv/1WkjRlyhS1b99e99xzj+6++26X96FOnTpq0qSJ5syZoxEjRjiXz507V927d5dhGDp27Ji6du2qu+++W2PHjlVqaqrGjh2rIUOGaO7cuS73e9ddd+mDDz7Q2rVr1bp1a0lSSkqKFixYoBdffFFS1nSSL730kqpXr679+/frmWee0dixYzVhwoSi/SByGD9+vP773/9qwoQJio2N1Y8//qhhw4apYsWKatWqVbH3KxHqS2zyusNKupD19LQ7MtT24C+KTinbQ13KAle/VLN/CWTY/JRusxewjXQouLL+DLtSGTY/nfMvL4fBzKwAUFZlOBzMXiXp/dUH8wV6KevzE3tPp+r91Qf1ZIcaHj1mfHy8TNPMdYY7p7p16+rMmTM6efKkKleuLEm6/vrr9X//93+SpNq1a2vDhg2aNGmSy1C/evVq/fbbb9qxY4fKlcsaUJh91n7RokUaMGCA/vWvf6lr16565plnnLfLPosfEREhPz8/BQcHKyoqqsD70aNHD3388cfOUP/nn39qy5YteueddyRlvTlo1KiRnnvuOedt/v3vf6tp06b6888/Vbt27Xz7rF+/vpo3b64vvvjCGeoXLlwoh8Oh7t27S1Kucf41atTQiBEj9PTTTxc71KekpOiDDz7QnDlzdN1110mSatasqfXr1+s///kPod7XfvjzrPPra4/vzBXokwKCdKxChByGTaYhOWSTaRhZ/yQ5ZCjNz18X/P73YVgzx2ueq7N2OYNuzvW5zqdnz4aQ83Y59+XiGOZfX2fa/JRhsynT8FOmYVOmzS/3cYxCjm3kX5b7uJf3CzoAXI78bLbLPtBL0g9/Fjxzm8OUVv2Z4PFQfzE5Z1nK1qJFi1zbtGjRosDx5Vu2bFFKSorq16+fa3lqaqr27t0rSdq+fbvuueeeEtXZrVs3jRkzRps2bVKLFi00e/ZsNWzY0HncrVu3as2aNapZs2a+2+7du9dlqJek/v3764UXXtArr7yi4OBgTZ8+XXfccYfCwsIkZb1peeutt7Rz504lJSUpMzNTqampSklJUVBQUJHvx86dO5WamqpevXrlWp6enq5GjRoVeX95EepLwDRNZeYYnlLlXIIkafWVjXUgJIozyACAy16b2FBfl+Bzpmkqw1H4cNZ0R8HT4BZXrVq1ZBiGdu7cqTvuuCPf+t27dys8PNzluHN3OBwORUVFad68efnWZQfj8uVL/nH7qKgotW7dWnPnzlWLFi00b948DRgwIFcdHTt2dI7Lz3vbgnTr1k0vvPCC5s+fr1atWmn9+vXOvygcOHBA/fv318CBAzVixAhFRERo/fr1Gj58uDIyMlzuz9WU5Onp6bnqlKTp06crOjo613bZf+koCUJ9CRiGIXuOH6D9rw9tJvtXINADAC57NSPKaeiNV/i6DJ/LyguFh3W7zfD4XzQiIyPVtm1bTZkyRQ888ECucfXHjh3TnDlz1KtXr1zH3bw59+xsmzdvLnD4TuPGjXX8+HHZ7XbFxMS43Oaaa67RDz/8oH79+rlc7+/vr8zMi0960bNnT40dO1bdunXT3r171a1bt1x1LF68WDExMbLb3Y+2wcHBuvPOO/XFF19o3759qlGjhnMozi+//KKMjAyNGTPGGdYXLFhQ6P4qVqyo33//Pdeybdu2yd/fX1LWkJ9y5crp4MGDJR5q4wqhvoTaxIZq9paTMiXZzawHZUYJLh4F91Twt+nWemF62I0ZA7x5RdmUtEx1/WS7c/51ALhclfMzNLl3Paaz/MtNtSM065djcnXC3mZkrS8Nr7zyiv72t7+pT58+GjlyZK4pLaOjo/Xss8/m2n7Dhg2aOHGi7rjjDq1cuVILFy7U559/7nLfbdu2VYsWLTRw4EDnB2qPHj2q7777TrfffruaNm2qJ598Uj169FDNmjXVrVs3ZWRk6LvvvtMjjzwiSapevbp+/PFHdevWTQEBAQX+1eBvf/ubnn76aT399NNq3bq1qlat6lx333336bPPPtMDDzyghx9+WJGRkdqzZ4/mz5+vN998U35+BT8G+/fvrzvvvFM7d+7UsGHDnL/ja9asqYyMDH300Ufq2LGjNmzYoGnTphXa67i4OL377rv68ssvdd1112nWrFn6/fffnUNrgoODNWzYML344otyOBy64YYblJycrA0bNigoKEh9+/YtdP8XQ6gvoaE3XqFNB5K193Sq80x9puH+C5jzQi/+hs5dyFRaZskuIlKSC5FEBvpp4f0NXYbWgsJsSlqmPvzraqLZVx2Ny3N10LzbrtpzVumZps6lZfXLlJSWaSrAz6bQ8jbdFBuW76qlORmGoSuuuEJHjhxx60qG7oR1d8+OXGy74HJ2/e2aSM3ZetLlC7fVVPC3qWP9cD0cV00V/LMeXYW9sTmX7tDQmTu173Rqvgvq1Igop8m96ym4nL1YV6As6hVJs+V8jKZnOmQzpNDyfkpKc8jhyJoRKuf3Nplq48aVf7OP12PqDh1NSitwu8pBds2/r6FS0jL17+8P6isXF/WSsl4LKgbZdVNsWL6r6763+pC+/uO0UvP8xTfnhY9M838XVPrfhZD+95zK3mfW1XoPa82eJGU4TPkZ0g01QpThkJbvStD5HNfZMCSVsxsKLe+nG2uEKsMhrdh9RqkZDufxyvnbFBRgk90w1KZ21nN38rojmvtrwc+BLg0i9HSescP/+v6A5v56yuVtDEk9GlfUY22rF+tN+r++P1hoPbUiAjSpd+651Avqe3k/qWpYOSVeyFDCuUy3n+eGpPJ2Q21qhWnd/kTnBAuFCczR2xtqhMgwDK3fl6QLGRlKOJ//woWGss6MT/orSOftg/P5mZCaq+6ct5OkB2bt0t7Tqfn2HxJg6NO7rlaFAD99uO6I5hTSU0mKDPIv8HX8cvRQXDVt3H9We0/n7r/NkGpGBuqhuPwXQfKE2NhYLVu2TK+99pqGDBmihIQEValSRbfffruefPLJfHPUP/TQQ9q6daveeOMNBQUFacyYMerQoYPLfRuGoS+++EIvv/yyhg8frlOnTqlKlSpq2bKl84O3rVu31kcffaQ333xTEydOVEhIiFq2bOncxzPPPKMnn3xS119/vS5cuKDjx4+7PFZISIg6duyohQsX6t///neuddHR0Vq8eLHGjh2rPn36KC0tTdWqVVOHDh1cDonJqWXLlqpTp47i4+PVp08f5/JGjRpp7Nixmjhxol566SW1bNlSzz33nP7xj38UuK8OHTro8ccf19ixY3XhwgX169dPvXv31m+//ebcZsSIEapUqZLefvtt7du3T2FhYWrUqJGGDx9eaJ3uMMzL9PrOJ06cyDXOqSRS0jL14drDqrhwpjIzHfq6QQc1q13J+QvwfI6rxpa3577UffYLXmGXZHfFnUvU59V9ynYdSy74PkeHBGjuvQ0ueuyi1nSxbQu6vHpBDMNQ1apV3Q713paSlnnRX5wV/LM+OJb7TZFZrIBZ0r9EFPTG7IFWV6puzepF6vO/vj+gOVtOunxTaTOkHo0r+XS+6rx9uNj37vjX9wcKfBPn6j6npGVq8trDzp+53c/QbQ2vUP/GoW791SlbzufMxf4vbH951ydfyNDkPI+HvFelzvvhurz7Keg5YDOkmhHlnaEzp+Lcxl2FPSdrRV5833n7nnN59pWfbTZbrr7n3D5vv7LfWK2OT3TOVZ59MiTQnjUEo6DeZi87l+4ocB+F3ZfCjp3zjWTObfwMuXwduthjv2fjyhre1nNB1d/f3xkUfSU+Pl4hISHFvn32PPWr/kxQusOUv81Qm1Kep97TGjZsqBEjRhQ4BSU8LykpSbGxsRfdjlDvKZmZ8p+/QElJSSp3V38Zf42fcv4yyXPm0mZINSLKe/VPk0UNH2VVWQ/1knu/OPPyRMAsqZzHLE6fu0/ZXuhZ66ohAZpTgjeOZVFJwqhpmrLZbGXm8ezJ16viPAeKc5ui3LcP1x3R2v3JupCW4dF9l4QnnufF3YcnTggU9NivUyVY73Wv7fwLnydcCqE+J6tN83nu3Dlt2LBBffr00eLFi51TMqL0uRvq+buYp2R/EtqQlGPs1uR1h/P9gpSypq/al5Dq1SvtZQ8VKih88GEmzwkK8NNjbavrsbbuv3Dn3cYXL/YlOWbW7A6FDynIKIXZHXwtKMBPk3vXK1YYLWt98OTrVXGeA8W5jbuCAvz0WLvqmlC1qg4fPuyx/ZaUJ+5jcfdR0qGJBT3228SG6cXu1yrp9Amfv1Ety8ra8/9iPv30U7355psaOnQogb6MItR7iPlXqDf8/GTkGL+1Kj6x0HlpvXmlvZKEDxSf1V64iyvvbFCu+JXC7A5lQWmGUW8qrder4vSjNHt4sc9nwH2uHvuGYSi4nF1Jvi4OHvXAAw/kuhgTyh5Cvadkn6nPMZVSWTxzeamED5RNbWJDCx3idTnMV23V51RZfL2CtfC4AHyLuRc9xUWoL+tnLnkBhqcNvfEK1Ygor7zTMTPEq+wr669XAIDCEeo9JXv4TZ6LHrSJDc0XcLJdLmcucfnIHuLVo3ElVQ0JUOUgf1UNCVCPxpVKNHsJvIPXKwCwLobfeIiZfTU0v9wt5cOpuNwwxMu6eL0CAOsi1HuKi+E3Eh9OxeWNQG8tvF4BgHUR6j0lPXv4Tf5fepy5BGAVvF4BgDUxpt5DzMzsM/X+hW7HL0gAVsHrFYCy5pFHHtGAAQN8XUaZRKj3FOfwG/48DQAAyoZHHnlEVapUcf6rX7+++vTpo+3bt3vsGBMmTFD79u0L3WbkyJG64YYbXK47cuSIoqOjtXjxYo/VdDki1HtKAbPfAAAA+FKHDh3066+/6tdff9Xs2bNlt9t19913e7WG/v37a8+ePfrxxx/zrZsxY4YiIyPVqVMnr9Z0qSHUe4iZkSlTZr4PygIAAPhSQECAoqKiFBUVpUaNGumRRx7RoUOHdPLkSec2R44c0ZAhQ1S3bl3Vr19fAwYM0P79+53r16xZo06dOqlmzZqqU6eO/va3v+nAgQOaMWOGXn/9dW3fvt3514AZM2bkq6FRo0Zq3Lixpk+fnm/djBkz1KtXL9lsNg0fPlwtWrRQTEyMbrzxRk2ePLnQ+9a8eXNNmjQp17L27dtrwoQJzu8TExP1xBNP6JprrlFsbKy6d++ubdu2ud0/qyCBllBKWqYmrzuss6t3q9aJfdoVZajiuWhmigAA4BJmmub/ht56m91e7M+8JCcna/bs2apVq5YiIyMlSefOnVO3bt3UsmVLLViwQHa7XW+++ab69u2rlStXymazaeDAgbr77rv1wQcfKD09XT/99JMMw1CXLl3022+/acWKFZo1a5YkKTTU9TUt+vfvr7Fjx+rll19WcHCwJGnt2rXas2eP+vfvL4fDoapVq+rDDz9UZGSkNm7cqCeffFJRUVHq0qVLse6vaZrq37+/IiIiNH36dIWGhmratGnq2bOn1q1bp4iIiGLttywi1JdASlqmhs7cqX2nU3Vd8gUlX8jQsfOmVm49qU0HkjWZi+0AAHBpysjQuU8/9cmhK9xzj+Rf+MQcOX3zzTeqWbOmpKwAHxUVpc8//1y2v64iPX/+fNlsNv3rX/9yvll4++23VbduXa1Zs0ZNmzZVYmKiOnbsqFq1akmS6tWr59x/UFCQ/Pz8FBUVVWgdPXr00OjRo7Vo0SL169dPkjR9+nS1aNFC9evXlyQ988wzzu1r1KihjRs3asGCBcUO9atXr9Zvv/2mHTt2qFy5cpKkMWPGaMmSJVq0aNEl9aFbQn0JTF53WPtOp8ohye7IuvhUps1PDlPal5CqyesO67G21X1bJAAAuKy1bt3aORzlzJkzmjJlivr27aulS5eqevXq2rJli/bs2eMM7NlSU1O1d+9etW/fXn379lWfPn3Utm1b3XTTTerSpctFQ3xeYWFhuuOOOzR9+nT169dPycnJWrx4scaNG+fcZurUqfr888918OBBnT9/Xunp6WrYsGGx7/uWLVuUkpLifNOQ975dSgj1JbAqPlGOv762m1mhPsOWdWbeYUqr4xP1WFsfFQcAAEqP3Z51xtxHxy6KChUqKDY21vl9kyZNVLt2bX322WcaOXKkHA6HmjRpovfeey/fbStVqiQp68z9kCFDtHz5cs2fP1/jx4/XrFmz1KJFiyLVctddd6lHjx6Kj4/X2rVrJUldu3aVJC1YsEAvvviiRo8ereuuu05BQUF699139dNPPxW4P8MwsoZC5ZCRY1iUw+FQVFSU5s2bl++2YWFhRaq9rCPUF5NpmspwOJzf+/31dYbxv88eZzhMLt4CAMAlyDCMIg2BKUsMw5DNZtP58+clSY0bN9aCBQtUuXJlhYSEFHi7Ro0aqVGjRnr00Ud1++23a+7cuWrRooUCAgLkyJGJChMXF6caNWpoxowZWr16tbp06eIcX//jjz/quuuu03333efc/mJn0ytVqqRjx445v09KSsr1Ad/GjRvr+PHjstvtiomJcatGq2L2m2IyDEN22//at6NiTf1YtaFOBoY7l/nZDAI9AADwqbS0NB07dkzHjh3Tzp07NXLkSKWkpDinkOzRo4ciIyM1YMAA/fjjj9q3b5/Wrl2r5557TocPH9a+ffs0btw4bdy4UQcOHNCKFSsUHx+vunXrSpKqV6+uffv26ddff9WpU6d04cKFAmsxDEP9+vXT1KlTtWnTJvXv39+5rlatWvrll1+0fPly/fnnn3rllVf0yy+/FHrf4uLiNGvWLP3444/67bff9I9//MP5WQFJatu2rVq0aKGBAwdq+fLl2r9/vzZs2KDx48dfdN9Ww5n6EmgTG6o5W0/KYUpHgyrmWmczstYDAAD40vLly9WoUSNJUnBwsOrWrauPPvpIrVu3lpQ1PGfBggX65z//qXvvvVfJycmKjo7WTTfdpJCQEJ0/f167du3Sl19+qYSEBEVFRem+++7TwIEDJUmdO3fWf//7X3Xv3l1nz57V22+/rb59+xZYT9++fTVhwgTVqVMn1wWpBg4cqG3btmno0KEyDEPdunXTvffeq++++67AfT366KPat2+f7rrrLoWGhuqZZ57JdabeMAx98cUXevnllzV8+HCdOnVKVapUUcuWLVW5cuUS9bWsMcy8A5EuEydOnFB6enqJ9uGc/SYhVY4cXbQZUs2I8prE7DelwjAMVa1aVUeOHMk3jg6eQ5+9gz57D732jtLqs7+/v89DWHx8fKHDU4DSkJSUlOszEQXhTH0JBAX4aXLvepq87rBW70mUKZsMORRXK5R56gEAAOA1hPoSCgrw02Ntq+vxdoaio6N19OhRzgABAADAq/igrAfxoVgAAAD4AqEeAAAAsDhCPQAAAFBGuTsShFAPAADgBldXLwVKk8PhINQDAAB4UkREhM6dO+frMnCZcDgcSkpKUlRUlFvbM/sNAACAG8LDw5WamqqkpCRfl4JLnGEYMgxDV155pQIDA926DaEeAADATdHR0b4uAXCJ4TcAAACAxRHqAQAAAIsj1AMAAAAWR6gHAAAALI5QDwAAAFgcoR4AAACwOEI9AAAAYHGEegAAAMDiCPUAAACAxRHqAQAAAIsj1AMAAAAWR6gHAAAALI5QDwAAAFic3dcFFMfSpUu1cOFCnTlzRtWqVdOgQYN09dVX+7osAAAAwCcsd6Z+7dq1mjp1qrp3765XX31VV199tV5++WWdPHnS16UBAAAAPmG5UL948WJ16NBBN998s/MsfaVKlbRs2TJflwYAAAD4hKVCfUZGhuLj49WkSZNcyxs3bqw//vjDR1UBAAAAvmWpMfWJiYlyOBwKCwvLtTwsLExnzpxxeZv09HSlp6c7vzcMQ4GBgc6vPSV7X57cJ1yj195Bn72DPnsPvfYO+gz4hqVCfTZXLxQFvXjMmzdPs2fPdn5fq1Ytvfrqq6pcuXKp1BYdHV0q+0V+9No76LN30GfvodfeQZ8B77JUqA8NDZXNZst3Vv7s2bP5zt5n69atmzp37uz8Pjv8nzhxQhkZGR6rzTAMRUdH6+jRozJN02P7RX702jvos3fQZ++h195RWn222+2ldkIOuBRYKtTb7XbFxsZq69atuv76653Lt27dquuuu87lbfz9/eXv7+9yXWm8qJumyS8LL6HX3kGfvYM+ew+99g76DHiXpUK9JHXu3FkTJ05UbGys6tWrp2+//VYnT57Urbfe6uvSAAAAAJ+wXKhv1aqVkpKSNGfOHCUkJKh69eoaOXIkf5IDAADAZctyoV6SOnXqpE6dOvm6DAAAAKBMsNQ89QAAAADyI9QDAAAAFkeoBwAAACyOUA8AAABYHKEeAAAAsDhCPQAAAGBxhHoAAADA4gj1AAAAgMUR6gEAAACLI9QDAAAAFkeoBwAAACyOUA8AAABYHKEeAAAAsDhCPQAAAGBxhHoAAADA4gj1AAAAgMUR6gEAAACLI9QDAAAAFkeoBwAAACyOUA8AAABYHKEeAAAAsDhCPQAAAGBxhHoAAADA4gj1AAAAgMUR6gEAAACLI9QDAAAAFkeoBwAAACyOUA8AAABYHKEeAAAAsDhCPQAAAGBxhHoAgEeYpunrEgDgsmX3dQEAAOtKScvU5HWHtSo+URkOh+w2m9rEhmrojVcoKMDP1+UBwGWDUA8AKJaUtEwNnblT+06nypFj+ZytJ7XpQLIm965HsAcAL2H4DQCgWCavO5wv0EuSw5T2JaRq8rrDPqkLAC5HhHoAQLGsik/MF+izOUxpdXyiV+sBgMsZoR4AUGSmaSrDUVCkz5LhMPnwLAB4CaEeAFBkhmHIbiv8V4ifzZBhGF6qCAAub4R6AECxtIkNla2AzG4zstYDALyDUA8AKJahN16hGhHl8wV7myHVjCivoTde4ZvCAOAyxJSWAIBiCQrw0+Te9TR53WGtjk9UhsOU3WYojnnqAcDrCPUAgGILCvDTY22r67G2WR+eZQw9APgGw28AAB5BoAcA3yn2mfpDhw5px44dSkpKUocOHRQeHq7Tp08rODhYAQEBnqwRAAAAQCGKHOodDocmTZqklStXOpc1bdpU4eHhmjx5smrVqqU+ffp4skYAAAAAhSjy8Ju5c+dq9erVuueee/TGG2/kWtesWTP98ssvnqoNAAAAgBuKfKZ+5cqV6tGjhzp37ixHnqsJVqlSRcePH/dYcQAAAAAurshn6k+fPq169eq5XOfv76/U1NQSFwUAAADAfUUO9WFhYQWejT98+LAiIyNLXBQAAAAA9xU51Ddr1kxz587V6dOnncsMw9C5c+e0ZMkSNW/e3KMFAgAAAChckcfU9+7dWz///LMee+wxNWjQQJL0xRdf6MCBA/Lz81PPnj09XiQAAACAghX5TH14eLjGjx+v1q1ba8+ePbLZbNq3b5+aNm2qcePGKTg4uDTqBAAAAFCAYl18Kjw8XEOHDvV0LQAAAACKochn6gEAAACULUU+U//ee+8Vut4wDD300EPFLggAAABA0RQ51G/fvj3fsuTkZKWmpqpChQoKCgrySGEAAAAA3FPkUP/uu++6XL5t2zZ99NFHevzxx0tcFAAAAAD3eWxMfcOGDXXbbbdpypQpntolAAAAADd49IOy1apV0+7duz25SwAAAAAX4dFQv2PHDoWGhnpylwAAAAAuoshj6mfPnp1vWXp6uvbt26dffvlFd955p0cKAwAAAOCeIof6WbNm5d+J3a4qVaqod+/ehHoAAADAy4oc6r/88svSqAMAAABAMXFFWQAAAMDiCPUAAACAxbk1/KZPnz5u79AwDM2YMaPYBQEAAAAoGrdCfY8ePWQYRmnXAgAAAKAY3Ar1vXv3Lu06AAAAABQTY+oBAAAAiyvylJbZ9u/fr0OHDiktLS3furZt25aoqILMnTtXP/30k/bu3Su73a6pU6eWynEAAAAAKylyqL9w4YImTJigbdu2FbhNaYX6jIwMtWzZUvXq1dPy5ctL5RgAAACA1RQ51M+ZM0fHjx/X6NGjNXr0aD3xxBMKDAzUN998o/3792v48OGlUGaW7LH9K1euLLVjAAAAAFZT5FC/ceNGdenSRfXr15ckVapUSbGxsWrUqJH+/e9/a9myZRo6dKjHCy2u9PR0paenO783DEOBgYHOrz0le1/MElT66LV30GfvoM/eQ6+9gz4DvlHkUH/ixAldeeWVstmyPmObc0x9mzZt9P7775epUD9v3jzNnj3b+X2tWrX06quvqnLlyqVyvOjo6FLZL/Kj195Bn72DPnsPvfYO+gx4V5FDfVBQkC5cuCBJCgsL05EjR3TVVVdJyhrznr3OXTNnzswVul0ZP368ateuXdRSJUndunVT586dnd9nnzk4ceKEMjIyirVPVwzDUHR0tI4ePSrTND22X+RHr72DPnsHffYeeu0dpdVnu91eaifkgEtBkUN9TEyMDh8+rKZNm6pBgwaaN2+eqlatKrvdrjlz5qhGjRpF2t9tt92m1q1bF7pNSZ7E/v7+8vf3d7muNF7UTdPkl4WX0GvvoM/eQZ+9h157B30GvKvIob59+/Y6evSoJKlfv3564YUXNGrUKElZZ/FHjhxZpP2FhoYqNDS0qGUAAAAA+ItboX7q1Knq0KGDYmJi1KpVK+fyKlWq6N///re2bdsmwzBUv359BQcHl1qxJ0+eVHJysk6ePCmHw6G9e/dKyhq3V758+VI7LgAAAFCWuRXqlyxZoiVLlig2NlYdOnRQ69atVaFCBUlS+fLl1aJFi1ItMtuXX36p77//3vn9008/LUkaNWqUGjRo4JUaAAAAgLLGMN0Y8Hb06FEtX75cq1at0unTpxUQEKAbbrhBHTp00DXXXOONOj3uxIkTuaa6LCnDMFS1alUdOXKEMYSljF57B332DvrsPfTaO0qrz/7+/nxQFiiEW2fqo6Oj1b9/f/Xt21dbtmzRihUrtG7dOq1atUpVqlRRhw4d1LZtW0VGRpZ2vQAAAADyKNIHZW02m5o1a6ZmzZopOTlZq1at0sqVKzVjxgzNnDlTjRs3VocOHXTDDTeUVr0AAAAA8ijy7DfZgoODdfvtt+v222/Xvn37tHTpUn333XfasmWLZsyY4ckaAQAAABSi2KE+W3x8vFasWKEff/xRkpieEgAAAPCyYoX6pKQkrVq1SitWrND+/ftls9nUpEkTdejQQc2bN/d0jQAAAAAK4XaoN01TP//8s1auXKnNmzcrIyNDUVFR6tu3r9q1a6eIiIjSrBMAAABAAdwK9dOnT9cPP/yghIQEBQQE6MYbb7T0dJYAAADApcStUL9gwQLFxsaqe/fuiouLc154CgAAAIDvuRXqJ0yYoBo1apR2LQAAAACKwebORgR6AAAAoOxyK9QDAAAAKLsI9QAAAIDFEeoBAAAAiyPUAwAAABZXrCvKStK5c+e0c+dOJSUlqVmzZgoODvZkXQAAAADcVKxQP3v2bC1YsEBpaWmSpPHjxys4OFhjx45V48aN1bVrV0/WCAAAAKAQRR5+s3TpUs2ePVvt27fXiBEjcq279tpr9dNPP3msOAAAAAAXV+Qz9V9//bU6d+6su+++Ww6HI9e6qlWr6siRIx4rDgAAAMDFFflM/fHjx9WkSROX6wIDA3Xu3LkSFwUAAADAfUUO9RUqVNDZs2ddrjt+/LhCQ0NLXBQAAAAA9xU51Dds2FALFixQamqqc5lhGMrMzNQ333xT4Fl8AAAAAKWjyGPq+/Tpo5EjR+rxxx/X9ddfLylrnP3evXt18uRJPfbYYx4vEgAAAEDBinymPjo6Wv/85z915ZVXaunSpZKkH374QSEhIRozZowqVark8SIBAAAAFKxY89RXq1ZNzz33nNLT05WUlKTg4GAFBAR4ujYAAAAAbijymfrNmzc7p7L09/dXZGQkgR4AAADwoSKfqZ8wYYLCwsJ00003qV27dqpWrVpp1AUAAADATUUO9SNGjNDKlSu1ZMkSLVq0SHXq1FH79u3VunVrBQYGlkaNAAAAAApR5FDfrFkzNWvWTCkpKVq9erW+//57ffjhh5o2bZquv/56tW/fXg0bNiyNWgEAAAC4UKwPykpSUFCQOnXqpE6dOungwYNauXKlvv/+e61Zs0YzZszwZI0AAAAAClHkD8rmZZqmTp06pZMnT+rcuXMyTdMTdQEAAABwU7HP1B89etR5dv706dOKjIxU586d1b59e0/WBwAAAOAiihzqV6xYoZUrV+r333+X3W5XixYt1L59ezVu3Fg2W4lP/AMAAAAooiKH+g8++EA1a9bUvffeq7i4OAUHB5dGXQAAAADcVKx56mvUqFEatQAAAAAohiKPlyHQAwAAAGWLW2fqZ8+erQ4dOigyMlKzZ8++6PY9e/YscWEAAAAA3ONWqJ81a5aaNm2qyMhIzZo166LbE+oBAAAA73Er1H/55ZcuvwYAAADge8xBCQAAAFhckUN9nz59tHv3bpfr4uPj1adPnxIXBQAAAMB9Hj1T73A4ZBiGJ3cJAAAA4CI8Gurj4+NVoUIFT+4SAAAAwEW49UHZr776Sl999ZXz+9dee03+/v65tklLS9PZs2fVsmVLz1YIAAAAoFBuhfrQ0FBVq1ZNknTixAlFRUXlOyPv7++vmJgY3XHHHZ6vEgAAAECB3Ar1cXFxiouLkySNGTNGgwcP1pVXXlmqhQEAAABwj1uhPqdRo0aVRh0AAAAAiqnIH5RdsWKFZs6c6XLdzJkz9f3335e4KAAAAADuK3KoX7JkiYKDg12uCw0N1ZIlS0pcFAAAAAD3FTnUHz16VNWrV3e5rlq1ajpy5EiJiwIAAADgvmLNU3/u3LkClzscjhIVBAAAAKBoihzqY2JitGbNGpfrVq9erZiYmBIXBQAAAMB9RQ71t912m9avX6933nlHu3bt0unTp7Vr1y69++67Wr9+vW677bbSqBMAAABAAYo8pWVcXJwOHTqk+fPna9WqVc7lNptNPXr0UJs2bTxaIAAAAIDCFTnUS1KfPn3Uvn17bd26VYmJiQoNDVWTJk1UuXJlT9cHAAAA4CKKFeolqUqVKrrllls8WQsAAACAYihWqE9PT9fKlSu1fft2JScn6/7771fVqlW1ceNGxcTEKCoqytN1AgAAAChAkUN9YmKixowZo4MHDyo8PFxnzpzR+fPnJUkbN27Uli1bNHjwYI8XCgAAAMC1Is9+89lnn+ncuXMaP3683nvvvVzrGjRooB07dnisOAAAAAAXV+RQ/9NPP6l3796KjY2VYRi51lWsWFGnTp3yWHEAAAAALq7Iof78+fMFznKTkZHBFWUBAAAALytyqK9SpYp27tzpct3u3bt1xRVXlLgoAAAAAO4rcqiPi4vTggULtHHjRpmmKUkyDEO7d+/WkiVLuPgUAAAA4GVFnv2mS5cu+uOPP/T6668rKChIkvTSSy8pKSlJTZs21R133OHxIgEAAAAUrMih3m63a+TIkVq7dq1++uknnT17ViEhIWrevLlatWolm63IJ/8BAAAAlECxLj5lGIZat26t1q1be7oeAAAAAEXEaXUAAADA4tw6Uz9mzBgNHjxYV155pcaMGVPotoZhKDg4WPXr11fHjh3l7+/vkUIBAAAAuFbk4Temaea76FTe9ceOHdPGjRt14MABPfjggyUqEAAAAEDh3Ar1o0aNcn49evRot3a8fPlyTZ8+vVhFuXL8+HHNmTNH27Zt05kzZxQZGak2bdqoe/fustuL9dEAAAAA4JJQamn46quv1rXXXuux/R0+fFimaWro0KGKjo7WgQMHNGnSJKWmpmrAgAEeOw4AAABgNcUK9Q6HQ2vXrtX27duVlJSkkJAQNWjQQDfeeKP8/PwkSVWrVtWwYcM8VmjTpk3VtGlT5/dRUVE6fPiwli1bRqgHAADAZa3IoT4xMVEvv/yy9uzZI5vNppCQECUlJWn58uVatGiRnnvuOYWGhpZGrfmcO3dOwcHBhW6Tnp6u9PR05/eGYSgwMND5tadk78uT+4Rr9No76LN30GfvodfeQZ8B3zBM0zSLcoOJEydq48aNGjp0qPNiU9ln7j/88EO1aNFCjzzySGnV63T06FE988wzGjBggG6++eYCt5s5c6Zmz57t/L5WrVp69dVXS70+AAAAwFuKfKZ+8+bN6tu3r+Li4pzLbDab4uLidPbsWc2aNatI+8sbul0ZP368ateu7fz+9OnTevnll3XjjTcWGuglqVu3burcubPz++wzBydOnFBGRkaRai2MYRiKjo7W0aNHVcT3SSgieu0d9Nk76LP30GvvKK0+2+12Va5c2WP7Ay41xZrSslq1ai7XVa9evchP4Ntuu+2iV6bN+SQ+ffq0xowZo3r16mno0KEX3b+/v3+Bc+WXxou6aZr8svASeu0d9Nk76LP30GvvoM+AdxU51Ddq1Ei//vqrGjdunG/d1q1b1aBBgyLtLzQ01O0x+NmBvlatWho2bJhsNi6ICwAAALgV6pOTk51f9+zZU6+//rocDofi4uIUHh6uM2fOaNWqVdqwYYOefPLJUin09OnTGj16tCpVqqQBAwYoMTHRuS48PLxUjgkAAABYgVuh/v7778+3bPHixVq8eHG+5c8884y+/PLLkleWx9atW3X06FEdPXo031VqZ86c6fHjAQAAAFbhVqjv0aOHz6emateundq1a+fTGgAAAICyyK1Q37t379KuAwAAAEAxFeuKsqZpKikpSYZhKDg42Odn8QEAAIDLWZFC/c6dOzV//nxt27ZNFy5ckCSVK1dODRs2VLdu3VS3bt1SKRIAAABAwdwO9UuXLtXUqVMlSbGxsc6540+cOKGff/5ZP//8swYNGqROnTqVSqEAAAAAXHMr1O/cuVNTpkxRs2bNNHjwYFWsWDHX+lOnTunDDz/U1KlTVbt2bdWpU6dUigUAAACQn1tXb1q8eLHq1q2rp556Kl+gl6SKFSvq6aefVp06dbRw4UKPFwkAAACgYG6F+t9//12dOnUq9AquNptNHTt21O+//+6x4gAAAABcnFuhPjk5WZUqVbrodpUrV8519VkAAAAApc+tUB8SEqITJ05cdLuTJ08qJCSkxEUBAAAAcJ9bob5+/fpatmyZHA5Hgds4HA59/fXXuuqqqzxWHAAAAICLcyvUd+7cWbt27dLrr7+uhISEfOtPnz6t119/XX/++af+/ve/e7xIAAAAAAVza0rLevXqaeDAgZo2bZqGDRum2rVrq0qVKpKk48eP688//5Rpmho0aBDTWQIAAABe5vbFp26//XbVqlVL8+fP1/bt27Vr1y5JUkBAgJo0aaJu3bqpfv36pVYoAAAAANfcDvWSdNVVV2nEiBFyOBxKSkqSlPUh2sKmugQAAABQuooU6rPZbDaFhYV5uhYAAAAAxcApdgAAAMDiCPUAAACAxRHqAQAAAIsj1AMAAAAWR6gHAAAALI5QDwAAAFgcoR4AAACwOEI9AAAAYHGEegAAAMDiCPUAAACAxRHqAQAAAIsj1AMAAAAWR6gHAAAALI5QDwAAAFgcoR4AAACwOEI9AAAAYHGEegAAAMDiCPUAAACAxRHqAQAAAIsj1AMAAAAWR6gHAAAALI5QDwAAAFgcoR4AAACwOEI9AAAAYHGEegAAAMDiCPUAAACAxRHqAQAAAIsj1AMAAAAWR6gHAAAALI5QDwAAAFgcoR4AAACwOEI9AAAAYHGEegAAAMDiCPUAAACAxRHqAQAAAIsj1AMAAAAWR6gHAAAALI5QDwAAAFgcoR4AAACwOEI9AAAAYHGEegAAAMDiCPUAAACAxRHqAQAAAIsj1AMAAAAWR6gHAAAALI5QDwAAAFgcoR4AAACwOEI9AAAAYHGEegAAAMDiCPUAAACAxRHqAQAAAIsj1AMAAAAWZ/d1AUXx6quvau/evUpMTFRQUJAaNWqku+66S5GRkb4uDQAAAPAZS4X6Bg0aqFu3boqIiNDp06f16aef6s0339S4ceN8XRoAAADgM5YaftO5c2fVq1dPlStXVv369dW1a1ft2rVLGRkZvi4NAAAA8BlLhfqckpOTtWrVKtWrV092u6X+4AAAAAB4lOXS8GeffaalS5fqwoULqlu3rkaMGFHo9unp6UpPT3d+bxiGAgMDnV97Sva+PLlPuEavvYM+ewd99h567R30GfANwzRN05cFzJw5U7Nnzy50m/Hjx6t27dqSpMTERCUnJ+vkyZOaNWuWKlSooBEjRhT44pF3/7Vq1dKrr77quTsAAAAA+JjPQ31iYqKSkpIK3aZy5coKCAjIt/zUqVN66KGHNG7cONWrV8/lbQs6U3/ixAmPjsU3DEPR0dE6evSofNzSSx699g767B302XvotXeUVp/tdrsqV67ssf0BlxqfD78JDQ1VaGhosW6b/WKRM7Tn5e/vL39//0Jv70mmafLLwkvotXfQZ++gz95Dr72DPgPe5fNQ767du3dr9+7duuqqqxQUFKRjx45p5syZioqKKvAsPQAAAHA5sEyoDwgI0Pr16zVz5kxduHBB4eHhatq0qYYPH17gmXgAAADgcmCZUB8TE6NRo0b5ugwAAACgzLHsPPUAAAAAshDqAQAAAIsj1AMAAAAWR6gHAAAALI5QDwAAAFgcoR4AAACwOEI9AAAAYHGEegAAAMDiCPUAAACAxRHqAQAAAIsj1AMAAAAWR6gHAAAALI5QDwAAAFgcoR4AAACwOEI9AAAAYHGEegAAAMDiCPUAAACAxRHqAQAAAIsj1AMAAAAWR6gHAAAALI5QDwAAAFgcoR4AAACwOEI9AAAAYHGEegAAAMDiCPUAAACAxRHqAQAAAIsj1AMAAAAWR6gHAAAALI5QDwAAAFgcoR4AAACwOEI9AAAAYHGEegAAAMDiCPUAAACAxRHqAQC4DJim6esSAJQiu68LAAAApSMlLVOT1x3WqvhEZTgcsttsahMbqqE3XqGgAD9flwfAgwj1AABcglLSMjV05k7tO50qR47lc7ae1KYDyZrcux7BHriEMPwGAIBL0OR1h/MFeklymNK+hFRNXnfYJ3UBKB2EegAALkGr4hPzBfpsDlNaHZ/o1XoAlC5CPQAAlxjTNJXhKCjSZ8lwmHx4FriEEOoBALjEGIYhu63wX/F+NkOGYXipIgCljVAPAMAlqE1sqGwFZHabkbUewKWDUA8AwCVo6I1XqEZE+XzB3mZINSPKa+iNV/imMAClgiktAQC4BAUF+Gly73qavO6wVscnKsNhym4zFMc89cAliVAPAMAlKijAT4+1ra7H2mZ9eJYx9MCli+E3AABcBgj0wKWNUA8AAABYHKEeAAAAsDhCPQAAAGBxhHoAAADA4gj1AAAAgMUR6gEAAACLI9QDAAAAFkeoBwAAACyOUA8AAABYnN3XBfiK3V46d7209ov86LV30GfvoM/eQ6+9w9N95ucGFM4wTdP0dREAAAAAio/hNx5y/vx5PfPMMzp//ryvS7nk0WvvoM/eQZ+9h157B30GfINQ7yGmaWrPnj3iDx+lj157B332DvrsPfTaO+gz4BuEegAAAMDiCPUAAACAxRHqPcTf3189e/aUv7+/r0u55NFr76DP3kGfvYdeewd9BnyD2W8AAAAAi+NMPQAAAGBxhHoAAADA4gj1AAAAgMUR6gEAAACLs/u6gEvF0qVLtXDhQp05c0bVqlXToEGDdPXVV/u6LMvYsWOHFi5cqD179ighIUFPPvmkrr/+eud60zQ1a9Ysfffdd0pOTlbdunV1//33q3r16s5t0tPT9emnn2rNmjVKS0tTw4YNNXjwYFWsWNEXd6lMmjdvnjZs2KBDhw4pICBA9erV0913360rrrjCuQ29Lrlly5Zp2bJlOnHihCSpWrVq6tmzp5o1ayaJHpeWefPm6YsvvtAdd9yhQYMGSaLXnjJz5kzNnj0717KwsDB9+OGHkugzUBZwpt4D1q5dq6lTp6p79+569dVXdfXVV+vll1/WyZMnfV2aZVy4cEE1a9bUfffd53L9ggUL9N///lf33Xefxo8fr/DwcI0bNy7XZcinTp2qDRs26NFHH9XYsWOVmpqqV155RQ6Hw1t3o8zbsWOHOnXqpJdeeknPP/+8HA6Hxo0bp9TUVOc29LrkIiMj1b9/f40fP17jx49Xw4YNNWHCBB04cEASPS4Nu3fv1rfffqsaNWrkWk6vPad69eqaPHmy898bb7zhXEefgTLARImNHDnSnDx5cq5lw4cPNz///HMfVWRtvXr1MtevX+/83uFwmEOGDDHnzZvnXJaWlmYOHDjQXLZsmWmappmSkmL27dvXXLNmjXObU6dOmb179zZ//vlnb5VuOWfPnjV79eplbt++3TRNel2aBg0aZH733Xf0uBScP3/e/L//+z9zy5Yt5qhRo8wpU6aYpsnj2ZO+/PJL88knn3S5jj4DZQNn6ksoIyND8fHxatKkSa7ljRs31h9//OGjqi4tx48f15kzZ3L12N/fX9dcc42zx/Hx8crMzFTjxo2d20RGRiomJkY7d+70es1Wce7cOUlScHCwJHpdGhwOh9asWaMLFy6oXr169LgUfPTRR2rWrFmufkk8nj3t6NGjeuCBB/Twww/rrbfe0rFjxyTRZ6CsYEx9CSUmJsrhcCgsLCzX8rCwMJ05c8Y3RV1isvvoqsfZQ5zOnDkju93uDKc5t+Hn4Jppmpo2bZquuuoqxcTESKLXnrR//34999xzSk9PV/ny5fXkk0+qWrVqzpBDjz1jzZo12rNnj8aPH59vHY9nz6lbt64efvhhXXHFFTpz5ozmzp2r559/Xm+++SZ9BsoIQr2HGIbh1jIUX95+mm5cDNmdbS5XH3/8sfbv36+xY8fmW0evS+6KK67Qa6+9ppSUFK1fv17vvvuuxowZ41xPj0vu5MmTmjp1qp577jkFBAQUuB29LrnsD3lLUkxMjOrVq6dHHnlE33//verWrSuJPgO+xvCbEgoNDZXNZst3puHs2bP5zlqgeMLDwyUpX48TExOdPQ4PD1dGRoaSk5PzbZN9e/zPJ598os2bN2vUqFG5Zp6g155jt9sVHR2t2rVrq3///qpZs6a++uoreuxB8fHxOnv2rEaMGKG+ffuqb9++2rFjh5YsWaK+ffs6+0mvPa98+fKKiYnRkSNHeEwDZQShvoTsdrtiY2O1devWXMu3bt2q+vXr+6iqS0uVKlUUHh6eq8cZGRnasWOHs8exsbHy8/PLtU1CQoL279+vevXqeb3msso0TX388cdav369XnzxRVWpUiXXenpdekzTVHp6Oj32oEaNGun111/XhAkTnP9q166tuLg4TZgwQVFRUfS6lKSnp+vQoUOKiIjgMQ2UEQy/8YDOnTtr4sSJio2NVb169fTtt9/q5MmTuvXWW31dmmWkpqbq6NGjzu+PHz+uvXv3Kjg4WJUqVdIdd9yhefPmqWrVqoqOjta8efNUrlw5xcXFSZIqVKigDh066NNPP1VISIiCg4P16aefKiYmJt+H5y5nH3/8sVavXq2nn35agYGBzjNrFSpUUEBAgAzDoNceMH36dDVr1kwVK1ZUamqq1qxZo+3bt+u5556jxx4UGBjo/DxItnLlyikkJMS5nF57xn/+8x+1aNFClSpV0tmzZzVnzhydP39ebdu25TENlBGGyYA2j8i++FRCQoKqV6+ugQMH6pprrvF1WZaxffv2XOONs7Vt21YPP/yw88Im3377rVJSUlSnTh3df//9uX6hp6Wl6bPPPtPq1atzXdikUqVK3rwrZVrv3r1dLh82bJjatWsnSfTaA95//31t27ZNCQkJqlChgmrUqKEuXbo4wws9Lj2jR49WzZo18118il6XzFtvvaXffvtNiYmJCg0NVd26ddW3b19Vq1ZNEn0GygJCPQAAAGBxjKkHAAAALI5QDwAAAFgcoR4AAACwOEI9AAAAYHGEegAAAMDiCPUAAACAxRHqAQAAAIvjirIAyoyCLo6V16hRo9SgQYN8y0ePHp3r/6IoyW0BAPA1Qj2AMmPcuHG5vp8zZ462b9+uF198Mdfy7KtY5jV48OBSqw0AgLKMUA+gzKhXr16u70NDQ2UYRr7leV24cEHlypUrMOwDAHCpI9QDsJTRo0crKSlJ999/v6ZPn669e/eqRYsWGj58uMshNLNmzdLPP/+sI0eOyOFwKDo6Wp06dVL79u1lGIZv7gQAAB5GqAdgOQkJCZo4caK6dOmifv36FRrOT5w4oVtuuUWVKlWSJO3atUuffPKJTp8+rZ49e3qrZAAAShWhHoDlJCcn6/HHH1fDhg0vuu2wYcOcXzscDjVo0ECmaWrJkiXq0aMHZ+sBAJcEQj0AywkKCnIr0EvStm3bNG/ePO3evVvnz5/Pte7s2bMKDw8vhQoBAPAuQj0Ay4mIiHBru927d2vcuHFq0KCBHnjgAVWsWFF2u10bN27U3LlzlZaWVsqVAgDgHYR6AJbj7pCZNWvWyM/PT88884wCAgKcyzdu3FhapQEA4BNcURbAJcswDPn5+clm+99LXVpamn744QcfVgUAgOdxph7AJevaa6/V4sWL9fbbb+uWW25RUlKSFi1aJH9/f1+XBgCAR3GmHsAlq2HDhnrooYe0f/9+vfrqq5oxY4ZatmypLl26+Lo0AAA8yjBN0/R1EQAAAACKjzP1AAAAgMUR6gEAAACLI9QDAAAAFkeoBwAAACyOUA8AAABYHKEeAAAAsDhCPQAAAGBxhHoAAADA4gj1AAAAgMUR6gEAAACLI9QDAAAAFkeoBwAAACzu/wEwV2g6RF6UpAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.720357</td>\n",
       "      <td>0.026407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>165.400000</td>\n",
       "      <td>10.394443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>89.800000</td>\n",
       "      <td>5.769652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>23.600000</td>\n",
       "      <td>6.058969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>4.001389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.858970</td>\n",
       "      <td>0.032930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.874620</td>\n",
       "      <td>0.034974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.899715</td>\n",
       "      <td>0.025559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.792980</td>\n",
       "      <td>0.043536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.886929</td>\n",
       "      <td>0.029915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.858275</td>\n",
       "      <td>0.033248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.849192</td>\n",
       "      <td>0.032942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.846354</td>\n",
       "      <td>0.033116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.699159</td>\n",
       "      <td>0.065406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.831080</td>\n",
       "      <td>0.032727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.846354</td>\n",
       "      <td>0.033116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.720357     0.026407\n",
       "1                    TP       165.400000    10.394443\n",
       "2                    TN        89.800000     5.769652\n",
       "3                    FP        23.600000     6.058969\n",
       "4                    FN        18.300000     4.001389\n",
       "5              Accuracy         0.858970     0.032930\n",
       "6             Precision         0.874620     0.034974\n",
       "7           Sensitivity         0.899715     0.025559\n",
       "8           Specificity         0.792980     0.043536\n",
       "9              F1 score         0.886929     0.029915\n",
       "10  F1 score (weighted)         0.858275     0.033248\n",
       "11     F1 score (macro)         0.849192     0.032942\n",
       "12    Balanced Accuracy         0.846354     0.033116\n",
       "13                  MCC         0.699159     0.065406\n",
       "14                  NPV         0.831080     0.032727\n",
       "15              ROC_AUC         0.846354     0.033116"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.733450</td>\n",
       "      <td>0.709249</td>\n",
       "      <td>0.685721</td>\n",
       "      <td>0.744080</td>\n",
       "      <td>0.680602</td>\n",
       "      <td>0.715920</td>\n",
       "      <td>0.704537</td>\n",
       "      <td>0.695594</td>\n",
       "      <td>0.717151</td>\n",
       "      <td>0.715255</td>\n",
       "      <td>0.710156</td>\n",
       "      <td>0.019767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>349.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>333.300000</td>\n",
       "      <td>8.654479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>175.200000</td>\n",
       "      <td>6.142746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>50.800000</td>\n",
       "      <td>6.214678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>35.700000</td>\n",
       "      <td>4.498148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.868908</td>\n",
       "      <td>0.842017</td>\n",
       "      <td>0.852101</td>\n",
       "      <td>0.867227</td>\n",
       "      <td>0.852101</td>\n",
       "      <td>0.875630</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.831933</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.865546</td>\n",
       "      <td>0.854622</td>\n",
       "      <td>0.014089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.888041</td>\n",
       "      <td>0.855643</td>\n",
       "      <td>0.870130</td>\n",
       "      <td>0.885638</td>\n",
       "      <td>0.852041</td>\n",
       "      <td>0.891473</td>\n",
       "      <td>0.861257</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.843915</td>\n",
       "      <td>0.871728</td>\n",
       "      <td>0.867701</td>\n",
       "      <td>0.016439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.911227</td>\n",
       "      <td>0.893151</td>\n",
       "      <td>0.898123</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.917582</td>\n",
       "      <td>0.915119</td>\n",
       "      <td>0.896458</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.903683</td>\n",
       "      <td>0.914835</td>\n",
       "      <td>0.903262</td>\n",
       "      <td>0.011832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.792500</td>\n",
       "      <td>0.760900</td>\n",
       "      <td>0.774800</td>\n",
       "      <td>0.809700</td>\n",
       "      <td>0.748900</td>\n",
       "      <td>0.807300</td>\n",
       "      <td>0.767500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.756200</td>\n",
       "      <td>0.787900</td>\n",
       "      <td>0.775570</td>\n",
       "      <td>0.022685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.899485</td>\n",
       "      <td>0.873995</td>\n",
       "      <td>0.883905</td>\n",
       "      <td>0.893960</td>\n",
       "      <td>0.883598</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.878505</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.872777</td>\n",
       "      <td>0.892761</td>\n",
       "      <td>0.885055</td>\n",
       "      <td>0.011880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.868169</td>\n",
       "      <td>0.840864</td>\n",
       "      <td>0.851217</td>\n",
       "      <td>0.866806</td>\n",
       "      <td>0.850069</td>\n",
       "      <td>0.874984</td>\n",
       "      <td>0.845989</td>\n",
       "      <td>0.831084</td>\n",
       "      <td>0.842114</td>\n",
       "      <td>0.864443</td>\n",
       "      <td>0.853574</td>\n",
       "      <td>0.014293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.855539</td>\n",
       "      <td>0.831141</td>\n",
       "      <td>0.840101</td>\n",
       "      <td>0.858216</td>\n",
       "      <td>0.840416</td>\n",
       "      <td>0.864716</td>\n",
       "      <td>0.836078</td>\n",
       "      <td>0.817931</td>\n",
       "      <td>0.835081</td>\n",
       "      <td>0.856291</td>\n",
       "      <td>0.843551</td>\n",
       "      <td>0.014639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.851840</td>\n",
       "      <td>0.827010</td>\n",
       "      <td>0.836449</td>\n",
       "      <td>0.856087</td>\n",
       "      <td>0.833250</td>\n",
       "      <td>0.861229</td>\n",
       "      <td>0.832001</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.829941</td>\n",
       "      <td>0.851357</td>\n",
       "      <td>0.839416</td>\n",
       "      <td>0.014878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.711657</td>\n",
       "      <td>0.663639</td>\n",
       "      <td>0.680989</td>\n",
       "      <td>0.716700</td>\n",
       "      <td>0.685119</td>\n",
       "      <td>0.730003</td>\n",
       "      <td>0.673361</td>\n",
       "      <td>0.636396</td>\n",
       "      <td>0.673418</td>\n",
       "      <td>0.714354</td>\n",
       "      <td>0.688564</td>\n",
       "      <td>0.029021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.831700</td>\n",
       "      <td>0.817800</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.835600</td>\n",
       "      <td>0.852200</td>\n",
       "      <td>0.846200</td>\n",
       "      <td>0.821600</td>\n",
       "      <td>0.785700</td>\n",
       "      <td>0.843300</td>\n",
       "      <td>0.854500</td>\n",
       "      <td>0.830760</td>\n",
       "      <td>0.020720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.851840</td>\n",
       "      <td>0.827010</td>\n",
       "      <td>0.836449</td>\n",
       "      <td>0.856087</td>\n",
       "      <td>0.833250</td>\n",
       "      <td>0.861229</td>\n",
       "      <td>0.832001</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.829941</td>\n",
       "      <td>0.851357</td>\n",
       "      <td>0.839416</td>\n",
       "      <td>0.014878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.733450    0.709249    0.685721    0.744080   \n",
       "1                    TP  349.000000  326.000000  335.000000  333.000000   \n",
       "2                    TN  168.000000  175.000000  172.000000  183.000000   \n",
       "3                    FP   44.000000   55.000000   50.000000   43.000000   \n",
       "4                    FN   34.000000   39.000000   38.000000   36.000000   \n",
       "5              Accuracy    0.868908    0.842017    0.852101    0.867227   \n",
       "6             Precision    0.888041    0.855643    0.870130    0.885638   \n",
       "7           Sensitivity    0.911227    0.893151    0.898123    0.902439   \n",
       "8           Specificity    0.792500    0.760900    0.774800    0.809700   \n",
       "9              F1 score    0.899485    0.873995    0.883905    0.893960   \n",
       "10  F1 score (weighted)    0.868169    0.840864    0.851217    0.866806   \n",
       "11     F1 score (macro)    0.855539    0.831141    0.840101    0.858216   \n",
       "12    Balanced Accuracy    0.851840    0.827010    0.836449    0.856087   \n",
       "13                  MCC    0.711657    0.663639    0.680989    0.716700   \n",
       "14                  NPV    0.831700    0.817800    0.819000    0.835600   \n",
       "15              ROC_AUC    0.851840    0.827010    0.836449    0.856087   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.680602    0.715920    0.704537    0.695594    0.717151    0.715255   \n",
       "1   334.000000  345.000000  329.000000  330.000000  319.000000  333.000000   \n",
       "2   173.000000  176.000000  175.000000  165.000000  183.000000  182.000000   \n",
       "3    58.000000   42.000000   53.000000   55.000000   59.000000   49.000000   \n",
       "4    30.000000   32.000000   38.000000   45.000000   34.000000   31.000000   \n",
       "5     0.852101    0.875630    0.847059    0.831933    0.843697    0.865546   \n",
       "6     0.852041    0.891473    0.861257    0.857143    0.843915    0.871728   \n",
       "7     0.917582    0.915119    0.896458    0.880000    0.903683    0.914835   \n",
       "8     0.748900    0.807300    0.767500    0.750000    0.756200    0.787900   \n",
       "9     0.883598    0.903141    0.878505    0.868421    0.872777    0.892761   \n",
       "10    0.850069    0.874984    0.845989    0.831084    0.842114    0.864443   \n",
       "11    0.840416    0.864716    0.836078    0.817931    0.835081    0.856291   \n",
       "12    0.833250    0.861229    0.832001    0.815000    0.829941    0.851357   \n",
       "13    0.685119    0.730003    0.673361    0.636396    0.673418    0.714354   \n",
       "14    0.852200    0.846200    0.821600    0.785700    0.843300    0.854500   \n",
       "15    0.833250    0.861229    0.832001    0.815000    0.829941    0.851357   \n",
       "\n",
       "           ave       std  \n",
       "0     0.710156  0.019767  \n",
       "1   333.300000  8.654479  \n",
       "2   175.200000  6.142746  \n",
       "3    50.800000  6.214678  \n",
       "4    35.700000  4.498148  \n",
       "5     0.854622  0.014089  \n",
       "6     0.867701  0.016439  \n",
       "7     0.903262  0.011832  \n",
       "8     0.775570  0.022685  \n",
       "9     0.885055  0.011880  \n",
       "10    0.853574  0.014293  \n",
       "11    0.843551  0.014639  \n",
       "12    0.839416  0.014878  \n",
       "13    0.688564  0.029021  \n",
       "14    0.830760  0.020720  \n",
       "15    0.839416  0.014878  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test['std'] = mat_met_xgb_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_xgb0</th>\n",
       "      <th>y_pred_xgb1</th>\n",
       "      <th>y_pred_xgb2</th>\n",
       "      <th>y_pred_xgb3</th>\n",
       "      <th>y_pred_xgb4</th>\n",
       "      <th>y_pred_xgb_ave</th>\n",
       "      <th>y_pred_xgb_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4084049</td>\n",
       "      <td>0</td>\n",
       "      <td>6.26</td>\n",
       "      <td>8.758532</td>\n",
       "      <td>8.784937</td>\n",
       "      <td>9.329721</td>\n",
       "      <td>8.791228</td>\n",
       "      <td>8.777309</td>\n",
       "      <td>8.450288</td>\n",
       "      <td>1.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL2178343</td>\n",
       "      <td>1</td>\n",
       "      <td>5.96</td>\n",
       "      <td>5.876702</td>\n",
       "      <td>6.181002</td>\n",
       "      <td>5.856022</td>\n",
       "      <td>5.723582</td>\n",
       "      <td>5.844631</td>\n",
       "      <td>5.906990</td>\n",
       "      <td>0.140792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL454672</td>\n",
       "      <td>2</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.450893</td>\n",
       "      <td>6.372425</td>\n",
       "      <td>6.311010</td>\n",
       "      <td>6.516962</td>\n",
       "      <td>6.680537</td>\n",
       "      <td>6.521971</td>\n",
       "      <td>0.170484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4299417</td>\n",
       "      <td>3</td>\n",
       "      <td>8.14</td>\n",
       "      <td>7.459725</td>\n",
       "      <td>7.450311</td>\n",
       "      <td>7.620901</td>\n",
       "      <td>7.762241</td>\n",
       "      <td>7.369499</td>\n",
       "      <td>7.633780</td>\n",
       "      <td>0.260361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3692580</td>\n",
       "      <td>4</td>\n",
       "      <td>5.24</td>\n",
       "      <td>5.672783</td>\n",
       "      <td>5.919975</td>\n",
       "      <td>5.909253</td>\n",
       "      <td>5.757923</td>\n",
       "      <td>5.891068</td>\n",
       "      <td>5.731834</td>\n",
       "      <td>0.237451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL3775269</td>\n",
       "      <td>2966</td>\n",
       "      <td>7.40</td>\n",
       "      <td>6.757438</td>\n",
       "      <td>6.755647</td>\n",
       "      <td>6.088509</td>\n",
       "      <td>6.794027</td>\n",
       "      <td>6.651505</td>\n",
       "      <td>6.741188</td>\n",
       "      <td>0.381080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL3339019</td>\n",
       "      <td>2967</td>\n",
       "      <td>8.07</td>\n",
       "      <td>8.239470</td>\n",
       "      <td>7.852266</td>\n",
       "      <td>8.144821</td>\n",
       "      <td>8.125732</td>\n",
       "      <td>8.141168</td>\n",
       "      <td>8.095576</td>\n",
       "      <td>0.119703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL3771312</td>\n",
       "      <td>2968</td>\n",
       "      <td>6.52</td>\n",
       "      <td>6.254598</td>\n",
       "      <td>6.044411</td>\n",
       "      <td>6.240855</td>\n",
       "      <td>6.159820</td>\n",
       "      <td>6.196447</td>\n",
       "      <td>6.236022</td>\n",
       "      <td>0.144395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL3589701</td>\n",
       "      <td>2969</td>\n",
       "      <td>6.49</td>\n",
       "      <td>7.020974</td>\n",
       "      <td>7.020853</td>\n",
       "      <td>6.991595</td>\n",
       "      <td>7.008939</td>\n",
       "      <td>6.980704</td>\n",
       "      <td>6.918844</td>\n",
       "      <td>0.192345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL4092997</td>\n",
       "      <td>2970</td>\n",
       "      <td>7.47</td>\n",
       "      <td>7.180530</td>\n",
       "      <td>7.156816</td>\n",
       "      <td>7.018005</td>\n",
       "      <td>7.200287</td>\n",
       "      <td>7.073430</td>\n",
       "      <td>7.183178</td>\n",
       "      <td>0.142938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_xgb0  y_pred_xgb1  \\\n",
       "0         CHEMBL4084049            0     6.26     8.758532     8.784937   \n",
       "1         CHEMBL2178343            1     5.96     5.876702     6.181002   \n",
       "2          CHEMBL454672            2     6.80     6.450893     6.372425   \n",
       "3         CHEMBL4299417            3     8.14     7.459725     7.450311   \n",
       "4         CHEMBL3692580            4     5.24     5.672783     5.919975   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "2966      CHEMBL3775269         2966     7.40     6.757438     6.755647   \n",
       "2967      CHEMBL3339019         2967     8.07     8.239470     7.852266   \n",
       "2968      CHEMBL3771312         2968     6.52     6.254598     6.044411   \n",
       "2969      CHEMBL3589701         2969     6.49     7.020974     7.020853   \n",
       "2970      CHEMBL4092997         2970     7.47     7.180530     7.156816   \n",
       "\n",
       "      y_pred_xgb2  y_pred_xgb3  y_pred_xgb4  y_pred_xgb_ave  y_pred_xgb_std  \n",
       "0        9.329721     8.791228     8.777309        8.450288        1.000079  \n",
       "1        5.856022     5.723582     5.844631        5.906990        0.140792  \n",
       "2        6.311010     6.516962     6.680537        6.521971        0.170484  \n",
       "3        7.620901     7.762241     7.369499        7.633780        0.260361  \n",
       "4        5.909253     5.757923     5.891068        5.731834        0.237451  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "2966     6.088509     6.794027     6.651505        6.741188        0.381080  \n",
       "2967     8.144821     8.125732     8.141168        8.095576        0.119703  \n",
       "2968     6.240855     6.159820     6.196447        6.236022        0.144395  \n",
       "2969     6.991595     7.008939     6.980704        6.918844        0.192345  \n",
       "2970     7.018005     7.200287     7.073430        7.183178        0.142938  \n",
       "\n",
       "[2971 rows x 10 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"rmse\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_xgb_cat = np.where((y_pred_optimized_xgb >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_xgb_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_xgb))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "xgb_run0 = data_xgb[['y_test_idx0', 'y_test0', 'y_pred_xgb0']]\n",
    "xgb_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "xgb_run0.reset_index(inplace=True, drop=True)\n",
    "xgb_run1 = data_xgb[['y_test_idx1', 'y_test1', 'y_pred_xgb1']]\n",
    "xgb_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "xgb_run1.reset_index(inplace=True, drop=True)\n",
    "xgb_run2 = data_xgb[['y_test_idx2', 'y_test2', 'y_pred_xgb2']]\n",
    "xgb_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "xgb_run2.reset_index(inplace=True, drop=True)\n",
    "xgb_run3 = data_xgb[['y_test_idx3', 'y_test3', 'y_pred_xgb3']]\n",
    "xgb_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "xgb_run3.reset_index(inplace=True, drop=True)\n",
    "xgb_run4 = data_xgb[['y_test_idx4', 'y_test4', 'y_pred_xgb4']]\n",
    "xgb_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "xgb_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "xgb_5preds = pd.concat([chembl_id, xgb_run0, xgb_run1, xgb_run2, xgb_run3, xgb_run4], axis=1)\n",
    "xgb_5preds = xgb_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_xgb0', 'y_pred_xgb1', 'y_pred_xgb2', 'y_pred_xgb3', 'y_pred_xgb4']]\n",
    "xgb_5preds['y_pred_xgb_ave'] = xgb_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "xgb_5preds['y_pred_xgb_std'] = xgb_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "xgb_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "02aaad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b19aca7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt  \n",
    "# retrieve performance metrics\n",
    "results = optimized_xgb_0.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "    \n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "pyplot.show()\n",
    "\n",
    " # plot classification error\n",
    "#fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "#ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "#ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "#ax.legend()\n",
    "    \n",
    "#pyplot.ylabel('Classification Error')\n",
    "#pyplot.title('XGBoost Classification Error')\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost baseline model r2_score 0.6498 with a standard deviation of 0.0344\n",
      "XGBoost optimized model r2_score 0.7140 with a standard deviation of 0.0265\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_reg, X, Y, cv=10, scoring=\"r2\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_xgb.joblib']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_reg, \"OUTPUT/xgb_reg.joblib\")\n",
    "#joblib.dump(optimized_xgb, \"OUTPUT/optimized_xgb.joblib\")\n",
    "joblib.dump(optimizedCV_xgb, \"OUTPUT/optimizedCV_xgb.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.637460     0.029924\n",
      "1                    TP       163.800000     9.307106\n",
      "2                    TN        81.600000     4.599517\n",
      "3                    FP        31.800000     4.685676\n",
      "4                    FN        19.900000     4.748099\n",
      "5              Accuracy         0.825993     0.022815\n",
      "6             Precision         0.837056     0.026363\n",
      "7           Sensitivity         0.891376     0.026515\n",
      "8           Specificity         0.720460     0.029060\n",
      "9              F1 score         0.863135     0.021845\n",
      "10  F1 score (weighted)         0.823946     0.022766\n",
      "11     F1 score (macro)         0.811518     0.021414\n",
      "12    Balanced Accuracy         0.805920     0.020142\n",
      "13                  MCC         0.626924     0.043524\n",
      "14                  NPV         0.805480     0.038829\n",
      "15              ROC_AUC         0.805920     0.020142\n",
      "CPU times: user 3.01 s, sys: 7.56 s, total: 10.6 s\n",
      "Wall time: 374 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    \n",
    "    knn_reg.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:01:36,554] A new study created in memory with name: KNNregressor\n",
      "[I 2023-12-20 14:01:36,780] Trial 0 finished with value: 0.5823101921648949 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 80}. Best is trial 0 with value: 0.5823101921648949.\n",
      "[I 2023-12-20 14:01:37,262] Trial 1 finished with value: 0.5066941880927178 and parameters: {'n_neighbors': 30, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 0 with value: 0.5823101921648949.\n",
      "[I 2023-12-20 14:01:37,461] Trial 2 finished with value: 0.5262759924393624 and parameters: {'n_neighbors': 24, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 55}. Best is trial 0 with value: 0.5823101921648949.\n",
      "[I 2023-12-20 14:01:37,662] Trial 3 finished with value: 0.5505914056917073 and parameters: {'n_neighbors': 16, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 51}. Best is trial 0 with value: 0.5823101921648949.\n",
      "[I 2023-12-20 14:01:38,080] Trial 4 finished with value: 0.6165036484011546 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 4 with value: 0.6165036484011546.\n",
      "[I 2023-12-20 14:01:38,284] Trial 5 finished with value: 0.5184426697514382 and parameters: {'n_neighbors': 26, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 87}. Best is trial 4 with value: 0.6165036484011546.\n",
      "[I 2023-12-20 14:01:38,491] Trial 6 finished with value: 0.5890656729364891 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 75}. Best is trial 4 with value: 0.6165036484011546.\n",
      "[I 2023-12-20 14:01:38,995] Trial 7 finished with value: 0.5218591565168735 and parameters: {'n_neighbors': 25, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 4 with value: 0.6165036484011546.\n",
      "[I 2023-12-20 14:01:39,199] Trial 8 finished with value: 0.5565983865478085 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 48}. Best is trial 4 with value: 0.6165036484011546.\n",
      "[I 2023-12-20 14:01:39,704] Trial 9 finished with value: 0.6139987883998423 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 4 with value: 0.6165036484011546.\n",
      "[I 2023-12-20 14:01:40,215] Trial 10 finished with value: 0.6207183340703503 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 10 with value: 0.6207183340703503.\n",
      "[I 2023-12-20 14:01:40,726] Trial 11 finished with value: 0.6207183340703503 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 10 with value: 0.6207183340703503.\n",
      "[I 2023-12-20 14:01:41,236] Trial 12 finished with value: 0.6261758476377135 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:41,737] Trial 13 finished with value: 0.6261758476377135 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:42,217] Trial 14 finished with value: 0.6261758476377135 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:42,731] Trial 15 finished with value: 0.5912158836553285 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:43,241] Trial 16 finished with value: 0.6261758476377135 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:43,753] Trial 17 finished with value: 0.6240477251928491 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:43,963] Trial 18 finished with value: 0.5869983298440341 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 91}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:44,178] Trial 19 finished with value: 0.5666187873409173 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:44,643] Trial 20 finished with value: 0.6042710809120568 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:45,113] Trial 21 finished with value: 0.6250385288351852 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:45,592] Trial 22 finished with value: 0.6260235409659772 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:46,091] Trial 23 finished with value: 0.6109186410453968 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:46,594] Trial 24 finished with value: 0.6260235409659772 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:47,076] Trial 25 finished with value: 0.6261758476377135 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:47,559] Trial 26 finished with value: 0.6240477251928491 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:48,050] Trial 27 finished with value: 0.6165036484011546 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:48,256] Trial 28 finished with value: 0.6162828988224558 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 81}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:48,468] Trial 29 finished with value: 0.5803938242360369 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 94}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:48,683] Trial 30 finished with value: 0.561239414545057 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 57}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:49,176] Trial 31 finished with value: 0.6261758476377135 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:49,668] Trial 32 finished with value: 0.6261758476377135 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:50,171] Trial 33 finished with value: 0.6207183340703503 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:50,606] Trial 34 finished with value: 0.6260235409659772 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:51,019] Trial 35 finished with value: 0.5687325229991665 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:51,226] Trial 36 finished with value: 0.6069823161050747 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:51,657] Trial 37 finished with value: 0.6165036484011546 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:52,159] Trial 38 finished with value: 0.5890656729364891 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:52,371] Trial 39 finished with value: 0.6167401579974842 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 47}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:52,587] Trial 40 finished with value: 0.6018247608881558 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:53,077] Trial 41 finished with value: 0.6261758476377135 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:53,567] Trial 42 finished with value: 0.6261758476377135 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:54,000] Trial 43 finished with value: 0.6250385288351852 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:54,504] Trial 44 finished with value: 0.6194099854561889 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:54,977] Trial 45 finished with value: 0.6240477251928491 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:55,490] Trial 46 finished with value: 0.6139987883998423 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:55,984] Trial 47 finished with value: 0.6207183340703503 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:56,476] Trial 48 finished with value: 0.6260235409659772 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:56,689] Trial 49 finished with value: 0.5518582926825221 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 83}. Best is trial 12 with value: 0.6261758476377135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6262\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNregressor\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.661926\n",
      "1                    TP  341.000000\n",
      "2                    TN  161.000000\n",
      "3                    FP   51.000000\n",
      "4                    FN   42.000000\n",
      "5              Accuracy    0.843697\n",
      "6             Precision    0.869898\n",
      "7           Sensitivity    0.890339\n",
      "8           Specificity    0.759400\n",
      "9              F1 score    0.880000\n",
      "10  F1 score (weighted)    0.842910\n",
      "11     F1 score (macro)    0.827952\n",
      "12    Balanced Accuracy    0.824887\n",
      "13                  MCC    0.656354\n",
      "14                  NPV    0.793100\n",
      "15              ROC_AUC    0.824887\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_knn_0)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_0_cat = np.where((y_pred_knn_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_knn_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:01:57,329] Trial 50 finished with value: 0.6222750741265637 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:57,850] Trial 51 finished with value: 0.6243489896683279 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:58,351] Trial 52 finished with value: 0.6243489896683279 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:58,853] Trial 53 finished with value: 0.6200758461806724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:59,334] Trial 54 finished with value: 0.6222750741265637 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:01:59,815] Trial 55 finished with value: 0.6205023793354134 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:00,037] Trial 56 finished with value: 0.6078324411585918 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 85}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:00,534] Trial 57 finished with value: 0.5718830029374837 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:01,033] Trial 58 finished with value: 0.5981865931513088 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:01,530] Trial 59 finished with value: 0.6222750741265637 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:02,039] Trial 60 finished with value: 0.5585227455927357 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:02,546] Trial 61 finished with value: 0.6243489896683279 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:03,023] Trial 62 finished with value: 0.6243489896683279 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:03,513] Trial 63 finished with value: 0.6205023793354134 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:03,931] Trial 64 finished with value: 0.5830966854479624 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:04,351] Trial 65 finished with value: 0.6200758461806724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:04,568] Trial 66 finished with value: 0.6142919951037531 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 87}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:04,790] Trial 67 finished with value: 0.610739279596243 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 83}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:05,213] Trial 68 finished with value: 0.6243489896683279 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:05,693] Trial 69 finished with value: 0.5912363738795985 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:06,202] Trial 70 finished with value: 0.6222750741265637 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:06,716] Trial 71 finished with value: 0.6243489896683279 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:07,197] Trial 72 finished with value: 0.6243489896683279 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:07,708] Trial 73 finished with value: 0.6222750741265637 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:08,219] Trial 74 finished with value: 0.6200758461806724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:08,730] Trial 75 finished with value: 0.6205023793354134 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:09,229] Trial 76 finished with value: 0.6243489896683279 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:09,721] Trial 77 finished with value: 0.6222750741265637 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:09,938] Trial 78 finished with value: 0.610739279596243 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 43}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:10,160] Trial 79 finished with value: 0.6078324411585918 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 65}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:10,659] Trial 80 finished with value: 0.6138555007241592 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:11,145] Trial 81 finished with value: 0.6243489896683279 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:11,619] Trial 82 finished with value: 0.6222750741265637 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:12,128] Trial 83 finished with value: 0.6222750741265637 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:12,639] Trial 84 finished with value: 0.5634254213447107 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:13,150] Trial 85 finished with value: 0.6200758461806724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:13,658] Trial 86 finished with value: 0.5913822198430104 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:14,145] Trial 87 finished with value: 0.6205023793354134 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:14,633] Trial 88 finished with value: 0.6243489896683279 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:15,121] Trial 89 finished with value: 0.6222750741265637 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:15,591] Trial 90 finished with value: 0.6243489896683279 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:16,081] Trial 91 finished with value: 0.6205023793354134 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:16,578] Trial 92 finished with value: 0.6243489896683279 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:17,011] Trial 93 finished with value: 0.6222750741265637 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:17,440] Trial 94 finished with value: 0.6200758461806724 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:17,657] Trial 95 finished with value: 0.6188753323808556 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 33}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:17,880] Trial 96 finished with value: 0.606692488739052 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:18,303] Trial 97 finished with value: 0.6205023793354134 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:18,726] Trial 98 finished with value: 0.6243489896683279 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 12 with value: 0.6261758476377135.\n",
      "[I 2023-12-20 14:02:19,160] Trial 99 finished with value: 0.6205023793354134 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 12 with value: 0.6261758476377135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6262\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.661926    0.678335\n",
      "1                    TP  341.000000  336.000000\n",
      "2                    TN  161.000000  169.000000\n",
      "3                    FP   51.000000   61.000000\n",
      "4                    FN   42.000000   29.000000\n",
      "5              Accuracy    0.843697    0.848739\n",
      "6             Precision    0.869898    0.846348\n",
      "7           Sensitivity    0.890339    0.920548\n",
      "8           Specificity    0.759400    0.734800\n",
      "9              F1 score    0.880000    0.881890\n",
      "10  F1 score (weighted)    0.842910    0.846261\n",
      "11     F1 score (macro)    0.827952    0.835805\n",
      "12    Balanced Accuracy    0.824887    0.827665\n",
      "13                  MCC    0.656354    0.677240\n",
      "14                  NPV    0.793100    0.853500\n",
      "15              ROC_AUC    0.824887    0.827665\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_knn_1)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_1_cat = np.where((y_pred_knn_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_knn_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:02:19,708] Trial 100 finished with value: 0.6363010333218743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:20,199] Trial 101 finished with value: 0.6363010333218743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:20,723] Trial 102 finished with value: 0.6363010333218743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:21,238] Trial 103 finished with value: 0.6363010333218743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:21,763] Trial 104 finished with value: 0.6363010333218743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:22,279] Trial 105 finished with value: 0.6316310908325993 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:22,803] Trial 106 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:23,308] Trial 107 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:23,814] Trial 108 finished with value: 0.6305542009636135 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:24,339] Trial 109 finished with value: 0.6277890046387007 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:24,855] Trial 110 finished with value: 0.6243464927033011 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:25,371] Trial 111 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:25,877] Trial 112 finished with value: 0.6277890046387007 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:26,392] Trial 113 finished with value: 0.6277890046387007 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:26,907] Trial 114 finished with value: 0.6305542009636135 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:27,412] Trial 115 finished with value: 0.6305542009636135 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:27,918] Trial 116 finished with value: 0.6305542009636135 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:28,404] Trial 117 finished with value: 0.6305542009636135 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:28,840] Trial 118 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:29,263] Trial 119 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:29,684] Trial 120 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:30,127] Trial 121 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:30,547] Trial 122 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:30,957] Trial 123 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:31,391] Trial 124 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:31,807] Trial 125 finished with value: 0.6243464927033011 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:32,247] Trial 126 finished with value: 0.6316310908325993 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:32,759] Trial 127 finished with value: 0.6020703277853252 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:32,981] Trial 128 finished with value: 0.6005569643713284 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 53}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:33,212] Trial 129 finished with value: 0.6169132530459056 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 56}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:33,733] Trial 130 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:34,246] Trial 131 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:34,758] Trial 132 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:35,272] Trial 133 finished with value: 0.6316310908325993 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:35,753] Trial 134 finished with value: 0.6243464927033011 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:36,260] Trial 135 finished with value: 0.6316310908325993 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:36,778] Trial 136 finished with value: 0.6176994954601341 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:37,280] Trial 137 finished with value: 0.6305542009636135 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:37,791] Trial 138 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:38,293] Trial 139 finished with value: 0.6277890046387007 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:38,810] Trial 140 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:39,326] Trial 141 finished with value: 0.6316310908325993 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:39,841] Trial 142 finished with value: 0.6321264883601898 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:40,368] Trial 143 finished with value: 0.5949801792587591 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:40,883] Trial 144 finished with value: 0.6305542009636135 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:41,410] Trial 145 finished with value: 0.6336048127383348 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:41,920] Trial 146 finished with value: 0.6336048127383348 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:42,434] Trial 147 finished with value: 0.6336048127383348 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:42,902] Trial 148 finished with value: 0.6336048127383348 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:43,369] Trial 149 finished with value: 0.6336048127383348 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 100 with value: 0.6363010333218743.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6363\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 46\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.661926    0.678335    0.647956\n",
      "1                    TP  341.000000  336.000000  332.000000\n",
      "2                    TN  161.000000  169.000000  170.000000\n",
      "3                    FP   51.000000   61.000000   52.000000\n",
      "4                    FN   42.000000   29.000000   41.000000\n",
      "5              Accuracy    0.843697    0.848739    0.843697\n",
      "6             Precision    0.869898    0.846348    0.864583\n",
      "7           Sensitivity    0.890339    0.920548    0.890080\n",
      "8           Specificity    0.759400    0.734800    0.765800\n",
      "9              F1 score    0.880000    0.881890    0.877147\n",
      "10  F1 score (weighted)    0.842910    0.846261    0.842848\n",
      "11     F1 score (macro)    0.827952    0.835805    0.831183\n",
      "12    Balanced Accuracy    0.824887    0.827665    0.827923\n",
      "13                  MCC    0.656354    0.677240    0.663019\n",
      "14                  NPV    0.793100    0.853500    0.805700\n",
      "15              ROC_AUC    0.824887    0.827665    0.827923\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_knn_2)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_2_cat = np.where((y_pred_knn_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_knn_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:02:43,907] Trial 150 finished with value: 0.6235227263789899 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:44,345] Trial 151 finished with value: 0.6235227263789899 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:44,756] Trial 152 finished with value: 0.6235227263789899 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:45,170] Trial 153 finished with value: 0.6242172085556572 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:45,588] Trial 154 finished with value: 0.6046412833122836 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:45,997] Trial 155 finished with value: 0.6242172085556572 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:46,406] Trial 156 finished with value: 0.6191711455847277 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:46,630] Trial 157 finished with value: 0.6141958863848113 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 51}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:47,056] Trial 158 finished with value: 0.6235227263789899 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:47,278] Trial 159 finished with value: 0.6081370732306719 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 47}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:47,703] Trial 160 finished with value: 0.6242172085556572 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:48,112] Trial 161 finished with value: 0.6191711455847277 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:48,521] Trial 162 finished with value: 0.6242172085556572 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:48,944] Trial 163 finished with value: 0.6191711455847277 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:49,411] Trial 164 finished with value: 0.6235227263789899 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:49,876] Trial 165 finished with value: 0.6242172085556572 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:50,354] Trial 166 finished with value: 0.6235227263789899 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:50,865] Trial 167 finished with value: 0.6191711455847277 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:51,366] Trial 168 finished with value: 0.6242172085556572 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:51,878] Trial 169 finished with value: 0.6235227263789899 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:52,383] Trial 170 finished with value: 0.6235227263789899 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:52,899] Trial 171 finished with value: 0.6139796948370663 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:53,411] Trial 172 finished with value: 0.6191711455847277 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:53,905] Trial 173 finished with value: 0.6161187163141842 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:54,416] Trial 174 finished with value: 0.6242172085556572 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:54,942] Trial 175 finished with value: 0.6161187163141842 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:55,468] Trial 176 finished with value: 0.6139796948370663 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:55,994] Trial 177 finished with value: 0.6191711455847277 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:56,429] Trial 178 finished with value: 0.6046412833122836 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:56,863] Trial 179 finished with value: 0.5895372736182717 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:57,293] Trial 180 finished with value: 0.6161187163141842 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:57,724] Trial 181 finished with value: 0.6161187163141842 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:58,238] Trial 182 finished with value: 0.6161187163141842 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:58,753] Trial 183 finished with value: 0.6191711455847277 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:59,258] Trial 184 finished with value: 0.6139796948370663 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:02:59,778] Trial 185 finished with value: 0.6191711455847277 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:03:00,288] Trial 186 finished with value: 0.6242172085556572 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:03:00,775] Trial 187 finished with value: 0.6139796948370663 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:03:00,993] Trial 188 finished with value: 0.6039597295739796 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 50}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:03:01,217] Trial 189 finished with value: 0.6141958863848113 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 53}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:03:01,714] Trial 190 finished with value: 0.6112796977797899 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:03:02,230] Trial 191 finished with value: 0.6161187163141842 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:03:02,736] Trial 192 finished with value: 0.6161187163141842 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:03:03,232] Trial 193 finished with value: 0.6242172085556572 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:03:03,738] Trial 194 finished with value: 0.6191711455847277 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:03:04,265] Trial 195 finished with value: 0.6161187163141842 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:03:04,781] Trial 196 finished with value: 0.6191711455847277 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:03:05,308] Trial 197 finished with value: 0.6139796948370663 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:03:05,825] Trial 198 finished with value: 0.6235227263789899 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 100 with value: 0.6363010333218743.\n",
      "[I 2023-12-20 14:03:06,312] Trial 199 finished with value: 0.594962668098989 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 100 with value: 0.6363010333218743.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6363\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 46\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.661926    0.678335    0.647956    0.697543\n",
      "1                    TP  341.000000  336.000000  332.000000  334.000000\n",
      "2                    TN  161.000000  169.000000  170.000000  169.000000\n",
      "3                    FP   51.000000   61.000000   52.000000   57.000000\n",
      "4                    FN   42.000000   29.000000   41.000000   35.000000\n",
      "5              Accuracy    0.843697    0.848739    0.843697    0.845378\n",
      "6             Precision    0.869898    0.846348    0.864583    0.854220\n",
      "7           Sensitivity    0.890339    0.920548    0.890080    0.905149\n",
      "8           Specificity    0.759400    0.734800    0.765800    0.747800\n",
      "9              F1 score    0.880000    0.881890    0.877147    0.878947\n",
      "10  F1 score (weighted)    0.842910    0.846261    0.842848    0.843661\n",
      "11     F1 score (macro)    0.827952    0.835805    0.831183    0.832497\n",
      "12    Balanced Accuracy    0.824887    0.827665    0.827923    0.826468\n",
      "13                  MCC    0.656354    0.677240    0.663019    0.667629\n",
      "14                  NPV    0.793100    0.853500    0.805700    0.828400\n",
      "15              ROC_AUC    0.824887    0.827665    0.827923    0.826468\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_knn_3)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_3_cat = np.where((y_pred_knn_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_knn_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:03:06,929] Trial 200 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:07,434] Trial 201 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:07,929] Trial 202 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:08,444] Trial 203 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:08,949] Trial 204 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:09,434] Trial 205 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:09,948] Trial 206 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:10,438] Trial 207 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:10,880] Trial 208 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:11,310] Trial 209 finished with value: 0.6278852795645643 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:11,814] Trial 210 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:12,318] Trial 211 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:12,816] Trial 212 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:13,258] Trial 213 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:13,723] Trial 214 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:14,188] Trial 215 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:14,689] Trial 216 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:15,204] Trial 217 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:15,719] Trial 218 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:16,235] Trial 219 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:16,751] Trial 220 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:17,265] Trial 221 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:17,781] Trial 222 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:18,297] Trial 223 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:18,813] Trial 224 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:19,330] Trial 225 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:19,837] Trial 226 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:20,352] Trial 227 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:20,858] Trial 228 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:21,353] Trial 229 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:21,570] Trial 230 finished with value: 0.6408688806638123 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:22,049] Trial 231 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:22,545] Trial 232 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:23,032] Trial 233 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:23,536] Trial 234 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:23,995] Trial 235 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:24,446] Trial 236 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:24,869] Trial 237 finished with value: 0.6477337802012979 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:25,079] Trial 238 finished with value: 0.6408688806638123 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:25,498] Trial 239 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:25,900] Trial 240 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:26,301] Trial 241 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:26,785] Trial 242 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:27,270] Trial 243 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:27,775] Trial 244 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:28,279] Trial 245 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:28,783] Trial 246 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:29,276] Trial 247 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:29,784] Trial 248 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:30,288] Trial 249 finished with value: 0.6479457806927199 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 200 with value: 0.6479457806927199.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6479\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 34\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.661926    0.678335    0.647956    0.697543   \n",
      "1                    TP  341.000000  336.000000  332.000000  334.000000   \n",
      "2                    TN  161.000000  169.000000  170.000000  169.000000   \n",
      "3                    FP   51.000000   61.000000   52.000000   57.000000   \n",
      "4                    FN   42.000000   29.000000   41.000000   35.000000   \n",
      "5              Accuracy    0.843697    0.848739    0.843697    0.845378   \n",
      "6             Precision    0.869898    0.846348    0.864583    0.854220   \n",
      "7           Sensitivity    0.890339    0.920548    0.890080    0.905149   \n",
      "8           Specificity    0.759400    0.734800    0.765800    0.747800   \n",
      "9              F1 score    0.880000    0.881890    0.877147    0.878947   \n",
      "10  F1 score (weighted)    0.842910    0.846261    0.842848    0.843661   \n",
      "11     F1 score (macro)    0.827952    0.835805    0.831183    0.832497   \n",
      "12    Balanced Accuracy    0.824887    0.827665    0.827923    0.826468   \n",
      "13                  MCC    0.656354    0.677240    0.663019    0.667629   \n",
      "14                  NPV    0.793100    0.853500    0.805700    0.828400   \n",
      "15              ROC_AUC    0.824887    0.827665    0.827923    0.826468   \n",
      "\n",
      "          Set4  \n",
      "0     0.603395  \n",
      "1   332.000000  \n",
      "2   152.000000  \n",
      "3    79.000000  \n",
      "4    32.000000  \n",
      "5     0.813445  \n",
      "6     0.807786  \n",
      "7     0.912088  \n",
      "8     0.658000  \n",
      "9     0.856774  \n",
      "10    0.808538  \n",
      "11    0.794652  \n",
      "12    0.785048  \n",
      "13    0.601140  \n",
      "14    0.826100  \n",
      "15    0.785048  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_knn_4)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_4_cat = np.where((y_pred_knn_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_knn_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:03:30,889] Trial 250 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:31,374] Trial 251 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:31,868] Trial 252 finished with value: 0.6157069412110296 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:32,363] Trial 253 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:32,860] Trial 254 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:33,354] Trial 255 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:33,852] Trial 256 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:34,338] Trial 257 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:34,823] Trial 258 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:35,319] Trial 259 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:35,803] Trial 260 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:36,288] Trial 261 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:36,783] Trial 262 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:36,979] Trial 263 finished with value: 0.6256357665070302 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:37,197] Trial 264 finished with value: 0.5581921624191982 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:37,711] Trial 265 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:38,246] Trial 266 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:38,729] Trial 267 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:39,216] Trial 268 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:39,711] Trial 269 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:40,207] Trial 270 finished with value: 0.6282884961638311 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:40,702] Trial 271 finished with value: 0.6157069412110296 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:41,197] Trial 272 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:41,703] Trial 273 finished with value: 0.6282884961638311 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:42,209] Trial 274 finished with value: 0.6282884961638311 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:42,715] Trial 275 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:43,199] Trial 276 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:43,705] Trial 277 finished with value: 0.632327986023816 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:44,211] Trial 278 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:44,717] Trial 279 finished with value: 0.6282884961638311 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:45,223] Trial 280 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:45,709] Trial 281 finished with value: 0.632327986023816 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:46,215] Trial 282 finished with value: 0.6282884961638311 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:46,720] Trial 283 finished with value: 0.632327986023816 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:47,223] Trial 284 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:47,729] Trial 285 finished with value: 0.6282884961638311 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:47,934] Trial 286 finished with value: 0.6256357665070302 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:48,144] Trial 287 finished with value: 0.6268291279699667 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:48,646] Trial 288 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:49,131] Trial 289 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:49,616] Trial 290 finished with value: 0.632327986023816 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:50,113] Trial 291 finished with value: 0.6056388880301206 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:50,600] Trial 292 finished with value: 0.632327986023816 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:51,062] Trial 293 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:51,598] Trial 294 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:52,126] Trial 295 finished with value: 0.632327986023816 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:52,527] Trial 296 finished with value: 0.6282884961638311 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:52,983] Trial 297 finished with value: 0.6282884961638311 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:53,468] Trial 298 finished with value: 0.6332199095096005 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:53,952] Trial 299 finished with value: 0.632327986023816 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6479\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 34\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.661926    0.678335    0.647956    0.697543   \n",
      "1                    TP  341.000000  336.000000  332.000000  334.000000   \n",
      "2                    TN  161.000000  169.000000  170.000000  169.000000   \n",
      "3                    FP   51.000000   61.000000   52.000000   57.000000   \n",
      "4                    FN   42.000000   29.000000   41.000000   35.000000   \n",
      "5              Accuracy    0.843697    0.848739    0.843697    0.845378   \n",
      "6             Precision    0.869898    0.846348    0.864583    0.854220   \n",
      "7           Sensitivity    0.890339    0.920548    0.890080    0.905149   \n",
      "8           Specificity    0.759400    0.734800    0.765800    0.747800   \n",
      "9              F1 score    0.880000    0.881890    0.877147    0.878947   \n",
      "10  F1 score (weighted)    0.842910    0.846261    0.842848    0.843661   \n",
      "11     F1 score (macro)    0.827952    0.835805    0.831183    0.832497   \n",
      "12    Balanced Accuracy    0.824887    0.827665    0.827923    0.826468   \n",
      "13                  MCC    0.656354    0.677240    0.663019    0.667629   \n",
      "14                  NPV    0.793100    0.853500    0.805700    0.828400   \n",
      "15              ROC_AUC    0.824887    0.827665    0.827923    0.826468   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.603395    0.669008  \n",
      "1   332.000000  342.000000  \n",
      "2   152.000000  170.000000  \n",
      "3    79.000000   48.000000  \n",
      "4    32.000000   35.000000  \n",
      "5     0.813445    0.860504  \n",
      "6     0.807786    0.876923  \n",
      "7     0.912088    0.907162  \n",
      "8     0.658000    0.779800  \n",
      "9     0.856774    0.891786  \n",
      "10    0.808538    0.859543  \n",
      "11    0.794652    0.847784  \n",
      "12    0.785048    0.843489  \n",
      "13    0.601140    0.696519  \n",
      "14    0.826100    0.829300  \n",
      "15    0.785048    0.843489  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_knn_5)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_5_cat = np.where((y_pred_knn_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_knn_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:03:54,591] Trial 300 finished with value: 0.6290989875609789 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:55,100] Trial 301 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:55,609] Trial 302 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:56,120] Trial 303 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:56,619] Trial 304 finished with value: 0.5903187751514922 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:57,121] Trial 305 finished with value: 0.6290989875609789 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:57,632] Trial 306 finished with value: 0.6375549141996121 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:58,105] Trial 307 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:58,321] Trial 308 finished with value: 0.6237896503260656 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 37}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:58,837] Trial 309 finished with value: 0.621782913491298 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:59,057] Trial 310 finished with value: 0.6268492138138303 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:03:59,536] Trial 311 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:00,016] Trial 312 finished with value: 0.6290989875609789 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:00,503] Trial 313 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:00,978] Trial 314 finished with value: 0.6375549141996121 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:01,484] Trial 315 finished with value: 0.6290989875609789 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:01,992] Trial 316 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:02,501] Trial 317 finished with value: 0.6375549141996121 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:03,001] Trial 318 finished with value: 0.6290989875609789 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:03,508] Trial 319 finished with value: 0.6375549141996121 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:04,008] Trial 320 finished with value: 0.6098055979281403 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:04,495] Trial 321 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:05,004] Trial 322 finished with value: 0.6290989875609789 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:05,411] Trial 323 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:05,821] Trial 324 finished with value: 0.6290989875609789 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:06,244] Trial 325 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:06,661] Trial 326 finished with value: 0.6375549141996121 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:07,083] Trial 327 finished with value: 0.6290989875609789 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:07,494] Trial 328 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:07,908] Trial 329 finished with value: 0.6190726528444097 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:08,130] Trial 330 finished with value: 0.6237896503260656 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:08,632] Trial 331 finished with value: 0.6153420328001074 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:08,865] Trial 332 finished with value: 0.6268492138138303 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:09,381] Trial 333 finished with value: 0.6375549141996121 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:09,895] Trial 334 finished with value: 0.6290989875609789 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:10,397] Trial 335 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:10,918] Trial 336 finished with value: 0.6375549141996121 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:11,438] Trial 337 finished with value: 0.6290989875609789 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:11,950] Trial 338 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:12,462] Trial 339 finished with value: 0.6375549141996121 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:12,973] Trial 340 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:13,495] Trial 341 finished with value: 0.6290989875609789 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:14,005] Trial 342 finished with value: 0.6290989875609789 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:14,525] Trial 343 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:15,037] Trial 344 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:15,559] Trial 345 finished with value: 0.6332637323209335 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:16,070] Trial 346 finished with value: 0.6375549141996121 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:16,582] Trial 347 finished with value: 0.6290989875609789 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:17,103] Trial 348 finished with value: 0.6190726528444097 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 200 with value: 0.6479457806927199.\n",
      "[I 2023-12-20 14:04:17,625] Trial 349 finished with value: 0.6375549141996121 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 200 with value: 0.6479457806927199.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6479\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 34\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.661926    0.678335    0.647956    0.697543   \n",
      "1                    TP  341.000000  336.000000  332.000000  334.000000   \n",
      "2                    TN  161.000000  169.000000  170.000000  169.000000   \n",
      "3                    FP   51.000000   61.000000   52.000000   57.000000   \n",
      "4                    FN   42.000000   29.000000   41.000000   35.000000   \n",
      "5              Accuracy    0.843697    0.848739    0.843697    0.845378   \n",
      "6             Precision    0.869898    0.846348    0.864583    0.854220   \n",
      "7           Sensitivity    0.890339    0.920548    0.890080    0.905149   \n",
      "8           Specificity    0.759400    0.734800    0.765800    0.747800   \n",
      "9              F1 score    0.880000    0.881890    0.877147    0.878947   \n",
      "10  F1 score (weighted)    0.842910    0.846261    0.842848    0.843661   \n",
      "11     F1 score (macro)    0.827952    0.835805    0.831183    0.832497   \n",
      "12    Balanced Accuracy    0.824887    0.827665    0.827923    0.826468   \n",
      "13                  MCC    0.656354    0.677240    0.663019    0.667629   \n",
      "14                  NPV    0.793100    0.853500    0.805700    0.828400   \n",
      "15              ROC_AUC    0.824887    0.827665    0.827923    0.826468   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.603395    0.669008    0.663318  \n",
      "1   332.000000  342.000000  334.000000  \n",
      "2   152.000000  170.000000  167.000000  \n",
      "3    79.000000   48.000000   61.000000  \n",
      "4    32.000000   35.000000   33.000000  \n",
      "5     0.813445    0.860504    0.842017  \n",
      "6     0.807786    0.876923    0.845570  \n",
      "7     0.912088    0.907162    0.910082  \n",
      "8     0.658000    0.779800    0.732500  \n",
      "9     0.856774    0.891786    0.876640  \n",
      "10    0.808538    0.859543    0.839752  \n",
      "11    0.794652    0.847784    0.828507  \n",
      "12    0.785048    0.843489    0.821269  \n",
      "13    0.601140    0.696519    0.661280  \n",
      "14    0.826100    0.829300    0.835000  \n",
      "15    0.785048    0.843489    0.821269  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_knn_6)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_6_cat = np.where((y_pred_knn_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_knn_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:04:18,301] Trial 350 finished with value: 0.6487213913834766 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 350 with value: 0.6487213913834766.\n",
      "[I 2023-12-20 14:04:18,776] Trial 351 finished with value: 0.5902543768934779 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 350 with value: 0.6487213913834766.\n",
      "[I 2023-12-20 14:04:19,206] Trial 352 finished with value: 0.6489391696349673 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:19,430] Trial 353 finished with value: 0.6332654719015018 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:19,894] Trial 354 finished with value: 0.6489391696349673 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:20,118] Trial 355 finished with value: 0.6332654719015018 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:20,607] Trial 356 finished with value: 0.6489391696349673 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:21,092] Trial 357 finished with value: 0.6489391696349673 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:21,562] Trial 358 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:22,041] Trial 359 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:22,562] Trial 360 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:23,072] Trial 361 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:23,602] Trial 362 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:24,122] Trial 363 finished with value: 0.6454909002222166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:24,651] Trial 364 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:25,182] Trial 365 finished with value: 0.6454909002222166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:25,714] Trial 366 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:26,243] Trial 367 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:26,764] Trial 368 finished with value: 0.6454909002222166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:27,294] Trial 369 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:27,820] Trial 370 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:28,352] Trial 371 finished with value: 0.6454909002222166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:28,879] Trial 372 finished with value: 0.618775400716262 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:29,411] Trial 373 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:29,941] Trial 374 finished with value: 0.6454909002222166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:30,471] Trial 375 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:31,003] Trial 376 finished with value: 0.6454909002222166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:31,535] Trial 377 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:32,051] Trial 378 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:32,469] Trial 379 finished with value: 0.6454909002222166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:32,885] Trial 380 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:33,109] Trial 381 finished with value: 0.6332654719015018 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 61}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:33,342] Trial 382 finished with value: 0.6279540401680139 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 62}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:33,789] Trial 383 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:34,202] Trial 384 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:34,626] Trial 385 finished with value: 0.6454909002222166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:35,091] Trial 386 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:35,607] Trial 387 finished with value: 0.6454909002222166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:36,115] Trial 388 finished with value: 0.5793280368824149 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:36,630] Trial 389 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:37,149] Trial 390 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:37,646] Trial 391 finished with value: 0.6454909002222166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:38,135] Trial 392 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:38,615] Trial 393 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:39,123] Trial 394 finished with value: 0.6454909002222166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:39,639] Trial 395 finished with value: 0.618775400716262 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:40,156] Trial 396 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:40,677] Trial 397 finished with value: 0.6454909002222166 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:41,171] Trial 398 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:41,686] Trial 399 finished with value: 0.6486941350012454 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 352 with value: 0.6489391696349673.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6489\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 24\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.661926    0.678335    0.647956    0.697543   \n",
      "1                    TP  341.000000  336.000000  332.000000  334.000000   \n",
      "2                    TN  161.000000  169.000000  170.000000  169.000000   \n",
      "3                    FP   51.000000   61.000000   52.000000   57.000000   \n",
      "4                    FN   42.000000   29.000000   41.000000   35.000000   \n",
      "5              Accuracy    0.843697    0.848739    0.843697    0.845378   \n",
      "6             Precision    0.869898    0.846348    0.864583    0.854220   \n",
      "7           Sensitivity    0.890339    0.920548    0.890080    0.905149   \n",
      "8           Specificity    0.759400    0.734800    0.765800    0.747800   \n",
      "9              F1 score    0.880000    0.881890    0.877147    0.878947   \n",
      "10  F1 score (weighted)    0.842910    0.846261    0.842848    0.843661   \n",
      "11     F1 score (macro)    0.827952    0.835805    0.831183    0.832497   \n",
      "12    Balanced Accuracy    0.824887    0.827665    0.827923    0.826468   \n",
      "13                  MCC    0.656354    0.677240    0.663019    0.667629   \n",
      "14                  NPV    0.793100    0.853500    0.805700    0.828400   \n",
      "15              ROC_AUC    0.824887    0.827665    0.827923    0.826468   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.603395    0.669008    0.663318    0.647779  \n",
      "1   332.000000  342.000000  334.000000  339.000000  \n",
      "2   152.000000  170.000000  167.000000  153.000000  \n",
      "3    79.000000   48.000000   61.000000   67.000000  \n",
      "4    32.000000   35.000000   33.000000   36.000000  \n",
      "5     0.813445    0.860504    0.842017    0.826891  \n",
      "6     0.807786    0.876923    0.845570    0.834975  \n",
      "7     0.912088    0.907162    0.910082    0.904000  \n",
      "8     0.658000    0.779800    0.732500    0.695500  \n",
      "9     0.856774    0.891786    0.876640    0.868118  \n",
      "10    0.808538    0.859543    0.839752    0.823766  \n",
      "11    0.794652    0.847784    0.828507    0.808142  \n",
      "12    0.785048    0.843489    0.821269    0.799727  \n",
      "13    0.601140    0.696519    0.661280    0.621569  \n",
      "14    0.826100    0.829300    0.835000    0.809500  \n",
      "15    0.785048    0.843489    0.821269    0.799727  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_knn_7)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_7_cat = np.where((y_pred_knn_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_knn_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:04:42,328] Trial 400 finished with value: 0.6366106723197243 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:42,825] Trial 401 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:43,332] Trial 402 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:43,840] Trial 403 finished with value: 0.6366106723197243 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:44,349] Trial 404 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:44,568] Trial 405 finished with value: 0.6269150919339197 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 59}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:45,051] Trial 406 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:45,270] Trial 407 finished with value: 0.6322166318378558 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:45,710] Trial 408 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:46,125] Trial 409 finished with value: 0.6366106723197243 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:46,577] Trial 410 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:47,045] Trial 411 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:47,534] Trial 412 finished with value: 0.6366106723197243 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:48,045] Trial 413 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:48,547] Trial 414 finished with value: 0.6139871118774184 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:49,010] Trial 415 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:49,483] Trial 416 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:49,956] Trial 417 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:50,466] Trial 418 finished with value: 0.6366106723197243 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:50,986] Trial 419 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:51,498] Trial 420 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:52,019] Trial 421 finished with value: 0.6366106723197243 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:52,540] Trial 422 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:53,052] Trial 423 finished with value: 0.6366106723197243 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:53,576] Trial 424 finished with value: 0.6079742125152375 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:54,099] Trial 425 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:54,619] Trial 426 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:54,842] Trial 427 finished with value: 0.6322166318378558 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 64}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:55,360] Trial 428 finished with value: 0.6366106723197243 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:55,585] Trial 429 finished with value: 0.5914752815812501 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:56,107] Trial 430 finished with value: 0.6366106723197243 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:56,617] Trial 431 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:57,139] Trial 432 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:57,661] Trial 433 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:58,185] Trial 434 finished with value: 0.6139871118774184 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:58,714] Trial 435 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:59,245] Trial 436 finished with value: 0.6366106723197243 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:04:59,705] Trial 437 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:00,149] Trial 438 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:00,575] Trial 439 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:00,982] Trial 440 finished with value: 0.6366106723197243 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:01,399] Trial 441 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:01,817] Trial 442 finished with value: 0.6366106723197243 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:02,256] Trial 443 finished with value: 0.5991076908895867 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:02,754] Trial 444 finished with value: 0.6366106723197243 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:03,246] Trial 445 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:03,747] Trial 446 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:04,231] Trial 447 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:04,740] Trial 448 finished with value: 0.6393657670785843 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:05,248] Trial 449 finished with value: 0.6228179851156995 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 352 with value: 0.6489391696349673.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6489\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 24\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.661926    0.678335    0.647956    0.697543   \n",
      "1                    TP  341.000000  336.000000  332.000000  334.000000   \n",
      "2                    TN  161.000000  169.000000  170.000000  169.000000   \n",
      "3                    FP   51.000000   61.000000   52.000000   57.000000   \n",
      "4                    FN   42.000000   29.000000   41.000000   35.000000   \n",
      "5              Accuracy    0.843697    0.848739    0.843697    0.845378   \n",
      "6             Precision    0.869898    0.846348    0.864583    0.854220   \n",
      "7           Sensitivity    0.890339    0.920548    0.890080    0.905149   \n",
      "8           Specificity    0.759400    0.734800    0.765800    0.747800   \n",
      "9              F1 score    0.880000    0.881890    0.877147    0.878947   \n",
      "10  F1 score (weighted)    0.842910    0.846261    0.842848    0.843661   \n",
      "11     F1 score (macro)    0.827952    0.835805    0.831183    0.832497   \n",
      "12    Balanced Accuracy    0.824887    0.827665    0.827923    0.826468   \n",
      "13                  MCC    0.656354    0.677240    0.663019    0.667629   \n",
      "14                  NPV    0.793100    0.853500    0.805700    0.828400   \n",
      "15              ROC_AUC    0.824887    0.827665    0.827923    0.826468   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.603395    0.669008    0.663318    0.647779    0.606425  \n",
      "1   332.000000  342.000000  334.000000  339.000000  320.000000  \n",
      "2   152.000000  170.000000  167.000000  153.000000  170.000000  \n",
      "3    79.000000   48.000000   61.000000   67.000000   72.000000  \n",
      "4    32.000000   35.000000   33.000000   36.000000   33.000000  \n",
      "5     0.813445    0.860504    0.842017    0.826891    0.823529  \n",
      "6     0.807786    0.876923    0.845570    0.834975    0.816327  \n",
      "7     0.912088    0.907162    0.910082    0.904000    0.906516  \n",
      "8     0.658000    0.779800    0.732500    0.695500    0.702500  \n",
      "9     0.856774    0.891786    0.876640    0.868118    0.859060  \n",
      "10    0.808538    0.859543    0.839752    0.823766    0.820415  \n",
      "11    0.794652    0.847784    0.828507    0.808142    0.811553  \n",
      "12    0.785048    0.843489    0.821269    0.799727    0.804497  \n",
      "13    0.601140    0.696519    0.661280    0.621569    0.630983  \n",
      "14    0.826100    0.829300    0.835000    0.809500    0.837400  \n",
      "15    0.785048    0.843489    0.821269    0.799727    0.804497  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_knn_8)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_8_cat = np.where((y_pred_knn_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_knn_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:05:05,597] Trial 450 finished with value: 0.6263455227122081 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 58}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:06,109] Trial 451 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:06,628] Trial 452 finished with value: 0.6388598415223881 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:06,854] Trial 453 finished with value: 0.61239439511933 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:07,323] Trial 454 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:07,811] Trial 455 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:08,269] Trial 456 finished with value: 0.6388598415223881 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:08,738] Trial 457 finished with value: 0.584247881315799 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:09,204] Trial 458 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:09,675] Trial 459 finished with value: 0.6388598415223881 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:10,198] Trial 460 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:10,715] Trial 461 finished with value: 0.6388598415223881 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:11,226] Trial 462 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:11,693] Trial 463 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:12,213] Trial 464 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:12,684] Trial 465 finished with value: 0.6388598415223881 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:13,191] Trial 466 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:13,668] Trial 467 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:14,167] Trial 468 finished with value: 0.6388598415223881 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:14,666] Trial 469 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:15,155] Trial 470 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:15,378] Trial 471 finished with value: 0.6263455227122081 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 64}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:15,909] Trial 472 finished with value: 0.61239439511933 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:16,454] Trial 473 finished with value: 0.6388598415223881 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:16,695] Trial 474 finished with value: 0.6318145418175134 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 70}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:17,211] Trial 475 finished with value: 0.6410929673974636 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:17,677] Trial 476 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:18,223] Trial 477 finished with value: 0.6158551953565459 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:18,748] Trial 478 finished with value: 0.6410929673974636 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:19,280] Trial 479 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:19,811] Trial 480 finished with value: 0.6388598415223881 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:20,329] Trial 481 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:20,852] Trial 482 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:21,363] Trial 483 finished with value: 0.6410929673974636 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:21,877] Trial 484 finished with value: 0.6388598415223881 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:22,411] Trial 485 finished with value: 0.6025415638242274 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:22,942] Trial 486 finished with value: 0.6410929673974636 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:23,465] Trial 487 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:23,990] Trial 488 finished with value: 0.6204810995047785 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:24,493] Trial 489 finished with value: 0.6410929673974636 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:25,030] Trial 490 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:25,507] Trial 491 finished with value: 0.6388598415223881 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:25,937] Trial 492 finished with value: 0.61239439511933 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:26,366] Trial 493 finished with value: 0.6410929673974636 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:26,610] Trial 494 finished with value: 0.6263455227122081 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 60}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:27,121] Trial 495 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:27,350] Trial 496 finished with value: 0.6318145418175134 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 62}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:27,878] Trial 497 finished with value: 0.6349603202281527 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:28,411] Trial 498 finished with value: 0.6388598415223881 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 352 with value: 0.6489391696349673.\n",
      "[I 2023-12-20 14:05:28,944] Trial 499 finished with value: 0.6409343576082964 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 352 with value: 0.6489391696349673.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6489\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 24\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.661926    0.678335    0.647956    0.697543   \n",
      "1                    TP  341.000000  336.000000  332.000000  334.000000   \n",
      "2                    TN  161.000000  169.000000  170.000000  169.000000   \n",
      "3                    FP   51.000000   61.000000   52.000000   57.000000   \n",
      "4                    FN   42.000000   29.000000   41.000000   35.000000   \n",
      "5              Accuracy    0.843697    0.848739    0.843697    0.845378   \n",
      "6             Precision    0.869898    0.846348    0.864583    0.854220   \n",
      "7           Sensitivity    0.890339    0.920548    0.890080    0.905149   \n",
      "8           Specificity    0.759400    0.734800    0.765800    0.747800   \n",
      "9              F1 score    0.880000    0.881890    0.877147    0.878947   \n",
      "10  F1 score (weighted)    0.842910    0.846261    0.842848    0.843661   \n",
      "11     F1 score (macro)    0.827952    0.835805    0.831183    0.832497   \n",
      "12    Balanced Accuracy    0.824887    0.827665    0.827923    0.826468   \n",
      "13                  MCC    0.656354    0.677240    0.663019    0.667629   \n",
      "14                  NPV    0.793100    0.853500    0.805700    0.828400   \n",
      "15              ROC_AUC    0.824887    0.827665    0.827923    0.826468   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.603395    0.669008    0.663318    0.647779    0.606425    0.640998  \n",
      "1   332.000000  342.000000  334.000000  339.000000  320.000000  328.000000  \n",
      "2   152.000000  170.000000  167.000000  153.000000  170.000000  176.000000  \n",
      "3    79.000000   48.000000   61.000000   67.000000   72.000000   55.000000  \n",
      "4    32.000000   35.000000   33.000000   36.000000   33.000000   36.000000  \n",
      "5     0.813445    0.860504    0.842017    0.826891    0.823529    0.847059  \n",
      "6     0.807786    0.876923    0.845570    0.834975    0.816327    0.856397  \n",
      "7     0.912088    0.907162    0.910082    0.904000    0.906516    0.901099  \n",
      "8     0.658000    0.779800    0.732500    0.695500    0.702500    0.761900  \n",
      "9     0.856774    0.891786    0.876640    0.868118    0.859060    0.878179  \n",
      "10    0.808538    0.859543    0.839752    0.823766    0.820415    0.845724  \n",
      "11    0.794652    0.847784    0.828507    0.808142    0.811553    0.836381  \n",
      "12    0.785048    0.843489    0.821269    0.799727    0.804497    0.831502  \n",
      "13    0.601140    0.696519    0.661280    0.621569    0.630983    0.674692  \n",
      "14    0.826100    0.829300    0.835000    0.809500    0.837400    0.830200  \n",
      "15    0.785048    0.843489    0.821269    0.799727    0.804497    0.831502  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_knn_9)\n",
    "# now convert the resuls to binary with cutoff \n",
    "y_pred_knn_9_cat = np.where((y_pred_knn_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_knn_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.650994</td>\n",
       "      <td>0.032138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>165.600000</td>\n",
       "      <td>9.453982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>83.900000</td>\n",
       "      <td>4.581363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>4.904646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>4.629615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.839792</td>\n",
       "      <td>0.025938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.848356</td>\n",
       "      <td>0.028102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.901135</td>\n",
       "      <td>0.026089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.740860</td>\n",
       "      <td>0.030746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.873775</td>\n",
       "      <td>0.023872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.838021</td>\n",
       "      <td>0.026010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.826705</td>\n",
       "      <td>0.025220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.820998</td>\n",
       "      <td>0.023814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.656855</td>\n",
       "      <td>0.050856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.823800</td>\n",
       "      <td>0.040115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.820998</td>\n",
       "      <td>0.023814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.650994     0.032138\n",
       "1                    TP       165.600000     9.453982\n",
       "2                    TN        83.900000     4.581363\n",
       "3                    FP        29.500000     4.904646\n",
       "4                    FN        18.100000     4.629615\n",
       "5              Accuracy         0.839792     0.025938\n",
       "6             Precision         0.848356     0.028102\n",
       "7           Sensitivity         0.901135     0.026089\n",
       "8           Specificity         0.740860     0.030746\n",
       "9              F1 score         0.873775     0.023872\n",
       "10  F1 score (weighted)         0.838021     0.026010\n",
       "11     F1 score (macro)         0.826705     0.025220\n",
       "12    Balanced Accuracy         0.820998     0.023814\n",
       "13                  MCC         0.656855     0.050856\n",
       "14                  NPV         0.823800     0.040115\n",
       "15              ROC_AUC         0.820998     0.023814"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.661926</td>\n",
       "      <td>0.678335</td>\n",
       "      <td>0.647956</td>\n",
       "      <td>0.697543</td>\n",
       "      <td>0.603395</td>\n",
       "      <td>0.669008</td>\n",
       "      <td>0.663318</td>\n",
       "      <td>0.647779</td>\n",
       "      <td>0.606425</td>\n",
       "      <td>0.640998</td>\n",
       "      <td>0.651668</td>\n",
       "      <td>0.029584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>333.800000</td>\n",
       "      <td>6.511528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>165.700000</td>\n",
       "      <td>7.860591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>60.300000</td>\n",
       "      <td>9.877584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>35.200000</td>\n",
       "      <td>3.938415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.845378</td>\n",
       "      <td>0.813445</td>\n",
       "      <td>0.860504</td>\n",
       "      <td>0.842017</td>\n",
       "      <td>0.826891</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.839496</td>\n",
       "      <td>0.013955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.869898</td>\n",
       "      <td>0.846348</td>\n",
       "      <td>0.864583</td>\n",
       "      <td>0.854220</td>\n",
       "      <td>0.807786</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.845570</td>\n",
       "      <td>0.834975</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.856397</td>\n",
       "      <td>0.847303</td>\n",
       "      <td>0.022346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.890339</td>\n",
       "      <td>0.920548</td>\n",
       "      <td>0.890080</td>\n",
       "      <td>0.905149</td>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.907162</td>\n",
       "      <td>0.910082</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.906516</td>\n",
       "      <td>0.901099</td>\n",
       "      <td>0.904706</td>\n",
       "      <td>0.009296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.759400</td>\n",
       "      <td>0.734800</td>\n",
       "      <td>0.765800</td>\n",
       "      <td>0.747800</td>\n",
       "      <td>0.658000</td>\n",
       "      <td>0.779800</td>\n",
       "      <td>0.732500</td>\n",
       "      <td>0.695500</td>\n",
       "      <td>0.702500</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.733800</td>\n",
       "      <td>0.037971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.877147</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.856774</td>\n",
       "      <td>0.891786</td>\n",
       "      <td>0.876640</td>\n",
       "      <td>0.868118</td>\n",
       "      <td>0.859060</td>\n",
       "      <td>0.878179</td>\n",
       "      <td>0.874854</td>\n",
       "      <td>0.010649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.842910</td>\n",
       "      <td>0.846261</td>\n",
       "      <td>0.842848</td>\n",
       "      <td>0.843661</td>\n",
       "      <td>0.808538</td>\n",
       "      <td>0.859543</td>\n",
       "      <td>0.839752</td>\n",
       "      <td>0.823766</td>\n",
       "      <td>0.820415</td>\n",
       "      <td>0.845724</td>\n",
       "      <td>0.837342</td>\n",
       "      <td>0.015093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.827952</td>\n",
       "      <td>0.835805</td>\n",
       "      <td>0.831183</td>\n",
       "      <td>0.832497</td>\n",
       "      <td>0.794652</td>\n",
       "      <td>0.847784</td>\n",
       "      <td>0.828507</td>\n",
       "      <td>0.808142</td>\n",
       "      <td>0.811553</td>\n",
       "      <td>0.836381</td>\n",
       "      <td>0.825446</td>\n",
       "      <td>0.015864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.824887</td>\n",
       "      <td>0.827665</td>\n",
       "      <td>0.827923</td>\n",
       "      <td>0.826468</td>\n",
       "      <td>0.785048</td>\n",
       "      <td>0.843489</td>\n",
       "      <td>0.821269</td>\n",
       "      <td>0.799727</td>\n",
       "      <td>0.804497</td>\n",
       "      <td>0.831502</td>\n",
       "      <td>0.819248</td>\n",
       "      <td>0.017449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.656354</td>\n",
       "      <td>0.677240</td>\n",
       "      <td>0.663019</td>\n",
       "      <td>0.667629</td>\n",
       "      <td>0.601140</td>\n",
       "      <td>0.696519</td>\n",
       "      <td>0.661280</td>\n",
       "      <td>0.621569</td>\n",
       "      <td>0.630983</td>\n",
       "      <td>0.674692</td>\n",
       "      <td>0.655042</td>\n",
       "      <td>0.028803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.793100</td>\n",
       "      <td>0.853500</td>\n",
       "      <td>0.805700</td>\n",
       "      <td>0.828400</td>\n",
       "      <td>0.826100</td>\n",
       "      <td>0.829300</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.809500</td>\n",
       "      <td>0.837400</td>\n",
       "      <td>0.830200</td>\n",
       "      <td>0.824820</td>\n",
       "      <td>0.017494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.824887</td>\n",
       "      <td>0.827665</td>\n",
       "      <td>0.827923</td>\n",
       "      <td>0.826468</td>\n",
       "      <td>0.785048</td>\n",
       "      <td>0.843489</td>\n",
       "      <td>0.821269</td>\n",
       "      <td>0.799727</td>\n",
       "      <td>0.804497</td>\n",
       "      <td>0.831502</td>\n",
       "      <td>0.819248</td>\n",
       "      <td>0.017449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.661926    0.678335    0.647956    0.697543   \n",
       "1                    TP  341.000000  336.000000  332.000000  334.000000   \n",
       "2                    TN  161.000000  169.000000  170.000000  169.000000   \n",
       "3                    FP   51.000000   61.000000   52.000000   57.000000   \n",
       "4                    FN   42.000000   29.000000   41.000000   35.000000   \n",
       "5              Accuracy    0.843697    0.848739    0.843697    0.845378   \n",
       "6             Precision    0.869898    0.846348    0.864583    0.854220   \n",
       "7           Sensitivity    0.890339    0.920548    0.890080    0.905149   \n",
       "8           Specificity    0.759400    0.734800    0.765800    0.747800   \n",
       "9              F1 score    0.880000    0.881890    0.877147    0.878947   \n",
       "10  F1 score (weighted)    0.842910    0.846261    0.842848    0.843661   \n",
       "11     F1 score (macro)    0.827952    0.835805    0.831183    0.832497   \n",
       "12    Balanced Accuracy    0.824887    0.827665    0.827923    0.826468   \n",
       "13                  MCC    0.656354    0.677240    0.663019    0.667629   \n",
       "14                  NPV    0.793100    0.853500    0.805700    0.828400   \n",
       "15              ROC_AUC    0.824887    0.827665    0.827923    0.826468   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.603395    0.669008    0.663318    0.647779    0.606425    0.640998   \n",
       "1   332.000000  342.000000  334.000000  339.000000  320.000000  328.000000   \n",
       "2   152.000000  170.000000  167.000000  153.000000  170.000000  176.000000   \n",
       "3    79.000000   48.000000   61.000000   67.000000   72.000000   55.000000   \n",
       "4    32.000000   35.000000   33.000000   36.000000   33.000000   36.000000   \n",
       "5     0.813445    0.860504    0.842017    0.826891    0.823529    0.847059   \n",
       "6     0.807786    0.876923    0.845570    0.834975    0.816327    0.856397   \n",
       "7     0.912088    0.907162    0.910082    0.904000    0.906516    0.901099   \n",
       "8     0.658000    0.779800    0.732500    0.695500    0.702500    0.761900   \n",
       "9     0.856774    0.891786    0.876640    0.868118    0.859060    0.878179   \n",
       "10    0.808538    0.859543    0.839752    0.823766    0.820415    0.845724   \n",
       "11    0.794652    0.847784    0.828507    0.808142    0.811553    0.836381   \n",
       "12    0.785048    0.843489    0.821269    0.799727    0.804497    0.831502   \n",
       "13    0.601140    0.696519    0.661280    0.621569    0.630983    0.674692   \n",
       "14    0.826100    0.829300    0.835000    0.809500    0.837400    0.830200   \n",
       "15    0.785048    0.843489    0.821269    0.799727    0.804497    0.831502   \n",
       "\n",
       "           ave       std  \n",
       "0     0.651668  0.029584  \n",
       "1   333.800000  6.511528  \n",
       "2   165.700000  7.860591  \n",
       "3    60.300000  9.877584  \n",
       "4    35.200000  3.938415  \n",
       "5     0.839496  0.013955  \n",
       "6     0.847303  0.022346  \n",
       "7     0.904706  0.009296  \n",
       "8     0.733800  0.037971  \n",
       "9     0.874854  0.010649  \n",
       "10    0.837342  0.015093  \n",
       "11    0.825446  0.015864  \n",
       "12    0.819248  0.017449  \n",
       "13    0.655042  0.028803  \n",
       "14    0.824820  0.017494  \n",
       "15    0.819248  0.017449  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test['std'] = mat_met_knn_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_knn0</th>\n",
       "      <th>y_pred_knn1</th>\n",
       "      <th>y_pred_knn2</th>\n",
       "      <th>y_pred_knn3</th>\n",
       "      <th>y_pred_knn4</th>\n",
       "      <th>y_pred_knn_ave</th>\n",
       "      <th>y_pred_knn_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4084049</td>\n",
       "      <td>0</td>\n",
       "      <td>6.26</td>\n",
       "      <td>8.314242</td>\n",
       "      <td>8.580536</td>\n",
       "      <td>8.713683</td>\n",
       "      <td>8.345944</td>\n",
       "      <td>8.314242</td>\n",
       "      <td>8.088108</td>\n",
       "      <td>0.831120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL2178343</td>\n",
       "      <td>1</td>\n",
       "      <td>5.96</td>\n",
       "      <td>5.692657</td>\n",
       "      <td>5.714293</td>\n",
       "      <td>5.462840</td>\n",
       "      <td>5.462840</td>\n",
       "      <td>5.692657</td>\n",
       "      <td>5.664215</td>\n",
       "      <td>0.169665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL454672</td>\n",
       "      <td>2</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.429171</td>\n",
       "      <td>6.407538</td>\n",
       "      <td>6.339414</td>\n",
       "      <td>6.339414</td>\n",
       "      <td>6.407538</td>\n",
       "      <td>6.453846</td>\n",
       "      <td>0.158593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4299417</td>\n",
       "      <td>3</td>\n",
       "      <td>8.14</td>\n",
       "      <td>7.110000</td>\n",
       "      <td>7.110000</td>\n",
       "      <td>7.149945</td>\n",
       "      <td>7.146060</td>\n",
       "      <td>7.110000</td>\n",
       "      <td>7.294334</td>\n",
       "      <td>0.378577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3692580</td>\n",
       "      <td>4</td>\n",
       "      <td>5.24</td>\n",
       "      <td>5.328651</td>\n",
       "      <td>5.397189</td>\n",
       "      <td>5.328651</td>\n",
       "      <td>5.368068</td>\n",
       "      <td>5.322712</td>\n",
       "      <td>5.330878</td>\n",
       "      <td>0.048504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL3775269</td>\n",
       "      <td>2966</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.416433</td>\n",
       "      <td>7.416433</td>\n",
       "      <td>7.139804</td>\n",
       "      <td>7.416433</td>\n",
       "      <td>7.478761</td>\n",
       "      <td>7.377978</td>\n",
       "      <td>0.109396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL3339019</td>\n",
       "      <td>2967</td>\n",
       "      <td>8.07</td>\n",
       "      <td>7.928212</td>\n",
       "      <td>8.050778</td>\n",
       "      <td>7.966713</td>\n",
       "      <td>7.977354</td>\n",
       "      <td>7.985335</td>\n",
       "      <td>7.996399</td>\n",
       "      <td>0.048973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL3771312</td>\n",
       "      <td>2968</td>\n",
       "      <td>6.52</td>\n",
       "      <td>5.985449</td>\n",
       "      <td>5.985449</td>\n",
       "      <td>5.985449</td>\n",
       "      <td>5.972569</td>\n",
       "      <td>6.029393</td>\n",
       "      <td>6.079718</td>\n",
       "      <td>0.197704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL3589701</td>\n",
       "      <td>2969</td>\n",
       "      <td>6.49</td>\n",
       "      <td>6.121612</td>\n",
       "      <td>5.960083</td>\n",
       "      <td>6.121612</td>\n",
       "      <td>6.121612</td>\n",
       "      <td>6.207435</td>\n",
       "      <td>6.170392</td>\n",
       "      <td>0.160670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL4092997</td>\n",
       "      <td>2970</td>\n",
       "      <td>7.47</td>\n",
       "      <td>7.046455</td>\n",
       "      <td>7.026920</td>\n",
       "      <td>7.046455</td>\n",
       "      <td>7.046455</td>\n",
       "      <td>7.011444</td>\n",
       "      <td>7.107955</td>\n",
       "      <td>0.162432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_knn0  y_pred_knn1  \\\n",
       "0         CHEMBL4084049            0     6.26     8.314242     8.580536   \n",
       "1         CHEMBL2178343            1     5.96     5.692657     5.714293   \n",
       "2          CHEMBL454672            2     6.80     6.429171     6.407538   \n",
       "3         CHEMBL4299417            3     8.14     7.110000     7.110000   \n",
       "4         CHEMBL3692580            4     5.24     5.328651     5.397189   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "2966      CHEMBL3775269         2966     7.40     7.416433     7.416433   \n",
       "2967      CHEMBL3339019         2967     8.07     7.928212     8.050778   \n",
       "2968      CHEMBL3771312         2968     6.52     5.985449     5.985449   \n",
       "2969      CHEMBL3589701         2969     6.49     6.121612     5.960083   \n",
       "2970      CHEMBL4092997         2970     7.47     7.046455     7.026920   \n",
       "\n",
       "      y_pred_knn2  y_pred_knn3  y_pred_knn4  y_pred_knn_ave  y_pred_knn_std  \n",
       "0        8.713683     8.345944     8.314242        8.088108        0.831120  \n",
       "1        5.462840     5.462840     5.692657        5.664215        0.169665  \n",
       "2        6.339414     6.339414     6.407538        6.453846        0.158593  \n",
       "3        7.149945     7.146060     7.110000        7.294334        0.378577  \n",
       "4        5.328651     5.368068     5.322712        5.330878        0.048504  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "2966     7.139804     7.416433     7.478761        7.377978        0.109396  \n",
       "2967     7.966713     7.977354     7.985335        7.996399        0.048973  \n",
       "2968     5.985449     5.972569     6.029393        6.079718        0.197704  \n",
       "2969     6.121612     6.121612     6.207435        6.170392        0.160670  \n",
       "2970     7.046455     7.046455     7.011444        7.107955        0.162432  \n",
       "\n",
       "[2971 rows x 10 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_knn_cat = np.where((y_pred_optimized_knn >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_knn_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_knn))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "knn_run0 = data_knn[['y_test_idx0', 'y_test0', 'y_pred_knn0']]\n",
    "knn_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "knn_run0.reset_index(inplace=True, drop=True)\n",
    "knn_run1 = data_knn[['y_test_idx1', 'y_test1', 'y_pred_knn1']]\n",
    "knn_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "knn_run1.reset_index(inplace=True, drop=True)\n",
    "knn_run2 = data_knn[['y_test_idx2', 'y_test2', 'y_pred_knn2']]\n",
    "knn_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "knn_run2.reset_index(inplace=True, drop=True)\n",
    "knn_run3 = data_knn[['y_test_idx3', 'y_test3', 'y_pred_knn3']]\n",
    "knn_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "knn_run3.reset_index(inplace=True, drop=True)\n",
    "knn_run4 = data_knn[['y_test_idx4', 'y_test4', 'y_pred_knn4']]\n",
    "knn_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "knn_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "knn_5preds = pd.concat([chembl_id, knn_run0, knn_run1, knn_run2, knn_run3, knn_run4], axis=1)\n",
    "knn_5preds = knn_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_knn0', 'y_pred_knn1', 'y_pred_knn2', 'y_pred_knn3', 'y_pred_knn4']]\n",
    "knn_5preds['y_pred_knn_ave'] = knn_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "knn_5preds['y_pred_knn_std'] = knn_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "knn_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0bc43db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model r2_score 0.6466 with a standard deviation of 0.0297\n",
      "KNN optimized model r2_score 0.6573 with a standard deviation of 0.0222\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"KNN baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_knn.joblib']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_reg, \"OUTPUT/knn_reg.joblib\")\n",
    "#joblib.dump(optimized_knn, \"OUTPUT/optimized_knn.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"OUTPUT/optimizedCV_knn.joblib\")\n",
    "#loaded_rf = joblib.load(\"OUTPUT/optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.691559     0.022876\n",
      "1                    TP       165.100000    10.343543\n",
      "2                    TN        85.900000     5.724218\n",
      "3                    FP        27.500000     4.169999\n",
      "4                    FN        18.600000     4.247875\n",
      "5              Accuracy         0.844843     0.026360\n",
      "6             Precision         0.856622     0.025192\n",
      "7           Sensitivity         0.898135     0.025995\n",
      "8           Specificity         0.757970     0.029203\n",
      "9              F1 score         0.876838     0.024574\n",
      "10  F1 score (weighted)         0.843525     0.026428\n",
      "11     F1 score (macro)         0.832892     0.025758\n",
      "12    Balanced Accuracy         0.828047     0.024767\n",
      "13                  MCC         0.667698     0.051819\n",
      "14                  NPV         0.822930     0.034363\n",
      "15              ROC_AUC         0.828047     0.024767\n",
      "CPU times: user 9.66 s, sys: 4 ms, total: 9.66 s\n",
      "Wall time: 9.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    \n",
    "    svm_reg.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.6\n",
    "    y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "    y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.6\n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_cat = np.where((y_pred >= 6.6), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:05:46,592] A new study created in memory with name: SVM_regressor_CV\n",
      "[I 2023-12-20 14:05:52,760] Trial 0 finished with value: 0.3426338352793038 and parameters: {'C': 0.5, 'gamma': 0.00048828125}. Best is trial 0 with value: 0.3426338352793038.\n",
      "[I 2023-12-20 14:05:59,068] Trial 1 finished with value: 0.1438927384545503 and parameters: {'C': 0.0625, 'gamma': 0.0625}. Best is trial 0 with value: 0.3426338352793038.\n",
      "[I 2023-12-20 14:06:05,162] Trial 2 finished with value: 0.34758069504845274 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 2 with value: 0.34758069504845274.\n",
      "[I 2023-12-20 14:06:11,084] Trial 3 finished with value: 0.434676215199475 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 3 with value: 0.434676215199475.\n",
      "[I 2023-12-20 14:06:17,350] Trial 4 finished with value: 0.08191806173880276 and parameters: {'C': 0.0078125, 'gamma': 0.0078125}. Best is trial 3 with value: 0.434676215199475.\n",
      "[I 2023-12-20 14:06:23,804] Trial 5 finished with value: 0.01898948243798838 and parameters: {'C': 0.5, 'gamma': 4.0}. Best is trial 3 with value: 0.434676215199475.\n",
      "[I 2023-12-20 14:06:31,509] Trial 6 finished with value: 0.020463023850425022 and parameters: {'C': 64.0, 'gamma': 8.0}. Best is trial 3 with value: 0.434676215199475.\n",
      "[I 2023-12-20 14:06:37,730] Trial 7 finished with value: 0.5462741574542962 and parameters: {'C': 32.0, 'gamma': 0.0625}. Best is trial 7 with value: 0.5462741574542962.\n",
      "[I 2023-12-20 14:06:44,421] Trial 8 finished with value: 0.02097505331516858 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 7 with value: 0.5462741574542962.\n",
      "[I 2023-12-20 14:06:50,476] Trial 9 finished with value: 0.6414908850723673 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 9 with value: 0.6414908850723673.\n",
      "[I 2023-12-20 14:06:56,515] Trial 10 finished with value: 0.6414908850723673 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 9 with value: 0.6414908850723673.\n",
      "[I 2023-12-20 14:07:02,605] Trial 11 finished with value: 0.6414908850723673 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 9 with value: 0.6414908850723673.\n",
      "[I 2023-12-20 14:07:08,658] Trial 12 finished with value: 0.6414908850723673 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 9 with value: 0.6414908850723673.\n",
      "[I 2023-12-20 14:07:14,913] Trial 13 finished with value: 0.010532545766390178 and parameters: {'C': 0.03125, 'gamma': 0.125}. Best is trial 9 with value: 0.6414908850723673.\n",
      "[I 2023-12-20 14:07:20,799] Trial 14 finished with value: 0.5921583430073342 and parameters: {'C': 1.0, 'gamma': 0.00390625}. Best is trial 9 with value: 0.6414908850723673.\n",
      "[I 2023-12-20 14:07:27,129] Trial 15 finished with value: 0.6742300072203082 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:07:33,475] Trial 16 finished with value: 0.6742300072203082 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:07:39,781] Trial 17 finished with value: 0.16544129472411004 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:07:46,155] Trial 18 finished with value: 0.6742300072203082 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:07:55,616] Trial 19 finished with value: 0.5980595340810181 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:08:02,016] Trial 20 finished with value: 0.01146676259497167 and parameters: {'C': 0.25, 'gamma': 2.0}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:08:08,382] Trial 21 finished with value: 0.6742300072203082 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:08:14,757] Trial 22 finished with value: 0.6742300072203082 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:08:21,460] Trial 23 finished with value: 0.03161962285655785 and parameters: {'C': 2.0, 'gamma': 1.0}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:08:27,847] Trial 24 finished with value: 0.6742300072203082 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:08:33,545] Trial 25 finished with value: 0.5490216929212044 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:08:39,478] Trial 26 finished with value: 0.07644462399901256 and parameters: {'C': 0.125, 'gamma': 0.000244140625}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:08:46,092] Trial 27 finished with value: 0.0815679070169342 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:08:52,215] Trial 28 finished with value: 0.6614717616547516 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:08:58,965] Trial 29 finished with value: 0.04353001492045141 and parameters: {'C': 128.0, 'gamma': 0.5}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:09:05,432] Trial 30 finished with value: 0.03719995137215796 and parameters: {'C': 0.03125, 'gamma': 0.00048828125}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:09:11,731] Trial 31 finished with value: 0.6742300072203082 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:09:17,818] Trial 32 finished with value: 0.3884364292697687 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:09:27,346] Trial 33 finished with value: 0.5811644529201627 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:09:33,654] Trial 34 finished with value: 0.6742300072203082 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:09:40,157] Trial 35 finished with value: -0.004599846985155409 and parameters: {'C': 0.0078125, 'gamma': 3.0517578125e-05}. Best is trial 15 with value: 0.6742300072203082.\n",
      "[I 2023-12-20 14:09:46,458] Trial 36 finished with value: 0.6746159491922565 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 36 with value: 0.6746159491922565.\n",
      "[I 2023-12-20 14:09:52,331] Trial 37 finished with value: 0.5769480920365904 and parameters: {'C': 64.0, 'gamma': 6.103515625e-05}. Best is trial 36 with value: 0.6746159491922565.\n",
      "[I 2023-12-20 14:09:59,294] Trial 38 finished with value: 0.6571433227534816 and parameters: {'C': 64.0, 'gamma': 0.0078125}. Best is trial 36 with value: 0.6746159491922565.\n",
      "[I 2023-12-20 14:10:06,729] Trial 39 finished with value: 0.01873617464863364 and parameters: {'C': 0.5, 'gamma': 8.0}. Best is trial 36 with value: 0.6746159491922565.\n",
      "[I 2023-12-20 14:10:13,117] Trial 40 finished with value: 0.6761120797908642 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 40 with value: 0.6761120797908642.\n",
      "[I 2023-12-20 14:10:19,454] Trial 41 finished with value: 0.6761120797908642 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 40 with value: 0.6761120797908642.\n",
      "[I 2023-12-20 14:10:25,722] Trial 42 finished with value: 0.6761120797908642 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 40 with value: 0.6761120797908642.\n",
      "[I 2023-12-20 14:10:31,883] Trial 43 finished with value: 0.5462741574542962 and parameters: {'C': 32.0, 'gamma': 0.0625}. Best is trial 40 with value: 0.6761120797908642.\n",
      "[I 2023-12-20 14:10:38,184] Trial 44 finished with value: 0.6761120797908642 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 40 with value: 0.6761120797908642.\n",
      "[I 2023-12-20 14:10:44,899] Trial 45 finished with value: 0.020975297394274773 and parameters: {'C': 32.0, 'gamma': 4.0}. Best is trial 40 with value: 0.6761120797908642.\n",
      "[I 2023-12-20 14:10:51,442] Trial 46 finished with value: 0.27138630244786854 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 40 with value: 0.6761120797908642.\n",
      "[I 2023-12-20 14:10:58,037] Trial 47 finished with value: 0.08156790701693303 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 40 with value: 0.6761120797908642.\n",
      "[I 2023-12-20 14:11:05,398] Trial 48 finished with value: 0.6225910431125036 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 40 with value: 0.6761120797908642.\n",
      "[I 2023-12-20 14:11:11,657] Trial 49 finished with value: 0.6761120797908642 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 40 with value: 0.6761120797908642.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6761\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_regressor_CV\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.720102\n",
      "1                    TP  345.000000\n",
      "2                    TN  167.000000\n",
      "3                    FP   45.000000\n",
      "4                    FN   38.000000\n",
      "5              Accuracy    0.860504\n",
      "6             Precision    0.884615\n",
      "7           Sensitivity    0.900783\n",
      "8           Specificity    0.787700\n",
      "9              F1 score    0.892626\n",
      "10  F1 score (weighted)    0.859965\n",
      "11     F1 score (macro)    0.846793\n",
      "12    Balanced Accuracy    0.844260\n",
      "13                  MCC    0.693864\n",
      "14                  NPV    0.814600\n",
      "15              ROC_AUC    0.844260\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_svm_0)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_0_cat = np.where((y_pred_svm_0 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_svm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:11:19,794] Trial 50 finished with value: 0.02003868620953946 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 40 with value: 0.6761120797908642.\n",
      "[I 2023-12-20 14:11:26,278] Trial 51 finished with value: 0.6854062244733852 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:11:32,753] Trial 52 finished with value: 0.6854062244733852 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:11:40,014] Trial 53 finished with value: 0.013128252146970377 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:11:46,492] Trial 54 finished with value: 0.6854062244733852 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:11:52,766] Trial 55 finished with value: 0.5490573780481174 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:11:59,076] Trial 56 finished with value: 0.49758519218057906 and parameters: {'C': 1.0, 'gamma': 0.0009765625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:12:05,572] Trial 57 finished with value: 0.6673835409586986 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:12:12,190] Trial 58 finished with value: 0.16082280665298537 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:12:18,781] Trial 59 finished with value: 0.00840007699356543 and parameters: {'C': 0.25, 'gamma': 0.5}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:12:25,114] Trial 60 finished with value: 0.3415345698422026 and parameters: {'C': 2.0, 'gamma': 0.0001220703125}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:12:31,612] Trial 61 finished with value: 0.6854062244733852 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:12:37,968] Trial 62 finished with value: 0.6854062244733852 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:12:44,332] Trial 63 finished with value: 0.6854062244733852 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:12:50,463] Trial 64 finished with value: 0.6058410566440029 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:12:56,703] Trial 65 finished with value: 0.48481876163491544 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:13:02,738] Trial 66 finished with value: 0.5486371548777571 and parameters: {'C': 32.0, 'gamma': 6.103515625e-05}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:13:09,184] Trial 67 finished with value: 0.3849819784293578 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:13:15,506] Trial 68 finished with value: 0.4996859436019466 and parameters: {'C': 32.0, 'gamma': 3.0517578125e-05}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:13:22,100] Trial 69 finished with value: 0.07772180368552091 and parameters: {'C': 0.0078125, 'gamma': 0.0078125}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:13:29,873] Trial 70 finished with value: 0.006740139194563987 and parameters: {'C': 0.5, 'gamma': 8.0}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:13:36,264] Trial 71 finished with value: 0.6854062244733852 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:13:42,628] Trial 72 finished with value: 0.6854062244733852 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:13:49,000] Trial 73 finished with value: 0.6854062244733852 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:13:55,337] Trial 74 finished with value: 0.26975188800690625 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:14:01,666] Trial 75 finished with value: 0.5474335215158904 and parameters: {'C': 32.0, 'gamma': 0.0625}. Best is trial 51 with value: 0.6854062244733852.\n",
      "[I 2023-12-20 14:14:07,927] Trial 76 finished with value: 0.696802065242923 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:14:14,952] Trial 77 finished with value: 0.01005844859691355 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:14:21,656] Trial 78 finished with value: 0.26704108870203996 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:14:27,881] Trial 79 finished with value: 0.696802065242923 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:14:34,115] Trial 80 finished with value: 0.6625826149112111 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:14:40,355] Trial 81 finished with value: 0.696802065242923 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:14:46,605] Trial 82 finished with value: 0.696802065242923 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:14:52,842] Trial 83 finished with value: 0.696802065242923 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:14:59,132] Trial 84 finished with value: 0.696802065242923 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:15:05,392] Trial 85 finished with value: 0.696802065242923 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:15:11,465] Trial 86 finished with value: 0.62408351564009 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:15:18,536] Trial 87 finished with value: 0.020038686209447966 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:15:25,675] Trial 88 finished with value: 0.013129171593130273 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:15:32,626] Trial 89 finished with value: 0.07261414055840057 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:15:38,908] Trial 90 finished with value: 0.696802065242923 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:15:45,198] Trial 91 finished with value: 0.696802065242923 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:15:51,546] Trial 92 finished with value: 0.696802065242923 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:15:57,832] Trial 93 finished with value: 0.696802065242923 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:16:04,137] Trial 94 finished with value: 0.49933334630922666 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:16:10,348] Trial 95 finished with value: 0.4322662152001153 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:16:16,642] Trial 96 finished with value: 0.6734531237663465 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:16:22,899] Trial 97 finished with value: 0.696802065242923 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:16:29,182] Trial 98 finished with value: 0.696802065242923 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:16:36,168] Trial 99 finished with value: 0.03305582102682728 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 76 with value: 0.696802065242923.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6968\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.720102    0.712450\n",
      "1                    TP  345.000000  334.000000\n",
      "2                    TN  167.000000  170.000000\n",
      "3                    FP   45.000000   60.000000\n",
      "4                    FN   38.000000   31.000000\n",
      "5              Accuracy    0.860504    0.847059\n",
      "6             Precision    0.884615    0.847716\n",
      "7           Sensitivity    0.900783    0.915068\n",
      "8           Specificity    0.787700    0.739100\n",
      "9              F1 score    0.892626    0.880105\n",
      "10  F1 score (weighted)    0.859965    0.844835\n",
      "11     F1 score (macro)    0.846793    0.834484\n",
      "12    Balanced Accuracy    0.844260    0.827099\n",
      "13                  MCC    0.693864    0.673557\n",
      "14                  NPV    0.814600    0.845800\n",
      "15              ROC_AUC    0.844260    0.827099\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_svm_1)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_1_cat = np.where((y_pred_svm_1 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_svm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:16:42,996] Trial 100 finished with value: 0.5938516068674138 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 76 with value: 0.696802065242923.\n",
      "[I 2023-12-20 14:16:48,882] Trial 101 finished with value: 0.7009185511601342 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:16:54,764] Trial 102 finished with value: 0.7009185511601342 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:17:00,628] Trial 103 finished with value: 0.7009185511601342 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:17:06,426] Trial 104 finished with value: 0.55862995006968 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:17:12,356] Trial 105 finished with value: 0.7009185511601342 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:17:18,514] Trial 106 finished with value: 0.6993727847703098 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:17:24,217] Trial 107 finished with value: 0.35045254615777244 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:17:29,988] Trial 108 finished with value: 0.4382058180938224 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:17:35,782] Trial 109 finished with value: 0.6515047617397405 and parameters: {'C': 1.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:17:41,704] Trial 110 finished with value: 0.16568155806513818 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:17:47,544] Trial 111 finished with value: 0.5762183831662225 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:17:53,443] Trial 112 finished with value: 0.7009185511601342 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:17:59,093] Trial 113 finished with value: 0.6975426145468611 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:18:04,721] Trial 114 finished with value: 0.6975426145468611 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:18:12,246] Trial 115 finished with value: 0.02032156895702477 and parameters: {'C': 2.0, 'gamma': 8.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:18:17,937] Trial 116 finished with value: 0.6975426145468611 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:18:23,592] Trial 117 finished with value: 0.6975426145468611 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:18:29,417] Trial 118 finished with value: 0.5654663016504926 and parameters: {'C': 2.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:18:35,971] Trial 119 finished with value: 0.02095253753219086 and parameters: {'C': 2.0, 'gamma': 4.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:18:41,654] Trial 120 finished with value: 0.6975426145468611 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:18:47,346] Trial 121 finished with value: 0.6975426145468611 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:18:52,975] Trial 122 finished with value: 0.6975426145468611 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:18:59,217] Trial 123 finished with value: 0.28441994966537637 and parameters: {'C': 2.0, 'gamma': 0.125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:19:04,852] Trial 124 finished with value: 0.6975426145468611 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:19:10,492] Trial 125 finished with value: 0.6975426145468611 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:19:16,147] Trial 126 finished with value: 0.6409515241161952 and parameters: {'C': 2.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:19:21,793] Trial 127 finished with value: 0.6975426145468611 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:19:27,579] Trial 128 finished with value: 0.5993684506248995 and parameters: {'C': 2.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:19:33,213] Trial 129 finished with value: 0.6975426145468611 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:19:39,684] Trial 130 finished with value: 0.025043045365737048 and parameters: {'C': 2.0, 'gamma': 2.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:19:45,324] Trial 131 finished with value: 0.6975426145468611 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:19:50,993] Trial 132 finished with value: 0.6975426145468611 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:19:56,623] Trial 133 finished with value: 0.6975426145468611 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:20:03,293] Trial 134 finished with value: 0.033803340341351615 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:20:09,208] Trial 135 finished with value: 0.49448033335737795 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:20:14,892] Trial 136 finished with value: 0.6975426145468611 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:20:21,310] Trial 137 finished with value: 0.08991812134635163 and parameters: {'C': 2.0, 'gamma': 0.25}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:20:27,039] Trial 138 finished with value: 0.39308868170862177 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:20:33,122] Trial 139 finished with value: 0.6993727847703098 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:20:39,035] Trial 140 finished with value: 0.5582703691466506 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:20:45,073] Trial 141 finished with value: 0.08912018837945271 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:20:51,159] Trial 142 finished with value: 0.6993727847703098 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:20:56,973] Trial 143 finished with value: 0.6815261941873698 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:21:02,839] Trial 144 finished with value: 0.5067818109935506 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:21:08,975] Trial 145 finished with value: 0.6993727847703098 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:21:15,679] Trial 146 finished with value: 0.04877512919872725 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:21:21,476] Trial 147 finished with value: 0.6163964075489922 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:21:27,595] Trial 148 finished with value: 0.6993727847703098 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:21:33,238] Trial 149 finished with value: 0.5904755090710763 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7009185511601342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7009\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.720102    0.712450    0.690328\n",
      "1                    TP  345.000000  334.000000  340.000000\n",
      "2                    TN  167.000000  170.000000  173.000000\n",
      "3                    FP   45.000000   60.000000   49.000000\n",
      "4                    FN   38.000000   31.000000   33.000000\n",
      "5              Accuracy    0.860504    0.847059    0.862185\n",
      "6             Precision    0.884615    0.847716    0.874036\n",
      "7           Sensitivity    0.900783    0.915068    0.911528\n",
      "8           Specificity    0.787700    0.739100    0.779300\n",
      "9              F1 score    0.892626    0.880105    0.892388\n",
      "10  F1 score (weighted)    0.859965    0.844835    0.861056\n",
      "11     F1 score (macro)    0.846793    0.834484    0.850400\n",
      "12    Balanced Accuracy    0.844260    0.827099    0.845404\n",
      "13                  MCC    0.693864    0.673557    0.702230\n",
      "14                  NPV    0.814600    0.845800    0.839800\n",
      "15              ROC_AUC    0.844260    0.827099    0.845404\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_svm_2)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_2_cat = np.where((y_pred_svm_2 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_svm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:21:40,758] Trial 150 finished with value: 0.6728936072831033 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:21:47,241] Trial 151 finished with value: 0.27553453542165657 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:21:53,086] Trial 152 finished with value: 0.6279422680944446 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:21:59,495] Trial 153 finished with value: 0.6650204130864111 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:22:06,041] Trial 154 finished with value: 0.6728936072831033 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:22:12,294] Trial 155 finished with value: 0.4350504583881518 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:22:18,570] Trial 156 finished with value: 0.34904380495502324 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:22:24,475] Trial 157 finished with value: 0.6636349099081806 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:22:31,025] Trial 158 finished with value: 0.16448092860159033 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:22:38,312] Trial 159 finished with value: 0.0024552405627944474 and parameters: {'C': 0.25, 'gamma': 8.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:22:45,176] Trial 160 finished with value: 0.6612294607476938 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:22:51,318] Trial 161 finished with value: 0.6811929251429436 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:22:57,472] Trial 162 finished with value: 0.6811929251429436 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:23:03,870] Trial 163 finished with value: 0.6706229095367818 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:23:10,019] Trial 164 finished with value: 0.48662322815043596 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:23:16,161] Trial 165 finished with value: 0.6811929251429436 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:23:22,622] Trial 166 finished with value: 0.5483096484694314 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:23:28,728] Trial 167 finished with value: 0.6811929251429436 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:23:35,346] Trial 168 finished with value: -0.007411373524163356 and parameters: {'C': 0.0078125, 'gamma': 0.125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:23:41,179] Trial 169 finished with value: 0.6279422680944446 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:23:47,369] Trial 170 finished with value: 0.3893657661313853 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:23:53,483] Trial 171 finished with value: 0.6811929251429436 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:23:59,607] Trial 172 finished with value: 0.6811929251429436 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:24:06,599] Trial 173 finished with value: 0.01640427841461619 and parameters: {'C': 2.0, 'gamma': 4.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:24:12,649] Trial 174 finished with value: 0.6280885919924518 and parameters: {'C': 2.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:24:19,128] Trial 175 finished with value: 0.27553453542165657 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:24:25,552] Trial 176 finished with value: 0.6650204130864111 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:24:32,122] Trial 177 finished with value: 0.6227864010414408 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:24:38,254] Trial 178 finished with value: 0.6811929251429436 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:24:45,005] Trial 179 finished with value: 0.019319588231268316 and parameters: {'C': 2.0, 'gamma': 2.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:24:51,568] Trial 180 finished with value: 0.6728936072831033 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:24:57,744] Trial 181 finished with value: 0.6811929251429436 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:25:03,872] Trial 182 finished with value: 0.6811929251429436 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:25:10,023] Trial 183 finished with value: 0.6811929251429436 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:25:17,072] Trial 184 finished with value: 0.026553964369763892 and parameters: {'C': 2.0, 'gamma': 1.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:25:23,611] Trial 185 finished with value: 0.6728936072831033 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:25:29,734] Trial 186 finished with value: 0.6811929251429436 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:25:36,044] Trial 187 finished with value: 0.34705033362204 and parameters: {'C': 1.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:25:42,171] Trial 188 finished with value: 0.6811929251429436 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:25:48,815] Trial 189 finished with value: -0.00882493420895344 and parameters: {'C': 0.015625, 'gamma': 0.25}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:25:54,851] Trial 190 finished with value: 0.5011975964421073 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:26:01,021] Trial 191 finished with value: 0.6811929251429436 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:26:07,135] Trial 192 finished with value: 0.6811929251429436 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:26:13,259] Trial 193 finished with value: 0.6811929251429436 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:26:19,687] Trial 194 finished with value: 0.6633444016046426 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:26:25,592] Trial 195 finished with value: 0.5689673856533523 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:26:31,885] Trial 196 finished with value: 0.6775595593527608 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:26:41,412] Trial 197 finished with value: 0.5693597972503358 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:26:48,314] Trial 198 finished with value: 0.04026518178117946 and parameters: {'C': 2.0, 'gamma': 0.5}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:26:54,817] Trial 199 finished with value: 0.6728936072831033 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7009\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.720102    0.712450    0.690328    0.730473\n",
      "1                    TP  345.000000  334.000000  340.000000  336.000000\n",
      "2                    TN  167.000000  170.000000  173.000000  181.000000\n",
      "3                    FP   45.000000   60.000000   49.000000   45.000000\n",
      "4                    FN   38.000000   31.000000   33.000000   33.000000\n",
      "5              Accuracy    0.860504    0.847059    0.862185    0.868908\n",
      "6             Precision    0.884615    0.847716    0.874036    0.881890\n",
      "7           Sensitivity    0.900783    0.915068    0.911528    0.910569\n",
      "8           Specificity    0.787700    0.739100    0.779300    0.800900\n",
      "9              F1 score    0.892626    0.880105    0.892388    0.896000\n",
      "10  F1 score (weighted)    0.859965    0.844835    0.861056    0.868169\n",
      "11     F1 score (macro)    0.846793    0.834484    0.850400    0.859364\n",
      "12    Balanced Accuracy    0.844260    0.827099    0.845404    0.855727\n",
      "13                  MCC    0.693864    0.673557    0.702230    0.719523\n",
      "14                  NPV    0.814600    0.845800    0.839800    0.845800\n",
      "15              ROC_AUC    0.844260    0.827099    0.845404    0.855727\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_svm_3)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_3_cat = np.where((y_pred_svm_3 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_svm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:27:01,794] Trial 200 finished with value: 0.14399515316153116 and parameters: {'C': 0.125, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:27:07,699] Trial 201 finished with value: 0.6963755607727274 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:27:13,658] Trial 202 finished with value: 0.6963755607727274 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:27:19,614] Trial 203 finished with value: 0.6963755607727274 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:27:25,718] Trial 204 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:27:31,814] Trial 205 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:27:37,978] Trial 206 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:27:43,775] Trial 207 finished with value: 0.2446931907415529 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:27:49,828] Trial 208 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:27:55,740] Trial 209 finished with value: 0.34790418329727746 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:28:01,852] Trial 210 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:28:07,950] Trial 211 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:28:14,070] Trial 212 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:28:20,179] Trial 213 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:28:26,261] Trial 214 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:28:32,361] Trial 215 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:28:38,437] Trial 216 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:28:44,388] Trial 217 finished with value: 0.685423601669626 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:28:52,007] Trial 218 finished with value: 0.012875866536112123 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:28:58,077] Trial 219 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:29:04,142] Trial 220 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:29:10,194] Trial 221 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:29:16,288] Trial 222 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:29:22,396] Trial 223 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:29:28,470] Trial 224 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:29:34,520] Trial 225 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:29:40,557] Trial 226 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:29:46,493] Trial 227 finished with value: 0.5642076867890731 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:29:52,551] Trial 228 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:29:58,598] Trial 229 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:30:05,221] Trial 230 finished with value: 0.013372861253414681 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:30:11,271] Trial 231 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:30:17,305] Trial 232 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:30:23,356] Trial 233 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:30:29,410] Trial 234 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:30:35,538] Trial 235 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:30:41,657] Trial 236 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:30:48,034] Trial 237 finished with value: 0.2777185448939594 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:30:54,170] Trial 238 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:31:00,288] Trial 239 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:31:06,397] Trial 240 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:31:12,531] Trial 241 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:31:18,639] Trial 242 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:31:24,737] Trial 243 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:31:30,499] Trial 244 finished with value: 0.6247788798009793 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:31:36,636] Trial 245 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:31:43,275] Trial 246 finished with value: 0.01673033150356621 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:31:49,404] Trial 247 finished with value: 0.654720225666647 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:31:55,532] Trial 248 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:32:01,656] Trial 249 finished with value: 0.7009167596645087 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7009\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.720102    0.712450    0.690328    0.730473   \n",
      "1                    TP  345.000000  334.000000  340.000000  336.000000   \n",
      "2                    TN  167.000000  170.000000  173.000000  181.000000   \n",
      "3                    FP   45.000000   60.000000   49.000000   45.000000   \n",
      "4                    FN   38.000000   31.000000   33.000000   33.000000   \n",
      "5              Accuracy    0.860504    0.847059    0.862185    0.868908   \n",
      "6             Precision    0.884615    0.847716    0.874036    0.881890   \n",
      "7           Sensitivity    0.900783    0.915068    0.911528    0.910569   \n",
      "8           Specificity    0.787700    0.739100    0.779300    0.800900   \n",
      "9              F1 score    0.892626    0.880105    0.892388    0.896000   \n",
      "10  F1 score (weighted)    0.859965    0.844835    0.861056    0.868169   \n",
      "11     F1 score (macro)    0.846793    0.834484    0.850400    0.859364   \n",
      "12    Balanced Accuracy    0.844260    0.827099    0.845404    0.855727   \n",
      "13                  MCC    0.693864    0.673557    0.702230    0.719523   \n",
      "14                  NPV    0.814600    0.845800    0.839800    0.845800   \n",
      "15              ROC_AUC    0.844260    0.827099    0.845404    0.855727   \n",
      "\n",
      "          Set4  \n",
      "0     0.672656  \n",
      "1   337.000000  \n",
      "2   169.000000  \n",
      "3    62.000000  \n",
      "4    27.000000  \n",
      "5     0.850420  \n",
      "6     0.844612  \n",
      "7     0.925824  \n",
      "8     0.731600  \n",
      "9     0.883355  \n",
      "10    0.847721  \n",
      "11    0.837462  \n",
      "12    0.828713  \n",
      "13    0.681693  \n",
      "14    0.862200  \n",
      "15    0.828713  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_svm_4)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_4_cat = np.where((y_pred_svm_4 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_svm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:32:09,518] Trial 250 finished with value: 0.09016447081080263 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:32:16,133] Trial 251 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:32:23,306] Trial 252 finished with value: 0.03999394393869602 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:32:29,881] Trial 253 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:32:36,107] Trial 254 finished with value: 0.5051001176146578 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:32:42,722] Trial 255 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:32:48,982] Trial 256 finished with value: 0.4361944452550766 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:32:55,551] Trial 257 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:33:01,862] Trial 258 finished with value: 0.6740192496498879 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:33:08,425] Trial 259 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:33:14,607] Trial 260 finished with value: 0.5882583496009809 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:33:21,179] Trial 261 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:33:27,761] Trial 262 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:33:34,392] Trial 263 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:33:40,970] Trial 264 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:33:47,984] Trial 265 finished with value: 0.05322751146719491 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:33:54,381] Trial 266 finished with value: 0.5569948932338195 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:34:00,998] Trial 267 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:34:07,420] Trial 268 finished with value: 0.24030090212478772 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:34:13,756] Trial 269 finished with value: 0.34643925448224844 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:34:20,370] Trial 270 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:34:26,963] Trial 271 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:34:33,946] Trial 272 finished with value: 0.6720866808492658 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:34:42,027] Trial 273 finished with value: 0.02763013380885835 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:34:48,601] Trial 274 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:34:55,134] Trial 275 finished with value: 0.08643050280178802 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:35:01,722] Trial 276 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:35:08,279] Trial 277 finished with value: 0.3912385956526791 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:35:14,865] Trial 278 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:35:21,486] Trial 279 finished with value: 0.014765493668428942 and parameters: {'C': 0.5, 'gamma': 4.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:35:28,031] Trial 280 finished with value: 0.27322746018864874 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:35:34,630] Trial 281 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:35:41,310] Trial 282 finished with value: 0.5583768638173617 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:35:47,798] Trial 283 finished with value: 0.686112130535381 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:35:54,902] Trial 284 finished with value: 0.28125721050521957 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:36:01,473] Trial 285 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:36:08,047] Trial 286 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:36:14,639] Trial 287 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:36:21,002] Trial 288 finished with value: 0.6450398512356419 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:36:27,282] Trial 289 finished with value: 0.6154050695378354 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:36:33,919] Trial 290 finished with value: 0.027359010596754118 and parameters: {'C': 1.0, 'gamma': 2.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:36:40,472] Trial 291 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:36:46,893] Trial 292 finished with value: 0.16291353634317413 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:36:54,020] Trial 293 finished with value: 0.03999394348997938 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:37:00,552] Trial 294 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:37:06,776] Trial 295 finished with value: 0.5733671411171206 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:37:13,652] Trial 296 finished with value: 0.09016447081080263 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:37:19,853] Trial 297 finished with value: 0.5051001176146578 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:37:26,418] Trial 298 finished with value: 0.6900366134880949 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:37:32,697] Trial 299 finished with value: 0.4361944452550766 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7009185511601342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7009\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.720102    0.712450    0.690328    0.730473   \n",
      "1                    TP  345.000000  334.000000  340.000000  336.000000   \n",
      "2                    TN  167.000000  170.000000  173.000000  181.000000   \n",
      "3                    FP   45.000000   60.000000   49.000000   45.000000   \n",
      "4                    FN   38.000000   31.000000   33.000000   33.000000   \n",
      "5              Accuracy    0.860504    0.847059    0.862185    0.868908   \n",
      "6             Precision    0.884615    0.847716    0.874036    0.881890   \n",
      "7           Sensitivity    0.900783    0.915068    0.911528    0.910569   \n",
      "8           Specificity    0.787700    0.739100    0.779300    0.800900   \n",
      "9              F1 score    0.892626    0.880105    0.892388    0.896000   \n",
      "10  F1 score (weighted)    0.859965    0.844835    0.861056    0.868169   \n",
      "11     F1 score (macro)    0.846793    0.834484    0.850400    0.859364   \n",
      "12    Balanced Accuracy    0.844260    0.827099    0.845404    0.855727   \n",
      "13                  MCC    0.693864    0.673557    0.702230    0.719523   \n",
      "14                  NPV    0.814600    0.845800    0.839800    0.845800   \n",
      "15              ROC_AUC    0.844260    0.827099    0.845404    0.855727   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.672656    0.722036  \n",
      "1   337.000000  344.000000  \n",
      "2   169.000000  174.000000  \n",
      "3    62.000000   44.000000  \n",
      "4    27.000000   33.000000  \n",
      "5     0.850420    0.870588  \n",
      "6     0.844612    0.886598  \n",
      "7     0.925824    0.912467  \n",
      "8     0.731600    0.798200  \n",
      "9     0.883355    0.899346  \n",
      "10    0.847721    0.869844  \n",
      "11    0.837462    0.859085  \n",
      "12    0.828713    0.855316  \n",
      "13    0.681693    0.718857  \n",
      "14    0.862200    0.840600  \n",
      "15    0.828713    0.855316  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_svm_5)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_5_cat = np.where((y_pred_svm_5 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_svm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:37:39,804] Trial 300 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:37:45,852] Trial 301 finished with value: 0.4391141932842942 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:37:51,890] Trial 302 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:37:57,667] Trial 303 finished with value: 0.5945140017568091 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:38:03,811] Trial 304 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:38:10,568] Trial 305 finished with value: 0.038473883074328384 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:38:16,723] Trial 306 finished with value: 0.6983921110974574 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:38:22,338] Trial 307 finished with value: 0.5564675264544199 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:38:28,431] Trial 308 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:38:34,495] Trial 309 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:38:40,332] Trial 310 finished with value: 0.3870598006703177 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:38:46,317] Trial 311 finished with value: -0.010614034771398795 and parameters: {'C': 0.0078125, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:38:52,383] Trial 312 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:38:58,037] Trial 313 finished with value: 0.3453654652606386 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:39:04,044] Trial 314 finished with value: 0.6806069680624208 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:39:09,809] Trial 315 finished with value: 0.6293930419677473 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:39:15,960] Trial 316 finished with value: 0.27054496200025296 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:39:22,051] Trial 317 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:39:28,088] Trial 318 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:39:34,150] Trial 319 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:39:40,209] Trial 320 finished with value: 0.5536190524945519 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:39:48,051] Trial 321 finished with value: 0.010799041645934971 and parameters: {'C': 64.0, 'gamma': 8.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:39:54,200] Trial 322 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:40:00,831] Trial 323 finished with value: 0.011394460253914208 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:40:06,883] Trial 324 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:40:13,312] Trial 325 finished with value: 0.266307075671377 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:40:19,367] Trial 326 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:40:25,309] Trial 327 finished with value: 0.6718656911074519 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:40:31,402] Trial 328 finished with value: 0.6974508767965212 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:40:37,399] Trial 329 finished with value: 0.06321428199337939 and parameters: {'C': 0.015625, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:40:43,278] Trial 330 finished with value: 0.6547037849006028 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:40:49,387] Trial 331 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:40:55,209] Trial 332 finished with value: 0.5719609192113194 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:41:01,385] Trial 333 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:41:08,023] Trial 334 finished with value: 0.015296918036462003 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:41:14,110] Trial 335 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:41:20,029] Trial 336 finished with value: 0.4894558057340571 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:41:26,685] Trial 337 finished with value: 0.02423533834867082 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:41:32,761] Trial 338 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:41:38,823] Trial 339 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:41:45,218] Trial 340 finished with value: 0.07673418090643389 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:41:51,369] Trial 341 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:41:57,282] Trial 342 finished with value: 0.43737796488673497 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:42:03,004] Trial 343 finished with value: 0.5069209347765905 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:42:08,974] Trial 344 finished with value: 0.3218833011200523 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:42:15,029] Trial 345 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:42:21,252] Trial 346 finished with value: 0.01031596160426489 and parameters: {'C': 0.0078125, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:42:27,486] Trial 347 finished with value: 0.6983921110974574 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:42:34,245] Trial 348 finished with value: 0.038473883074328384 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 101 with value: 0.7009185511601342.\n",
      "[I 2023-12-20 14:42:40,294] Trial 349 finished with value: 0.698577105415312 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7009185511601342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7009\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.720102    0.712450    0.690328    0.730473   \n",
      "1                    TP  345.000000  334.000000  340.000000  336.000000   \n",
      "2                    TN  167.000000  170.000000  173.000000  181.000000   \n",
      "3                    FP   45.000000   60.000000   49.000000   45.000000   \n",
      "4                    FN   38.000000   31.000000   33.000000   33.000000   \n",
      "5              Accuracy    0.860504    0.847059    0.862185    0.868908   \n",
      "6             Precision    0.884615    0.847716    0.874036    0.881890   \n",
      "7           Sensitivity    0.900783    0.915068    0.911528    0.910569   \n",
      "8           Specificity    0.787700    0.739100    0.779300    0.800900   \n",
      "9              F1 score    0.892626    0.880105    0.892388    0.896000   \n",
      "10  F1 score (weighted)    0.859965    0.844835    0.861056    0.868169   \n",
      "11     F1 score (macro)    0.846793    0.834484    0.850400    0.859364   \n",
      "12    Balanced Accuracy    0.844260    0.827099    0.845404    0.855727   \n",
      "13                  MCC    0.693864    0.673557    0.702230    0.719523   \n",
      "14                  NPV    0.814600    0.845800    0.839800    0.845800   \n",
      "15              ROC_AUC    0.844260    0.827099    0.845404    0.855727   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.672656    0.722036    0.693380  \n",
      "1   337.000000  344.000000  328.000000  \n",
      "2   169.000000  174.000000  170.000000  \n",
      "3    62.000000   44.000000   58.000000  \n",
      "4    27.000000   33.000000   39.000000  \n",
      "5     0.850420    0.870588    0.836975  \n",
      "6     0.844612    0.886598    0.849741  \n",
      "7     0.925824    0.912467    0.893733  \n",
      "8     0.731600    0.798200    0.745600  \n",
      "9     0.883355    0.899346    0.871182  \n",
      "10    0.847721    0.869844    0.835488  \n",
      "11    0.837462    0.859085    0.824607  \n",
      "12    0.828713    0.855316    0.819674  \n",
      "13    0.681693    0.718857    0.651134  \n",
      "14    0.862200    0.840600    0.813400  \n",
      "15    0.828713    0.855316    0.819674  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_svm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_6_cat = np.where((y_pred_svm_6 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_svm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:42:47,753] Trial 350 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:42:53,802] Trial 351 finished with value: 0.6354155827393676 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:43:00,077] Trial 352 finished with value: 0.2797927106663252 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:43:06,538] Trial 353 finished with value: 0.2474140995210748 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:43:13,681] Trial 354 finished with value: 0.5873118368754955 and parameters: {'C': 64.0, 'gamma': 0.00048828125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:43:19,874] Trial 355 finished with value: 0.3534187147552219 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:43:26,302] Trial 356 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:43:32,812] Trial 357 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:43:39,293] Trial 358 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:43:45,786] Trial 359 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:43:52,256] Trial 360 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:43:58,603] Trial 361 finished with value: 0.6864584384914069 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:44:05,039] Trial 362 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:44:12,981] Trial 363 finished with value: 0.00794173388704309 and parameters: {'C': 1.0, 'gamma': 8.0}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:44:19,449] Trial 364 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:44:25,937] Trial 365 finished with value: 0.697106449204924 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:44:32,388] Trial 366 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:44:38,811] Trial 367 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:44:44,992] Trial 368 finished with value: 0.5750558444761895 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:44:51,666] Trial 369 finished with value: 0.036050403217957896 and parameters: {'C': 0.015625, 'gamma': 0.0625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:44:58,079] Trial 370 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:45:05,186] Trial 371 finished with value: 0.009182524668361624 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:45:11,759] Trial 372 finished with value: 0.28361287471034197 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:45:18,252] Trial 373 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:45:24,680] Trial 374 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:45:31,101] Trial 375 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:45:37,444] Trial 376 finished with value: 0.4144734617537743 and parameters: {'C': 0.125, 'gamma': 0.00390625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:45:43,871] Trial 377 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:45:50,321] Trial 378 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:45:56,281] Trial 379 finished with value: 0.617617348550797 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:46:02,701] Trial 380 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:46:09,135] Trial 381 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:46:15,600] Trial 382 finished with value: 0.6980454555231624 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:46:22,102] Trial 383 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:46:29,132] Trial 384 finished with value: 0.011577019158384516 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:46:36,185] Trial 385 finished with value: 0.0187912431364058 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:46:42,610] Trial 386 finished with value: 0.39636338564554247 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:46:49,449] Trial 387 finished with value: 0.07951800372856793 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:46:56,326] Trial 388 finished with value: 0.09146783708758927 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:47:02,813] Trial 389 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:47:08,874] Trial 390 finished with value: 0.5067732533248215 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:47:14,893] Trial 391 finished with value: 0.6354155827393676 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:47:21,415] Trial 392 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:47:27,450] Trial 393 finished with value: 0.44228679242266844 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:47:33,959] Trial 394 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:47:40,445] Trial 395 finished with value: 0.6971030840206006 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:47:46,874] Trial 396 finished with value: 0.6852989057607757 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:47:53,347] Trial 397 finished with value: 0.7013628681027785 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:47:59,683] Trial 398 finished with value: 0.2797927106663252 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:48:06,612] Trial 399 finished with value: 0.6073359273148562 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 350 with value: 0.7013628681027785.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7014\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.720102    0.712450    0.690328    0.730473   \n",
      "1                    TP  345.000000  334.000000  340.000000  336.000000   \n",
      "2                    TN  167.000000  170.000000  173.000000  181.000000   \n",
      "3                    FP   45.000000   60.000000   49.000000   45.000000   \n",
      "4                    FN   38.000000   31.000000   33.000000   33.000000   \n",
      "5              Accuracy    0.860504    0.847059    0.862185    0.868908   \n",
      "6             Precision    0.884615    0.847716    0.874036    0.881890   \n",
      "7           Sensitivity    0.900783    0.915068    0.911528    0.910569   \n",
      "8           Specificity    0.787700    0.739100    0.779300    0.800900   \n",
      "9              F1 score    0.892626    0.880105    0.892388    0.896000   \n",
      "10  F1 score (weighted)    0.859965    0.844835    0.861056    0.868169   \n",
      "11     F1 score (macro)    0.846793    0.834484    0.850400    0.859364   \n",
      "12    Balanced Accuracy    0.844260    0.827099    0.845404    0.855727   \n",
      "13                  MCC    0.693864    0.673557    0.702230    0.719523   \n",
      "14                  NPV    0.814600    0.845800    0.839800    0.845800   \n",
      "15              ROC_AUC    0.844260    0.827099    0.845404    0.855727   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.672656    0.722036    0.693380    0.695743  \n",
      "1   337.000000  344.000000  328.000000  331.000000  \n",
      "2   169.000000  174.000000  170.000000  163.000000  \n",
      "3    62.000000   44.000000   58.000000   57.000000  \n",
      "4    27.000000   33.000000   39.000000   44.000000  \n",
      "5     0.850420    0.870588    0.836975    0.830252  \n",
      "6     0.844612    0.886598    0.849741    0.853093  \n",
      "7     0.925824    0.912467    0.893733    0.882667  \n",
      "8     0.731600    0.798200    0.745600    0.740900  \n",
      "9     0.883355    0.899346    0.871182    0.867628  \n",
      "10    0.847721    0.869844    0.835488    0.829114  \n",
      "11    0.837462    0.859085    0.824607    0.815547  \n",
      "12    0.828713    0.855316    0.819674    0.811788  \n",
      "13    0.681693    0.718857    0.651134    0.631997  \n",
      "14    0.862200    0.840600    0.813400    0.787400  \n",
      "15    0.828713    0.855316    0.819674    0.811788  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_svm_7)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_7_cat = np.where((y_pred_svm_7 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_svm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:48:14,322] Trial 400 finished with value: 0.041294397815867225 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:48:20,282] Trial 401 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:48:26,219] Trial 402 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:48:32,130] Trial 403 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:48:38,096] Trial 404 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:48:44,020] Trial 405 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:48:49,755] Trial 406 finished with value: 0.4301979737154841 and parameters: {'C': 1.0, 'gamma': 0.00048828125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:48:55,668] Trial 407 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:49:01,960] Trial 408 finished with value: -0.007903360962044715 and parameters: {'C': 0.015625, 'gamma': 3.0517578125e-05}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:49:07,916] Trial 409 finished with value: 0.3453678428593173 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:49:13,894] Trial 410 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:49:20,028] Trial 411 finished with value: 0.6729437696235929 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:49:26,287] Trial 412 finished with value: 0.6829370812738828 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:49:33,939] Trial 413 finished with value: 0.012967672499873618 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:49:39,449] Trial 414 finished with value: 0.5697408526580077 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:49:45,392] Trial 415 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:49:51,614] Trial 416 finished with value: 0.5674198692788812 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:49:57,533] Trial 417 finished with value: 0.4901997481367356 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:50:03,535] Trial 418 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:50:09,511] Trial 419 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:50:16,185] Trial 420 finished with value: 0.013604275870801097 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:50:22,107] Trial 421 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:50:28,573] Trial 422 finished with value: 0.2871916966984761 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:50:34,874] Trial 423 finished with value: 0.6853138214793513 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:50:40,820] Trial 424 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:50:46,825] Trial 425 finished with value: 0.6458670772443458 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:50:52,644] Trial 426 finished with value: 0.612047375840679 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:50:58,527] Trial 427 finished with value: 0.3912691714719754 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:51:04,847] Trial 428 finished with value: -0.009270401262234352 and parameters: {'C': 0.0078125, 'gamma': 2.0}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:51:10,786] Trial 429 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:51:16,425] Trial 430 finished with value: 0.632359070161308 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:51:22,360] Trial 431 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:51:28,397] Trial 432 finished with value: -0.007426307440184821 and parameters: {'C': 0.03125, 'gamma': 1.0}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:51:34,336] Trial 433 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:51:40,864] Trial 434 finished with value: 0.08489954951376731 and parameters: {'C': 64.0, 'gamma': 0.25}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:51:46,778] Trial 435 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:51:52,510] Trial 436 finished with value: 0.4319964561255306 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:51:58,370] Trial 437 finished with value: 0.5889803119465744 and parameters: {'C': 32.0, 'gamma': 0.000244140625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:52:04,323] Trial 438 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:52:10,261] Trial 439 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:52:16,194] Trial 440 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:52:22,051] Trial 441 finished with value: 0.6760442824624817 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:52:27,637] Trial 442 finished with value: 0.6701539392821305 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:52:33,387] Trial 443 finished with value: 0.5843708227014434 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:52:39,688] Trial 444 finished with value: 0.6829370812738828 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:52:45,705] Trial 445 finished with value: 0.6887294773600174 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:52:52,360] Trial 446 finished with value: 0.041294397815867225 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:52:58,392] Trial 447 finished with value: 0.16506952193173147 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:53:04,484] Trial 448 finished with value: 0.23514151546353554 and parameters: {'C': 0.25, 'gamma': 0.00048828125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:53:10,511] Trial 449 finished with value: 0.24085951062434052 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 350 with value: 0.7013628681027785.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7014\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.720102    0.712450    0.690328    0.730473   \n",
      "1                    TP  345.000000  334.000000  340.000000  336.000000   \n",
      "2                    TN  167.000000  170.000000  173.000000  181.000000   \n",
      "3                    FP   45.000000   60.000000   49.000000   45.000000   \n",
      "4                    FN   38.000000   31.000000   33.000000   33.000000   \n",
      "5              Accuracy    0.860504    0.847059    0.862185    0.868908   \n",
      "6             Precision    0.884615    0.847716    0.874036    0.881890   \n",
      "7           Sensitivity    0.900783    0.915068    0.911528    0.910569   \n",
      "8           Specificity    0.787700    0.739100    0.779300    0.800900   \n",
      "9              F1 score    0.892626    0.880105    0.892388    0.896000   \n",
      "10  F1 score (weighted)    0.859965    0.844835    0.861056    0.868169   \n",
      "11     F1 score (macro)    0.846793    0.834484    0.850400    0.859364   \n",
      "12    Balanced Accuracy    0.844260    0.827099    0.845404    0.855727   \n",
      "13                  MCC    0.693864    0.673557    0.702230    0.719523   \n",
      "14                  NPV    0.814600    0.845800    0.839800    0.845800   \n",
      "15              ROC_AUC    0.844260    0.827099    0.845404    0.855727   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.672656    0.722036    0.693380    0.695743    0.702683  \n",
      "1   337.000000  344.000000  328.000000  331.000000  317.000000  \n",
      "2   169.000000  174.000000  170.000000  163.000000  185.000000  \n",
      "3    62.000000   44.000000   58.000000   57.000000   57.000000  \n",
      "4    27.000000   33.000000   39.000000   44.000000   36.000000  \n",
      "5     0.850420    0.870588    0.836975    0.830252    0.843697  \n",
      "6     0.844612    0.886598    0.849741    0.853093    0.847594  \n",
      "7     0.925824    0.912467    0.893733    0.882667    0.898017  \n",
      "8     0.731600    0.798200    0.745600    0.740900    0.764500  \n",
      "9     0.883355    0.899346    0.871182    0.867628    0.872077  \n",
      "10    0.847721    0.869844    0.835488    0.829114    0.842410  \n",
      "11    0.837462    0.859085    0.824607    0.815547    0.835607  \n",
      "12    0.828713    0.855316    0.819674    0.811788    0.831240  \n",
      "13    0.681693    0.718857    0.651134    0.631997    0.673497  \n",
      "14    0.862200    0.840600    0.813400    0.787400    0.837100  \n",
      "15    0.828713    0.855316    0.819674    0.811788    0.831240  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_svm_8)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_8_cat = np.where((y_pred_svm_8 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_svm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 14:53:18,110] Trial 450 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:53:24,700] Trial 451 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:53:31,257] Trial 452 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:53:37,599] Trial 453 finished with value: 0.34554501279597993 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:53:44,014] Trial 454 finished with value: 0.4858215676582967 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:53:50,588] Trial 455 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:53:58,559] Trial 456 finished with value: 0.017336894417157724 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:54:04,888] Trial 457 finished with value: 0.6763479263277123 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:54:11,430] Trial 458 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:54:17,914] Trial 459 finished with value: 0.5631651255505481 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:54:24,459] Trial 460 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:54:30,978] Trial 461 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:54:37,760] Trial 462 finished with value: -0.005854630571131314 and parameters: {'C': 0.0625, 'gamma': 4.0}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:54:44,107] Trial 463 finished with value: 0.08701065537731277 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:54:50,970] Trial 464 finished with value: 0.2843336345594433 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:54:57,524] Trial 465 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:55:04,062] Trial 466 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:55:10,497] Trial 467 finished with value: 0.645161881236624 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:55:16,505] Trial 468 finished with value: 0.6276314080947542 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:55:23,071] Trial 469 finished with value: 0.2725132120475113 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:55:29,323] Trial 470 finished with value: 0.614979508671072 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:55:35,998] Trial 471 finished with value: 0.6885707831060913 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:55:42,978] Trial 472 finished with value: 0.02179053477259113 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:55:49,548] Trial 473 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:55:56,105] Trial 474 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:56:03,151] Trial 475 finished with value: 0.03129731461456718 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:56:09,977] Trial 476 finished with value: 0.0888533141464212 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:56:16,601] Trial 477 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:56:23,181] Trial 478 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:56:29,226] Trial 479 finished with value: 0.4987557531922713 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:56:35,644] Trial 480 finished with value: 0.2392333104581433 and parameters: {'C': 1.0, 'gamma': 0.0001220703125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:56:42,248] Trial 481 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:56:48,930] Trial 482 finished with value: 0.6885652652318606 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:56:55,038] Trial 483 finished with value: 0.5383528379775353 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:57:01,628] Trial 484 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:57:08,158] Trial 485 finished with value: 0.16282874017571353 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:57:14,458] Trial 486 finished with value: 0.5868516408663017 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:57:21,418] Trial 487 finished with value: 0.04738524172719912 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:57:27,917] Trial 488 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:57:34,154] Trial 489 finished with value: 0.13901580872988978 and parameters: {'C': 0.125, 'gamma': 0.00048828125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:57:40,664] Trial 490 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:57:47,213] Trial 491 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:57:53,754] Trial 492 finished with value: 0.24036725760925437 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:58:00,302] Trial 493 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:58:06,830] Trial 494 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:58:13,056] Trial 495 finished with value: 0.6763479263277123 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:58:19,627] Trial 496 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:58:26,204] Trial 497 finished with value: 0.6934826205153579 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:58:32,682] Trial 498 finished with value: 0.3876956102342143 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 350 with value: 0.7013628681027785.\n",
      "[I 2023-12-20 14:58:40,721] Trial 499 finished with value: 0.01733687429466414 and parameters: {'C': 16.0, 'gamma': 8.0}. Best is trial 350 with value: 0.7013628681027785.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7014\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.720102    0.712450    0.690328    0.730473   \n",
      "1                    TP  345.000000  334.000000  340.000000  336.000000   \n",
      "2                    TN  167.000000  170.000000  173.000000  181.000000   \n",
      "3                    FP   45.000000   60.000000   49.000000   45.000000   \n",
      "4                    FN   38.000000   31.000000   33.000000   33.000000   \n",
      "5              Accuracy    0.860504    0.847059    0.862185    0.868908   \n",
      "6             Precision    0.884615    0.847716    0.874036    0.881890   \n",
      "7           Sensitivity    0.900783    0.915068    0.911528    0.910569   \n",
      "8           Specificity    0.787700    0.739100    0.779300    0.800900   \n",
      "9              F1 score    0.892626    0.880105    0.892388    0.896000   \n",
      "10  F1 score (weighted)    0.859965    0.844835    0.861056    0.868169   \n",
      "11     F1 score (macro)    0.846793    0.834484    0.850400    0.859364   \n",
      "12    Balanced Accuracy    0.844260    0.827099    0.845404    0.855727   \n",
      "13                  MCC    0.693864    0.673557    0.702230    0.719523   \n",
      "14                  NPV    0.814600    0.845800    0.839800    0.845800   \n",
      "15              ROC_AUC    0.844260    0.827099    0.845404    0.855727   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.672656    0.722036    0.693380    0.695743    0.702683    0.718044  \n",
      "1   337.000000  344.000000  328.000000  331.000000  317.000000  339.000000  \n",
      "2   169.000000  174.000000  170.000000  163.000000  185.000000  184.000000  \n",
      "3    62.000000   44.000000   58.000000   57.000000   57.000000   47.000000  \n",
      "4    27.000000   33.000000   39.000000   44.000000   36.000000   25.000000  \n",
      "5     0.850420    0.870588    0.836975    0.830252    0.843697    0.878992  \n",
      "6     0.844612    0.886598    0.849741    0.853093    0.847594    0.878238  \n",
      "7     0.925824    0.912467    0.893733    0.882667    0.898017    0.931319  \n",
      "8     0.731600    0.798200    0.745600    0.740900    0.764500    0.796500  \n",
      "9     0.883355    0.899346    0.871182    0.867628    0.872077    0.904000  \n",
      "10    0.847721    0.869844    0.835488    0.829114    0.842410    0.877741  \n",
      "11    0.837462    0.859085    0.824607    0.815547    0.835607    0.870182  \n",
      "12    0.828713    0.855316    0.819674    0.811788    0.831240    0.863928  \n",
      "13    0.681693    0.718857    0.651134    0.631997    0.673497    0.743079  \n",
      "14    0.862200    0.840600    0.813400    0.787400    0.837100    0.880400  \n",
      "15    0.828713    0.855316    0.819674    0.811788    0.831240    0.863928  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_svm_9)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "y_pred_svm_9_cat = np.where((y_pred_svm_9 >= 6.6), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_svm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7b0e56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7014\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "#plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.713829</td>\n",
       "      <td>0.024078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>166.600000</td>\n",
       "      <td>9.663218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>88.900000</td>\n",
       "      <td>6.773314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>4.301163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>4.040077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.859982</td>\n",
       "      <td>0.025928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.871384</td>\n",
       "      <td>0.024926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.906460</td>\n",
       "      <td>0.024376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.784040</td>\n",
       "      <td>0.034042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.888519</td>\n",
       "      <td>0.023524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.858999</td>\n",
       "      <td>0.026068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.849472</td>\n",
       "      <td>0.026495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.845250</td>\n",
       "      <td>0.026083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.700359</td>\n",
       "      <td>0.052931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.839010</td>\n",
       "      <td>0.034386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.845250</td>\n",
       "      <td>0.026083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.713829     0.024078\n",
       "1                    TP       166.600000     9.663218\n",
       "2                    TN        88.900000     6.773314\n",
       "3                    FP        24.500000     4.301163\n",
       "4                    FN        17.100000     4.040077\n",
       "5              Accuracy         0.859982     0.025928\n",
       "6             Precision         0.871384     0.024926\n",
       "7           Sensitivity         0.906460     0.024376\n",
       "8           Specificity         0.784040     0.034042\n",
       "9              F1 score         0.888519     0.023524\n",
       "10  F1 score (weighted)         0.858999     0.026068\n",
       "11     F1 score (macro)         0.849472     0.026495\n",
       "12    Balanced Accuracy         0.845250     0.026083\n",
       "13                  MCC         0.700359     0.052931\n",
       "14                  NPV         0.839010     0.034386\n",
       "15              ROC_AUC         0.845250     0.026083"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.720102</td>\n",
       "      <td>0.712450</td>\n",
       "      <td>0.690328</td>\n",
       "      <td>0.730473</td>\n",
       "      <td>0.672656</td>\n",
       "      <td>0.722036</td>\n",
       "      <td>0.693380</td>\n",
       "      <td>0.695743</td>\n",
       "      <td>0.702683</td>\n",
       "      <td>0.718044</td>\n",
       "      <td>0.705790</td>\n",
       "      <td>0.017870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>331.000000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>335.100000</td>\n",
       "      <td>8.279157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>173.600000</td>\n",
       "      <td>7.426679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>52.400000</td>\n",
       "      <td>7.026932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>33.900000</td>\n",
       "      <td>5.646041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.860504</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.862185</td>\n",
       "      <td>0.868908</td>\n",
       "      <td>0.850420</td>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.836975</td>\n",
       "      <td>0.830252</td>\n",
       "      <td>0.843697</td>\n",
       "      <td>0.878992</td>\n",
       "      <td>0.854958</td>\n",
       "      <td>0.015787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.847716</td>\n",
       "      <td>0.874036</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>0.844612</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>0.849741</td>\n",
       "      <td>0.853093</td>\n",
       "      <td>0.847594</td>\n",
       "      <td>0.878238</td>\n",
       "      <td>0.864813</td>\n",
       "      <td>0.017591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.900783</td>\n",
       "      <td>0.915068</td>\n",
       "      <td>0.911528</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.925824</td>\n",
       "      <td>0.912467</td>\n",
       "      <td>0.893733</td>\n",
       "      <td>0.882667</td>\n",
       "      <td>0.898017</td>\n",
       "      <td>0.931319</td>\n",
       "      <td>0.908198</td>\n",
       "      <td>0.014709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.787700</td>\n",
       "      <td>0.739100</td>\n",
       "      <td>0.779300</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>0.731600</td>\n",
       "      <td>0.798200</td>\n",
       "      <td>0.745600</td>\n",
       "      <td>0.740900</td>\n",
       "      <td>0.764500</td>\n",
       "      <td>0.796500</td>\n",
       "      <td>0.768430</td>\n",
       "      <td>0.027344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.892626</td>\n",
       "      <td>0.880105</td>\n",
       "      <td>0.892388</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.883355</td>\n",
       "      <td>0.899346</td>\n",
       "      <td>0.871182</td>\n",
       "      <td>0.867628</td>\n",
       "      <td>0.872077</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.885871</td>\n",
       "      <td>0.012821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.859965</td>\n",
       "      <td>0.844835</td>\n",
       "      <td>0.861056</td>\n",
       "      <td>0.868169</td>\n",
       "      <td>0.847721</td>\n",
       "      <td>0.869844</td>\n",
       "      <td>0.835488</td>\n",
       "      <td>0.829114</td>\n",
       "      <td>0.842410</td>\n",
       "      <td>0.877741</td>\n",
       "      <td>0.853634</td>\n",
       "      <td>0.016053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.846793</td>\n",
       "      <td>0.834484</td>\n",
       "      <td>0.850400</td>\n",
       "      <td>0.859364</td>\n",
       "      <td>0.837462</td>\n",
       "      <td>0.859085</td>\n",
       "      <td>0.824607</td>\n",
       "      <td>0.815547</td>\n",
       "      <td>0.835607</td>\n",
       "      <td>0.870182</td>\n",
       "      <td>0.843353</td>\n",
       "      <td>0.016941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.844260</td>\n",
       "      <td>0.827099</td>\n",
       "      <td>0.845404</td>\n",
       "      <td>0.855727</td>\n",
       "      <td>0.828713</td>\n",
       "      <td>0.855316</td>\n",
       "      <td>0.819674</td>\n",
       "      <td>0.811788</td>\n",
       "      <td>0.831240</td>\n",
       "      <td>0.863928</td>\n",
       "      <td>0.838315</td>\n",
       "      <td>0.017169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.693864</td>\n",
       "      <td>0.673557</td>\n",
       "      <td>0.702230</td>\n",
       "      <td>0.719523</td>\n",
       "      <td>0.681693</td>\n",
       "      <td>0.718857</td>\n",
       "      <td>0.651134</td>\n",
       "      <td>0.631997</td>\n",
       "      <td>0.673497</td>\n",
       "      <td>0.743079</td>\n",
       "      <td>0.688943</td>\n",
       "      <td>0.033602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.814600</td>\n",
       "      <td>0.845800</td>\n",
       "      <td>0.839800</td>\n",
       "      <td>0.845800</td>\n",
       "      <td>0.862200</td>\n",
       "      <td>0.840600</td>\n",
       "      <td>0.813400</td>\n",
       "      <td>0.787400</td>\n",
       "      <td>0.837100</td>\n",
       "      <td>0.880400</td>\n",
       "      <td>0.836710</td>\n",
       "      <td>0.026273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.844260</td>\n",
       "      <td>0.827099</td>\n",
       "      <td>0.845404</td>\n",
       "      <td>0.855727</td>\n",
       "      <td>0.828713</td>\n",
       "      <td>0.855316</td>\n",
       "      <td>0.819674</td>\n",
       "      <td>0.811788</td>\n",
       "      <td>0.831240</td>\n",
       "      <td>0.863928</td>\n",
       "      <td>0.838315</td>\n",
       "      <td>0.017169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.720102    0.712450    0.690328    0.730473   \n",
       "1                    TP  345.000000  334.000000  340.000000  336.000000   \n",
       "2                    TN  167.000000  170.000000  173.000000  181.000000   \n",
       "3                    FP   45.000000   60.000000   49.000000   45.000000   \n",
       "4                    FN   38.000000   31.000000   33.000000   33.000000   \n",
       "5              Accuracy    0.860504    0.847059    0.862185    0.868908   \n",
       "6             Precision    0.884615    0.847716    0.874036    0.881890   \n",
       "7           Sensitivity    0.900783    0.915068    0.911528    0.910569   \n",
       "8           Specificity    0.787700    0.739100    0.779300    0.800900   \n",
       "9              F1 score    0.892626    0.880105    0.892388    0.896000   \n",
       "10  F1 score (weighted)    0.859965    0.844835    0.861056    0.868169   \n",
       "11     F1 score (macro)    0.846793    0.834484    0.850400    0.859364   \n",
       "12    Balanced Accuracy    0.844260    0.827099    0.845404    0.855727   \n",
       "13                  MCC    0.693864    0.673557    0.702230    0.719523   \n",
       "14                  NPV    0.814600    0.845800    0.839800    0.845800   \n",
       "15              ROC_AUC    0.844260    0.827099    0.845404    0.855727   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.672656    0.722036    0.693380    0.695743    0.702683    0.718044   \n",
       "1   337.000000  344.000000  328.000000  331.000000  317.000000  339.000000   \n",
       "2   169.000000  174.000000  170.000000  163.000000  185.000000  184.000000   \n",
       "3    62.000000   44.000000   58.000000   57.000000   57.000000   47.000000   \n",
       "4    27.000000   33.000000   39.000000   44.000000   36.000000   25.000000   \n",
       "5     0.850420    0.870588    0.836975    0.830252    0.843697    0.878992   \n",
       "6     0.844612    0.886598    0.849741    0.853093    0.847594    0.878238   \n",
       "7     0.925824    0.912467    0.893733    0.882667    0.898017    0.931319   \n",
       "8     0.731600    0.798200    0.745600    0.740900    0.764500    0.796500   \n",
       "9     0.883355    0.899346    0.871182    0.867628    0.872077    0.904000   \n",
       "10    0.847721    0.869844    0.835488    0.829114    0.842410    0.877741   \n",
       "11    0.837462    0.859085    0.824607    0.815547    0.835607    0.870182   \n",
       "12    0.828713    0.855316    0.819674    0.811788    0.831240    0.863928   \n",
       "13    0.681693    0.718857    0.651134    0.631997    0.673497    0.743079   \n",
       "14    0.862200    0.840600    0.813400    0.787400    0.837100    0.880400   \n",
       "15    0.828713    0.855316    0.819674    0.811788    0.831240    0.863928   \n",
       "\n",
       "           ave       std  \n",
       "0     0.705790  0.017870  \n",
       "1   335.100000  8.279157  \n",
       "2   173.600000  7.426679  \n",
       "3    52.400000  7.026932  \n",
       "4    33.900000  5.646041  \n",
       "5     0.854958  0.015787  \n",
       "6     0.864813  0.017591  \n",
       "7     0.908198  0.014709  \n",
       "8     0.768430  0.027344  \n",
       "9     0.885871  0.012821  \n",
       "10    0.853634  0.016053  \n",
       "11    0.843353  0.016941  \n",
       "12    0.838315  0.017169  \n",
       "13    0.688943  0.033602  \n",
       "14    0.836710  0.026273  \n",
       "15    0.838315  0.017169  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test['std'] = mat_met_svm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_svm0</th>\n",
       "      <th>y_pred_svm1</th>\n",
       "      <th>y_pred_svm2</th>\n",
       "      <th>y_pred_svm3</th>\n",
       "      <th>y_pred_svm4</th>\n",
       "      <th>y_pred_svm_ave</th>\n",
       "      <th>y_pred_svm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4084049</td>\n",
       "      <td>0</td>\n",
       "      <td>6.26</td>\n",
       "      <td>9.219488</td>\n",
       "      <td>9.122708</td>\n",
       "      <td>9.520794</td>\n",
       "      <td>9.181708</td>\n",
       "      <td>9.237545</td>\n",
       "      <td>8.757041</td>\n",
       "      <td>1.123783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL2178343</td>\n",
       "      <td>1</td>\n",
       "      <td>5.96</td>\n",
       "      <td>5.947284</td>\n",
       "      <td>6.075208</td>\n",
       "      <td>5.932128</td>\n",
       "      <td>5.939215</td>\n",
       "      <td>6.001966</td>\n",
       "      <td>5.975967</td>\n",
       "      <td>0.049791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL454672</td>\n",
       "      <td>2</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.532088</td>\n",
       "      <td>6.535762</td>\n",
       "      <td>6.586912</td>\n",
       "      <td>6.467704</td>\n",
       "      <td>6.661123</td>\n",
       "      <td>6.597265</td>\n",
       "      <td>0.108092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4299417</td>\n",
       "      <td>3</td>\n",
       "      <td>8.14</td>\n",
       "      <td>7.290831</td>\n",
       "      <td>7.387670</td>\n",
       "      <td>7.741231</td>\n",
       "      <td>7.824869</td>\n",
       "      <td>7.421946</td>\n",
       "      <td>7.634424</td>\n",
       "      <td>0.296472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3692580</td>\n",
       "      <td>4</td>\n",
       "      <td>5.24</td>\n",
       "      <td>5.860366</td>\n",
       "      <td>5.826391</td>\n",
       "      <td>5.840933</td>\n",
       "      <td>5.966441</td>\n",
       "      <td>6.199734</td>\n",
       "      <td>5.822311</td>\n",
       "      <td>0.289862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>CHEMBL3775269</td>\n",
       "      <td>2966</td>\n",
       "      <td>7.40</td>\n",
       "      <td>6.935708</td>\n",
       "      <td>6.871409</td>\n",
       "      <td>6.671857</td>\n",
       "      <td>6.892574</td>\n",
       "      <td>7.018283</td>\n",
       "      <td>6.964972</td>\n",
       "      <td>0.220930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>CHEMBL3339019</td>\n",
       "      <td>2967</td>\n",
       "      <td>8.07</td>\n",
       "      <td>7.971295</td>\n",
       "      <td>7.873109</td>\n",
       "      <td>7.988693</td>\n",
       "      <td>7.968394</td>\n",
       "      <td>7.985231</td>\n",
       "      <td>7.976120</td>\n",
       "      <td>0.057371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>CHEMBL3771312</td>\n",
       "      <td>2968</td>\n",
       "      <td>6.52</td>\n",
       "      <td>6.000765</td>\n",
       "      <td>5.873185</td>\n",
       "      <td>5.965293</td>\n",
       "      <td>5.988346</td>\n",
       "      <td>5.834441</td>\n",
       "      <td>6.030338</td>\n",
       "      <td>0.227197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>CHEMBL3589701</td>\n",
       "      <td>2969</td>\n",
       "      <td>6.49</td>\n",
       "      <td>6.596601</td>\n",
       "      <td>6.639120</td>\n",
       "      <td>6.586621</td>\n",
       "      <td>6.640874</td>\n",
       "      <td>6.396958</td>\n",
       "      <td>6.558362</td>\n",
       "      <td>0.087859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>CHEMBL4092997</td>\n",
       "      <td>2970</td>\n",
       "      <td>7.47</td>\n",
       "      <td>7.075925</td>\n",
       "      <td>7.114666</td>\n",
       "      <td>7.065059</td>\n",
       "      <td>7.082739</td>\n",
       "      <td>7.120022</td>\n",
       "      <td>7.154735</td>\n",
       "      <td>0.142384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2971 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_svm0  y_pred_svm1  \\\n",
       "0         CHEMBL4084049            0     6.26     9.219488     9.122708   \n",
       "1         CHEMBL2178343            1     5.96     5.947284     6.075208   \n",
       "2          CHEMBL454672            2     6.80     6.532088     6.535762   \n",
       "3         CHEMBL4299417            3     8.14     7.290831     7.387670   \n",
       "4         CHEMBL3692580            4     5.24     5.860366     5.826391   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "2966      CHEMBL3775269         2966     7.40     6.935708     6.871409   \n",
       "2967      CHEMBL3339019         2967     8.07     7.971295     7.873109   \n",
       "2968      CHEMBL3771312         2968     6.52     6.000765     5.873185   \n",
       "2969      CHEMBL3589701         2969     6.49     6.596601     6.639120   \n",
       "2970      CHEMBL4092997         2970     7.47     7.075925     7.114666   \n",
       "\n",
       "      y_pred_svm2  y_pred_svm3  y_pred_svm4  y_pred_svm_ave  y_pred_svm_std  \n",
       "0        9.520794     9.181708     9.237545        8.757041        1.123783  \n",
       "1        5.932128     5.939215     6.001966        5.975967        0.049791  \n",
       "2        6.586912     6.467704     6.661123        6.597265        0.108092  \n",
       "3        7.741231     7.824869     7.421946        7.634424        0.296472  \n",
       "4        5.840933     5.966441     6.199734        5.822311        0.289862  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "2966     6.671857     6.892574     7.018283        6.964972        0.220930  \n",
       "2967     7.988693     7.968394     7.985231        7.976120        0.057371  \n",
       "2968     5.965293     5.988346     5.834441        6.030338        0.227197  \n",
       "2969     6.586621     6.640874     6.396958        6.558362        0.087859  \n",
       "2970     7.065059     7.082739     7.120022        7.154735        0.142384  \n",
       "\n",
       "[2971 rows x 10 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where((y_test>=6.6), 1, 0) \n",
    "        y_pred_optimized_svm_cat = np.where((y_pred_optimized_svm >= 6.6), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_svm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_svm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "svm_run0 = data_svm[['y_test_idx0', 'y_test0', 'y_pred_svm0']]\n",
    "svm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "svm_run0.reset_index(inplace=True, drop=True)\n",
    "svm_run1 = data_svm[['y_test_idx1', 'y_test1', 'y_pred_svm1']]\n",
    "svm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "svm_run1.reset_index(inplace=True, drop=True)\n",
    "svm_run2 = data_svm[['y_test_idx2', 'y_test2', 'y_pred_svm2']]\n",
    "svm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "svm_run2.reset_index(inplace=True, drop=True)\n",
    "svm_run3 = data_svm[['y_test_idx3', 'y_test3', 'y_pred_svm3']]\n",
    "svm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "svm_run3.reset_index(inplace=True, drop=True)\n",
    "svm_run4 = data_svm[['y_test_idx4', 'y_test4', 'y_pred_svm4']]\n",
    "svm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "svm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "svm_5preds = pd.concat([chembl_id, svm_run0, svm_run1, svm_run2, svm_run3, svm_run4], axis=1)\n",
    "svm_5preds = svm_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_svm0', 'y_pred_svm1', 'y_pred_svm2', 'y_pred_svm3', 'y_pred_svm4']]\n",
    "svm_5preds['y_pred_svm_ave'] = svm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "svm_5preds['y_pred_svm_std'] = svm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "svm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2869d8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = 3,12\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'])), (3, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model r2_score 0.6914 with a standard deviation of 0.0222\n",
      "SVM optimized model r2_score 0.7129 with a standard deviation of 0.0207\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVR \n",
    "svm_baseline_CVscore = cross_val_score(svm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"SVM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_svm.joblib']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_reg, \"OUTPUT/svm_reg.joblib\")\n",
    "#joblib.dump(optimized_svm, \"OUTPUT/optimized_svm.joblib\")\n",
    "joblib.dump(optimizedCV_svm, \"OUTPUT/optimizedCV_svm.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f66294e1-0f38-417c-a4cd-08c62b383269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/TestSet_EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_rf_test.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_lgbm_test.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_xgb_test.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_knn_test.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_svm_test.to_excel(writer, sheet_name=\"SVM\", )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9ee06d03-ed96-4b17-8147-bbc6648b7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_optimized_rf.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_optimized_lgbm.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_optimized_xgb.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_optimized_knn.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_optimized_svm.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4d6ff474-48d0-4e5c-bbe1-5be8c018975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/PredResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    rf_5preds.to_excel(writer, sheet_name=\"RF\", )\n",
    "    lgbm_5preds.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    xgb_5preds.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    knn_5preds.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    svm_5preds.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
