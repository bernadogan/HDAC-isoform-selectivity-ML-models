{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/miniforge3/envs/teachopencadd/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "HDAC1and6 = Path(HERE).resolve().parents[1]/'input'\n",
    "output = HERE/'OUTPUT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL2047673</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[799147, 169200, 1015231, 4630274, 193508, 129...</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL2047689</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[38870925, 23625102, 744327, 11863091, 1125019...</td>\n",
       "      <td>-2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3655973</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4856216, 2478511, 10872982, 4181618, 11932100...</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL473270</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[6411541, 10511, 137380, 9391761, 4030911, 163...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3353927</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[6279969, 177484, 2475943, 1255316, 2924166, 1...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL2047673  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "1      CHEMBL2047689  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3655973  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3       CHEMBL473270  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL3353927  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...   \n",
       "4  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  SelectivityWindow  \n",
       "0  [799147, 169200, 1015231, 4630274, 193508, 129...              -0.25  \n",
       "1  [38870925, 23625102, 744327, 11863091, 1125019...              -2.61  \n",
       "2  [4856216, 2478511, 10872982, 4181618, 11932100...               2.42  \n",
       "3  [6411541, 10511, 137380, 9391761, 4030911, 163...               0.00  \n",
       "4  [6279969, 177484, 2475943, 1255316, 2924166, 1...               0.00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC1and6/\"HDAC1and6_SemiSel_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>type_HDAC1</th>\n",
       "      <th>Standard_Value_HDAC1</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "      <th>type_HDAC6</th>\n",
       "      <th>Standard_Value_HDAC6</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "      <th>SelectivityRatio</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4098975</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>109.647820</td>\n",
       "      <td>6.96</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.141254</td>\n",
       "      <td>9.85</td>\n",
       "      <td>776.247117</td>\n",
       "      <td>2.89</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3912061</td>\n",
       "      <td>CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12</td>\n",
       "      <td>IC50</td>\n",
       "      <td>616.595002</td>\n",
       "      <td>6.21</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.169824</td>\n",
       "      <td>9.77</td>\n",
       "      <td>3630.780548</td>\n",
       "      <td>3.56</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4243347</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1.995262</td>\n",
       "      <td>8.70</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.199526</td>\n",
       "      <td>9.70</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4247128</td>\n",
       "      <td>C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>83.176377</td>\n",
       "      <td>7.08</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.251189</td>\n",
       "      <td>9.60</td>\n",
       "      <td>331.131122</td>\n",
       "      <td>2.52</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4126811</td>\n",
       "      <td>CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>436.515832</td>\n",
       "      <td>6.36</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.331131</td>\n",
       "      <td>9.48</td>\n",
       "      <td>1318.256739</td>\n",
       "      <td>3.12</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>CHEMBL4167599</td>\n",
       "      <td>NCCCCNCCCCNCCCN1C(=O)c2ccc3c4c(ccc(c24)C1=O)C(...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>4073.802778</td>\n",
       "      <td>5.39</td>\n",
       "      <td>IC50</td>\n",
       "      <td>50.118723</td>\n",
       "      <td>7.30</td>\n",
       "      <td>81.283052</td>\n",
       "      <td>1.91</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>CHEMBL4282471</td>\n",
       "      <td>CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(NC(=O)CCCCC(=O)NO)...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>3388.441561</td>\n",
       "      <td>5.47</td>\n",
       "      <td>IC50</td>\n",
       "      <td>117.489756</td>\n",
       "      <td>6.93</td>\n",
       "      <td>28.840315</td>\n",
       "      <td>1.46</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>CHEMBL470843</td>\n",
       "      <td>O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO</td>\n",
       "      <td>IC50</td>\n",
       "      <td>6309.573445</td>\n",
       "      <td>5.20</td>\n",
       "      <td>IC50</td>\n",
       "      <td>173.780083</td>\n",
       "      <td>6.76</td>\n",
       "      <td>36.307805</td>\n",
       "      <td>1.56</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>CHEMBL3215861</td>\n",
       "      <td>CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC</td>\n",
       "      <td>Ki</td>\n",
       "      <td>28.183829</td>\n",
       "      <td>7.55</td>\n",
       "      <td>Ki</td>\n",
       "      <td>245.470892</td>\n",
       "      <td>6.61</td>\n",
       "      <td>0.114815</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>CHEMBL3233708</td>\n",
       "      <td>O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>354.813389</td>\n",
       "      <td>6.45</td>\n",
       "      <td>IC50</td>\n",
       "      <td>295.120923</td>\n",
       "      <td>6.53</td>\n",
       "      <td>1.202264</td>\n",
       "      <td>0.08</td>\n",
       "      <td>Non-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1910 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                             smiles  \\\n",
       "0         CHEMBL4098975  O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...   \n",
       "1         CHEMBL3912061     CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12   \n",
       "2         CHEMBL4243347  O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...   \n",
       "3         CHEMBL4247128  C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...   \n",
       "4         CHEMBL4126811  CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...   \n",
       "...                 ...                                                ...   \n",
       "1905      CHEMBL4167599  NCCCCNCCCCNCCCN1C(=O)c2ccc3c4c(ccc(c24)C1=O)C(...   \n",
       "1906      CHEMBL4282471  CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(NC(=O)CCCCC(=O)NO)...   \n",
       "1907       CHEMBL470843          O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO   \n",
       "1908      CHEMBL3215861           CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC   \n",
       "1909      CHEMBL3233708  O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...   \n",
       "\n",
       "     type_HDAC1  Standard_Value_HDAC1  pChEMBL_HDAC1 type_HDAC6  \\\n",
       "0          IC50            109.647820           6.96       IC50   \n",
       "1          IC50            616.595002           6.21       IC50   \n",
       "2          IC50              1.995262           8.70       IC50   \n",
       "3          IC50             83.176377           7.08       IC50   \n",
       "4          IC50            436.515832           6.36       IC50   \n",
       "...         ...                   ...            ...        ...   \n",
       "1905       IC50           4073.802778           5.39       IC50   \n",
       "1906       IC50           3388.441561           5.47       IC50   \n",
       "1907       IC50           6309.573445           5.20       IC50   \n",
       "1908         Ki             28.183829           7.55         Ki   \n",
       "1909       IC50            354.813389           6.45       IC50   \n",
       "\n",
       "      Standard_Value_HDAC6  pChEMBL_HDAC6  SelectivityRatio  \\\n",
       "0                 0.141254           9.85        776.247117   \n",
       "1                 0.169824           9.77       3630.780548   \n",
       "2                 0.199526           9.70         10.000000   \n",
       "3                 0.251189           9.60        331.131122   \n",
       "4                 0.331131           9.48       1318.256739   \n",
       "...                    ...            ...               ...   \n",
       "1905             50.118723           7.30         81.283052   \n",
       "1906            117.489756           6.93         28.840315   \n",
       "1907            173.780083           6.76         36.307805   \n",
       "1908            245.470892           6.61          0.114815   \n",
       "1909            295.120923           6.53          1.202264   \n",
       "\n",
       "      SelectivityWindow            label  \n",
       "0                  2.89  HDAC6-selective  \n",
       "1                  3.56  HDAC6-selective  \n",
       "2                  1.00      Dual-binder  \n",
       "3                  2.52  HDAC6-selective  \n",
       "4                  3.12  HDAC6-selective  \n",
       "...                 ...              ...  \n",
       "1905               1.91   Semi-selective  \n",
       "1906               1.46   Semi-selective  \n",
       "1907               1.56   Semi-selective  \n",
       "1908              -0.94      Dual-binder  \n",
       "1909               0.08       Non-binder  \n",
       "\n",
       "[1910 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC1and6/\"HDAC1and6_SemiSel_dataset.csv\", )\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL2047673</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[799147, 169200, 1015231, 4630274, 193508, 129...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>Non-binder</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL2047689</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[38870925, 23625102, 744327, 11863091, 1125019...</td>\n",
       "      <td>-2.61</td>\n",
       "      <td>HDAC1-selective</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3655973</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4856216, 2478511, 10872982, 4181618, 11932100...</td>\n",
       "      <td>2.42</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL473270</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[6411541, 10511, 137380, 9391761, 4030911, 163...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL2047673  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "1      CHEMBL2047689  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3655973  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3       CHEMBL473270  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  SelectivityWindow  \\\n",
       "0  [799147, 169200, 1015231, 4630274, 193508, 129...              -0.25   \n",
       "1  [38870925, 23625102, 744327, 11863091, 1125019...              -2.61   \n",
       "2  [4856216, 2478511, 10872982, 4181618, 11932100...               2.42   \n",
       "3  [6411541, 10511, 137380, 9391761, 4030911, 163...               0.00   \n",
       "\n",
       "             label  Class  \n",
       "0       Non-binder    4.0  \n",
       "1  HDAC1-selective    0.0  \n",
       "2  HDAC6-selective    0.0  \n",
       "3      Dual-binder    3.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'hDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'hDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'hDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "df.loc[df[df.label == 'Semi-selective'].index, \"Class\"] = 5.0\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for selectivity\n",
    "df[\"selectivity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as selective if SelectivityWindow is >=2 or >=-2, 0 otherwise\n",
    "df.loc[df[df.SelectivityWindow >= 2.0].index, \"selectivity\"] = 1.0\n",
    "df.loc[df[df.SelectivityWindow <= -2.0].index, \"selectivity\"] = 1.0\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "indices =  np.array(df.index)\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y = df[\"SelectivityWindow\"].values\n",
    "Y_cat =  df[\"selectivity\"].values\n",
    "Y_class = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['SelectivityWindow'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_cat\"] = np.array(list(df.iloc[index_tr_all[i]]['selectivity'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['SelectivityWindow'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_cat\"] = np.array(list(df.iloc[index_te_all[i]]['selectivity'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.710687     0.035787\n",
      "1                    TP        16.800000     2.658320\n",
      "2                    TN       154.600000     2.270585\n",
      "3                    FP         3.000000     2.494438\n",
      "4                    FN        16.600000     2.913570\n",
      "5              Accuracy         0.897382     0.018337\n",
      "6             Precision         0.858768     0.109618\n",
      "7           Sensitivity         0.503595     0.084041\n",
      "8           Specificity         0.981010     0.015742\n",
      "9              F1 score         0.629529     0.073354\n",
      "10  F1 score (weighted)         0.886016     0.021266\n",
      "11     F1 score (macro)         0.784964     0.041604\n",
      "12    Balanced Accuracy         0.742296     0.041782\n",
      "13                  MCC         0.605535     0.080284\n",
      "14                  NPV         0.903250     0.015623\n",
      "15              ROC_AUC         0.742296     0.041782\n",
      "CPU times: user 1min 40s, sys: 151 ms, total: 1min 40s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_reg =  RandomForestRegressor(random_state=1121218, max_features = None, n_jobs=8,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_reg.fit(x_train, y_train)\n",
    "        y_pred = rf_reg.predict(x_test)  \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=8, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=8, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 19:13:53,439] A new study created in memory with name: RFRegressor\n",
      "[I 2023-12-11 19:14:50,596] Trial 0 finished with value: 0.7015603894214231 and parameters: {'n_estimators': 561}. Best is trial 0 with value: 0.7015603894214231.\n",
      "[I 2023-12-11 19:15:34,639] Trial 1 finished with value: 0.700516786862845 and parameters: {'n_estimators': 439}. Best is trial 0 with value: 0.7015603894214231.\n",
      "[I 2023-12-11 19:16:26,074] Trial 2 finished with value: 0.7015648239727807 and parameters: {'n_estimators': 515}. Best is trial 2 with value: 0.7015648239727807.\n",
      "[I 2023-12-11 19:17:51,593] Trial 3 finished with value: 0.7014482251705475 and parameters: {'n_estimators': 854}. Best is trial 2 with value: 0.7015648239727807.\n",
      "[I 2023-12-11 19:18:27,231] Trial 4 finished with value: 0.7003452777377122 and parameters: {'n_estimators': 337}. Best is trial 2 with value: 0.7015648239727807.\n",
      "[I 2023-12-11 19:20:14,408] Trial 5 finished with value: 0.7012117254832679 and parameters: {'n_estimators': 994}. Best is trial 2 with value: 0.7015648239727807.\n",
      "[I 2023-12-11 19:21:00,225] Trial 6 finished with value: 0.7006446651392195 and parameters: {'n_estimators': 412}. Best is trial 2 with value: 0.7015648239727807.\n",
      "[I 2023-12-11 19:21:46,493] Trial 7 finished with value: 0.7007614043668025 and parameters: {'n_estimators': 415}. Best is trial 2 with value: 0.7015648239727807.\n",
      "[I 2023-12-11 19:22:59,435] Trial 8 finished with value: 0.7019684553622701 and parameters: {'n_estimators': 681}. Best is trial 8 with value: 0.7019684553622701.\n",
      "[I 2023-12-11 19:24:13,448] Trial 9 finished with value: 0.7019627529538914 and parameters: {'n_estimators': 673}. Best is trial 8 with value: 0.7019684553622701.\n",
      "[I 2023-12-11 19:24:25,472] Trial 10 finished with value: 0.6974687073889484 and parameters: {'n_estimators': 110}. Best is trial 8 with value: 0.7019684553622701.\n",
      "[I 2023-12-11 19:25:45,293] Trial 11 finished with value: 0.7020568280550039 and parameters: {'n_estimators': 727}. Best is trial 11 with value: 0.7020568280550039.\n",
      "[I 2023-12-11 19:27:08,813] Trial 12 finished with value: 0.7020688901609649 and parameters: {'n_estimators': 745}. Best is trial 12 with value: 0.7020688901609649.\n",
      "[I 2023-12-11 19:28:36,846] Trial 13 finished with value: 0.7016990708058212 and parameters: {'n_estimators': 810}. Best is trial 12 with value: 0.7020688901609649.\n",
      "[I 2023-12-11 19:30:00,347] Trial 14 finished with value: 0.7018981565432443 and parameters: {'n_estimators': 779}. Best is trial 12 with value: 0.7020688901609649.\n",
      "[I 2023-12-11 19:31:43,016] Trial 15 finished with value: 0.7012270336211928 and parameters: {'n_estimators': 950}. Best is trial 12 with value: 0.7020688901609649.\n",
      "[I 2023-12-11 19:32:59,357] Trial 16 finished with value: 0.701873192558683 and parameters: {'n_estimators': 687}. Best is trial 12 with value: 0.7020688901609649.\n",
      "[I 2023-12-11 19:34:02,141] Trial 17 finished with value: 0.7015197039746827 and parameters: {'n_estimators': 583}. Best is trial 12 with value: 0.7020688901609649.\n",
      "[I 2023-12-11 19:35:38,421] Trial 18 finished with value: 0.7014386825227612 and parameters: {'n_estimators': 883}. Best is trial 12 with value: 0.7020688901609649.\n",
      "[I 2023-12-11 19:37:00,415] Trial 19 finished with value: 0.7020612964995607 and parameters: {'n_estimators': 724}. Best is trial 12 with value: 0.7020688901609649.\n",
      "[I 2023-12-11 19:37:33,967] Trial 20 finished with value: 0.6997393429684575 and parameters: {'n_estimators': 302}. Best is trial 12 with value: 0.7020688901609649.\n",
      "[I 2023-12-11 19:38:56,160] Trial 21 finished with value: 0.7020688901609649 and parameters: {'n_estimators': 745}. Best is trial 12 with value: 0.7020688901609649.\n",
      "[I 2023-12-11 19:40:03,855] Trial 22 finished with value: 0.702082526944348 and parameters: {'n_estimators': 629}. Best is trial 22 with value: 0.702082526944348.\n",
      "[I 2023-12-11 19:41:12,456] Trial 23 finished with value: 0.7021168944389337 and parameters: {'n_estimators': 620}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:42:17,479] Trial 24 finished with value: 0.7016762944855885 and parameters: {'n_estimators': 595}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:43:25,174] Trial 25 finished with value: 0.7021065296866708 and parameters: {'n_estimators': 624}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:44:20,726] Trial 26 finished with value: 0.7014132239092364 and parameters: {'n_estimators': 508}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:45:30,198] Trial 27 finished with value: 0.7020056000406287 and parameters: {'n_estimators': 626}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:46:25,071] Trial 28 finished with value: 0.7009523046135901 and parameters: {'n_estimators': 480}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:47:28,388] Trial 29 finished with value: 0.7015652853256465 and parameters: {'n_estimators': 566}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:48:39,427] Trial 30 finished with value: 0.7019830312107136 and parameters: {'n_estimators': 638}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:50:08,589] Trial 31 finished with value: 0.7016744938705657 and parameters: {'n_estimators': 812}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:51:15,753] Trial 32 finished with value: 0.7021113295863352 and parameters: {'n_estimators': 623}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:52:15,046] Trial 33 finished with value: 0.7015647723160251 and parameters: {'n_estimators': 542}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:53:06,449] Trial 34 finished with value: 0.7007528902622153 and parameters: {'n_estimators': 459}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:54:14,137] Trial 35 finished with value: 0.702075197583176 and parameters: {'n_estimators': 630}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:55:11,494] Trial 36 finished with value: 0.7015501744431686 and parameters: {'n_estimators': 528}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:55:51,617] Trial 37 finished with value: 0.7005914013573356 and parameters: {'n_estimators': 360}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:56:20,870] Trial 38 finished with value: 0.7004520352130308 and parameters: {'n_estimators': 258}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:57:27,354] Trial 39 finished with value: 0.7017607451810035 and parameters: {'n_estimators': 605}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:58:44,281] Trial 40 finished with value: 0.7019631256328485 and parameters: {'n_estimators': 680}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 19:59:50,520] Trial 41 finished with value: 0.7020443030255494 and parameters: {'n_estimators': 628}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:00:59,823] Trial 42 finished with value: 0.7020019503836834 and parameters: {'n_estimators': 642}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:01:59,990] Trial 43 finished with value: 0.7015603894214231 and parameters: {'n_estimators': 561}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:02:56,702] Trial 44 finished with value: 0.7014562641968194 and parameters: {'n_estimators': 510}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:04:15,076] Trial 45 finished with value: 0.7019392777742828 and parameters: {'n_estimators': 698}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:05:02,327] Trial 46 finished with value: 0.7005521471983721 and parameters: {'n_estimators': 435}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:06:27,635] Trial 47 finished with value: 0.7018759374291841 and parameters: {'n_estimators': 769}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:07:41,474] Trial 48 finished with value: 0.7020050005256218 and parameters: {'n_estimators': 646}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:09:17,842] Trial 49 finished with value: 0.7014321421837689 and parameters: {'n_estimators': 891}. Best is trial 23 with value: 0.7021168944389337.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.7021\n",
      "\tBest params:\n",
      "\t\tn_estimators: 620\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFRegressor\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.648886\n",
      "1                    TP   29.000000\n",
      "2                    TN  313.000000\n",
      "3                    FP    3.000000\n",
      "4                    FN   37.000000\n",
      "5              Accuracy    0.895288\n",
      "6             Precision    0.906250\n",
      "7           Sensitivity    0.439394\n",
      "8           Specificity    0.990500\n",
      "9              F1 score    0.591837\n",
      "10  F1 score (weighted)    0.879796\n",
      "11     F1 score (macro)    0.765888\n",
      "12    Balanced Accuracy    0.714950\n",
      "13                  MCC    0.586643\n",
      "14                  NPV    0.894300\n",
      "15              ROC_AUC    0.714950\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_rf_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet0 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_0_cat = np.where(((y_pred_rf_0 >= 2) | (y_pred_rf_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_rf_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 20:10:31,160] Trial 50 finished with value: 0.690526950908841 and parameters: {'n_estimators': 586}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:11:56,363] Trial 51 finished with value: 0.690140787534421 and parameters: {'n_estimators': 745}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:13:17,118] Trial 52 finished with value: 0.6901397095686598 and parameters: {'n_estimators': 701}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:14:52,892] Trial 53 finished with value: 0.6904101677138135 and parameters: {'n_estimators': 820}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:16:11,961] Trial 54 finished with value: 0.6901166826082387 and parameters: {'n_estimators': 668}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:17:40,164] Trial 55 finished with value: 0.6903109911207548 and parameters: {'n_estimators': 770}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:19:04,219] Trial 56 finished with value: 0.6901582848228033 and parameters: {'n_estimators': 721}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:20:12,057] Trial 57 finished with value: 0.6903378487050154 and parameters: {'n_estimators': 601}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:21:29,453] Trial 58 finished with value: 0.6902962007305832 and parameters: {'n_estimators': 659}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:23:08,532] Trial 59 finished with value: 0.6904716486751922 and parameters: {'n_estimators': 856}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:24:12,039] Trial 60 finished with value: 0.6900071853024331 and parameters: {'n_estimators': 554}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:25:35,494] Trial 61 finished with value: 0.6900668534932963 and parameters: {'n_estimators': 726}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:27:03,238] Trial 62 finished with value: 0.690140272840889 and parameters: {'n_estimators': 758}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:28:15,371] Trial 63 finished with value: 0.6905237104345134 and parameters: {'n_estimators': 614}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:29:49,700] Trial 64 finished with value: 0.6903033158229438 and parameters: {'n_estimators': 795}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:30:44,375] Trial 65 finished with value: 0.6892996412680868 and parameters: {'n_estimators': 484}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:32:31,616] Trial 66 finished with value: 0.6908032378041252 and parameters: {'n_estimators': 931}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:33:52,551] Trial 67 finished with value: 0.6901168308217362 and parameters: {'n_estimators': 700}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:35:28,898] Trial 68 finished with value: 0.6904070194408499 and parameters: {'n_estimators': 834}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:36:37,153] Trial 69 finished with value: 0.6903947894447278 and parameters: {'n_estimators': 581}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:37:51,927] Trial 70 finished with value: 0.6903290139510807 and parameters: {'n_estimators': 663}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:39:16,308] Trial 71 finished with value: 0.690117732225962 and parameters: {'n_estimators': 733}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:40:39,579] Trial 72 finished with value: 0.6901731372511544 and parameters: {'n_estimators': 711}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:41:51,556] Trial 73 finished with value: 0.6907026964296819 and parameters: {'n_estimators': 623}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:43:24,658] Trial 74 finished with value: 0.6901975055031259 and parameters: {'n_estimators': 793}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:44:42,524] Trial 75 finished with value: 0.6899421896440391 and parameters: {'n_estimators': 677}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:45:43,842] Trial 76 finished with value: 0.6897714998418832 and parameters: {'n_estimators': 529}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:47:10,163] Trial 77 finished with value: 0.690140787534421 and parameters: {'n_estimators': 745}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:48:25,069] Trial 78 finished with value: 0.690697443147802 and parameters: {'n_estimators': 641}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:49:31,635] Trial 79 finished with value: 0.6903147753447151 and parameters: {'n_estimators': 578}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:50:41,499] Trial 80 finished with value: 0.6903475554310313 and parameters: {'n_estimators': 600}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:50:57,591] Trial 81 finished with value: 0.684558369064102 and parameters: {'n_estimators': 137}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:52:17,359] Trial 82 finished with value: 0.6900661113323477 and parameters: {'n_estimators': 689}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:53:34,119] Trial 83 finished with value: 0.6905981717684481 and parameters: {'n_estimators': 649}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:55:04,660] Trial 84 finished with value: 0.6902469740447964 and parameters: {'n_estimators': 776}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:56:31,945] Trial 85 finished with value: 0.6901840737019832 and parameters: {'n_estimators': 742}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:57:40,089] Trial 86 finished with value: 0.6905946617428464 and parameters: {'n_estimators': 628}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 20:58:44,185] Trial 87 finished with value: 0.6898228100616903 and parameters: {'n_estimators': 545}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:00:02,211] Trial 88 finished with value: 0.6899421896440391 and parameters: {'n_estimators': 677}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:01:42,930] Trial 89 finished with value: 0.6904711235919263 and parameters: {'n_estimators': 851}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:03:02,901] Trial 90 finished with value: 0.6901055590286893 and parameters: {'n_estimators': 713}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:04:15,231] Trial 91 finished with value: 0.6907026964296819 and parameters: {'n_estimators': 623}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:05:32,259] Trial 92 finished with value: 0.6901669840194469 and parameters: {'n_estimators': 665}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:06:42,298] Trial 93 finished with value: 0.6903406591310913 and parameters: {'n_estimators': 608}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:07:47,790] Trial 94 finished with value: 0.6902137322496389 and parameters: {'n_estimators': 570}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:09:16,492] Trial 95 finished with value: 0.690254240576411 and parameters: {'n_estimators': 760}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:10:30,686] Trial 96 finished with value: 0.6906681013035532 and parameters: {'n_estimators': 643}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:11:48,068] Trial 97 finished with value: 0.6900661113323477 and parameters: {'n_estimators': 689}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:12:45,157] Trial 98 finished with value: 0.6894879071413269 and parameters: {'n_estimators': 495}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:14:15,350] Trial 99 finished with value: 0.6904409838446919 and parameters: {'n_estimators': 801}. Best is trial 23 with value: 0.7021168944389337.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7021\n",
      "\tBest params:\n",
      "\t\tn_estimators: 620\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.648886    0.676902\n",
      "1                    TP   29.000000   35.000000\n",
      "2                    TN  313.000000  309.000000\n",
      "3                    FP    3.000000    6.000000\n",
      "4                    FN   37.000000   32.000000\n",
      "5              Accuracy    0.895288    0.900524\n",
      "6             Precision    0.906250    0.853659\n",
      "7           Sensitivity    0.439394    0.522388\n",
      "8           Specificity    0.990500    0.981000\n",
      "9              F1 score    0.591837    0.648148\n",
      "10  F1 score (weighted)    0.879796    0.890521\n",
      "11     F1 score (macro)    0.765888    0.795111\n",
      "12    Balanced Accuracy    0.714950    0.751670\n",
      "13                  MCC    0.586643    0.618423\n",
      "14                  NPV    0.894300    0.906200\n",
      "15              ROC_AUC    0.714950    0.751670\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_rf_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_1_cat = np.where(((y_pred_rf_1 >= 2) | (y_pred_rf_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_rf_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 21:15:44,825] Trial 100 finished with value: 0.6948369324447503 and parameters: {'n_estimators': 714}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:16:54,787] Trial 101 finished with value: 0.694768154735337 and parameters: {'n_estimators': 619}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:18:10,339] Trial 102 finished with value: 0.6947906553599539 and parameters: {'n_estimators': 660}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:19:16,263] Trial 103 finished with value: 0.6945344566310204 and parameters: {'n_estimators': 587}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:20:15,788] Trial 104 finished with value: 0.6948841139027953 and parameters: {'n_estimators': 535}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:21:18,216] Trial 105 finished with value: 0.6945558791471873 and parameters: {'n_estimators': 559}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:22:40,917] Trial 106 finished with value: 0.6949410430413292 and parameters: {'n_estimators': 727}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:23:58,287] Trial 107 finished with value: 0.6948709726004056 and parameters: {'n_estimators': 698}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:25:11,705] Trial 108 finished with value: 0.6947247958596884 and parameters: {'n_estimators': 646}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:26:24,802] Trial 109 finished with value: 0.6947119363199075 and parameters: {'n_estimators': 629}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:27:32,201] Trial 110 finished with value: 0.6943779204521452 and parameters: {'n_estimators': 593}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:28:59,563] Trial 111 finished with value: 0.6948006104655026 and parameters: {'n_estimators': 753}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:30:14,830] Trial 112 finished with value: 0.6947070408609582 and parameters: {'n_estimators': 656}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:31:23,129] Trial 113 finished with value: 0.6948004903560918 and parameters: {'n_estimators': 610}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:32:40,915] Trial 114 finished with value: 0.6951446160149604 and parameters: {'n_estimators': 679}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:33:43,660] Trial 115 finished with value: 0.6946595853611394 and parameters: {'n_estimators': 572}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:35:06,445] Trial 116 finished with value: 0.6950564219825047 and parameters: {'n_estimators': 732}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:36:36,642] Trial 117 finished with value: 0.6950961314281063 and parameters: {'n_estimators': 783}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:37:46,354] Trial 118 finished with value: 0.694757530511234 and parameters: {'n_estimators': 630}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:38:58,013] Trial 119 finished with value: 0.6948269964202456 and parameters: {'n_estimators': 710}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:40:21,656] Trial 120 finished with value: 0.695446922265319 and parameters: {'n_estimators': 818}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:41:28,002] Trial 121 finished with value: 0.6947467088944956 and parameters: {'n_estimators': 643}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:42:29,638] Trial 122 finished with value: 0.6944636003860738 and parameters: {'n_estimators': 597}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:43:38,194] Trial 123 finished with value: 0.6951178933477913 and parameters: {'n_estimators': 674}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:44:40,162] Trial 124 finished with value: 0.6947358112073324 and parameters: {'n_estimators': 609}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:45:51,666] Trial 125 finished with value: 0.6949255953533073 and parameters: {'n_estimators': 694}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:46:59,599] Trial 126 finished with value: 0.6948402919350654 and parameters: {'n_estimators': 661}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:48:05,591] Trial 127 finished with value: 0.6948877610798037 and parameters: {'n_estimators': 638}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:49:01,535] Trial 128 finished with value: 0.6946697515927915 and parameters: {'n_estimators': 552}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:50:17,859] Trial 129 finished with value: 0.6949866984027404 and parameters: {'n_estimators': 741}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:51:21,139] Trial 130 finished with value: 0.6947590822176412 and parameters: {'n_estimators': 620}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:52:25,959] Trial 131 finished with value: 0.6947769259143791 and parameters: {'n_estimators': 648}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:53:25,514] Trial 132 finished with value: 0.6945344566310204 and parameters: {'n_estimators': 587}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:54:34,199] Trial 133 finished with value: 0.6950036227166905 and parameters: {'n_estimators': 666}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:55:43,305] Trial 134 finished with value: 0.6950017830314182 and parameters: {'n_estimators': 690}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:56:48,185] Trial 135 finished with value: 0.694810057241912 and parameters: {'n_estimators': 631}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:58:01,969] Trial 136 finished with value: 0.6948031455612533 and parameters: {'n_estimators': 715}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 21:59:04,413] Trial 137 finished with value: 0.6946024097431188 and parameters: {'n_estimators': 604}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 22:00:21,685] Trial 138 finished with value: 0.6948986312931372 and parameters: {'n_estimators': 767}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 22:01:20,197] Trial 139 finished with value: 0.6946255036090165 and parameters: {'n_estimators': 573}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 22:02:28,695] Trial 140 finished with value: 0.695058433134967 and parameters: {'n_estimators': 683}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 22:03:35,854] Trial 141 finished with value: 0.6947237093738068 and parameters: {'n_estimators': 649}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 22:04:47,553] Trial 142 finished with value: 0.694900588502444 and parameters: {'n_estimators': 700}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 22:05:55,315] Trial 143 finished with value: 0.6950036227166905 and parameters: {'n_estimators': 666}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 22:07:09,831] Trial 144 finished with value: 0.6949410430413292 and parameters: {'n_estimators': 727}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 22:08:12,852] Trial 145 finished with value: 0.6947704550534605 and parameters: {'n_estimators': 621}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 22:09:27,969] Trial 146 finished with value: 0.6948725716867152 and parameters: {'n_estimators': 633}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 22:11:01,214] Trial 147 finished with value: 0.6948006104655026 and parameters: {'n_estimators': 753}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 22:12:24,749] Trial 148 finished with value: 0.695136107474098 and parameters: {'n_estimators': 677}. Best is trial 23 with value: 0.7021168944389337.\n",
      "[I 2023-12-11 22:13:38,899] Trial 149 finished with value: 0.6946166375422058 and parameters: {'n_estimators': 603}. Best is trial 23 with value: 0.7021168944389337.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7021\n",
      "\tBest params:\n",
      "\t\tn_estimators: 620\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.648886    0.676902    0.673941\n",
      "1                    TP   29.000000   35.000000   34.000000\n",
      "2                    TN  313.000000  309.000000  311.000000\n",
      "3                    FP    3.000000    6.000000    4.000000\n",
      "4                    FN   37.000000   32.000000   33.000000\n",
      "5              Accuracy    0.895288    0.900524    0.903141\n",
      "6             Precision    0.906250    0.853659    0.894737\n",
      "7           Sensitivity    0.439394    0.522388    0.507463\n",
      "8           Specificity    0.990500    0.981000    0.987300\n",
      "9              F1 score    0.591837    0.648148    0.647619\n",
      "10  F1 score (weighted)    0.879796    0.890521    0.891897\n",
      "11     F1 score (macro)    0.765888    0.795111    0.795737\n",
      "12    Balanced Accuracy    0.714950    0.751670    0.747382\n",
      "13                  MCC    0.586643    0.618423    0.628666\n",
      "14                  NPV    0.894300    0.906200    0.904100\n",
      "15              ROC_AUC    0.714950    0.751670    0.747382\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_rf_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_2_cat = np.where(((y_pred_rf_2 >= 2) | (y_pred_rf_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_rf_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 22:14:50,553] Trial 150 finished with value: 0.703286046513856 and parameters: {'n_estimators': 650}. Best is trial 150 with value: 0.703286046513856.\n",
      "[I 2023-12-11 22:15:55,824] Trial 151 finished with value: 0.7033095747526031 and parameters: {'n_estimators': 646}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:17:00,732] Trial 152 finished with value: 0.7032271810721701 and parameters: {'n_estimators': 648}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:18:05,423] Trial 153 finished with value: 0.703178440878504 and parameters: {'n_estimators': 657}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:19:10,399] Trial 154 finished with value: 0.7031903414032261 and parameters: {'n_estimators': 655}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:20:15,863] Trial 155 finished with value: 0.7032778650401459 and parameters: {'n_estimators': 653}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:21:21,671] Trial 156 finished with value: 0.7031903414032261 and parameters: {'n_estimators': 655}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:22:27,798] Trial 157 finished with value: 0.7031621647545132 and parameters: {'n_estimators': 660}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:23:33,084] Trial 158 finished with value: 0.703192881873209 and parameters: {'n_estimators': 658}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:24:39,550] Trial 159 finished with value: 0.703178440878504 and parameters: {'n_estimators': 657}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:25:46,130] Trial 160 finished with value: 0.7032135857874676 and parameters: {'n_estimators': 656}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:26:52,010] Trial 161 finished with value: 0.7031903414032261 and parameters: {'n_estimators': 655}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:27:58,190] Trial 162 finished with value: 0.7031926391230835 and parameters: {'n_estimators': 654}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:29:03,935] Trial 163 finished with value: 0.7031926391230835 and parameters: {'n_estimators': 654}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:30:10,688] Trial 164 finished with value: 0.7031903414032261 and parameters: {'n_estimators': 655}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:31:15,575] Trial 165 finished with value: 0.7031926391230835 and parameters: {'n_estimators': 654}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:32:22,775] Trial 166 finished with value: 0.7031800375859796 and parameters: {'n_estimators': 659}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:33:27,920] Trial 167 finished with value: 0.7032778650401459 and parameters: {'n_estimators': 653}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:34:33,841] Trial 168 finished with value: 0.7032135857874676 and parameters: {'n_estimators': 656}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:35:43,308] Trial 169 finished with value: 0.7031515092778197 and parameters: {'n_estimators': 681}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:36:48,343] Trial 170 finished with value: 0.7032271810721701 and parameters: {'n_estimators': 648}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:37:53,667] Trial 171 finished with value: 0.7032784834896194 and parameters: {'n_estimators': 649}. Best is trial 151 with value: 0.7033095747526031.\n",
      "[I 2023-12-11 22:38:58,488] Trial 172 finished with value: 0.7033670794242666 and parameters: {'n_estimators': 642}. Best is trial 172 with value: 0.7033670794242666.\n",
      "[I 2023-12-11 22:40:03,578] Trial 173 finished with value: 0.7034047975694616 and parameters: {'n_estimators': 641}. Best is trial 173 with value: 0.7034047975694616.\n",
      "[I 2023-12-11 22:41:07,420] Trial 174 finished with value: 0.703460680911254 and parameters: {'n_estimators': 638}. Best is trial 174 with value: 0.703460680911254.\n",
      "[I 2023-12-11 22:42:12,437] Trial 175 finished with value: 0.7034047975694616 and parameters: {'n_estimators': 641}. Best is trial 174 with value: 0.703460680911254.\n",
      "[I 2023-12-11 22:43:16,411] Trial 176 finished with value: 0.7034329145666975 and parameters: {'n_estimators': 634}. Best is trial 174 with value: 0.703460680911254.\n",
      "[I 2023-12-11 22:44:21,226] Trial 177 finished with value: 0.7035025107075026 and parameters: {'n_estimators': 635}. Best is trial 177 with value: 0.7035025107075026.\n",
      "[I 2023-12-11 22:45:22,893] Trial 178 finished with value: 0.7031241643561887 and parameters: {'n_estimators': 617}. Best is trial 177 with value: 0.7035025107075026.\n",
      "[I 2023-12-11 22:46:26,697] Trial 179 finished with value: 0.703533233688111 and parameters: {'n_estimators': 637}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 22:47:29,882] Trial 180 finished with value: 0.7035025107075026 and parameters: {'n_estimators': 635}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 22:48:34,154] Trial 181 finished with value: 0.7034025096344291 and parameters: {'n_estimators': 632}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 22:49:39,653] Trial 182 finished with value: 0.7034481196523046 and parameters: {'n_estimators': 633}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 22:50:43,997] Trial 183 finished with value: 0.7035071851542953 and parameters: {'n_estimators': 636}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 22:51:48,219] Trial 184 finished with value: 0.7034329145666975 and parameters: {'n_estimators': 634}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 22:52:51,697] Trial 185 finished with value: 0.7034329145666975 and parameters: {'n_estimators': 634}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 22:53:52,767] Trial 186 finished with value: 0.7030998960799157 and parameters: {'n_estimators': 612}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 22:54:55,535] Trial 187 finished with value: 0.703246375186461 and parameters: {'n_estimators': 628}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 22:55:54,952] Trial 188 finished with value: 0.7032042618775638 and parameters: {'n_estimators': 591}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 22:56:58,504] Trial 189 finished with value: 0.703300219548505 and parameters: {'n_estimators': 630}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 22:58:00,919] Trial 190 finished with value: 0.703239463879311 and parameters: {'n_estimators': 609}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 22:59:04,203] Trial 191 finished with value: 0.703300219548505 and parameters: {'n_estimators': 630}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:00:07,002] Trial 192 finished with value: 0.7033484410259927 and parameters: {'n_estimators': 631}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:01:10,730] Trial 193 finished with value: 0.7034481196523046 and parameters: {'n_estimators': 633}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:02:13,960] Trial 194 finished with value: 0.7034329145666975 and parameters: {'n_estimators': 634}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:03:18,535] Trial 195 finished with value: 0.7034025096344291 and parameters: {'n_estimators': 632}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:04:18,410] Trial 196 finished with value: 0.7032685269177482 and parameters: {'n_estimators': 595}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:05:21,698] Trial 197 finished with value: 0.7032489408428323 and parameters: {'n_estimators': 627}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:06:23,311] Trial 198 finished with value: 0.7031609169357866 and parameters: {'n_estimators': 616}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:07:21,646] Trial 199 finished with value: 0.7033666114937106 and parameters: {'n_estimators': 580}. Best is trial 179 with value: 0.703533233688111.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7035\n",
      "\tBest params:\n",
      "\t\tn_estimators: 637\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.648886    0.676902    0.673941    0.680771\n",
      "1                    TP   29.000000   35.000000   34.000000   31.000000\n",
      "2                    TN  313.000000  309.000000  311.000000  312.000000\n",
      "3                    FP    3.000000    6.000000    4.000000    2.000000\n",
      "4                    FN   37.000000   32.000000   33.000000   37.000000\n",
      "5              Accuracy    0.895288    0.900524    0.903141    0.897906\n",
      "6             Precision    0.906250    0.853659    0.894737    0.939394\n",
      "7           Sensitivity    0.439394    0.522388    0.507463    0.455882\n",
      "8           Specificity    0.990500    0.981000    0.987300    0.993600\n",
      "9              F1 score    0.591837    0.648148    0.647619    0.613861\n",
      "10  F1 score (weighted)    0.879796    0.890521    0.891897    0.882911\n",
      "11     F1 score (macro)    0.765888    0.795111    0.795737    0.777519\n",
      "12    Balanced Accuracy    0.714950    0.751670    0.747382    0.724756\n",
      "13                  MCC    0.586643    0.618423    0.628666    0.612057\n",
      "14                  NPV    0.894300    0.906200    0.904100    0.894000\n",
      "15              ROC_AUC    0.714950    0.751670    0.747382    0.724756\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_rf_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_3_cat = np.where(((y_pred_rf_3 >= 2) | (y_pred_rf_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_rf_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:08:24,935] Trial 200 finished with value: 0.6879162641927911 and parameters: {'n_estimators': 572}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:09:27,520] Trial 201 finished with value: 0.6879936737932772 and parameters: {'n_estimators': 628}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:10:26,165] Trial 202 finished with value: 0.6878842866040913 and parameters: {'n_estimators': 597}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:11:28,265] Trial 203 finished with value: 0.6880312789442881 and parameters: {'n_estimators': 631}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:12:28,614] Trial 204 finished with value: 0.6879512920771987 and parameters: {'n_estimators': 606}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:13:28,868] Trial 205 finished with value: 0.6879369796306444 and parameters: {'n_estimators': 633}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:14:29,890] Trial 206 finished with value: 0.6878687841031008 and parameters: {'n_estimators': 617}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:15:26,900] Trial 207 finished with value: 0.6877278372749343 and parameters: {'n_estimators': 587}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:16:29,740] Trial 208 finished with value: 0.6879369796306444 and parameters: {'n_estimators': 633}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:17:30,015] Trial 209 finished with value: 0.6878933696171329 and parameters: {'n_estimators': 611}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:18:38,494] Trial 210 finished with value: 0.6884296848628615 and parameters: {'n_estimators': 678}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:19:39,777] Trial 211 finished with value: 0.6880081794318343 and parameters: {'n_estimators': 635}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:20:43,695] Trial 212 finished with value: 0.6879369796306444 and parameters: {'n_estimators': 633}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:21:51,435] Trial 213 finished with value: 0.6884099501506441 and parameters: {'n_estimators': 677}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:22:49,715] Trial 214 finished with value: 0.6877882492198772 and parameters: {'n_estimators': 588}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:23:53,307] Trial 215 finished with value: 0.6880081794318343 and parameters: {'n_estimators': 635}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:24:53,389] Trial 216 finished with value: 0.6878258627911654 and parameters: {'n_estimators': 613}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:25:51,792] Trial 217 finished with value: 0.6878910588367004 and parameters: {'n_estimators': 600}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:26:53,930] Trial 218 finished with value: 0.6879369796306444 and parameters: {'n_estimators': 633}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:28:00,358] Trial 219 finished with value: 0.6883627889078585 and parameters: {'n_estimators': 676}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:28:56,774] Trial 220 finished with value: 0.6878830178043661 and parameters: {'n_estimators': 569}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:29:59,116] Trial 221 finished with value: 0.6880307223504517 and parameters: {'n_estimators': 636}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:31:02,610] Trial 222 finished with value: 0.6881403870604896 and parameters: {'n_estimators': 639}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:32:04,920] Trial 223 finished with value: 0.687882935634889 and parameters: {'n_estimators': 619}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:33:05,835] Trial 224 finished with value: 0.6878579150014771 and parameters: {'n_estimators': 614}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:34:44,318] Trial 225 finished with value: 0.6889476492713796 and parameters: {'n_estimators': 997}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:35:46,767] Trial 226 finished with value: 0.6881547798698294 and parameters: {'n_estimators': 640}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:36:52,552] Trial 227 finished with value: 0.6883405614737331 and parameters: {'n_estimators': 674}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:37:53,349] Trial 228 finished with value: 0.6879020437259704 and parameters: {'n_estimators': 604}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:38:53,712] Trial 229 finished with value: 0.6878710780187165 and parameters: {'n_estimators': 620}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:40:01,620] Trial 230 finished with value: 0.688175303507079 and parameters: {'n_estimators': 690}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:41:03,292] Trial 231 finished with value: 0.6882110435264776 and parameters: {'n_estimators': 642}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:41:22,982] Trial 232 finished with value: 0.6875162113813472 and parameters: {'n_estimators': 192}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:42:23,686] Trial 233 finished with value: 0.6881689607687564 and parameters: {'n_estimators': 641}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:43:27,768] Trial 234 finished with value: 0.6882708744166127 and parameters: {'n_estimators': 666}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:44:28,185] Trial 235 finished with value: 0.6880557889718854 and parameters: {'n_estimators': 637}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:45:27,709] Trial 236 finished with value: 0.6878784892584873 and parameters: {'n_estimators': 622}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:46:32,284] Trial 237 finished with value: 0.6883812214755647 and parameters: {'n_estimators': 670}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:47:29,595] Trial 238 finished with value: 0.6878568402299733 and parameters: {'n_estimators': 592}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:48:31,930] Trial 239 finished with value: 0.68818227783813 and parameters: {'n_estimators': 643}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:49:32,577] Trial 240 finished with value: 0.6878829356348889 and parameters: {'n_estimators': 619}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:50:34,529] Trial 241 finished with value: 0.6881116686391233 and parameters: {'n_estimators': 645}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:51:38,066] Trial 242 finished with value: 0.6883546162277184 and parameters: {'n_estimators': 668}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:52:58,739] Trial 243 finished with value: 0.688193961517255 and parameters: {'n_estimators': 644}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:54:16,332] Trial 244 finished with value: 0.6878599186605633 and parameters: {'n_estimators': 618}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:55:40,941] Trial 245 finished with value: 0.6883812214755647 and parameters: {'n_estimators': 670}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:57:02,983] Trial 246 finished with value: 0.6881648436942882 and parameters: {'n_estimators': 649}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:58:17,727] Trial 247 finished with value: 0.6880062718278215 and parameters: {'n_estimators': 627}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-11 23:59:15,121] Trial 248 finished with value: 0.6879615579631969 and parameters: {'n_estimators': 609}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:00:21,753] Trial 249 finished with value: 0.6881353788425129 and parameters: {'n_estimators': 692}. Best is trial 179 with value: 0.703533233688111.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7035\n",
      "\tBest params:\n",
      "\t\tn_estimators: 637\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77894dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.648886    0.676902    0.673941    0.680771   \n",
      "1                    TP   29.000000   35.000000   34.000000   31.000000   \n",
      "2                    TN  313.000000  309.000000  311.000000  312.000000   \n",
      "3                    FP    3.000000    6.000000    4.000000    2.000000   \n",
      "4                    FN   37.000000   32.000000   33.000000   37.000000   \n",
      "5              Accuracy    0.895288    0.900524    0.903141    0.897906   \n",
      "6             Precision    0.906250    0.853659    0.894737    0.939394   \n",
      "7           Sensitivity    0.439394    0.522388    0.507463    0.455882   \n",
      "8           Specificity    0.990500    0.981000    0.987300    0.993600   \n",
      "9              F1 score    0.591837    0.648148    0.647619    0.613861   \n",
      "10  F1 score (weighted)    0.879796    0.890521    0.891897    0.882911   \n",
      "11     F1 score (macro)    0.765888    0.795111    0.795737    0.777519   \n",
      "12    Balanced Accuracy    0.714950    0.751670    0.747382    0.724756   \n",
      "13                  MCC    0.586643    0.618423    0.628666    0.612057   \n",
      "14                  NPV    0.894300    0.906200    0.904100    0.894000   \n",
      "15              ROC_AUC    0.714950    0.751670    0.747382    0.724756   \n",
      "\n",
      "          Set4  \n",
      "0     0.707861  \n",
      "1    32.000000  \n",
      "2   313.000000  \n",
      "3     3.000000  \n",
      "4    34.000000  \n",
      "5     0.903141  \n",
      "6     0.914286  \n",
      "7     0.484848  \n",
      "8     0.990500  \n",
      "9     0.633663  \n",
      "10    0.890541  \n",
      "11    0.788928  \n",
      "12    0.737677  \n",
      "13    0.622923  \n",
      "14    0.902000  \n",
      "15    0.737677  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_rf_4)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_4_cat = np.where(((y_pred_rf_4 >= 2) | (y_pred_rf_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_rf_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:01:35,040] Trial 250 finished with value: 0.6840773499256034 and parameters: {'n_estimators': 667}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:02:39,046] Trial 251 finished with value: 0.6840873384661807 and parameters: {'n_estimators': 642}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:03:38,845] Trial 252 finished with value: 0.6838704867761832 and parameters: {'n_estimators': 603}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:04:39,792] Trial 253 finished with value: 0.6840709056259444 and parameters: {'n_estimators': 626}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:05:43,454] Trial 254 finished with value: 0.684213616005765 and parameters: {'n_estimators': 652}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:06:41,681] Trial 255 finished with value: 0.683658114984741 and parameters: {'n_estimators': 581}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:07:43,709] Trial 256 finished with value: 0.6841002080921138 and parameters: {'n_estimators': 628}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:08:48,967] Trial 257 finished with value: 0.6840121677421183 and parameters: {'n_estimators': 668}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:09:50,411] Trial 258 finished with value: 0.6840417082340999 and parameters: {'n_estimators': 647}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:10:58,551] Trial 259 finished with value: 0.6838818252590101 and parameters: {'n_estimators': 599}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:12:16,369] Trial 260 finished with value: 0.6839964666437488 and parameters: {'n_estimators': 622}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:13:42,904] Trial 261 finished with value: 0.6838391998178462 and parameters: {'n_estimators': 685}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:15:04,650] Trial 262 finished with value: 0.6840831714928248 and parameters: {'n_estimators': 644}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:16:19,006] Trial 263 finished with value: 0.683966535504408 and parameters: {'n_estimators': 608}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:17:25,863] Trial 264 finished with value: 0.684163717765984 and parameters: {'n_estimators': 658}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:18:27,738] Trial 265 finished with value: 0.6840809956011455 and parameters: {'n_estimators': 633}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:19:33,384] Trial 266 finished with value: 0.6839772832445502 and parameters: {'n_estimators': 669}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:20:34,574] Trial 267 finished with value: 0.6840374667169975 and parameters: {'n_estimators': 617}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:21:30,074] Trial 268 finished with value: 0.6839154227808665 and parameters: {'n_estimators': 589}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:22:34,654] Trial 269 finished with value: 0.6839773077484226 and parameters: {'n_estimators': 696}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:23:34,915] Trial 270 finished with value: 0.6841834977562967 and parameters: {'n_estimators': 639}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:24:36,243] Trial 271 finished with value: 0.6841830871628554 and parameters: {'n_estimators': 654}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:25:10,431] Trial 272 finished with value: 0.684922408972896 and parameters: {'n_estimators': 369}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:26:08,981] Trial 273 finished with value: 0.6841002080921138 and parameters: {'n_estimators': 628}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:27:05,815] Trial 274 finished with value: 0.6839098404227032 and parameters: {'n_estimators': 605}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:27:58,701] Trial 275 finished with value: 0.6837604666247706 and parameters: {'n_estimators': 560}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:29:01,958] Trial 276 finished with value: 0.6838134617426453 and parameters: {'n_estimators': 677}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:30:01,812] Trial 277 finished with value: 0.6841143226421201 and parameters: {'n_estimators': 645}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:30:59,067] Trial 278 finished with value: 0.6840479070709616 and parameters: {'n_estimators': 620}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:32:00,009] Trial 279 finished with value: 0.6842037756908411 and parameters: {'n_estimators': 664}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:32:57,769] Trial 280 finished with value: 0.684191447141522 and parameters: {'n_estimators': 638}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:33:50,676] Trial 281 finished with value: 0.6837996540674036 and parameters: {'n_estimators': 584}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:34:46,323] Trial 282 finished with value: 0.6839304379061691 and parameters: {'n_estimators': 606}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:35:47,009] Trial 283 finished with value: 0.6841286492055696 and parameters: {'n_estimators': 660}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:36:44,852] Trial 284 finished with value: 0.6840553843203135 and parameters: {'n_estimators': 631}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:37:46,961] Trial 285 finished with value: 0.6838442288169937 and parameters: {'n_estimators': 687}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:38:35,436] Trial 286 finished with value: 0.683692227884419 and parameters: {'n_estimators': 521}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:39:34,952] Trial 287 finished with value: 0.6841159041658329 and parameters: {'n_estimators': 648}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:40:31,246] Trial 288 finished with value: 0.6840374667169975 and parameters: {'n_estimators': 617}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:41:33,246] Trial 289 finished with value: 0.6840121677421183 and parameters: {'n_estimators': 668}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:42:29,460] Trial 290 finished with value: 0.6838262528617548 and parameters: {'n_estimators': 597}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:43:28,636] Trial 291 finished with value: 0.6840296801851884 and parameters: {'n_estimators': 632}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:44:28,761] Trial 292 finished with value: 0.6840831714928248 and parameters: {'n_estimators': 644}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:45:26,713] Trial 293 finished with value: 0.6840374667169975 and parameters: {'n_estimators': 617}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:46:32,162] Trial 294 finished with value: 0.6840555207719523 and parameters: {'n_estimators': 705}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:47:35,117] Trial 295 finished with value: 0.684112577076738 and parameters: {'n_estimators': 662}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:48:38,450] Trial 296 finished with value: 0.6838768250599616 and parameters: {'n_estimators': 682}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:50:06,582] Trial 297 finished with value: 0.6851581160668185 and parameters: {'n_estimators': 968}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:51:05,821] Trial 298 finished with value: 0.6841143226421201 and parameters: {'n_estimators': 645}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:51:58,322] Trial 299 finished with value: 0.68371591987054 and parameters: {'n_estimators': 572}. Best is trial 179 with value: 0.703533233688111.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7035\n",
      "\tBest params:\n",
      "\t\tn_estimators: 637\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.648886    0.676902    0.673941    0.680771   \n",
      "1                    TP   29.000000   35.000000   34.000000   31.000000   \n",
      "2                    TN  313.000000  309.000000  311.000000  312.000000   \n",
      "3                    FP    3.000000    6.000000    4.000000    2.000000   \n",
      "4                    FN   37.000000   32.000000   33.000000   37.000000   \n",
      "5              Accuracy    0.895288    0.900524    0.903141    0.897906   \n",
      "6             Precision    0.906250    0.853659    0.894737    0.939394   \n",
      "7           Sensitivity    0.439394    0.522388    0.507463    0.455882   \n",
      "8           Specificity    0.990500    0.981000    0.987300    0.993600   \n",
      "9              F1 score    0.591837    0.648148    0.647619    0.613861   \n",
      "10  F1 score (weighted)    0.879796    0.890521    0.891897    0.882911   \n",
      "11     F1 score (macro)    0.765888    0.795111    0.795737    0.777519   \n",
      "12    Balanced Accuracy    0.714950    0.751670    0.747382    0.724756   \n",
      "13                  MCC    0.586643    0.618423    0.628666    0.612057   \n",
      "14                  NPV    0.894300    0.906200    0.904100    0.894000   \n",
      "15              ROC_AUC    0.714950    0.751670    0.747382    0.724756   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.707861    0.711177  \n",
      "1    32.000000   32.000000  \n",
      "2   313.000000  311.000000  \n",
      "3     3.000000    5.000000  \n",
      "4    34.000000   34.000000  \n",
      "5     0.903141    0.897906  \n",
      "6     0.914286    0.864865  \n",
      "7     0.484848    0.484848  \n",
      "8     0.990500    0.984200  \n",
      "9     0.633663    0.621359  \n",
      "10    0.890541    0.885773  \n",
      "11    0.788928    0.781179  \n",
      "12    0.737677    0.734513  \n",
      "13    0.622923    0.599517  \n",
      "14    0.902000    0.901400  \n",
      "15    0.737677    0.734513  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_rf_5)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_5_cat = np.where(((y_pred_rf_5 >= 2) | (y_pred_rf_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_rf_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:52:59,642] Trial 300 finished with value: 0.6884195930827457 and parameters: {'n_estimators': 623}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:53:52,372] Trial 301 finished with value: 0.6884672906707944 and parameters: {'n_estimators': 601}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:54:50,329] Trial 302 finished with value: 0.6887494265998975 and parameters: {'n_estimators': 656}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:55:46,325] Trial 303 finished with value: 0.6886043861432245 and parameters: {'n_estimators': 633}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:56:41,593] Trial 304 finished with value: 0.6885986936026478 and parameters: {'n_estimators': 618}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:57:38,577] Trial 305 finished with value: 0.6886594876194441 and parameters: {'n_estimators': 645}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:58:37,121] Trial 306 finished with value: 0.688762682090921 and parameters: {'n_estimators': 667}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 00:59:28,989] Trial 307 finished with value: 0.6885672747557445 and parameters: {'n_estimators': 593}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:00:24,293] Trial 308 finished with value: 0.6885769828773939 and parameters: {'n_estimators': 630}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:01:24,707] Trial 309 finished with value: 0.6885054734177365 and parameters: {'n_estimators': 687}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:02:18,224] Trial 310 finished with value: 0.6884625166464028 and parameters: {'n_estimators': 610}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:02:43,530] Trial 311 finished with value: 0.6860629153188811 and parameters: {'n_estimators': 275}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:03:41,108] Trial 312 finished with value: 0.6887733727805907 and parameters: {'n_estimators': 652}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:04:36,672] Trial 313 finished with value: 0.6885893274335503 and parameters: {'n_estimators': 634}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:05:35,350] Trial 314 finished with value: 0.6887514541780437 and parameters: {'n_estimators': 672}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:06:33,175] Trial 315 finished with value: 0.6887733727805907 and parameters: {'n_estimators': 652}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:07:24,394] Trial 316 finished with value: 0.6885014304427992 and parameters: {'n_estimators': 577}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:08:19,710] Trial 317 finished with value: 0.6885200947386034 and parameters: {'n_estimators': 611}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:09:15,847] Trial 318 finished with value: 0.6884672117244314 and parameters: {'n_estimators': 628}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:10:13,027] Trial 319 finished with value: 0.6886006429205018 and parameters: {'n_estimators': 643}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:11:12,047] Trial 320 finished with value: 0.6887862462183216 and parameters: {'n_estimators': 669}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:12:04,633] Trial 321 finished with value: 0.6885072003693937 and parameters: {'n_estimators': 596}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:13:06,096] Trial 322 finished with value: 0.6885181613026814 and parameters: {'n_estimators': 701}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:14:01,175] Trial 323 finished with value: 0.6884489867473809 and parameters: {'n_estimators': 625}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:14:59,312] Trial 324 finished with value: 0.6888611993680327 and parameters: {'n_estimators': 659}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:15:53,390] Trial 325 finished with value: 0.6884625166464028 and parameters: {'n_estimators': 610}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:16:50,405] Trial 326 finished with value: 0.6885691269295002 and parameters: {'n_estimators': 639}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:17:49,212] Trial 327 finished with value: 0.6886833131951562 and parameters: {'n_estimators': 650}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:18:49,442] Trial 328 finished with value: 0.6885237470409151 and parameters: {'n_estimators': 681}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:19:44,389] Trial 329 finished with value: 0.6884195930827456 and parameters: {'n_estimators': 623}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:20:36,358] Trial 330 finished with value: 0.6882687855899776 and parameters: {'n_estimators': 589}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:21:32,402] Trial 331 finished with value: 0.6885587212789662 and parameters: {'n_estimators': 641}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:22:26,640] Trial 332 finished with value: 0.6887036959038702 and parameters: {'n_estimators': 545}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:23:25,179] Trial 333 finished with value: 0.6887921323521272 and parameters: {'n_estimators': 668}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:24:17,382] Trial 334 finished with value: 0.6885293524637458 and parameters: {'n_estimators': 607}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:25:09,858] Trial 335 finished with value: 0.6884489867473809 and parameters: {'n_estimators': 625}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:26:04,551] Trial 336 finished with value: 0.6886833131951562 and parameters: {'n_estimators': 650}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:27:01,557] Trial 337 finished with value: 0.6885237470409151 and parameters: {'n_estimators': 681}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:27:54,244] Trial 338 finished with value: 0.6885594461244967 and parameters: {'n_estimators': 631}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:28:49,246] Trial 339 finished with value: 0.6887822775326713 and parameters: {'n_estimators': 657}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:29:37,733] Trial 340 finished with value: 0.6884068767277738 and parameters: {'n_estimators': 579}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:30:28,440] Trial 341 finished with value: 0.6885245164911984 and parameters: {'n_estimators': 605}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:31:21,794] Trial 342 finished with value: 0.688579776481735 and parameters: {'n_estimators': 638}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:32:13,442] Trial 343 finished with value: 0.688550487768099 and parameters: {'n_estimators': 616}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:33:08,985] Trial 344 finished with value: 0.6888087507328999 and parameters: {'n_estimators': 664}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:34:07,767] Trial 345 finished with value: 0.6885813477528759 and parameters: {'n_estimators': 703}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:35:01,179] Trial 346 finished with value: 0.688579776481735 and parameters: {'n_estimators': 638}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:35:50,957] Trial 347 finished with value: 0.6885298732527338 and parameters: {'n_estimators': 594}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:36:47,613] Trial 348 finished with value: 0.688463016821617 and parameters: {'n_estimators': 683}. Best is trial 179 with value: 0.703533233688111.\n",
      "[I 2023-12-12 01:37:33,795] Trial 349 finished with value: 0.6888806667623065 and parameters: {'n_estimators': 555}. Best is trial 179 with value: 0.703533233688111.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7035\n",
      "\tBest params:\n",
      "\t\tn_estimators: 637\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.648886    0.676902    0.673941    0.680771   \n",
      "1                    TP   29.000000   35.000000   34.000000   31.000000   \n",
      "2                    TN  313.000000  309.000000  311.000000  312.000000   \n",
      "3                    FP    3.000000    6.000000    4.000000    2.000000   \n",
      "4                    FN   37.000000   32.000000   33.000000   37.000000   \n",
      "5              Accuracy    0.895288    0.900524    0.903141    0.897906   \n",
      "6             Precision    0.906250    0.853659    0.894737    0.939394   \n",
      "7           Sensitivity    0.439394    0.522388    0.507463    0.455882   \n",
      "8           Specificity    0.990500    0.981000    0.987300    0.993600   \n",
      "9              F1 score    0.591837    0.648148    0.647619    0.613861   \n",
      "10  F1 score (weighted)    0.879796    0.890521    0.891897    0.882911   \n",
      "11     F1 score (macro)    0.765888    0.795111    0.795737    0.777519   \n",
      "12    Balanced Accuracy    0.714950    0.751670    0.747382    0.724756   \n",
      "13                  MCC    0.586643    0.618423    0.628666    0.612057   \n",
      "14                  NPV    0.894300    0.906200    0.904100    0.894000   \n",
      "15              ROC_AUC    0.714950    0.751670    0.747382    0.724756   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.707861    0.711177    0.704579  \n",
      "1    32.000000   32.000000   26.000000  \n",
      "2   313.000000  311.000000  310.000000  \n",
      "3     3.000000    5.000000    4.000000  \n",
      "4    34.000000   34.000000   42.000000  \n",
      "5     0.903141    0.897906    0.879581  \n",
      "6     0.914286    0.864865    0.866667  \n",
      "7     0.484848    0.484848    0.382353  \n",
      "8     0.990500    0.984200    0.987300  \n",
      "9     0.633663    0.621359    0.530612  \n",
      "10    0.890541    0.885773    0.859670  \n",
      "11    0.788928    0.781179    0.730772  \n",
      "12    0.737677    0.734513    0.684807  \n",
      "13    0.622923    0.599517    0.525576  \n",
      "14    0.902000    0.901400    0.880700  \n",
      "15    0.737677    0.734513    0.684807  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_rf_6)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_6_cat = np.where(((y_pred_rf_6 >= 2) | (y_pred_rf_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_rf_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:38:30,226] Trial 350 finished with value: 0.7082594836354945 and parameters: {'n_estimators': 620}. Best is trial 350 with value: 0.7082594836354945.\n",
      "[I 2023-12-12 01:39:20,318] Trial 351 finished with value: 0.708076954232989 and parameters: {'n_estimators': 614}. Best is trial 350 with value: 0.7082594836354945.\n",
      "[I 2023-12-12 01:40:06,471] Trial 352 finished with value: 0.7083328945924433 and parameters: {'n_estimators': 566}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:40:52,587] Trial 353 finished with value: 0.7082988980871535 and parameters: {'n_estimators': 565}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:41:38,211] Trial 354 finished with value: 0.7082561451768936 and parameters: {'n_estimators': 560}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:42:24,126] Trial 355 finished with value: 0.7082769886573346 and parameters: {'n_estimators': 564}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:43:06,622] Trial 356 finished with value: 0.7081490072381367 and parameters: {'n_estimators': 518}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:43:50,160] Trial 357 finished with value: 0.7081047782166908 and parameters: {'n_estimators': 534}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:44:31,967] Trial 358 finished with value: 0.7081398249388913 and parameters: {'n_estimators': 514}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:45:12,885] Trial 359 finished with value: 0.708240973611009 and parameters: {'n_estimators': 502}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:45:53,755] Trial 360 finished with value: 0.708240973611009 and parameters: {'n_estimators': 502}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:46:35,724] Trial 361 finished with value: 0.7081757634755272 and parameters: {'n_estimators': 515}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:47:16,352] Trial 362 finished with value: 0.708215066740109 and parameters: {'n_estimators': 498}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:47:54,320] Trial 363 finished with value: 0.7081077717420466 and parameters: {'n_estimators': 465}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:48:31,360] Trial 364 finished with value: 0.7082263773141442 and parameters: {'n_estimators': 453}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:49:10,306] Trial 365 finished with value: 0.7080023535852218 and parameters: {'n_estimators': 478}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:49:50,873] Trial 366 finished with value: 0.7081341349725905 and parameters: {'n_estimators': 483}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:50:28,921] Trial 367 finished with value: 0.7080681886317314 and parameters: {'n_estimators': 456}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:51:07,923] Trial 368 finished with value: 0.7080518902540989 and parameters: {'n_estimators': 477}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:51:45,621] Trial 369 finished with value: 0.70805121693711 and parameters: {'n_estimators': 462}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:52:22,738] Trial 370 finished with value: 0.7081194168545514 and parameters: {'n_estimators': 455}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:52:59,836] Trial 371 finished with value: 0.7081194168545514 and parameters: {'n_estimators': 455}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:53:37,782] Trial 372 finished with value: 0.7082228723816454 and parameters: {'n_estimators': 466}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:54:15,253] Trial 373 finished with value: 0.708143718292576 and parameters: {'n_estimators': 460}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:54:53,478] Trial 374 finished with value: 0.7080885710825431 and parameters: {'n_estimators': 469}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:55:31,193] Trial 375 finished with value: 0.70805121693711 and parameters: {'n_estimators': 462}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:56:07,789] Trial 376 finished with value: 0.7081444735920189 and parameters: {'n_estimators': 448}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:56:42,048] Trial 377 finished with value: 0.7082521648636997 and parameters: {'n_estimators': 419}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:57:17,204] Trial 378 finished with value: 0.7082775201860685 and parameters: {'n_estimators': 431}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:57:52,119] Trial 379 finished with value: 0.7082167683784638 and parameters: {'n_estimators': 427}. Best is trial 352 with value: 0.7083328945924433.\n",
      "[I 2023-12-12 01:58:26,519] Trial 380 finished with value: 0.7083546445596478 and parameters: {'n_estimators': 421}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 01:59:00,413] Trial 381 finished with value: 0.7083042106666927 and parameters: {'n_estimators': 415}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 01:59:34,256] Trial 382 finished with value: 0.7083042106666927 and parameters: {'n_estimators': 415}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:00:08,223] Trial 383 finished with value: 0.7082909961569145 and parameters: {'n_estimators': 417}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:00:42,544] Trial 384 finished with value: 0.708335308299465 and parameters: {'n_estimators': 411}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:01:16,311] Trial 385 finished with value: 0.708319458612998 and parameters: {'n_estimators': 414}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:01:49,938] Trial 386 finished with value: 0.7083136572970652 and parameters: {'n_estimators': 412}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:02:23,190] Trial 387 finished with value: 0.708257756582206 and parameters: {'n_estimators': 408}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:02:56,676] Trial 388 finished with value: 0.708335308299465 and parameters: {'n_estimators': 411}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:03:29,936] Trial 389 finished with value: 0.708257756582206 and parameters: {'n_estimators': 408}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:04:04,594] Trial 390 finished with value: 0.7082651533349985 and parameters: {'n_estimators': 420}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:04:38,218] Trial 391 finished with value: 0.7083042106666927 and parameters: {'n_estimators': 415}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:05:12,464] Trial 392 finished with value: 0.7082521648636997 and parameters: {'n_estimators': 419}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:05:46,606] Trial 393 finished with value: 0.7082385717068949 and parameters: {'n_estimators': 418}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:06:19,258] Trial 394 finished with value: 0.7081666439700329 and parameters: {'n_estimators': 399}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:06:51,982] Trial 395 finished with value: 0.7082250160047392 and parameters: {'n_estimators': 400}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:07:25,114] Trial 396 finished with value: 0.7083167285746284 and parameters: {'n_estimators': 406}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:07:59,103] Trial 397 finished with value: 0.7083269958232404 and parameters: {'n_estimators': 416}. Best is trial 380 with value: 0.7083546445596478.\n",
      "[I 2023-12-12 02:08:29,682] Trial 398 finished with value: 0.7085648196120624 and parameters: {'n_estimators': 373}. Best is trial 398 with value: 0.7085648196120624.\n",
      "[I 2023-12-12 02:08:59,772] Trial 399 finished with value: 0.7086965412676856 and parameters: {'n_estimators': 368}. Best is trial 399 with value: 0.7086965412676856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7087\n",
      "\tBest params:\n",
      "\t\tn_estimators: 368\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.648886    0.676902    0.673941    0.680771   \n",
      "1                    TP   29.000000   35.000000   34.000000   31.000000   \n",
      "2                    TN  313.000000  309.000000  311.000000  312.000000   \n",
      "3                    FP    3.000000    6.000000    4.000000    2.000000   \n",
      "4                    FN   37.000000   32.000000   33.000000   37.000000   \n",
      "5              Accuracy    0.895288    0.900524    0.903141    0.897906   \n",
      "6             Precision    0.906250    0.853659    0.894737    0.939394   \n",
      "7           Sensitivity    0.439394    0.522388    0.507463    0.455882   \n",
      "8           Specificity    0.990500    0.981000    0.987300    0.993600   \n",
      "9              F1 score    0.591837    0.648148    0.647619    0.613861   \n",
      "10  F1 score (weighted)    0.879796    0.890521    0.891897    0.882911   \n",
      "11     F1 score (macro)    0.765888    0.795111    0.795737    0.777519   \n",
      "12    Balanced Accuracy    0.714950    0.751670    0.747382    0.724756   \n",
      "13                  MCC    0.586643    0.618423    0.628666    0.612057   \n",
      "14                  NPV    0.894300    0.906200    0.904100    0.894000   \n",
      "15              ROC_AUC    0.714950    0.751670    0.747382    0.724756   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.707861    0.711177    0.704579    0.666527  \n",
      "1    32.000000   32.000000   26.000000   32.000000  \n",
      "2   313.000000  311.000000  310.000000  311.000000  \n",
      "3     3.000000    5.000000    4.000000    4.000000  \n",
      "4    34.000000   34.000000   42.000000   35.000000  \n",
      "5     0.903141    0.897906    0.879581    0.897906  \n",
      "6     0.914286    0.864865    0.866667    0.888889  \n",
      "7     0.484848    0.484848    0.382353    0.477612  \n",
      "8     0.990500    0.984200    0.987300    0.987300  \n",
      "9     0.633663    0.621359    0.530612    0.621359  \n",
      "10    0.890541    0.885773    0.859670    0.884936  \n",
      "11    0.788928    0.781179    0.730772    0.781179  \n",
      "12    0.737677    0.734513    0.684807    0.732457  \n",
      "13    0.622923    0.599517    0.525576    0.605167  \n",
      "14    0.902000    0.901400    0.880700    0.898800  \n",
      "15    0.737677    0.734513    0.684807    0.732457  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_rf_7)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_7_cat = np.where(((y_pred_rf_7 >= 2) | (y_pred_rf_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_rf_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 02:09:34,102] Trial 400 finished with value: 0.6961621967862964 and parameters: {'n_estimators': 377}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:10:01,325] Trial 401 finished with value: 0.6961322996359092 and parameters: {'n_estimators': 332}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:10:33,380] Trial 402 finished with value: 0.6961394115508946 and parameters: {'n_estimators': 391}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:11:09,423] Trial 403 finished with value: 0.6962366628689658 and parameters: {'n_estimators': 434}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:11:37,940] Trial 404 finished with value: 0.6964371885648694 and parameters: {'n_estimators': 347}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:12:11,065] Trial 405 finished with value: 0.695989428133042 and parameters: {'n_estimators': 407}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:12:42,344] Trial 406 finished with value: 0.6961260948470755 and parameters: {'n_estimators': 381}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:13:16,454] Trial 407 finished with value: 0.6962393595187633 and parameters: {'n_estimators': 417}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:13:51,911] Trial 408 finished with value: 0.6962308299228167 and parameters: {'n_estimators': 435}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:14:25,149] Trial 409 finished with value: 0.6959314829425337 and parameters: {'n_estimators': 408}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:14:54,599] Trial 410 finished with value: 0.6963447992703632 and parameters: {'n_estimators': 360}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:15:26,560] Trial 411 finished with value: 0.6961410291347275 and parameters: {'n_estimators': 390}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:16:01,479] Trial 412 finished with value: 0.6964265345757601 and parameters: {'n_estimators': 428}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:16:34,963] Trial 413 finished with value: 0.6960597357584689 and parameters: {'n_estimators': 411}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:17:06,908] Trial 414 finished with value: 0.6962028223438592 and parameters: {'n_estimators': 388}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:17:41,318] Trial 415 finished with value: 0.6962285675866208 and parameters: {'n_estimators': 422}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:18:10,994] Trial 416 finished with value: 0.6963671090981909 and parameters: {'n_estimators': 362}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:18:46,701] Trial 417 finished with value: 0.6962683724873826 and parameters: {'n_estimators': 438}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:19:20,099] Trial 418 finished with value: 0.6959735549132425 and parameters: {'n_estimators': 409}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:19:51,170] Trial 419 finished with value: 0.6960934608488047 and parameters: {'n_estimators': 379}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:20:18,244] Trial 420 finished with value: 0.6961146852695617 and parameters: {'n_estimators': 331}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:20:50,591] Trial 421 finished with value: 0.6961707268563132 and parameters: {'n_estimators': 397}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:21:25,014] Trial 422 finished with value: 0.696228567586621 and parameters: {'n_estimators': 422}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:21:57,449] Trial 423 finished with value: 0.6961282787951836 and parameters: {'n_estimators': 396}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:22:33,191] Trial 424 finished with value: 0.6962730837061585 and parameters: {'n_estimators': 439}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:23:02,251] Trial 425 finished with value: 0.6965218689559258 and parameters: {'n_estimators': 355}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:23:33,115] Trial 426 finished with value: 0.6960950932254637 and parameters: {'n_estimators': 376}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:24:07,268] Trial 427 finished with value: 0.6962393595187634 and parameters: {'n_estimators': 417}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:24:42,200] Trial 428 finished with value: 0.6964042287400749 and parameters: {'n_estimators': 427}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:25:15,917] Trial 429 finished with value: 0.6960415144337361 and parameters: {'n_estimators': 412}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:25:48,189] Trial 430 finished with value: 0.6961434274528588 and parameters: {'n_estimators': 394}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:26:24,138] Trial 431 finished with value: 0.6962290602899776 and parameters: {'n_estimators': 440}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:26:57,330] Trial 432 finished with value: 0.6960463911348137 and parameters: {'n_estimators': 406}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:27:27,943] Trial 433 finished with value: 0.6961876179448758 and parameters: {'n_estimators': 374}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:28:03,671] Trial 434 finished with value: 0.6962683724873826 and parameters: {'n_estimators': 438}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:28:35,591] Trial 435 finished with value: 0.696176586501718 and parameters: {'n_estimators': 392}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:29:09,928] Trial 436 finished with value: 0.6961883504776227 and parameters: {'n_estimators': 421}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:29:37,897] Trial 437 finished with value: 0.6963987500011629 and parameters: {'n_estimators': 343}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:30:11,026] Trial 438 finished with value: 0.695989428133042 and parameters: {'n_estimators': 407}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:30:42,309] Trial 439 finished with value: 0.6961493873402685 and parameters: {'n_estimators': 384}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:31:17,259] Trial 440 finished with value: 0.6963322656234877 and parameters: {'n_estimators': 431}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:31:47,415] Trial 441 finished with value: 0.696196856242979 and parameters: {'n_estimators': 369}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:32:31,181] Trial 442 finished with value: 0.6960470181675861 and parameters: {'n_estimators': 539}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:33:04,282] Trial 443 finished with value: 0.695989428133042 and parameters: {'n_estimators': 407}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:33:40,023] Trial 444 finished with value: 0.6962290602899776 and parameters: {'n_estimators': 440}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:34:14,830] Trial 445 finished with value: 0.6963255437420368 and parameters: {'n_estimators': 426}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:34:47,541] Trial 446 finished with value: 0.6961755738652352 and parameters: {'n_estimators': 399}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:35:19,210] Trial 447 finished with value: 0.6962574246130983 and parameters: {'n_estimators': 385}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:35:44,608] Trial 448 finished with value: 0.6961128801450494 and parameters: {'n_estimators': 308}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:36:14,509] Trial 449 finished with value: 0.6963350508764081 and parameters: {'n_estimators': 364}. Best is trial 399 with value: 0.7086965412676856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7087\n",
      "\tBest params:\n",
      "\t\tn_estimators: 368\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.648886    0.676902    0.673941    0.680771   \n",
      "1                    TP   29.000000   35.000000   34.000000   31.000000   \n",
      "2                    TN  313.000000  309.000000  311.000000  312.000000   \n",
      "3                    FP    3.000000    6.000000    4.000000    2.000000   \n",
      "4                    FN   37.000000   32.000000   33.000000   37.000000   \n",
      "5              Accuracy    0.895288    0.900524    0.903141    0.897906   \n",
      "6             Precision    0.906250    0.853659    0.894737    0.939394   \n",
      "7           Sensitivity    0.439394    0.522388    0.507463    0.455882   \n",
      "8           Specificity    0.990500    0.981000    0.987300    0.993600   \n",
      "9              F1 score    0.591837    0.648148    0.647619    0.613861   \n",
      "10  F1 score (weighted)    0.879796    0.890521    0.891897    0.882911   \n",
      "11     F1 score (macro)    0.765888    0.795111    0.795737    0.777519   \n",
      "12    Balanced Accuracy    0.714950    0.751670    0.747382    0.724756   \n",
      "13                  MCC    0.586643    0.618423    0.628666    0.612057   \n",
      "14                  NPV    0.894300    0.906200    0.904100    0.894000   \n",
      "15              ROC_AUC    0.714950    0.751670    0.747382    0.724756   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.707861    0.711177    0.704579    0.666527    0.671576  \n",
      "1    32.000000   32.000000   26.000000   32.000000   32.000000  \n",
      "2   313.000000  311.000000  310.000000  311.000000  310.000000  \n",
      "3     3.000000    5.000000    4.000000    4.000000    3.000000  \n",
      "4    34.000000   34.000000   42.000000   35.000000   37.000000  \n",
      "5     0.903141    0.897906    0.879581    0.897906    0.895288  \n",
      "6     0.914286    0.864865    0.866667    0.888889    0.914286  \n",
      "7     0.484848    0.484848    0.382353    0.477612    0.463768  \n",
      "8     0.990500    0.984200    0.987300    0.987300    0.990400  \n",
      "9     0.633663    0.621359    0.530612    0.621359    0.615385  \n",
      "10    0.890541    0.885773    0.859670    0.884936    0.880869  \n",
      "11    0.788928    0.781179    0.730772    0.781179    0.777389  \n",
      "12    0.737677    0.734513    0.684807    0.732457    0.727092  \n",
      "13    0.622923    0.599517    0.525576    0.605167    0.605661  \n",
      "14    0.902000    0.901400    0.880700    0.898800    0.893400  \n",
      "15    0.737677    0.734513    0.684807    0.732457    0.727092  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_rf_8)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_8_cat = np.where(((y_pred_rf_8 >= 2) | (y_pred_rf_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_rf_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 02:36:52,632] Trial 450 finished with value: 0.693287887542311 and parameters: {'n_estimators': 420}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:37:29,352] Trial 451 finished with value: 0.6933858658972875 and parameters: {'n_estimators': 445}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:38:03,237] Trial 452 finished with value: 0.6933038696810534 and parameters: {'n_estimators': 409}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:38:35,520] Trial 453 finished with value: 0.693859257617528 and parameters: {'n_estimators': 390}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:39:10,201] Trial 454 finished with value: 0.693287887542311 and parameters: {'n_estimators': 420}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:39:46,728] Trial 455 finished with value: 0.6934336233997211 and parameters: {'n_estimators': 442}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:40:17,586] Trial 456 finished with value: 0.6938654960358127 and parameters: {'n_estimators': 372}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:40:50,828] Trial 457 finished with value: 0.6935680826017172 and parameters: {'n_estimators': 402}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:41:36,446] Trial 458 finished with value: 0.6942631626371336 and parameters: {'n_estimators': 555}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:42:11,694] Trial 459 finished with value: 0.6934981894132136 and parameters: {'n_estimators': 427}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:42:43,677] Trial 460 finished with value: 0.6936768189104614 and parameters: {'n_estimators': 387}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:43:18,105] Trial 461 finished with value: 0.6933314411310023 and parameters: {'n_estimators': 416}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:43:54,950] Trial 462 finished with value: 0.6933324056276516 and parameters: {'n_estimators': 447}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:44:28,099] Trial 463 finished with value: 0.693538954010678 and parameters: {'n_estimators': 400}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:44:57,225] Trial 464 finished with value: 0.6935448767799582 and parameters: {'n_estimators': 352}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:45:23,935] Trial 465 finished with value: 0.6937081734994822 and parameters: {'n_estimators': 323}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:46:08,301] Trial 466 finished with value: 0.6940765947436462 and parameters: {'n_estimators': 540}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:46:39,215] Trial 467 finished with value: 0.6938654960358127 and parameters: {'n_estimators': 372}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:47:19,504] Trial 468 finished with value: 0.6933905976085404 and parameters: {'n_estimators': 489}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:47:54,999] Trial 469 finished with value: 0.6936006166198224 and parameters: {'n_estimators': 429}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:48:28,847] Trial 470 finished with value: 0.6932548884137896 and parameters: {'n_estimators': 408}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:49:00,892] Trial 471 finished with value: 0.6936768189104613 and parameters: {'n_estimators': 387}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:49:37,434] Trial 472 finished with value: 0.6934336233997211 and parameters: {'n_estimators': 442}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:50:10,700] Trial 473 finished with value: 0.6935680826017172 and parameters: {'n_estimators': 402}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:50:45,451] Trial 474 finished with value: 0.6932836307833709 and parameters: {'n_estimators': 421}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:51:31,705] Trial 475 finished with value: 0.6943933323034314 and parameters: {'n_estimators': 561}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:52:01,575] Trial 476 finished with value: 0.6935850987298209 and parameters: {'n_estimators': 360}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:52:37,377] Trial 477 finished with value: 0.6935366659085433 and parameters: {'n_estimators': 432}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:53:09,153] Trial 478 finished with value: 0.6937089927819133 and parameters: {'n_estimators': 383}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:53:43,451] Trial 479 finished with value: 0.6931687011146973 and parameters: {'n_estimators': 414}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:54:16,560] Trial 480 finished with value: 0.6935913250199273 and parameters: {'n_estimators': 399}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:54:53,668] Trial 481 finished with value: 0.6933608712348635 and parameters: {'n_estimators': 449}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:55:28,707] Trial 482 finished with value: 0.6934257167679891 and parameters: {'n_estimators': 424}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:55:59,839] Trial 483 finished with value: 0.693816545136867 and parameters: {'n_estimators': 374}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:56:32,843] Trial 484 finished with value: 0.6936583525892044 and parameters: {'n_estimators': 398}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:57:16,489] Trial 485 finished with value: 0.6940940105508461 and parameters: {'n_estimators': 530}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:57:52,666] Trial 486 finished with value: 0.693384414869381 and parameters: {'n_estimators': 440}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:58:21,329] Trial 487 finished with value: 0.6933279308477833 and parameters: {'n_estimators': 347}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:58:55,554] Trial 488 finished with value: 0.6931687011146973 and parameters: {'n_estimators': 414}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 02:59:34,817] Trial 489 finished with value: 0.6932023712167842 and parameters: {'n_estimators': 477}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 03:00:06,683] Trial 490 finished with value: 0.6937151994433408 and parameters: {'n_estimators': 384}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 03:00:52,014] Trial 491 finished with value: 0.6941019205262375 and parameters: {'n_estimators': 551}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 03:01:29,455] Trial 492 finished with value: 0.6934234738279591 and parameters: {'n_estimators': 454}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 03:02:03,921] Trial 493 finished with value: 0.6933314411310023 and parameters: {'n_estimators': 416}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 03:02:39,731] Trial 494 finished with value: 0.6935366659085433 and parameters: {'n_estimators': 432}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 03:03:12,388] Trial 495 finished with value: 0.6936501816365072 and parameters: {'n_estimators': 395}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 03:03:42,515] Trial 496 finished with value: 0.6938037371140755 and parameters: {'n_estimators': 364}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 03:04:16,475] Trial 497 finished with value: 0.6932134728907838 and parameters: {'n_estimators': 410}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 03:04:52,503] Trial 498 finished with value: 0.6934316401425651 and parameters: {'n_estimators': 437}. Best is trial 399 with value: 0.7086965412676856.\n",
      "[I 2023-12-12 03:05:23,463] Trial 499 finished with value: 0.693846256563684 and parameters: {'n_estimators': 373}. Best is trial 399 with value: 0.7086965412676856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7087\n",
      "\tBest params:\n",
      "\t\tn_estimators: 368\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.648886    0.676902    0.673941    0.680771   \n",
      "1                    TP   29.000000   35.000000   34.000000   31.000000   \n",
      "2                    TN  313.000000  309.000000  311.000000  312.000000   \n",
      "3                    FP    3.000000    6.000000    4.000000    2.000000   \n",
      "4                    FN   37.000000   32.000000   33.000000   37.000000   \n",
      "5              Accuracy    0.895288    0.900524    0.903141    0.897906   \n",
      "6             Precision    0.906250    0.853659    0.894737    0.939394   \n",
      "7           Sensitivity    0.439394    0.522388    0.507463    0.455882   \n",
      "8           Specificity    0.990500    0.981000    0.987300    0.993600   \n",
      "9              F1 score    0.591837    0.648148    0.647619    0.613861   \n",
      "10  F1 score (weighted)    0.879796    0.890521    0.891897    0.882911   \n",
      "11     F1 score (macro)    0.765888    0.795111    0.795737    0.777519   \n",
      "12    Balanced Accuracy    0.714950    0.751670    0.747382    0.724756   \n",
      "13                  MCC    0.586643    0.618423    0.628666    0.612057   \n",
      "14                  NPV    0.894300    0.906200    0.904100    0.894000   \n",
      "15              ROC_AUC    0.714950    0.751670    0.747382    0.724756   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.707861    0.711177    0.704579    0.666527    0.671576    0.713789  \n",
      "1    32.000000   32.000000   26.000000   32.000000   32.000000   34.000000  \n",
      "2   313.000000  311.000000  310.000000  311.000000  310.000000  312.000000  \n",
      "3     3.000000    5.000000    4.000000    4.000000    3.000000    3.000000  \n",
      "4    34.000000   34.000000   42.000000   35.000000   37.000000   33.000000  \n",
      "5     0.903141    0.897906    0.879581    0.897906    0.895288    0.905759  \n",
      "6     0.914286    0.864865    0.866667    0.888889    0.914286    0.918919  \n",
      "7     0.484848    0.484848    0.382353    0.477612    0.463768    0.507463  \n",
      "8     0.990500    0.984200    0.987300    0.987300    0.990400    0.990500  \n",
      "9     0.633663    0.621359    0.530612    0.621359    0.615385    0.653846  \n",
      "10    0.890541    0.885773    0.859670    0.884936    0.880869    0.894309  \n",
      "11    0.788928    0.781179    0.730772    0.781179    0.777389    0.799650  \n",
      "12    0.737677    0.734513    0.684807    0.732457    0.727092    0.748969  \n",
      "13    0.622923    0.599517    0.525576    0.605167    0.605661    0.640263  \n",
      "14    0.902000    0.901400    0.880700    0.898800    0.893400    0.904300  \n",
      "15    0.737677    0.734513    0.684807    0.732457    0.727092    0.748969  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_rf_9)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_9_cat = np.where(((y_pred_rf_9 >= 2) | (y_pred_rf_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_rf_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7087\n",
      "\tBest params:\n",
      "\t\tn_estimators: 368\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.714101</td>\n",
       "      <td>0.036192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>2.529822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>2.160247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>2.319004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>16.600000</td>\n",
       "      <td>2.796824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.899476</td>\n",
       "      <td>0.015765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.876224</td>\n",
       "      <td>0.097241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.503595</td>\n",
       "      <td>0.080316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.983530</td>\n",
       "      <td>0.014637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.634265</td>\n",
       "      <td>0.065840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.887897</td>\n",
       "      <td>0.018765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.787975</td>\n",
       "      <td>0.037042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.743562</td>\n",
       "      <td>0.038652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.614187</td>\n",
       "      <td>0.069330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.014801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.743562</td>\n",
       "      <td>0.038652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.714101     0.036192\n",
       "1                    TP        16.800000     2.529822\n",
       "2                    TN       155.000000     2.160247\n",
       "3                    FP         2.600000     2.319004\n",
       "4                    FN        16.600000     2.796824\n",
       "5              Accuracy         0.899476     0.015765\n",
       "6             Precision         0.876224     0.097241\n",
       "7           Sensitivity         0.503595     0.080316\n",
       "8           Specificity         0.983530     0.014637\n",
       "9              F1 score         0.634265     0.065840\n",
       "10  F1 score (weighted)         0.887897     0.018765\n",
       "11     F1 score (macro)         0.787975     0.037042\n",
       "12    Balanced Accuracy         0.743562     0.038652\n",
       "13                  MCC         0.614187     0.069330\n",
       "14                  NPV         0.903500     0.014801\n",
       "15              ROC_AUC         0.743562     0.038652"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.648886</td>\n",
       "      <td>0.676902</td>\n",
       "      <td>0.673941</td>\n",
       "      <td>0.680771</td>\n",
       "      <td>0.707861</td>\n",
       "      <td>0.711177</td>\n",
       "      <td>0.704579</td>\n",
       "      <td>0.666527</td>\n",
       "      <td>0.671576</td>\n",
       "      <td>0.713789</td>\n",
       "      <td>0.685601</td>\n",
       "      <td>0.022231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>31.700000</td>\n",
       "      <td>2.626785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>311.200000</td>\n",
       "      <td>1.316561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1.159502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>35.400000</td>\n",
       "      <td>2.951459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.879581</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.905759</td>\n",
       "      <td>0.897644</td>\n",
       "      <td>0.007243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.896195</td>\n",
       "      <td>0.027601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.472602</td>\n",
       "      <td>0.040606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.990500</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>0.993600</td>\n",
       "      <td>0.990500</td>\n",
       "      <td>0.984200</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>0.990400</td>\n",
       "      <td>0.990500</td>\n",
       "      <td>0.988260</td>\n",
       "      <td>0.003653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.648148</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>0.613861</td>\n",
       "      <td>0.633663</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.617769</td>\n",
       "      <td>0.036030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.879796</td>\n",
       "      <td>0.890521</td>\n",
       "      <td>0.891897</td>\n",
       "      <td>0.882911</td>\n",
       "      <td>0.890541</td>\n",
       "      <td>0.885773</td>\n",
       "      <td>0.859670</td>\n",
       "      <td>0.884936</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.894309</td>\n",
       "      <td>0.884122</td>\n",
       "      <td>0.009879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.765888</td>\n",
       "      <td>0.795111</td>\n",
       "      <td>0.795737</td>\n",
       "      <td>0.777519</td>\n",
       "      <td>0.788928</td>\n",
       "      <td>0.781179</td>\n",
       "      <td>0.730772</td>\n",
       "      <td>0.781179</td>\n",
       "      <td>0.777389</td>\n",
       "      <td>0.799650</td>\n",
       "      <td>0.779335</td>\n",
       "      <td>0.019932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.714950</td>\n",
       "      <td>0.751670</td>\n",
       "      <td>0.747382</td>\n",
       "      <td>0.724756</td>\n",
       "      <td>0.737677</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>0.684807</td>\n",
       "      <td>0.732457</td>\n",
       "      <td>0.727092</td>\n",
       "      <td>0.748969</td>\n",
       "      <td>0.730427</td>\n",
       "      <td>0.019776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.586643</td>\n",
       "      <td>0.618423</td>\n",
       "      <td>0.628666</td>\n",
       "      <td>0.612057</td>\n",
       "      <td>0.622923</td>\n",
       "      <td>0.599517</td>\n",
       "      <td>0.525576</td>\n",
       "      <td>0.605167</td>\n",
       "      <td>0.605661</td>\n",
       "      <td>0.640263</td>\n",
       "      <td>0.604490</td>\n",
       "      <td>0.031672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.894300</td>\n",
       "      <td>0.906200</td>\n",
       "      <td>0.904100</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.902000</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>0.880700</td>\n",
       "      <td>0.898800</td>\n",
       "      <td>0.893400</td>\n",
       "      <td>0.904300</td>\n",
       "      <td>0.897920</td>\n",
       "      <td>0.007619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.714950</td>\n",
       "      <td>0.751670</td>\n",
       "      <td>0.747382</td>\n",
       "      <td>0.724756</td>\n",
       "      <td>0.737677</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>0.684807</td>\n",
       "      <td>0.732457</td>\n",
       "      <td>0.727092</td>\n",
       "      <td>0.748969</td>\n",
       "      <td>0.730427</td>\n",
       "      <td>0.019776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.648886    0.676902    0.673941    0.680771   \n",
       "1                    TP   29.000000   35.000000   34.000000   31.000000   \n",
       "2                    TN  313.000000  309.000000  311.000000  312.000000   \n",
       "3                    FP    3.000000    6.000000    4.000000    2.000000   \n",
       "4                    FN   37.000000   32.000000   33.000000   37.000000   \n",
       "5              Accuracy    0.895288    0.900524    0.903141    0.897906   \n",
       "6             Precision    0.906250    0.853659    0.894737    0.939394   \n",
       "7           Sensitivity    0.439394    0.522388    0.507463    0.455882   \n",
       "8           Specificity    0.990500    0.981000    0.987300    0.993600   \n",
       "9              F1 score    0.591837    0.648148    0.647619    0.613861   \n",
       "10  F1 score (weighted)    0.879796    0.890521    0.891897    0.882911   \n",
       "11     F1 score (macro)    0.765888    0.795111    0.795737    0.777519   \n",
       "12    Balanced Accuracy    0.714950    0.751670    0.747382    0.724756   \n",
       "13                  MCC    0.586643    0.618423    0.628666    0.612057   \n",
       "14                  NPV    0.894300    0.906200    0.904100    0.894000   \n",
       "15              ROC_AUC    0.714950    0.751670    0.747382    0.724756   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.707861    0.711177    0.704579    0.666527    0.671576    0.713789   \n",
       "1    32.000000   32.000000   26.000000   32.000000   32.000000   34.000000   \n",
       "2   313.000000  311.000000  310.000000  311.000000  310.000000  312.000000   \n",
       "3     3.000000    5.000000    4.000000    4.000000    3.000000    3.000000   \n",
       "4    34.000000   34.000000   42.000000   35.000000   37.000000   33.000000   \n",
       "5     0.903141    0.897906    0.879581    0.897906    0.895288    0.905759   \n",
       "6     0.914286    0.864865    0.866667    0.888889    0.914286    0.918919   \n",
       "7     0.484848    0.484848    0.382353    0.477612    0.463768    0.507463   \n",
       "8     0.990500    0.984200    0.987300    0.987300    0.990400    0.990500   \n",
       "9     0.633663    0.621359    0.530612    0.621359    0.615385    0.653846   \n",
       "10    0.890541    0.885773    0.859670    0.884936    0.880869    0.894309   \n",
       "11    0.788928    0.781179    0.730772    0.781179    0.777389    0.799650   \n",
       "12    0.737677    0.734513    0.684807    0.732457    0.727092    0.748969   \n",
       "13    0.622923    0.599517    0.525576    0.605167    0.605661    0.640263   \n",
       "14    0.902000    0.901400    0.880700    0.898800    0.893400    0.904300   \n",
       "15    0.737677    0.734513    0.684807    0.732457    0.727092    0.748969   \n",
       "\n",
       "           ave       std  \n",
       "0     0.685601  0.022231  \n",
       "1    31.700000  2.626785  \n",
       "2   311.200000  1.316561  \n",
       "3     3.700000  1.159502  \n",
       "4    35.400000  2.951459  \n",
       "5     0.897644  0.007243  \n",
       "6     0.896195  0.027601  \n",
       "7     0.472602  0.040606  \n",
       "8     0.988260  0.003653  \n",
       "9     0.617769  0.036030  \n",
       "10    0.884122  0.009879  \n",
       "11    0.779335  0.019932  \n",
       "12    0.730427  0.019776  \n",
       "13    0.604490  0.031672  \n",
       "14    0.897920  0.007619  \n",
       "15    0.730427  0.019776  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test['std'] = mat_met_rf_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.707966</td>\n",
       "      <td>0.051611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.898010</td>\n",
       "      <td>0.016497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.856029</td>\n",
       "      <td>0.079875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.504590</td>\n",
       "      <td>0.084355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.981334</td>\n",
       "      <td>0.010982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.630480</td>\n",
       "      <td>0.074143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.886553</td>\n",
       "      <td>0.020225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.785633</td>\n",
       "      <td>0.041390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.742964</td>\n",
       "      <td>0.041957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.605745</td>\n",
       "      <td>0.074533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.903630</td>\n",
       "      <td>0.014760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.742964</td>\n",
       "      <td>0.041957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.707966     0.051611\n",
       "1              Accuracy         0.898010     0.016497\n",
       "2             Precision         0.856029     0.079875\n",
       "3           Sensitivity         0.504590     0.084355\n",
       "4           Specificity         0.981334     0.010982\n",
       "5              F1 score         0.630480     0.074143\n",
       "6   F1 score (weighted)         0.886553     0.020225\n",
       "7      F1 score (macro)         0.785633     0.041390\n",
       "8     Balanced Accuracy         0.742964     0.041957\n",
       "9                   MCC         0.605745     0.074533\n",
       "10                  NPV         0.903630     0.014760\n",
       "11              ROC_AUC         0.742964     0.041957"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_rf_cat = np.where(((y_pred_optimized_rf >= 2) | (y_pred_optimized_rf <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_rf_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_rf))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "rf_run0 = data_rf[['y_test_idx0', 'y_test0', 'y_pred_rf0']]\n",
    "rf_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "rf_run0.reset_index(inplace=True, drop=True)\n",
    "rf_run1 = data_rf[['y_test_idx1', 'y_test1', 'y_pred_rf1']]\n",
    "rf_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "rf_run1.reset_index(inplace=True, drop=True)\n",
    "rf_run2 = data_rf[['y_test_idx2', 'y_test2', 'y_pred_rf2']]\n",
    "rf_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "rf_run2.reset_index(inplace=True, drop=True)\n",
    "rf_run3 = data_rf[['y_test_idx3', 'y_test3', 'y_pred_rf3']]\n",
    "rf_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "rf_run3.reset_index(inplace=True, drop=True)\n",
    "rf_run4 = data_rf[['y_test_idx4', 'y_test4', 'y_pred_rf4']]\n",
    "rf_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "rf_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "rf_5preds = pd.concat([chembl_id, rf_run0, rf_run1, rf_run2, rf_run3, rf_run4], axis=1)\n",
    "rf_5preds = rf_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_rf0', 'y_pred_rf1', 'y_pred_rf2', 'y_pred_rf3', 'y_pred_rf4']]\n",
    "rf_5preds['y_pred_rf_ave'] = rf_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "rf_5preds['y_pred_rf_std'] = rf_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "\n",
    "mat_met_optimized_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfc78124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (RF)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model r2_score 0.7061 with a standard deviation of 0.0433\n",
      "RF optimized model r2_score 0.7087 with a standard deviation of 0.0441\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"RF baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_rf_withSemiSel.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf_reg, \"OUTPUT/rf_reg_withSemiSel_withSemiSel_withSemiSel_withSemiSel_withSemiSel.joblib\")\n",
    "joblib.dump(optimizedCV_rf, \"OUTPUT/optimizedCV_rf_withSemiSel.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.712088     0.034120\n",
      "1                    TP        17.500000     2.505549\n",
      "2                    TN       153.600000     2.756810\n",
      "3                    FP         4.000000     2.943920\n",
      "4                    FN        15.900000     2.766867\n",
      "5              Accuracy         0.895812     0.019504\n",
      "6             Precision         0.825744     0.113416\n",
      "7           Sensitivity         0.524539     0.078058\n",
      "8           Specificity         0.974650     0.018585\n",
      "9              F1 score         0.636343     0.071224\n",
      "10  F1 score (weighted)         0.886164     0.021668\n",
      "11     F1 score (macro)         0.787744     0.040884\n",
      "12    Balanced Accuracy         0.749595     0.039090\n",
      "13                  MCC         0.602582     0.081131\n",
      "14                  NPV         0.906400     0.014775\n",
      "15              ROC_AUC         0.749595     0.039090\n",
      "CPU times: user 11.7 s, sys: 96.1 ms, total: 11.8 s\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_reg = lgbm.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=8,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_reg.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"rmse\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_reg.predict(X_test) \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "         # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:10:14,847] A new study created in memory with name: lgbmRegressor\n",
      "[I 2023-12-12 03:10:16,345] Trial 0 finished with value: 0.7023856881391042 and parameters: {'n_estimators': 809, 'learning_rate': 0.12448359535377414, 'max_depth': 9, 'max_bin': 239, 'num_leaves': 713}. Best is trial 0 with value: 0.7023856881391042.\n",
      "[I 2023-12-12 03:10:17,559] Trial 1 finished with value: 0.6694796335245015 and parameters: {'n_estimators': 347, 'learning_rate': 0.06981109798156886, 'max_depth': 4, 'max_bin': 266, 'num_leaves': 368}. Best is trial 0 with value: 0.7023856881391042.\n",
      "[I 2023-12-12 03:10:18,901] Trial 2 finished with value: 0.7001204366389489 and parameters: {'n_estimators': 192, 'learning_rate': 0.09253424204480196, 'max_depth': 9, 'max_bin': 240, 'num_leaves': 178}. Best is trial 0 with value: 0.7023856881391042.\n",
      "[I 2023-12-12 03:10:20,608] Trial 3 finished with value: 0.6965712607745214 and parameters: {'n_estimators': 881, 'learning_rate': 0.09859443882764618, 'max_depth': 6, 'max_bin': 221, 'num_leaves': 370}. Best is trial 0 with value: 0.7023856881391042.\n",
      "[I 2023-12-12 03:10:22,075] Trial 4 finished with value: 0.6675070215523233 and parameters: {'n_estimators': 359, 'learning_rate': 0.04634638132570723, 'max_depth': 5, 'max_bin': 193, 'num_leaves': 352}. Best is trial 0 with value: 0.7023856881391042.\n",
      "[I 2023-12-12 03:10:23,892] Trial 5 finished with value: 0.7057970020076294 and parameters: {'n_estimators': 832, 'learning_rate': 0.10133638932508963, 'max_depth': 8, 'max_bin': 207, 'num_leaves': 599}. Best is trial 5 with value: 0.7057970020076294.\n",
      "[I 2023-12-12 03:10:25,568] Trial 6 finished with value: 0.6787692897897587 and parameters: {'n_estimators': 787, 'learning_rate': 0.0611329402255695, 'max_depth': 4, 'max_bin': 227, 'num_leaves': 685}. Best is trial 5 with value: 0.7057970020076294.\n",
      "[I 2023-12-12 03:10:27,087] Trial 7 finished with value: 0.6974709158608394 and parameters: {'n_estimators': 434, 'learning_rate': 0.11107378926831375, 'max_depth': 7, 'max_bin': 238, 'num_leaves': 445}. Best is trial 5 with value: 0.7057970020076294.\n",
      "[I 2023-12-12 03:10:28,043] Trial 8 finished with value: 0.6654925044340482 and parameters: {'n_estimators': 602, 'learning_rate': 0.14318627207504353, 'max_depth': 3, 'max_bin': 187, 'num_leaves': 565}. Best is trial 5 with value: 0.7057970020076294.\n",
      "[I 2023-12-12 03:10:29,376] Trial 9 finished with value: 0.7040601344017503 and parameters: {'n_estimators': 441, 'learning_rate': 0.15584970982362026, 'max_depth': 9, 'max_bin': 277, 'num_leaves': 373}. Best is trial 5 with value: 0.7057970020076294.\n",
      "[I 2023-12-12 03:10:33,977] Trial 10 finished with value: 0.6984372556341889 and parameters: {'n_estimators': 659, 'learning_rate': 0.019025697078928552, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 101}. Best is trial 5 with value: 0.7057970020076294.\n",
      "[I 2023-12-12 03:10:34,765] Trial 11 finished with value: 0.6879482161506448 and parameters: {'n_estimators': 74, 'learning_rate': 0.1779921833657473, 'max_depth': 9, 'max_bin': 299, 'num_leaves': 538}. Best is trial 5 with value: 0.7057970020076294.\n",
      "[I 2023-12-12 03:10:36,409] Trial 12 finished with value: 0.6992476418695651 and parameters: {'n_estimators': 571, 'learning_rate': 0.19842653256023102, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 234}. Best is trial 5 with value: 0.7057970020076294.\n",
      "[I 2023-12-12 03:10:37,761] Trial 13 finished with value: 0.6980094103729942 and parameters: {'n_estimators': 256, 'learning_rate': 0.15358673796003647, 'max_depth': 8, 'max_bin': 202, 'num_leaves': 552}. Best is trial 5 with value: 0.7057970020076294.\n",
      "[I 2023-12-12 03:10:39,120] Trial 14 finished with value: 0.6980345929902996 and parameters: {'n_estimators': 477, 'learning_rate': 0.15134416820928887, 'max_depth': 10, 'max_bin': 266, 'num_leaves': 262}. Best is trial 5 with value: 0.7057970020076294.\n",
      "[I 2023-12-12 03:10:40,816] Trial 15 finished with value: 0.6992677484396284 and parameters: {'n_estimators': 684, 'learning_rate': 0.12881206582373111, 'max_depth': 7, 'max_bin': 172, 'num_leaves': 475}. Best is trial 5 with value: 0.7057970020076294.\n",
      "[I 2023-12-12 03:10:42,405] Trial 16 finished with value: 0.70612343138617 and parameters: {'n_estimators': 515, 'learning_rate': 0.16793734733600987, 'max_depth': 11, 'max_bin': 286, 'num_leaves': 634}. Best is trial 16 with value: 0.70612343138617.\n",
      "[I 2023-12-12 03:10:43,941] Trial 17 finished with value: 0.6995334207387621 and parameters: {'n_estimators': 735, 'learning_rate': 0.18866393752829944, 'max_depth': 11, 'max_bin': 299, 'num_leaves': 646}. Best is trial 16 with value: 0.70612343138617.\n",
      "[I 2023-12-12 03:10:45,560] Trial 18 finished with value: 0.7062212740365905 and parameters: {'n_estimators': 549, 'learning_rate': 0.17202665723254398, 'max_depth': 11, 'max_bin': 207, 'num_leaves': 636}. Best is trial 18 with value: 0.7062212740365905.\n",
      "[I 2023-12-12 03:10:47,483] Trial 19 finished with value: 0.7021672795169254 and parameters: {'n_estimators': 538, 'learning_rate': 0.1766072732906662, 'max_depth': 12, 'max_bin': 173, 'num_leaves': 748}. Best is trial 18 with value: 0.7062212740365905.\n",
      "[I 2023-12-12 03:10:49,181] Trial 20 finished with value: 0.7115737467939659 and parameters: {'n_estimators': 333, 'learning_rate': 0.17150319112713197, 'max_depth': 11, 'max_bin': 252, 'num_leaves': 631}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:10:50,660] Trial 21 finished with value: 0.7023730368922754 and parameters: {'n_estimators': 344, 'learning_rate': 0.17458679758986023, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 648}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:10:52,304] Trial 22 finished with value: 0.7015971351272001 and parameters: {'n_estimators': 512, 'learning_rate': 0.16660988161943674, 'max_depth': 10, 'max_bin': 282, 'num_leaves': 486}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:10:54,100] Trial 23 finished with value: 0.7019119190064746 and parameters: {'n_estimators': 252, 'learning_rate': 0.19743599198678324, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 612}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:10:55,698] Trial 24 finished with value: 0.7063360858064642 and parameters: {'n_estimators': 410, 'learning_rate': 0.16339897596847408, 'max_depth': 10, 'max_bin': 216, 'num_leaves': 683}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:10:57,385] Trial 25 finished with value: 0.703030315991487 and parameters: {'n_estimators': 389, 'learning_rate': 0.14349335952391992, 'max_depth': 10, 'max_bin': 218, 'num_leaves': 735}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:10:59,024] Trial 26 finished with value: 0.704315677888702 and parameters: {'n_estimators': 269, 'learning_rate': 0.1330601817521712, 'max_depth': 10, 'max_bin': 213, 'num_leaves': 677}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:11:00,299] Trial 27 finished with value: 0.7042968073664918 and parameters: {'n_estimators': 130, 'learning_rate': 0.1852291915250722, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 529}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:11:01,762] Trial 28 finished with value: 0.7022640231583905 and parameters: {'n_estimators': 621, 'learning_rate': 0.16139711667005174, 'max_depth': 11, 'max_bin': 196, 'num_leaves': 442}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:11:03,499] Trial 29 finished with value: 0.6987505526537076 and parameters: {'n_estimators': 423, 'learning_rate': 0.11913588662871678, 'max_depth': 9, 'max_bin': 252, 'num_leaves': 698}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:11:05,081] Trial 30 finished with value: 0.70335142992398 and parameters: {'n_estimators': 297, 'learning_rate': 0.140102792129855, 'max_depth': 10, 'max_bin': 177, 'num_leaves': 575}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:11:06,794] Trial 31 finished with value: 0.7082718184481913 and parameters: {'n_estimators': 495, 'learning_rate': 0.1667624352133108, 'max_depth': 11, 'max_bin': 230, 'num_leaves': 631}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:11:08,583] Trial 32 finished with value: 0.7086643751521796 and parameters: {'n_estimators': 492, 'learning_rate': 0.16363112728905801, 'max_depth': 11, 'max_bin': 228, 'num_leaves': 703}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:11:10,341] Trial 33 finished with value: 0.7034338873916435 and parameters: {'n_estimators': 477, 'learning_rate': 0.15994612569738345, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 686}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:11:11,990] Trial 34 finished with value: 0.7026105294051499 and parameters: {'n_estimators': 338, 'learning_rate': 0.18461539318246445, 'max_depth': 10, 'max_bin': 244, 'num_leaves': 720}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:11:13,647] Trial 35 finished with value: 0.7023753397094559 and parameters: {'n_estimators': 196, 'learning_rate': 0.1248621620956171, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 600}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:11:15,322] Trial 36 finished with value: 0.704222088066752 and parameters: {'n_estimators': 380, 'learning_rate': 0.14869013736745856, 'max_depth': 9, 'max_bin': 261, 'num_leaves': 508}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:11:16,554] Trial 37 finished with value: 0.6955710290299061 and parameters: {'n_estimators': 319, 'learning_rate': 0.1619425093617839, 'max_depth': 8, 'max_bin': 228, 'num_leaves': 750}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:11:17,954] Trial 38 finished with value: 0.6971424885409514 and parameters: {'n_estimators': 408, 'learning_rate': 0.13735947995378775, 'max_depth': 6, 'max_bin': 217, 'num_leaves': 670}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:11:19,624] Trial 39 finished with value: 0.7083620274951083 and parameters: {'n_estimators': 467, 'learning_rate': 0.18752120938978628, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 316}. Best is trial 20 with value: 0.7115737467939659.\n",
      "[I 2023-12-12 03:11:21,198] Trial 40 finished with value: 0.7129096258380114 and parameters: {'n_estimators': 464, 'learning_rate': 0.18701046015179237, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 322}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:22,749] Trial 41 finished with value: 0.7067674726273625 and parameters: {'n_estimators': 476, 'learning_rate': 0.18993465782274654, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 285}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:24,371] Trial 42 finished with value: 0.7064676380528743 and parameters: {'n_estimators': 464, 'learning_rate': 0.19809691987856204, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 335}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:25,871] Trial 43 finished with value: 0.7050540870226649 and parameters: {'n_estimators': 584, 'learning_rate': 0.1776055151482538, 'max_depth': 11, 'max_bin': 234, 'num_leaves': 317}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:27,599] Trial 44 finished with value: 0.699346888597651 and parameters: {'n_estimators': 625, 'learning_rate': 0.18239690620574187, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 403}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:29,302] Trial 45 finished with value: 0.7015718209239463 and parameters: {'n_estimators': 524, 'learning_rate': 0.1513530018487624, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 198}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:30,862] Trial 46 finished with value: 0.7054142189438466 and parameters: {'n_estimators': 453, 'learning_rate': 0.17085831921585562, 'max_depth': 11, 'max_bin': 265, 'num_leaves': 120}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:31,941] Trial 47 finished with value: 0.6845489492499938 and parameters: {'n_estimators': 367, 'learning_rate': 0.19304937053639964, 'max_depth': 5, 'max_bin': 249, 'num_leaves': 400}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:32,975] Trial 48 finished with value: 0.6702858282997479 and parameters: {'n_estimators': 899, 'learning_rate': 0.1866542900507004, 'max_depth': 3, 'max_bin': 223, 'num_leaves': 36}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:34,685] Trial 49 finished with value: 0.7070767740934013 and parameters: {'n_estimators': 564, 'learning_rate': 0.1995281648947738, 'max_depth': 11, 'max_bin': 232, 'num_leaves': 307}. Best is trial 40 with value: 0.7129096258380114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.7129\n",
      "\tBest params:\n",
      "\t\tn_estimators: 464\n",
      "\t\tlearning_rate: 0.18701046015179237\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 233\n",
      "\t\tnum_leaves: 322\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.655901\n",
      "1                    TP   28.000000\n",
      "2                    TN  305.000000\n",
      "3                    FP   11.000000\n",
      "4                    FN   38.000000\n",
      "5              Accuracy    0.871728\n",
      "6             Precision    0.717949\n",
      "7           Sensitivity    0.424242\n",
      "8           Specificity    0.965200\n",
      "9              F1 score    0.533333\n",
      "10  F1 score (weighted)    0.857863\n",
      "11     F1 score (macro)    0.729489\n",
      "12    Balanced Accuracy    0.694716\n",
      "13                  MCC    0.486260\n",
      "14                  NPV    0.889200\n",
      "15              ROC_AUC    0.694716\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_lgbm_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_0_cat = np.where(((y_pred_lgbm_0 >= 2) | (y_pred_lgbm_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:11:36,417] Trial 50 finished with value: 0.6943464933854152 and parameters: {'n_estimators': 675, 'learning_rate': 0.1795357279415723, 'max_depth': 9, 'max_bin': 208, 'num_leaves': 359}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:38,050] Trial 51 finished with value: 0.6946493560841416 and parameters: {'n_estimators': 500, 'learning_rate': 0.19286743260065833, 'max_depth': 11, 'max_bin': 231, 'num_leaves': 300}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:40,043] Trial 52 finished with value: 0.7028728579500705 and parameters: {'n_estimators': 563, 'learning_rate': 0.17003821398206564, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 254}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:41,578] Trial 53 finished with value: 0.704481437358626 and parameters: {'n_estimators': 727, 'learning_rate': 0.19884426017367762, 'max_depth': 11, 'max_bin': 237, 'num_leaves': 333}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:43,253] Trial 54 finished with value: 0.6958612040902805 and parameters: {'n_estimators': 599, 'learning_rate': 0.18054689470842758, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 227}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:45,252] Trial 55 finished with value: 0.6963894555281062 and parameters: {'n_estimators': 437, 'learning_rate': 0.15643197565697592, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 431}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:46,824] Trial 56 finished with value: 0.707175921108216 and parameters: {'n_estimators': 543, 'learning_rate': 0.1893276737054583, 'max_depth': 10, 'max_bin': 211, 'num_leaves': 292}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:48,300] Trial 57 finished with value: 0.6985850351236584 and parameters: {'n_estimators': 504, 'learning_rate': 0.1658884012419056, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 162}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:49,760] Trial 58 finished with value: 0.7017563435541967 and parameters: {'n_estimators': 220, 'learning_rate': 0.17638195458095945, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 275}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:51,649] Trial 59 finished with value: 0.6924283635848957 and parameters: {'n_estimators': 540, 'learning_rate': 0.18844921047148683, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 380}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:53,193] Trial 60 finished with value: 0.7016758580680923 and parameters: {'n_estimators': 397, 'learning_rate': 0.17223034384921967, 'max_depth': 11, 'max_bin': 199, 'num_leaves': 223}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:54,911] Trial 61 finished with value: 0.6976929269693877 and parameters: {'n_estimators': 642, 'learning_rate': 0.19038847673911855, 'max_depth': 11, 'max_bin': 233, 'num_leaves': 303}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:56,737] Trial 62 finished with value: 0.6953145490120666 and parameters: {'n_estimators': 569, 'learning_rate': 0.1851099534958371, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 350}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:58,294] Trial 63 finished with value: 0.6915816946101885 and parameters: {'n_estimators': 442, 'learning_rate': 0.1943652305490815, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 253}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:11:59,733] Trial 64 finished with value: 0.6906048643963375 and parameters: {'n_estimators': 490, 'learning_rate': 0.19929806892311347, 'max_depth': 10, 'max_bin': 189, 'num_leaves': 570}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:01,611] Trial 65 finished with value: 0.6957290187099782 and parameters: {'n_estimators': 537, 'learning_rate': 0.17404609858426448, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 322}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:02,828] Trial 66 finished with value: 0.690158390943483 and parameters: {'n_estimators': 142, 'learning_rate': 0.1664539517641553, 'max_depth': 11, 'max_bin': 250, 'num_leaves': 282}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:04,398] Trial 67 finished with value: 0.6971028242623183 and parameters: {'n_estimators': 597, 'learning_rate': 0.18070459475536718, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 618}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:06,112] Trial 68 finished with value: 0.6961484678752894 and parameters: {'n_estimators': 706, 'learning_rate': 0.16013107212101121, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 425}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:07,917] Trial 69 finished with value: 0.6979624546924734 and parameters: {'n_estimators': 824, 'learning_rate': 0.19261374004131096, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 710}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:09,647] Trial 70 finished with value: 0.7007904079746428 and parameters: {'n_estimators': 519, 'learning_rate': 0.14688535056546623, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 205}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:11,220] Trial 71 finished with value: 0.6854953293660969 and parameters: {'n_estimators': 479, 'learning_rate': 0.1879080196411301, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 288}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:12,896] Trial 72 finished with value: 0.6987648690136193 and parameters: {'n_estimators': 467, 'learning_rate': 0.18175875447444317, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 347}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:14,438] Trial 73 finished with value: 0.6940090230926238 and parameters: {'n_estimators': 554, 'learning_rate': 0.1916556239701785, 'max_depth': 11, 'max_bin': 240, 'num_leaves': 666}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:16,308] Trial 74 finished with value: 0.6992624453064578 and parameters: {'n_estimators': 421, 'learning_rate': 0.19970293033378783, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 264}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:17,955] Trial 75 finished with value: 0.7002537632514556 and parameters: {'n_estimators': 488, 'learning_rate': 0.1693586164374427, 'max_depth': 9, 'max_bin': 205, 'num_leaves': 309}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:19,273] Trial 76 finished with value: 0.6928018918460289 and parameters: {'n_estimators': 778, 'learning_rate': 0.17482894829690004, 'max_depth': 7, 'max_bin': 226, 'num_leaves': 371}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:20,939] Trial 77 finished with value: 0.692322613729752 and parameters: {'n_estimators': 328, 'learning_rate': 0.1842832088444525, 'max_depth': 11, 'max_bin': 152, 'num_leaves': 471}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:22,297] Trial 78 finished with value: 0.6927498121063521 and parameters: {'n_estimators': 446, 'learning_rate': 0.15717823945421877, 'max_depth': 8, 'max_bin': 243, 'num_leaves': 164}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:23,994] Trial 79 finished with value: 0.6925492253479549 and parameters: {'n_estimators': 359, 'learning_rate': 0.18868502754793218, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 650}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:25,770] Trial 80 finished with value: 0.6994944952754392 and parameters: {'n_estimators': 301, 'learning_rate': 0.17783916067725627, 'max_depth': 11, 'max_bin': 235, 'num_leaves': 591}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:27,474] Trial 81 finished with value: 0.6970624393876428 and parameters: {'n_estimators': 465, 'learning_rate': 0.19482009752060422, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 347}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:29,289] Trial 82 finished with value: 0.6942736155743379 and parameters: {'n_estimators': 523, 'learning_rate': 0.1953071410981136, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 326}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:30,987] Trial 83 finished with value: 0.6969321614123946 and parameters: {'n_estimators': 459, 'learning_rate': 0.18352102071402016, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 296}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:32,376] Trial 84 finished with value: 0.6978880710173814 and parameters: {'n_estimators': 574, 'learning_rate': 0.19952723241339373, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 246}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:34,054] Trial 85 finished with value: 0.688324921736227 and parameters: {'n_estimators': 389, 'learning_rate': 0.1876764197657576, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 395}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:35,714] Trial 86 finished with value: 0.6997755404582067 and parameters: {'n_estimators': 415, 'learning_rate': 0.16472454800049013, 'max_depth': 11, 'max_bin': 254, 'num_leaves': 336}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:37,565] Trial 87 finished with value: 0.6973915986085257 and parameters: {'n_estimators': 506, 'learning_rate': 0.1775395677673001, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 269}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:39,568] Trial 88 finished with value: 0.7076739305614488 and parameters: {'n_estimators': 540, 'learning_rate': 0.17217323091011016, 'max_depth': 11, 'max_bin': 232, 'num_leaves': 727}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:41,475] Trial 89 finished with value: 0.7063709678444782 and parameters: {'n_estimators': 626, 'learning_rate': 0.16760591868432254, 'max_depth': 10, 'max_bin': 241, 'num_leaves': 713}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:43,525] Trial 90 finished with value: 0.7074614376243253 and parameters: {'n_estimators': 546, 'learning_rate': 0.1718218784462973, 'max_depth': 11, 'max_bin': 231, 'num_leaves': 739}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:45,299] Trial 91 finished with value: 0.7024256064002643 and parameters: {'n_estimators': 538, 'learning_rate': 0.172034051710609, 'max_depth': 11, 'max_bin': 230, 'num_leaves': 735}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:46,395] Trial 92 finished with value: 0.6769936976341543 and parameters: {'n_estimators': 584, 'learning_rate': 0.15392102751469178, 'max_depth': 4, 'max_bin': 233, 'num_leaves': 687}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:48,312] Trial 93 finished with value: 0.7029769500919738 and parameters: {'n_estimators': 552, 'learning_rate': 0.16233844618926968, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 725}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:50,165] Trial 94 finished with value: 0.7022628877673316 and parameters: {'n_estimators': 608, 'learning_rate': 0.17412274134248218, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 694}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:51,749] Trial 95 finished with value: 0.6910539253368595 and parameters: {'n_estimators': 491, 'learning_rate': 0.17984140574617258, 'max_depth': 11, 'max_bin': 293, 'num_leaves': 638}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:53,345] Trial 96 finished with value: 0.6960046254958973 and parameters: {'n_estimators': 658, 'learning_rate': 0.19050590003903578, 'max_depth': 10, 'max_bin': 227, 'num_leaves': 662}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:55,231] Trial 97 finished with value: 0.7067748969717769 and parameters: {'n_estimators': 531, 'learning_rate': 0.16803157992830406, 'max_depth': 11, 'max_bin': 236, 'num_leaves': 736}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:57,025] Trial 98 finished with value: 0.7024839323843551 and parameters: {'n_estimators': 530, 'learning_rate': 0.16847302604642092, 'max_depth': 11, 'max_bin': 232, 'num_leaves': 744}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:12:58,772] Trial 99 finished with value: 0.700317427445992 and parameters: {'n_estimators': 562, 'learning_rate': 0.1558026717215216, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 704}. Best is trial 40 with value: 0.7129096258380114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7129\n",
      "\tBest params:\n",
      "\t\tn_estimators: 464\n",
      "\t\tlearning_rate: 0.18701046015179237\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 233\n",
      "\t\tnum_leaves: 322\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.655901    0.686027\n",
      "1                    TP   28.000000   34.000000\n",
      "2                    TN  305.000000  309.000000\n",
      "3                    FP   11.000000    6.000000\n",
      "4                    FN   38.000000   33.000000\n",
      "5              Accuracy    0.871728    0.897906\n",
      "6             Precision    0.717949    0.850000\n",
      "7           Sensitivity    0.424242    0.507463\n",
      "8           Specificity    0.965200    0.981000\n",
      "9              F1 score    0.533333    0.635514\n",
      "10  F1 score (weighted)    0.857863    0.887123\n",
      "11     F1 score (macro)    0.729489    0.788077\n",
      "12    Balanced Accuracy    0.694716    0.744208\n",
      "13                  MCC    0.486260    0.606651\n",
      "14                  NPV    0.889200    0.903500\n",
      "15              ROC_AUC    0.694716    0.744208\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_lgbm_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_1_cat = np.where(((y_pred_lgbm_1 >= 2) | (y_pred_lgbm_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:13:00,673] Trial 100 finished with value: 0.6986764292950023 and parameters: {'n_estimators': 505, 'learning_rate': 0.15943825351813587, 'max_depth': 11, 'max_bin': 225, 'num_leaves': 725}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:02,306] Trial 101 finished with value: 0.7000955063822546 and parameters: {'n_estimators': 585, 'learning_rate': 0.1812864553676082, 'max_depth': 11, 'max_bin': 236, 'num_leaves': 621}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:03,842] Trial 102 finished with value: 0.6974862338299312 and parameters: {'n_estimators': 515, 'learning_rate': 0.18471713802725206, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 544}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:05,573] Trial 103 finished with value: 0.6937496727459702 and parameters: {'n_estimators': 486, 'learning_rate': 0.17249689796094356, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 742}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:07,287] Trial 104 finished with value: 0.692378384603028 and parameters: {'n_estimators': 546, 'learning_rate': 0.1631783200036558, 'max_depth': 11, 'max_bin': 244, 'num_leaves': 677}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:08,985] Trial 105 finished with value: 0.6908513234600827 and parameters: {'n_estimators': 429, 'learning_rate': 0.17817583515940785, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 700}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:10,682] Trial 106 finished with value: 0.6946957590658959 and parameters: {'n_estimators': 473, 'learning_rate': 0.19084338272631798, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 721}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:12,265] Trial 107 finished with value: 0.6900557197830525 and parameters: {'n_estimators': 526, 'learning_rate': 0.1847225150100673, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 284}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:13,626] Trial 108 finished with value: 0.6962625572428698 and parameters: {'n_estimators': 606, 'learning_rate': 0.19526623978610547, 'max_depth': 10, 'max_bin': 236, 'num_leaves': 313}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:15,286] Trial 109 finished with value: 0.6982524141427385 and parameters: {'n_estimators': 454, 'learning_rate': 0.1702703557268278, 'max_depth': 11, 'max_bin': 228, 'num_leaves': 241}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:16,807] Trial 110 finished with value: 0.6919822233303263 and parameters: {'n_estimators': 571, 'learning_rate': 0.15199168850600223, 'max_depth': 9, 'max_bin': 241, 'num_leaves': 361}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:18,408] Trial 111 finished with value: 0.6942726117972049 and parameters: {'n_estimators': 502, 'learning_rate': 0.19643472750978413, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 337}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:19,640] Trial 112 finished with value: 0.6863318457904637 and parameters: {'n_estimators': 402, 'learning_rate': 0.18965000513782654, 'max_depth': 6, 'max_bin': 231, 'num_leaves': 304}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:20,459] Trial 113 finished with value: 0.6780633329437236 and parameters: {'n_estimators': 61, 'learning_rate': 0.1754777703455847, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 516}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:21,931] Trial 114 finished with value: 0.6876234942272965 and parameters: {'n_estimators': 375, 'learning_rate': 0.18622402362477175, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 382}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:23,487] Trial 115 finished with value: 0.6976988501363508 and parameters: {'n_estimators': 463, 'learning_rate': 0.18263020631486296, 'max_depth': 11, 'max_bin': 226, 'num_leaves': 656}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:25,119] Trial 116 finished with value: 0.68635045275096 and parameters: {'n_estimators': 554, 'learning_rate': 0.19519677377088415, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 324}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:26,600] Trial 117 finished with value: 0.6991146261602769 and parameters: {'n_estimators': 476, 'learning_rate': 0.16605106941794914, 'max_depth': 11, 'max_bin': 235, 'num_leaves': 293}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:27,980] Trial 118 finished with value: 0.6879712802519384 and parameters: {'n_estimators': 439, 'learning_rate': 0.19980870320555658, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 410}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:29,311] Trial 119 finished with value: 0.6906948606395475 and parameters: {'n_estimators': 515, 'learning_rate': 0.19231706744298616, 'max_depth': 11, 'max_bin': 238, 'num_leaves': 268}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:30,869] Trial 120 finished with value: 0.6872840672966577 and parameters: {'n_estimators': 538, 'learning_rate': 0.17856079999668628, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 587}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:32,556] Trial 121 finished with value: 0.6893018043526962 and parameters: {'n_estimators': 653, 'learning_rate': 0.16803713787841132, 'max_depth': 10, 'max_bin': 244, 'num_leaves': 711}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:34,135] Trial 122 finished with value: 0.6970712973428197 and parameters: {'n_estimators': 621, 'learning_rate': 0.17227008968661436, 'max_depth': 10, 'max_bin': 240, 'num_leaves': 730}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:35,837] Trial 123 finished with value: 0.701654100792745 and parameters: {'n_estimators': 634, 'learning_rate': 0.15850424031266397, 'max_depth': 11, 'max_bin': 242, 'num_leaves': 749}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:37,490] Trial 124 finished with value: 0.696554966116621 and parameters: {'n_estimators': 493, 'learning_rate': 0.16309697259863654, 'max_depth': 9, 'max_bin': 230, 'num_leaves': 711}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:39,079] Trial 125 finished with value: 0.6973737568809141 and parameters: {'n_estimators': 592, 'learning_rate': 0.14799581782946156, 'max_depth': 10, 'max_bin': 234, 'num_leaves': 689}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:40,607] Trial 126 finished with value: 0.6922329397509767 and parameters: {'n_estimators': 236, 'learning_rate': 0.18754988260744487, 'max_depth': 11, 'max_bin': 227, 'num_leaves': 679}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:42,101] Trial 127 finished with value: 0.6953512701241799 and parameters: {'n_estimators': 697, 'learning_rate': 0.17577979252801404, 'max_depth': 10, 'max_bin': 216, 'num_leaves': 212}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:43,834] Trial 128 finished with value: 0.6894871222816951 and parameters: {'n_estimators': 568, 'learning_rate': 0.16849119759002137, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 717}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:45,516] Trial 129 finished with value: 0.7009145893821327 and parameters: {'n_estimators': 446, 'learning_rate': 0.18171000854281583, 'max_depth': 11, 'max_bin': 233, 'num_leaves': 278}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:47,196] Trial 130 finished with value: 0.6972957680319112 and parameters: {'n_estimators': 533, 'learning_rate': 0.19273159701071813, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 732}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:48,815] Trial 131 finished with value: 0.691742046623085 and parameters: {'n_estimators': 414, 'learning_rate': 0.16022170404519193, 'max_depth': 10, 'max_bin': 217, 'num_leaves': 627}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:50,505] Trial 132 finished with value: 0.6968108737396135 and parameters: {'n_estimators': 428, 'learning_rate': 0.16402457238897336, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 699}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:52,079] Trial 133 finished with value: 0.6993636830198345 and parameters: {'n_estimators': 289, 'learning_rate': 0.1538134567424454, 'max_depth': 11, 'max_bin': 229, 'num_leaves': 603}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:53,358] Trial 134 finished with value: 0.6910548948728423 and parameters: {'n_estimators': 346, 'learning_rate': 0.1736122127574869, 'max_depth': 8, 'max_bin': 220, 'num_leaves': 669}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:54,866] Trial 135 finished with value: 0.6909104840200058 and parameters: {'n_estimators': 478, 'learning_rate': 0.16750821633251045, 'max_depth': 9, 'max_bin': 223, 'num_leaves': 335}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:56,333] Trial 136 finished with value: 0.6942583880552967 and parameters: {'n_estimators': 506, 'learning_rate': 0.1866116002937373, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 312}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:58,109] Trial 137 finished with value: 0.6982338542858315 and parameters: {'n_estimators': 459, 'learning_rate': 0.14461236580564016, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 690}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:13:59,957] Trial 138 finished with value: 0.6980442138955365 and parameters: {'n_estimators': 493, 'learning_rate': 0.17925296409699726, 'max_depth': 11, 'max_bin': 231, 'num_leaves': 710}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:01,664] Trial 139 finished with value: 0.6994345530316288 and parameters: {'n_estimators': 518, 'learning_rate': 0.19652607941327493, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 638}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:03,328] Trial 140 finished with value: 0.6984509805609043 and parameters: {'n_estimators': 549, 'learning_rate': 0.15660621882399386, 'max_depth': 11, 'max_bin': 227, 'num_leaves': 738}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:05,034] Trial 141 finished with value: 0.6885729540659604 and parameters: {'n_estimators': 578, 'learning_rate': 0.17088774738738413, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 673}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:06,661] Trial 142 finished with value: 0.6969158737908063 and parameters: {'n_estimators': 607, 'learning_rate': 0.16464446530566695, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 650}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:08,100] Trial 143 finished with value: 0.698405040238959 and parameters: {'n_estimators': 556, 'learning_rate': 0.1749308389937898, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 259}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:09,573] Trial 144 finished with value: 0.6929351845646113 and parameters: {'n_estimators': 526, 'learning_rate': 0.1808758428660006, 'max_depth': 11, 'max_bin': 240, 'num_leaves': 351}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:10,883] Trial 145 finished with value: 0.6850398001055156 and parameters: {'n_estimators': 471, 'learning_rate': 0.1899185944624924, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 297}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:12,514] Trial 146 finished with value: 0.6947866225674383 and parameters: {'n_estimators': 498, 'learning_rate': 0.16108838431201636, 'max_depth': 11, 'max_bin': 201, 'num_leaves': 749}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:14,095] Trial 147 finished with value: 0.6890106403082734 and parameters: {'n_estimators': 393, 'learning_rate': 0.18353189950330537, 'max_depth': 12, 'max_bin': 192, 'num_leaves': 561}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:15,828] Trial 148 finished with value: 0.6974141472612929 and parameters: {'n_estimators': 539, 'learning_rate': 0.15011236685695872, 'max_depth': 11, 'max_bin': 204, 'num_leaves': 608}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:17,366] Trial 149 finished with value: 0.6977008803524783 and parameters: {'n_estimators': 452, 'learning_rate': 0.17165523486249631, 'max_depth': 10, 'max_bin': 233, 'num_leaves': 659}. Best is trial 40 with value: 0.7129096258380114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7129\n",
      "\tBest params:\n",
      "\t\tn_estimators: 464\n",
      "\t\tlearning_rate: 0.18701046015179237\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 233\n",
      "\t\tnum_leaves: 322\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.655901    0.686027    0.679052\n",
      "1                    TP   28.000000   34.000000   39.000000\n",
      "2                    TN  305.000000  309.000000  308.000000\n",
      "3                    FP   11.000000    6.000000    7.000000\n",
      "4                    FN   38.000000   33.000000   28.000000\n",
      "5              Accuracy    0.871728    0.897906    0.908377\n",
      "6             Precision    0.717949    0.850000    0.847826\n",
      "7           Sensitivity    0.424242    0.507463    0.582090\n",
      "8           Specificity    0.965200    0.981000    0.977800\n",
      "9              F1 score    0.533333    0.635514    0.690265\n",
      "10  F1 score (weighted)    0.857863    0.887123    0.901341\n",
      "11     F1 score (macro)    0.729489    0.788077    0.818251\n",
      "12    Balanced Accuracy    0.694716    0.744208    0.779934\n",
      "13                  MCC    0.486260    0.606651    0.654228\n",
      "14                  NPV    0.889200    0.903500    0.916700\n",
      "15              ROC_AUC    0.694716    0.744208    0.779934\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_lgbm_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_2_cat = np.where(((y_pred_lgbm_2 >= 2) | (y_pred_lgbm_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:14:19,264] Trial 150 finished with value: 0.7036686735759602 and parameters: {'n_estimators': 510, 'learning_rate': 0.17608356335924502, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 366}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:21,148] Trial 151 finished with value: 0.7000107714318674 and parameters: {'n_estimators': 488, 'learning_rate': 0.1667591863632257, 'max_depth': 11, 'max_bin': 286, 'num_leaves': 638}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:22,851] Trial 152 finished with value: 0.7070534261747718 and parameters: {'n_estimators': 560, 'learning_rate': 0.16977592436981595, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 625}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:24,571] Trial 153 finished with value: 0.7080116471015087 and parameters: {'n_estimators': 557, 'learning_rate': 0.16968977959473341, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 622}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:25,746] Trial 154 finished with value: 0.6979908256941202 and parameters: {'n_estimators': 588, 'learning_rate': 0.1579099755220501, 'max_depth': 5, 'max_bin': 294, 'num_leaves': 583}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:27,617] Trial 155 finished with value: 0.7037822826404072 and parameters: {'n_estimators': 549, 'learning_rate': 0.16898899036383722, 'max_depth': 11, 'max_bin': 293, 'num_leaves': 323}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:28,993] Trial 156 finished with value: 0.7025360639465503 and parameters: {'n_estimators': 571, 'learning_rate': 0.11449565309167407, 'max_depth': 7, 'max_bin': 298, 'num_leaves': 623}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:30,717] Trial 157 finished with value: 0.7070287348987474 and parameters: {'n_estimators': 429, 'learning_rate': 0.19372453690176097, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 725}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:32,188] Trial 158 finished with value: 0.7019618958926298 and parameters: {'n_estimators': 526, 'learning_rate': 0.1996857671836043, 'max_depth': 11, 'max_bin': 290, 'num_leaves': 715}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:33,792] Trial 159 finished with value: 0.7005300804501048 and parameters: {'n_estimators': 438, 'learning_rate': 0.19357481959705494, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 704}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:35,399] Trial 160 finished with value: 0.6975344577097679 and parameters: {'n_estimators': 617, 'learning_rate': 0.1909619901512509, 'max_depth': 12, 'max_bin': 297, 'num_leaves': 726}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:37,136] Trial 161 finished with value: 0.706461522372688 and parameters: {'n_estimators': 414, 'learning_rate': 0.18766566685604402, 'max_depth': 11, 'max_bin': 283, 'num_leaves': 728}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:38,734] Trial 162 finished with value: 0.7029772948626797 and parameters: {'n_estimators': 471, 'learning_rate': 0.18770956334400363, 'max_depth': 11, 'max_bin': 290, 'num_leaves': 726}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:40,553] Trial 163 finished with value: 0.7039847472794414 and parameters: {'n_estimators': 433, 'learning_rate': 0.18310542345881367, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 287}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:42,162] Trial 164 finished with value: 0.7016986183010366 and parameters: {'n_estimators': 406, 'learning_rate': 0.19447668907344648, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 733}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:43,122] Trial 165 finished with value: 0.6778260276082421 and parameters: {'n_estimators': 455, 'learning_rate': 0.17858773842526499, 'max_depth': 3, 'max_bin': 280, 'num_leaves': 750}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:44,871] Trial 166 finished with value: 0.70443464939088 and parameters: {'n_estimators': 375, 'learning_rate': 0.19675560961849445, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 690}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:46,655] Trial 167 finished with value: 0.70645148976453 and parameters: {'n_estimators': 514, 'learning_rate': 0.18539798669800972, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 308}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:48,233] Trial 168 finished with value: 0.703959888904006 and parameters: {'n_estimators': 506, 'learning_rate': 0.18488632229986987, 'max_depth': 12, 'max_bin': 297, 'num_leaves': 312}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:49,875] Trial 169 finished with value: 0.6997348217039809 and parameters: {'n_estimators': 560, 'learning_rate': 0.1895086703393437, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 340}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:51,418] Trial 170 finished with value: 0.7038865775252483 and parameters: {'n_estimators': 489, 'learning_rate': 0.19998276259900638, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 275}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:53,252] Trial 171 finished with value: 0.7117090284729698 and parameters: {'n_estimators': 533, 'learning_rate': 0.1327896441587968, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 320}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:54,903] Trial 172 finished with value: 0.7096780782993172 and parameters: {'n_estimators': 533, 'learning_rate': 0.1389768395393067, 'max_depth': 11, 'max_bin': 283, 'num_leaves': 318}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:56,657] Trial 173 finished with value: 0.7048122615338659 and parameters: {'n_estimators': 533, 'learning_rate': 0.14522428842512036, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 292}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:14:58,474] Trial 174 finished with value: 0.7012389593150995 and parameters: {'n_estimators': 543, 'learning_rate': 0.1274783731272675, 'max_depth': 11, 'max_bin': 291, 'num_leaves': 249}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:15:00,247] Trial 175 finished with value: 0.7050216869853484 and parameters: {'n_estimators': 565, 'learning_rate': 0.15146533074120286, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 325}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:15:02,079] Trial 176 finished with value: 0.7088252082605432 and parameters: {'n_estimators': 477, 'learning_rate': 0.13363935670966445, 'max_depth': 11, 'max_bin': 282, 'num_leaves': 300}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:15:03,977] Trial 177 finished with value: 0.7116697895088919 and parameters: {'n_estimators': 518, 'learning_rate': 0.13186823116712207, 'max_depth': 11, 'max_bin': 283, 'num_leaves': 317}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:15:05,632] Trial 178 finished with value: 0.7106052008570002 and parameters: {'n_estimators': 514, 'learning_rate': 0.12278135221067077, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 307}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:15:07,155] Trial 179 finished with value: 0.7048127824611655 and parameters: {'n_estimators': 524, 'learning_rate': 0.139975889537809, 'max_depth': 11, 'max_bin': 279, 'num_leaves': 299}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:15:08,787] Trial 180 finished with value: 0.708808648848769 and parameters: {'n_estimators': 549, 'learning_rate': 0.13303564278564878, 'max_depth': 11, 'max_bin': 296, 'num_leaves': 318}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:15:10,537] Trial 181 finished with value: 0.710615186547215 and parameters: {'n_estimators': 548, 'learning_rate': 0.13395433888686498, 'max_depth': 11, 'max_bin': 281, 'num_leaves': 321}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:15:12,416] Trial 182 finished with value: 0.7112277065090139 and parameters: {'n_estimators': 553, 'learning_rate': 0.1328120073504263, 'max_depth': 11, 'max_bin': 282, 'num_leaves': 321}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:15:14,073] Trial 183 finished with value: 0.7070826937441457 and parameters: {'n_estimators': 578, 'learning_rate': 0.13443483614249763, 'max_depth': 11, 'max_bin': 281, 'num_leaves': 318}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:15:15,960] Trial 184 finished with value: 0.7101576777752078 and parameters: {'n_estimators': 588, 'learning_rate': 0.13067287575187547, 'max_depth': 11, 'max_bin': 280, 'num_leaves': 321}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:15:17,702] Trial 185 finished with value: 0.7021765115666226 and parameters: {'n_estimators': 593, 'learning_rate': 0.13263226594179423, 'max_depth': 11, 'max_bin': 283, 'num_leaves': 352}. Best is trial 40 with value: 0.7129096258380114.\n",
      "[I 2023-12-12 03:15:19,415] Trial 186 finished with value: 0.714111055926004 and parameters: {'n_estimators': 579, 'learning_rate': 0.13492960495800832, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 325}. Best is trial 186 with value: 0.714111055926004.\n",
      "[I 2023-12-12 03:15:20,534] Trial 187 finished with value: 0.698956557871173 and parameters: {'n_estimators': 114, 'learning_rate': 0.12979485556206305, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 322}. Best is trial 186 with value: 0.714111055926004.\n",
      "[I 2023-12-12 03:15:22,230] Trial 188 finished with value: 0.7103471022911259 and parameters: {'n_estimators': 550, 'learning_rate': 0.1373085048198886, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 333}. Best is trial 186 with value: 0.714111055926004.\n",
      "[I 2023-12-12 03:15:24,044] Trial 189 finished with value: 0.7075888793729165 and parameters: {'n_estimators': 544, 'learning_rate': 0.13780022920541593, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 336}. Best is trial 186 with value: 0.714111055926004.\n",
      "[I 2023-12-12 03:15:26,020] Trial 190 finished with value: 0.7110259145333468 and parameters: {'n_estimators': 596, 'learning_rate': 0.13740122246548103, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 363}. Best is trial 186 with value: 0.714111055926004.\n",
      "[I 2023-12-12 03:15:27,893] Trial 191 finished with value: 0.7147343094409851 and parameters: {'n_estimators': 602, 'learning_rate': 0.13609095642941718, 'max_depth': 11, 'max_bin': 273, 'num_leaves': 376}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:29,565] Trial 192 finished with value: 0.7127223970159239 and parameters: {'n_estimators': 602, 'learning_rate': 0.12429634343792019, 'max_depth': 11, 'max_bin': 268, 'num_leaves': 384}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:31,135] Trial 193 finished with value: 0.709237706398973 and parameters: {'n_estimators': 602, 'learning_rate': 0.12261455380438156, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 385}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:32,932] Trial 194 finished with value: 0.7099257216961844 and parameters: {'n_estimators': 606, 'learning_rate': 0.12362470443061911, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 380}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:34,795] Trial 195 finished with value: 0.7104368487886323 and parameters: {'n_estimators': 639, 'learning_rate': 0.12567845355687277, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 386}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:36,715] Trial 196 finished with value: 0.7083325294273632 and parameters: {'n_estimators': 637, 'learning_rate': 0.12308098453890158, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 399}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:38,639] Trial 197 finished with value: 0.7122597885797739 and parameters: {'n_estimators': 606, 'learning_rate': 0.1342675394181844, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 386}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:40,054] Trial 198 finished with value: 0.7020056666991914 and parameters: {'n_estimators': 610, 'learning_rate': 0.1347197532939405, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 370}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:41,819] Trial 199 finished with value: 0.7057017157043978 and parameters: {'n_estimators': 671, 'learning_rate': 0.13021233979570923, 'max_depth': 11, 'max_bin': 268, 'num_leaves': 419}. Best is trial 191 with value: 0.7147343094409851.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7147\n",
      "\tBest params:\n",
      "\t\tn_estimators: 602\n",
      "\t\tlearning_rate: 0.13609095642941718\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 273\n",
      "\t\tnum_leaves: 376\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.655901    0.686027    0.679052    0.679859\n",
      "1                    TP   28.000000   34.000000   39.000000   31.000000\n",
      "2                    TN  305.000000  309.000000  308.000000  307.000000\n",
      "3                    FP   11.000000    6.000000    7.000000    7.000000\n",
      "4                    FN   38.000000   33.000000   28.000000   37.000000\n",
      "5              Accuracy    0.871728    0.897906    0.908377    0.884817\n",
      "6             Precision    0.717949    0.850000    0.847826    0.815789\n",
      "7           Sensitivity    0.424242    0.507463    0.582090    0.455882\n",
      "8           Specificity    0.965200    0.981000    0.977800    0.977700\n",
      "9              F1 score    0.533333    0.635514    0.690265    0.584906\n",
      "10  F1 score (weighted)    0.857863    0.887123    0.901341    0.871143\n",
      "11     F1 score (macro)    0.729489    0.788077    0.818251    0.759018\n",
      "12    Balanced Accuracy    0.694716    0.744208    0.779934    0.716795\n",
      "13                  MCC    0.486260    0.606651    0.654228    0.554149\n",
      "14                  NPV    0.889200    0.903500    0.916700    0.892400\n",
      "15              ROC_AUC    0.694716    0.744208    0.779934    0.716795\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_lgbm_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_3_cat = np.where(((y_pred_lgbm_3 >= 2) | (y_pred_lgbm_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:15:43,798] Trial 200 finished with value: 0.6943293920542286 and parameters: {'n_estimators': 646, 'learning_rate': 0.1404000169278535, 'max_depth': 11, 'max_bin': 263, 'num_leaves': 389}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:45,650] Trial 201 finished with value: 0.6849901033784209 and parameters: {'n_estimators': 601, 'learning_rate': 0.12479144858156765, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 361}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:47,313] Trial 202 finished with value: 0.6877216267071715 and parameters: {'n_estimators': 626, 'learning_rate': 0.1342788236412381, 'max_depth': 11, 'max_bin': 272, 'num_leaves': 388}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:49,266] Trial 203 finished with value: 0.6887916483544332 and parameters: {'n_estimators': 592, 'learning_rate': 0.12741573501499165, 'max_depth': 11, 'max_bin': 283, 'num_leaves': 372}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:51,198] Trial 204 finished with value: 0.6975636249517426 and parameters: {'n_estimators': 611, 'learning_rate': 0.13717452138670105, 'max_depth': 11, 'max_bin': 269, 'num_leaves': 412}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:52,957] Trial 205 finished with value: 0.6897066144883304 and parameters: {'n_estimators': 582, 'learning_rate': 0.12079581665203758, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 351}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:54,861] Trial 206 finished with value: 0.6927791757706858 and parameters: {'n_estimators': 629, 'learning_rate': 0.13065618955467137, 'max_depth': 11, 'max_bin': 282, 'num_leaves': 443}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:56,821] Trial 207 finished with value: 0.6916634495870536 and parameters: {'n_estimators': 593, 'learning_rate': 0.12709715790025866, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 342}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:15:58,947] Trial 208 finished with value: 0.6945202152990777 and parameters: {'n_estimators': 665, 'learning_rate': 0.13197702352687932, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 381}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:00,868] Trial 209 finished with value: 0.6895299316264077 and parameters: {'n_estimators': 611, 'learning_rate': 0.14062841758687944, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 363}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:02,775] Trial 210 finished with value: 0.6880106566153871 and parameters: {'n_estimators': 644, 'learning_rate': 0.11727716266859314, 'max_depth': 11, 'max_bin': 270, 'num_leaves': 331}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:04,735] Trial 211 finished with value: 0.6914801072837663 and parameters: {'n_estimators': 578, 'learning_rate': 0.12363130042575601, 'max_depth': 11, 'max_bin': 280, 'num_leaves': 393}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:06,573] Trial 212 finished with value: 0.6922896322911527 and parameters: {'n_estimators': 618, 'learning_rate': 0.13527474884136984, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 350}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:08,394] Trial 213 finished with value: 0.6935234611701735 and parameters: {'n_estimators': 594, 'learning_rate': 0.14255089696537324, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 309}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:10,047] Trial 214 finished with value: 0.6926486758339319 and parameters: {'n_estimators': 572, 'learning_rate': 0.13780293967397444, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 376}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:11,732] Trial 215 finished with value: 0.6886178202516555 and parameters: {'n_estimators': 598, 'learning_rate': 0.1304273704365544, 'max_depth': 11, 'max_bin': 273, 'num_leaves': 332}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:13,568] Trial 216 finished with value: 0.6935446443019899 and parameters: {'n_estimators': 631, 'learning_rate': 0.12622720303055504, 'max_depth': 11, 'max_bin': 277, 'num_leaves': 405}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:15,538] Trial 217 finished with value: 0.6956200754141765 and parameters: {'n_estimators': 579, 'learning_rate': 0.12235655380979903, 'max_depth': 11, 'max_bin': 284, 'num_leaves': 316}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:17,362] Trial 218 finished with value: 0.690022751385456 and parameters: {'n_estimators': 558, 'learning_rate': 0.13395906275239927, 'max_depth': 11, 'max_bin': 288, 'num_leaves': 429}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:18,891] Trial 219 finished with value: 0.6911477501248486 and parameters: {'n_estimators': 176, 'learning_rate': 0.11879618940035312, 'max_depth': 11, 'max_bin': 282, 'num_leaves': 357}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:20,747] Trial 220 finished with value: 0.6956818015626568 and parameters: {'n_estimators': 606, 'learning_rate': 0.1430227531730324, 'max_depth': 11, 'max_bin': 280, 'num_leaves': 303}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:22,657] Trial 221 finished with value: 0.6953875090443685 and parameters: {'n_estimators': 681, 'learning_rate': 0.12343256416582203, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 402}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:24,412] Trial 222 finished with value: 0.6900738939692614 and parameters: {'n_estimators': 637, 'learning_rate': 0.1281606175704633, 'max_depth': 11, 'max_bin': 269, 'num_leaves': 341}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:26,140] Trial 223 finished with value: 0.6928327979203475 and parameters: {'n_estimators': 636, 'learning_rate': 0.1376617050838323, 'max_depth': 11, 'max_bin': 262, 'num_leaves': 379}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:27,862] Trial 224 finished with value: 0.692732886484366 and parameters: {'n_estimators': 653, 'learning_rate': 0.1307418171172265, 'max_depth': 11, 'max_bin': 277, 'num_leaves': 321}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:29,686] Trial 225 finished with value: 0.6958658526426305 and parameters: {'n_estimators': 621, 'learning_rate': 0.12201651508094406, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 328}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:31,551] Trial 226 finished with value: 0.6931333375497022 and parameters: {'n_estimators': 573, 'learning_rate': 0.13377236975547935, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 393}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:33,183] Trial 227 finished with value: 0.687011109213333 and parameters: {'n_estimators': 590, 'learning_rate': 0.11472150147694093, 'max_depth': 11, 'max_bin': 280, 'num_leaves': 299}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:35,137] Trial 228 finished with value: 0.691830382373988 and parameters: {'n_estimators': 553, 'learning_rate': 0.12516618981294506, 'max_depth': 11, 'max_bin': 265, 'num_leaves': 358}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:36,962] Trial 229 finished with value: 0.6859223373048343 and parameters: {'n_estimators': 523, 'learning_rate': 0.13741011387732818, 'max_depth': 11, 'max_bin': 284, 'num_leaves': 342}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:38,810] Trial 230 finished with value: 0.6898665150908609 and parameters: {'n_estimators': 607, 'learning_rate': 0.12918951312234656, 'max_depth': 11, 'max_bin': 288, 'num_leaves': 466}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:40,476] Trial 231 finished with value: 0.6894742166575699 and parameters: {'n_estimators': 518, 'learning_rate': 0.14687523151646587, 'max_depth': 11, 'max_bin': 259, 'num_leaves': 120}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:42,347] Trial 232 finished with value: 0.6935655980664578 and parameters: {'n_estimators': 499, 'learning_rate': 0.10963390710325596, 'max_depth': 11, 'max_bin': 279, 'num_leaves': 50}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:44,273] Trial 233 finished with value: 0.6965947774844737 and parameters: {'n_estimators': 533, 'learning_rate': 0.13242418423778216, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 311}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:45,901] Trial 234 finished with value: 0.691207436085864 and parameters: {'n_estimators': 479, 'learning_rate': 0.14040087288519404, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 371}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:47,696] Trial 235 finished with value: 0.6944981479397002 and parameters: {'n_estimators': 505, 'learning_rate': 0.12681645143255546, 'max_depth': 11, 'max_bin': 282, 'num_leaves': 288}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:49,073] Trial 236 finished with value: 0.6803362272107638 and parameters: {'n_estimators': 586, 'learning_rate': 0.13551782559520173, 'max_depth': 6, 'max_bin': 272, 'num_leaves': 325}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:51,142] Trial 237 finished with value: 0.693707436743219 and parameters: {'n_estimators': 571, 'learning_rate': 0.12234259955777349, 'max_depth': 11, 'max_bin': 273, 'num_leaves': 312}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:53,005] Trial 238 finished with value: 0.698494744551261 and parameters: {'n_estimators': 620, 'learning_rate': 0.1315848386274943, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 339}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:55,007] Trial 239 finished with value: 0.6873133658053311 and parameters: {'n_estimators': 549, 'learning_rate': 0.12671050386020763, 'max_depth': 10, 'max_bin': 278, 'num_leaves': 391}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:56,582] Trial 240 finished with value: 0.6888184178457177 and parameters: {'n_estimators': 657, 'learning_rate': 0.14330640108634832, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 300}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:16:58,258] Trial 241 finished with value: 0.6864758819875533 and parameters: {'n_estimators': 564, 'learning_rate': 0.13586628770198605, 'max_depth': 11, 'max_bin': 280, 'num_leaves': 325}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:00,274] Trial 242 finished with value: 0.6993347690361931 and parameters: {'n_estimators': 541, 'learning_rate': 0.11982082227128077, 'max_depth': 11, 'max_bin': 269, 'num_leaves': 362}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:02,111] Trial 243 finished with value: 0.6971259921434271 and parameters: {'n_estimators': 513, 'learning_rate': 0.139986349289544, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 495}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:04,114] Trial 244 finished with value: 0.6931559500591491 and parameters: {'n_estimators': 558, 'learning_rate': 0.13035972587854094, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 381}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:05,972] Trial 245 finished with value: 0.6854367388304257 and parameters: {'n_estimators': 598, 'learning_rate': 0.12475654223233971, 'max_depth': 11, 'max_bin': 283, 'num_leaves': 348}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:07,632] Trial 246 finished with value: 0.6941212031903993 and parameters: {'n_estimators': 490, 'learning_rate': 0.13382781970673321, 'max_depth': 11, 'max_bin': 264, 'num_leaves': 316}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:09,679] Trial 247 finished with value: 0.6937539485280096 and parameters: {'n_estimators': 583, 'learning_rate': 0.1287182428174396, 'max_depth': 11, 'max_bin': 272, 'num_leaves': 527}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:11,783] Trial 248 finished with value: 0.6932476540898291 and parameters: {'n_estimators': 522, 'learning_rate': 0.13929135966812026, 'max_depth': 11, 'max_bin': 286, 'num_leaves': 276}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:13,595] Trial 249 finished with value: 0.6866391173454023 and parameters: {'n_estimators': 541, 'learning_rate': 0.14443238793009525, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 411}. Best is trial 191 with value: 0.7147343094409851.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7147\n",
      "\tBest params:\n",
      "\t\tn_estimators: 602\n",
      "\t\tlearning_rate: 0.13609095642941718\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 273\n",
      "\t\tnum_leaves: 376\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.655901    0.686027    0.679052    0.679859   \n",
      "1                    TP   28.000000   34.000000   39.000000   31.000000   \n",
      "2                    TN  305.000000  309.000000  308.000000  307.000000   \n",
      "3                    FP   11.000000    6.000000    7.000000    7.000000   \n",
      "4                    FN   38.000000   33.000000   28.000000   37.000000   \n",
      "5              Accuracy    0.871728    0.897906    0.908377    0.884817   \n",
      "6             Precision    0.717949    0.850000    0.847826    0.815789   \n",
      "7           Sensitivity    0.424242    0.507463    0.582090    0.455882   \n",
      "8           Specificity    0.965200    0.981000    0.977800    0.977700   \n",
      "9              F1 score    0.533333    0.635514    0.690265    0.584906   \n",
      "10  F1 score (weighted)    0.857863    0.887123    0.901341    0.871143   \n",
      "11     F1 score (macro)    0.729489    0.788077    0.818251    0.759018   \n",
      "12    Balanced Accuracy    0.694716    0.744208    0.779934    0.716795   \n",
      "13                  MCC    0.486260    0.606651    0.654228    0.554149   \n",
      "14                  NPV    0.889200    0.903500    0.916700    0.892400   \n",
      "15              ROC_AUC    0.694716    0.744208    0.779934    0.716795   \n",
      "\n",
      "          Set4  \n",
      "0     0.729972  \n",
      "1    34.000000  \n",
      "2   312.000000  \n",
      "3     4.000000  \n",
      "4    32.000000  \n",
      "5     0.905759  \n",
      "6     0.894737  \n",
      "7     0.515152  \n",
      "8     0.987300  \n",
      "9     0.653846  \n",
      "10    0.895072  \n",
      "11    0.799650  \n",
      "12    0.751247  \n",
      "13    0.634709  \n",
      "14    0.907000  \n",
      "15    0.751247  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_lgbm_4)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_4_cat = np.where(((y_pred_lgbm_4 >= 2) | (y_pred_lgbm_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:17:15,585] Trial 250 finished with value: 0.681554014695315 and parameters: {'n_estimators': 566, 'learning_rate': 0.11867877220030928, 'max_depth': 11, 'max_bin': 281, 'num_leaves': 298}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:17,232] Trial 251 finished with value: 0.6746563334132148 and parameters: {'n_estimators': 612, 'learning_rate': 0.14832199760279324, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 332}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:19,171] Trial 252 finished with value: 0.6784855864395809 and parameters: {'n_estimators': 641, 'learning_rate': 0.13636846815090978, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 401}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:20,749] Trial 253 finished with value: 0.6770703653575343 and parameters: {'n_estimators': 310, 'learning_rate': 0.13076327354059483, 'max_depth': 11, 'max_bin': 268, 'num_leaves': 375}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:22,414] Trial 254 finished with value: 0.6840349901376508 and parameters: {'n_estimators': 599, 'learning_rate': 0.12194833417132302, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 315}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:24,112] Trial 255 finished with value: 0.6803564809679032 and parameters: {'n_estimators': 551, 'learning_rate': 0.12540019929062693, 'max_depth': 11, 'max_bin': 281, 'num_leaves': 348}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:26,008] Trial 256 finished with value: 0.6826007872829243 and parameters: {'n_estimators': 625, 'learning_rate': 0.1327388403548767, 'max_depth': 11, 'max_bin': 290, 'num_leaves': 332}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:27,550] Trial 257 finished with value: 0.6794822020535902 and parameters: {'n_estimators': 531, 'learning_rate': 0.10866788315825052, 'max_depth': 10, 'max_bin': 285, 'num_leaves': 363}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:29,217] Trial 258 finished with value: 0.6769916122603439 and parameters: {'n_estimators': 579, 'learning_rate': 0.14083346721657403, 'max_depth': 11, 'max_bin': 179, 'num_leaves': 290}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:30,990] Trial 259 finished with value: 0.6768673464757264 and parameters: {'n_estimators': 496, 'learning_rate': 0.1364889129657452, 'max_depth': 11, 'max_bin': 277, 'num_leaves': 306}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:32,802] Trial 260 finished with value: 0.6797005654928967 and parameters: {'n_estimators': 472, 'learning_rate': 0.11666370993331274, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 323}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:34,690] Trial 261 finished with value: 0.6861598690274947 and parameters: {'n_estimators': 861, 'learning_rate': 0.12886549229160765, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 392}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:36,671] Trial 262 finished with value: 0.6871060811045033 and parameters: {'n_estimators': 558, 'learning_rate': 0.08784150430644479, 'max_depth': 11, 'max_bin': 279, 'num_leaves': 339}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:38,335] Trial 263 finished with value: 0.6823186852521939 and parameters: {'n_estimators': 600, 'learning_rate': 0.14568388209007913, 'max_depth': 11, 'max_bin': 282, 'num_leaves': 360}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:40,150] Trial 264 finished with value: 0.6805063205443596 and parameters: {'n_estimators': 513, 'learning_rate': 0.1339163519525814, 'max_depth': 11, 'max_bin': 297, 'num_leaves': 306}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:41,938] Trial 265 finished with value: 0.6793518157533205 and parameters: {'n_estimators': 535, 'learning_rate': 0.12471479110834541, 'max_depth': 10, 'max_bin': 273, 'num_leaves': 382}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:43,755] Trial 266 finished with value: 0.6831515748147192 and parameters: {'n_estimators': 272, 'learning_rate': 0.13875559322667344, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 419}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:45,732] Trial 267 finished with value: 0.6854810574730319 and parameters: {'n_estimators': 580, 'learning_rate': 0.11352390581397843, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 288}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:47,582] Trial 268 finished with value: 0.6795413197732509 and parameters: {'n_estimators': 618, 'learning_rate': 0.1519001228419362, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 607}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:49,251] Trial 269 finished with value: 0.6801262380019605 and parameters: {'n_estimators': 483, 'learning_rate': 0.12077709223132153, 'max_depth': 10, 'max_bin': 292, 'num_leaves': 323}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:51,028] Trial 270 finished with value: 0.6828513473382964 and parameters: {'n_estimators': 636, 'learning_rate': 0.12847541838990678, 'max_depth': 11, 'max_bin': 281, 'num_leaves': 351}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:52,942] Trial 271 finished with value: 0.6789042664185506 and parameters: {'n_estimators': 569, 'learning_rate': 0.1328276392644012, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 336}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:54,446] Trial 272 finished with value: 0.6773504941660967 and parameters: {'n_estimators': 506, 'learning_rate': 0.1435673039566703, 'max_depth': 11, 'max_bin': 284, 'num_leaves': 371}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:56,452] Trial 273 finished with value: 0.6788052651057058 and parameters: {'n_estimators': 549, 'learning_rate': 0.13830067875821814, 'max_depth': 11, 'max_bin': 277, 'num_leaves': 553}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:58,268] Trial 274 finished with value: 0.6809022572310376 and parameters: {'n_estimators': 594, 'learning_rate': 0.12655065232088797, 'max_depth': 11, 'max_bin': 273, 'num_leaves': 310}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:17:59,789] Trial 275 finished with value: 0.676512827525573 and parameters: {'n_estimators': 755, 'learning_rate': 0.1543777472302401, 'max_depth': 11, 'max_bin': 264, 'num_leaves': 398}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:01,387] Trial 276 finished with value: 0.6807343015387511 and parameters: {'n_estimators': 525, 'learning_rate': 0.11847283682240584, 'max_depth': 11, 'max_bin': 279, 'num_leaves': 346}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:02,895] Trial 277 finished with value: 0.6806324007764438 and parameters: {'n_estimators': 695, 'learning_rate': 0.13213183693229347, 'max_depth': 11, 'max_bin': 268, 'num_leaves': 275}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:04,480] Trial 278 finished with value: 0.676413904755643 and parameters: {'n_estimators': 654, 'learning_rate': 0.1623775970765404, 'max_depth': 10, 'max_bin': 284, 'num_leaves': 326}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:06,239] Trial 279 finished with value: 0.6820819654820669 and parameters: {'n_estimators': 559, 'learning_rate': 0.136027478742553, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 300}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:07,519] Trial 280 finished with value: 0.6651953086435388 and parameters: {'n_estimators': 457, 'learning_rate': 0.12429375439529951, 'max_depth': 5, 'max_bin': 274, 'num_leaves': 381}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:09,350] Trial 281 finished with value: 0.6791735682856543 and parameters: {'n_estimators': 586, 'learning_rate': 0.14853445576794405, 'max_depth': 11, 'max_bin': 280, 'num_leaves': 456}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:10,873] Trial 282 finished with value: 0.6794942797558938 and parameters: {'n_estimators': 210, 'learning_rate': 0.14271921577830993, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 319}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:12,525] Trial 283 finished with value: 0.6786686357280013 and parameters: {'n_estimators': 608, 'learning_rate': 0.12942247769997936, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 369}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:14,015] Trial 284 finished with value: 0.6757192152159011 and parameters: {'n_estimators': 539, 'learning_rate': 0.13531233859707856, 'max_depth': 11, 'max_bin': 282, 'num_leaves': 644}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:15,767] Trial 285 finished with value: 0.6752678953008251 and parameters: {'n_estimators': 491, 'learning_rate': 0.1585944740494655, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 287}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:17,465] Trial 286 finished with value: 0.6766282587748511 and parameters: {'n_estimators': 627, 'learning_rate': 0.12282710755084739, 'max_depth': 11, 'max_bin': 260, 'num_leaves': 336}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:19,223] Trial 287 finished with value: 0.6797818135644835 and parameters: {'n_estimators': 569, 'learning_rate': 0.14043796906990544, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 354}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:21,100] Trial 288 finished with value: 0.6802806608660663 and parameters: {'n_estimators': 516, 'learning_rate': 0.12950759186654734, 'max_depth': 11, 'max_bin': 293, 'num_leaves': 408}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:22,928] Trial 289 finished with value: 0.6825758925475285 and parameters: {'n_estimators': 602, 'learning_rate': 0.11849616666748952, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 572}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:25,338] Trial 290 finished with value: 0.6659525787068302 and parameters: {'n_estimators': 472, 'learning_rate': 0.026254298543337218, 'max_depth': 8, 'max_bin': 283, 'num_leaves': 178}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:27,020] Trial 291 finished with value: 0.6798528405121259 and parameters: {'n_estimators': 531, 'learning_rate': 0.13087310756646708, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 305}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:28,634] Trial 292 finished with value: 0.6804440678266721 and parameters: {'n_estimators': 503, 'learning_rate': 0.16476522197063798, 'max_depth': 11, 'max_bin': 229, 'num_leaves': 432}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:30,156] Trial 293 finished with value: 0.677376907393444 and parameters: {'n_estimators': 557, 'learning_rate': 0.13612605327798302, 'max_depth': 10, 'max_bin': 254, 'num_leaves': 321}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:32,266] Trial 294 finished with value: 0.6802651421019908 and parameters: {'n_estimators': 583, 'learning_rate': 0.11272394793860972, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 385}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:33,927] Trial 295 finished with value: 0.6822175281596027 and parameters: {'n_estimators': 618, 'learning_rate': 0.12696119806430203, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 344}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:35,903] Trial 296 finished with value: 0.6811544574569165 and parameters: {'n_estimators': 542, 'learning_rate': 0.10683273894195387, 'max_depth': 11, 'max_bin': 279, 'num_leaves': 593}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:37,916] Trial 297 finished with value: 0.6856411387798472 and parameters: {'n_estimators': 641, 'learning_rate': 0.12131173345324801, 'max_depth': 11, 'max_bin': 268, 'num_leaves': 365}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:39,715] Trial 298 finished with value: 0.6762966178396452 and parameters: {'n_estimators': 573, 'learning_rate': 0.1160547248181626, 'max_depth': 11, 'max_bin': 297, 'num_leaves': 311}. Best is trial 191 with value: 0.7147343094409851.\n",
      "[I 2023-12-12 03:18:41,369] Trial 299 finished with value: 0.6779414142273867 and parameters: {'n_estimators': 518, 'learning_rate': 0.14827047027689189, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 331}. Best is trial 191 with value: 0.7147343094409851.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7147\n",
      "\tBest params:\n",
      "\t\tn_estimators: 602\n",
      "\t\tlearning_rate: 0.13609095642941718\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 273\n",
      "\t\tnum_leaves: 376\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.655901    0.686027    0.679052    0.679859   \n",
      "1                    TP   28.000000   34.000000   39.000000   31.000000   \n",
      "2                    TN  305.000000  309.000000  308.000000  307.000000   \n",
      "3                    FP   11.000000    6.000000    7.000000    7.000000   \n",
      "4                    FN   38.000000   33.000000   28.000000   37.000000   \n",
      "5              Accuracy    0.871728    0.897906    0.908377    0.884817   \n",
      "6             Precision    0.717949    0.850000    0.847826    0.815789   \n",
      "7           Sensitivity    0.424242    0.507463    0.582090    0.455882   \n",
      "8           Specificity    0.965200    0.981000    0.977800    0.977700   \n",
      "9              F1 score    0.533333    0.635514    0.690265    0.584906   \n",
      "10  F1 score (weighted)    0.857863    0.887123    0.901341    0.871143   \n",
      "11     F1 score (macro)    0.729489    0.788077    0.818251    0.759018   \n",
      "12    Balanced Accuracy    0.694716    0.744208    0.779934    0.716795   \n",
      "13                  MCC    0.486260    0.606651    0.654228    0.554149   \n",
      "14                  NPV    0.889200    0.903500    0.916700    0.892400   \n",
      "15              ROC_AUC    0.694716    0.744208    0.779934    0.716795   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.729972    0.737187  \n",
      "1    34.000000   36.000000  \n",
      "2   312.000000  312.000000  \n",
      "3     4.000000    4.000000  \n",
      "4    32.000000   30.000000  \n",
      "5     0.905759    0.910995  \n",
      "6     0.894737    0.900000  \n",
      "7     0.515152    0.545455  \n",
      "8     0.987300    0.987300  \n",
      "9     0.653846    0.679245  \n",
      "10    0.895072    0.901837  \n",
      "11    0.799650    0.813787  \n",
      "12    0.751247    0.766398  \n",
      "13    0.634709    0.657860  \n",
      "14    0.907000    0.912300  \n",
      "15    0.751247    0.766398  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_lgbm_5)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_5_cat = np.where(((y_pred_lgbm_5 >= 2) | (y_pred_lgbm_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:18:43,754] Trial 300 finished with value: 0.7153329958600628 and parameters: {'n_estimators': 666, 'learning_rate': 0.1415546036841192, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 289}. Best is trial 300 with value: 0.7153329958600628.\n",
      "[I 2023-12-12 03:18:45,935] Trial 301 finished with value: 0.7181206849520032 and parameters: {'n_estimators': 669, 'learning_rate': 0.1424966575717498, 'max_depth': 10, 'max_bin': 274, 'num_leaves': 270}. Best is trial 301 with value: 0.7181206849520032.\n",
      "[I 2023-12-12 03:18:47,803] Trial 302 finished with value: 0.7109508072567801 and parameters: {'n_estimators': 670, 'learning_rate': 0.14147286656183458, 'max_depth': 10, 'max_bin': 275, 'num_leaves': 263}. Best is trial 301 with value: 0.7181206849520032.\n",
      "[I 2023-12-12 03:18:49,424] Trial 303 finished with value: 0.7120314358014048 and parameters: {'n_estimators': 713, 'learning_rate': 0.14330466316652288, 'max_depth': 10, 'max_bin': 275, 'num_leaves': 256}. Best is trial 301 with value: 0.7181206849520032.\n",
      "[I 2023-12-12 03:18:50,933] Trial 304 finished with value: 0.7049318257573758 and parameters: {'n_estimators': 698, 'learning_rate': 0.1448471210368045, 'max_depth': 9, 'max_bin': 276, 'num_leaves': 237}. Best is trial 301 with value: 0.7181206849520032.\n",
      "[I 2023-12-12 03:18:52,594] Trial 305 finished with value: 0.7148456345551061 and parameters: {'n_estimators': 721, 'learning_rate': 0.14106012299841583, 'max_depth': 10, 'max_bin': 275, 'num_leaves': 265}. Best is trial 301 with value: 0.7181206849520032.\n",
      "[I 2023-12-12 03:18:54,417] Trial 306 finished with value: 0.7163471833985338 and parameters: {'n_estimators': 730, 'learning_rate': 0.14064081720657465, 'max_depth': 10, 'max_bin': 274, 'num_leaves': 267}. Best is trial 301 with value: 0.7181206849520032.\n",
      "[I 2023-12-12 03:18:56,123] Trial 307 finished with value: 0.7173274497251799 and parameters: {'n_estimators': 763, 'learning_rate': 0.14126279592677743, 'max_depth': 10, 'max_bin': 273, 'num_leaves': 246}. Best is trial 301 with value: 0.7181206849520032.\n",
      "[I 2023-12-12 03:18:58,024] Trial 308 finished with value: 0.7148139718828166 and parameters: {'n_estimators': 729, 'learning_rate': 0.1425777746583533, 'max_depth': 10, 'max_bin': 273, 'num_leaves': 258}. Best is trial 301 with value: 0.7181206849520032.\n",
      "[I 2023-12-12 03:18:59,678] Trial 309 finished with value: 0.7094331114955224 and parameters: {'n_estimators': 727, 'learning_rate': 0.1407054063744428, 'max_depth': 10, 'max_bin': 273, 'num_leaves': 251}. Best is trial 301 with value: 0.7181206849520032.\n",
      "[I 2023-12-12 03:19:01,415] Trial 310 finished with value: 0.7091610583547137 and parameters: {'n_estimators': 733, 'learning_rate': 0.14396730327465415, 'max_depth': 10, 'max_bin': 275, 'num_leaves': 259}. Best is trial 301 with value: 0.7181206849520032.\n",
      "[I 2023-12-12 03:19:03,165] Trial 311 finished with value: 0.7115129021543865 and parameters: {'n_estimators': 745, 'learning_rate': 0.14599711270682975, 'max_depth': 10, 'max_bin': 272, 'num_leaves': 271}. Best is trial 301 with value: 0.7181206849520032.\n",
      "[I 2023-12-12 03:19:05,124] Trial 312 finished with value: 0.7182264793556408 and parameters: {'n_estimators': 770, 'learning_rate': 0.14870415287000716, 'max_depth': 10, 'max_bin': 271, 'num_leaves': 265}. Best is trial 312 with value: 0.7182264793556408.\n",
      "[I 2023-12-12 03:19:07,070] Trial 313 finished with value: 0.7171399728389255 and parameters: {'n_estimators': 755, 'learning_rate': 0.14847237753133916, 'max_depth': 10, 'max_bin': 271, 'num_leaves': 224}. Best is trial 312 with value: 0.7182264793556408.\n",
      "[I 2023-12-12 03:19:08,715] Trial 314 finished with value: 0.7122963923107205 and parameters: {'n_estimators': 761, 'learning_rate': 0.14669608631099018, 'max_depth': 10, 'max_bin': 270, 'num_leaves': 264}. Best is trial 312 with value: 0.7182264793556408.\n",
      "[I 2023-12-12 03:19:10,631] Trial 315 finished with value: 0.7178173054890186 and parameters: {'n_estimators': 757, 'learning_rate': 0.1500431846936622, 'max_depth': 10, 'max_bin': 270, 'num_leaves': 222}. Best is trial 312 with value: 0.7182264793556408.\n",
      "[I 2023-12-12 03:19:12,500] Trial 316 finished with value: 0.7150498501738191 and parameters: {'n_estimators': 764, 'learning_rate': 0.1498844520849554, 'max_depth': 10, 'max_bin': 270, 'num_leaves': 236}. Best is trial 312 with value: 0.7182264793556408.\n",
      "[I 2023-12-12 03:19:14,295] Trial 317 finished with value: 0.7163878464714367 and parameters: {'n_estimators': 769, 'learning_rate': 0.14990441269817825, 'max_depth': 10, 'max_bin': 270, 'num_leaves': 212}. Best is trial 312 with value: 0.7182264793556408.\n",
      "[I 2023-12-12 03:19:16,223] Trial 318 finished with value: 0.7206236275315592 and parameters: {'n_estimators': 785, 'learning_rate': 0.15026461092052895, 'max_depth': 10, 'max_bin': 269, 'num_leaves': 218}. Best is trial 318 with value: 0.7206236275315592.\n",
      "[I 2023-12-12 03:19:17,840] Trial 319 finished with value: 0.7180502949111796 and parameters: {'n_estimators': 789, 'learning_rate': 0.15082278391038265, 'max_depth': 10, 'max_bin': 270, 'num_leaves': 217}. Best is trial 318 with value: 0.7206236275315592.\n",
      "[I 2023-12-12 03:19:19,473] Trial 320 finished with value: 0.7180234900365638 and parameters: {'n_estimators': 778, 'learning_rate': 0.15137097267717142, 'max_depth': 10, 'max_bin': 270, 'num_leaves': 219}. Best is trial 318 with value: 0.7206236275315592.\n",
      "[I 2023-12-12 03:19:21,201] Trial 321 finished with value: 0.7144683309916976 and parameters: {'n_estimators': 784, 'learning_rate': 0.15058676198311757, 'max_depth': 10, 'max_bin': 269, 'num_leaves': 207}. Best is trial 318 with value: 0.7206236275315592.\n",
      "[I 2023-12-12 03:19:22,841] Trial 322 finished with value: 0.715265556921387 and parameters: {'n_estimators': 788, 'learning_rate': 0.15049649997399406, 'max_depth': 10, 'max_bin': 270, 'num_leaves': 221}. Best is trial 318 with value: 0.7206236275315592.\n",
      "[I 2023-12-12 03:19:24,273] Trial 323 finished with value: 0.7111711311381987 and parameters: {'n_estimators': 793, 'learning_rate': 0.1515307705987051, 'max_depth': 10, 'max_bin': 269, 'num_leaves': 220}. Best is trial 318 with value: 0.7206236275315592.\n",
      "[I 2023-12-12 03:19:25,936] Trial 324 finished with value: 0.7221064142747474 and parameters: {'n_estimators': 766, 'learning_rate': 0.15097403367434395, 'max_depth': 10, 'max_bin': 271, 'num_leaves': 192}. Best is trial 324 with value: 0.7221064142747474.\n",
      "[I 2023-12-12 03:19:27,633] Trial 325 finished with value: 0.7126172341407271 and parameters: {'n_estimators': 769, 'learning_rate': 0.15351186878406492, 'max_depth': 10, 'max_bin': 270, 'num_leaves': 193}. Best is trial 324 with value: 0.7221064142747474.\n",
      "[I 2023-12-12 03:19:29,414] Trial 326 finished with value: 0.7201881302999554 and parameters: {'n_estimators': 777, 'learning_rate': 0.15380565977008953, 'max_depth': 10, 'max_bin': 270, 'num_leaves': 195}. Best is trial 324 with value: 0.7221064142747474.\n",
      "[I 2023-12-12 03:19:31,032] Trial 327 finished with value: 0.7162261918227327 and parameters: {'n_estimators': 768, 'learning_rate': 0.15467512046559112, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 197}. Best is trial 324 with value: 0.7221064142747474.\n",
      "[I 2023-12-12 03:19:32,787] Trial 328 finished with value: 0.7224104278978583 and parameters: {'n_estimators': 770, 'learning_rate': 0.15103002977132218, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 197}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:34,278] Trial 329 finished with value: 0.7154296048049409 and parameters: {'n_estimators': 781, 'learning_rate': 0.15422542018096946, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 192}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:36,141] Trial 330 finished with value: 0.717417825547558 and parameters: {'n_estimators': 796, 'learning_rate': 0.15514956623997078, 'max_depth': 10, 'max_bin': 266, 'num_leaves': 199}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:37,891] Trial 331 finished with value: 0.7166339102254674 and parameters: {'n_estimators': 796, 'learning_rate': 0.15535501572392452, 'max_depth': 10, 'max_bin': 262, 'num_leaves': 198}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:39,647] Trial 332 finished with value: 0.7188713364172511 and parameters: {'n_estimators': 797, 'learning_rate': 0.15576720008275607, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 198}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:41,210] Trial 333 finished with value: 0.716418739770175 and parameters: {'n_estimators': 798, 'learning_rate': 0.15476299407829552, 'max_depth': 9, 'max_bin': 265, 'num_leaves': 198}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:42,910] Trial 334 finished with value: 0.7196766947008963 and parameters: {'n_estimators': 817, 'learning_rate': 0.1551545738833694, 'max_depth': 9, 'max_bin': 263, 'num_leaves': 181}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:44,898] Trial 335 finished with value: 0.7205104720062783 and parameters: {'n_estimators': 812, 'learning_rate': 0.15588666340551238, 'max_depth': 10, 'max_bin': 263, 'num_leaves': 190}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:46,362] Trial 336 finished with value: 0.7187520700391172 and parameters: {'n_estimators': 814, 'learning_rate': 0.1554289687857951, 'max_depth': 9, 'max_bin': 263, 'num_leaves': 188}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:48,073] Trial 337 finished with value: 0.7196874479407821 and parameters: {'n_estimators': 803, 'learning_rate': 0.15540549321560793, 'max_depth': 9, 'max_bin': 261, 'num_leaves': 187}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:49,666] Trial 338 finished with value: 0.7177174545867038 and parameters: {'n_estimators': 819, 'learning_rate': 0.15550413952252712, 'max_depth': 9, 'max_bin': 262, 'num_leaves': 185}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:51,214] Trial 339 finished with value: 0.7125285063279071 and parameters: {'n_estimators': 819, 'learning_rate': 0.15607380705399682, 'max_depth': 9, 'max_bin': 262, 'num_leaves': 192}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:52,781] Trial 340 finished with value: 0.7141094059959007 and parameters: {'n_estimators': 809, 'learning_rate': 0.15618300896585283, 'max_depth': 9, 'max_bin': 258, 'num_leaves': 166}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:54,583] Trial 341 finished with value: 0.7217302260055887 and parameters: {'n_estimators': 837, 'learning_rate': 0.1546624011808108, 'max_depth': 9, 'max_bin': 263, 'num_leaves': 180}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:56,346] Trial 342 finished with value: 0.7172716044859062 and parameters: {'n_estimators': 850, 'learning_rate': 0.1549679987695801, 'max_depth': 9, 'max_bin': 263, 'num_leaves': 182}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:57,896] Trial 343 finished with value: 0.7095972699827254 and parameters: {'n_estimators': 841, 'learning_rate': 0.15871124345306994, 'max_depth': 9, 'max_bin': 262, 'num_leaves': 151}. Best is trial 328 with value: 0.7224104278978583.\n",
      "[I 2023-12-12 03:19:59,544] Trial 344 finished with value: 0.7240465774291549 and parameters: {'n_estimators': 807, 'learning_rate': 0.1545739174645877, 'max_depth': 9, 'max_bin': 258, 'num_leaves': 176}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:01,200] Trial 345 finished with value: 0.7060863676884612 and parameters: {'n_estimators': 804, 'learning_rate': 0.15873503253418542, 'max_depth': 9, 'max_bin': 258, 'num_leaves': 175}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:02,851] Trial 346 finished with value: 0.7193470471505432 and parameters: {'n_estimators': 840, 'learning_rate': 0.153537575026213, 'max_depth': 9, 'max_bin': 260, 'num_leaves': 182}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:04,547] Trial 347 finished with value: 0.7149337494977398 and parameters: {'n_estimators': 852, 'learning_rate': 0.15333644115429101, 'max_depth': 9, 'max_bin': 256, 'num_leaves': 212}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:06,278] Trial 348 finished with value: 0.7185747376157685 and parameters: {'n_estimators': 831, 'learning_rate': 0.1564300830059162, 'max_depth': 9, 'max_bin': 260, 'num_leaves': 155}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:07,819] Trial 349 finished with value: 0.7159375735229491 and parameters: {'n_estimators': 828, 'learning_rate': 0.15722432190942975, 'max_depth': 9, 'max_bin': 261, 'num_leaves': 146}. Best is trial 344 with value: 0.7240465774291549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.724047\n",
      "\tBest params:\n",
      "\t\tn_estimators: 807\n",
      "\t\tlearning_rate: 0.1545739174645877\n",
      "\t\tmax_depth: 9\n",
      "\t\tmax_bin: 258\n",
      "\t\tnum_leaves: 176\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.655901    0.686027    0.679052    0.679859   \n",
      "1                    TP   28.000000   34.000000   39.000000   31.000000   \n",
      "2                    TN  305.000000  309.000000  308.000000  307.000000   \n",
      "3                    FP   11.000000    6.000000    7.000000    7.000000   \n",
      "4                    FN   38.000000   33.000000   28.000000   37.000000   \n",
      "5              Accuracy    0.871728    0.897906    0.908377    0.884817   \n",
      "6             Precision    0.717949    0.850000    0.847826    0.815789   \n",
      "7           Sensitivity    0.424242    0.507463    0.582090    0.455882   \n",
      "8           Specificity    0.965200    0.981000    0.977800    0.977700   \n",
      "9              F1 score    0.533333    0.635514    0.690265    0.584906   \n",
      "10  F1 score (weighted)    0.857863    0.887123    0.901341    0.871143   \n",
      "11     F1 score (macro)    0.729489    0.788077    0.818251    0.759018   \n",
      "12    Balanced Accuracy    0.694716    0.744208    0.779934    0.716795   \n",
      "13                  MCC    0.486260    0.606651    0.654228    0.554149   \n",
      "14                  NPV    0.889200    0.903500    0.916700    0.892400   \n",
      "15              ROC_AUC    0.694716    0.744208    0.779934    0.716795   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.729972    0.737187    0.704752  \n",
      "1    34.000000   36.000000   33.000000  \n",
      "2   312.000000  312.000000  307.000000  \n",
      "3     4.000000    4.000000    7.000000  \n",
      "4    32.000000   30.000000   35.000000  \n",
      "5     0.905759    0.910995    0.890052  \n",
      "6     0.894737    0.900000    0.825000  \n",
      "7     0.515152    0.545455    0.485294  \n",
      "8     0.987300    0.987300    0.977700  \n",
      "9     0.653846    0.679245    0.611111  \n",
      "10    0.895072    0.901837    0.878146  \n",
      "11    0.799650    0.813787    0.773543  \n",
      "12    0.751247    0.766398    0.731501  \n",
      "13    0.634709    0.657860    0.578440  \n",
      "14    0.907000    0.912300    0.897700  \n",
      "15    0.751247    0.766398    0.731501  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_lgbm_6)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_6_cat = np.where(((y_pred_lgbm_6 >= 2) | (y_pred_lgbm_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:20:09,600] Trial 350 finished with value: 0.7143524829357083 and parameters: {'n_estimators': 807, 'learning_rate': 0.16113618123847753, 'max_depth': 9, 'max_bin': 260, 'num_leaves': 179}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:11,099] Trial 351 finished with value: 0.7104224772563896 and parameters: {'n_estimators': 878, 'learning_rate': 0.15442808360986238, 'max_depth': 9, 'max_bin': 264, 'num_leaves': 182}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:12,517] Trial 352 finished with value: 0.7148400280703496 and parameters: {'n_estimators': 841, 'learning_rate': 0.15796355454200142, 'max_depth': 9, 'max_bin': 252, 'num_leaves': 153}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:14,099] Trial 353 finished with value: 0.7105380473839379 and parameters: {'n_estimators': 798, 'learning_rate': 0.15269271464235498, 'max_depth': 9, 'max_bin': 259, 'num_leaves': 201}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:15,548] Trial 354 finished with value: 0.7124943237000944 and parameters: {'n_estimators': 822, 'learning_rate': 0.15984118720000107, 'max_depth': 9, 'max_bin': 256, 'num_leaves': 166}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:17,157] Trial 355 finished with value: 0.7127874421051319 and parameters: {'n_estimators': 817, 'learning_rate': 0.15083725961317965, 'max_depth': 9, 'max_bin': 264, 'num_leaves': 230}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:18,593] Trial 356 finished with value: 0.7066963310568316 and parameters: {'n_estimators': 792, 'learning_rate': 0.15540309665557436, 'max_depth': 8, 'max_bin': 261, 'num_leaves': 185}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:20,240] Trial 357 finished with value: 0.7088916217249176 and parameters: {'n_estimators': 858, 'learning_rate': 0.1498573830588171, 'max_depth': 9, 'max_bin': 265, 'num_leaves': 203}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:21,626] Trial 358 finished with value: 0.710839190192606 and parameters: {'n_estimators': 833, 'learning_rate': 0.1554436968245362, 'max_depth': 9, 'max_bin': 262, 'num_leaves': 133}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:23,405] Trial 359 finished with value: 0.7049149851043488 and parameters: {'n_estimators': 749, 'learning_rate': 0.16221266464412956, 'max_depth': 9, 'max_bin': 265, 'num_leaves': 169}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:24,969] Trial 360 finished with value: 0.7082113002482717 and parameters: {'n_estimators': 804, 'learning_rate': 0.1527427479834646, 'max_depth': 9, 'max_bin': 258, 'num_leaves': 223}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:26,633] Trial 361 finished with value: 0.7032343486039652 and parameters: {'n_estimators': 782, 'learning_rate': 0.14849949982737043, 'max_depth': 8, 'max_bin': 255, 'num_leaves': 190}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:28,106] Trial 362 finished with value: 0.7016889439751336 and parameters: {'n_estimators': 883, 'learning_rate': 0.15930797000107044, 'max_depth': 9, 'max_bin': 266, 'num_leaves': 207}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:29,664] Trial 363 finished with value: 0.7122017057472826 and parameters: {'n_estimators': 817, 'learning_rate': 0.15613890099404928, 'max_depth': 10, 'max_bin': 262, 'num_leaves': 179}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:31,382] Trial 364 finished with value: 0.7156240239240492 and parameters: {'n_estimators': 839, 'learning_rate': 0.14801301670319394, 'max_depth': 10, 'max_bin': 260, 'num_leaves': 160}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:32,972] Trial 365 finished with value: 0.7133193526619366 and parameters: {'n_estimators': 794, 'learning_rate': 0.15316527659301307, 'max_depth': 9, 'max_bin': 265, 'num_leaves': 219}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:34,670] Trial 366 finished with value: 0.7118583777085453 and parameters: {'n_estimators': 777, 'learning_rate': 0.1629080492835687, 'max_depth': 10, 'max_bin': 263, 'num_leaves': 200}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:36,472] Trial 367 finished with value: 0.716526125074169 and parameters: {'n_estimators': 809, 'learning_rate': 0.15812389523244938, 'max_depth': 10, 'max_bin': 259, 'num_leaves': 234}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:38,222] Trial 368 finished with value: 0.7126042536773853 and parameters: {'n_estimators': 753, 'learning_rate': 0.15982138061440812, 'max_depth': 10, 'max_bin': 258, 'num_leaves': 238}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:39,962] Trial 369 finished with value: 0.7114487486519735 and parameters: {'n_estimators': 826, 'learning_rate': 0.1492123379642035, 'max_depth': 10, 'max_bin': 251, 'num_leaves': 229}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:41,600] Trial 370 finished with value: 0.7146831574174464 and parameters: {'n_estimators': 848, 'learning_rate': 0.15678704760334777, 'max_depth': 10, 'max_bin': 260, 'num_leaves': 178}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:43,407] Trial 371 finished with value: 0.717363625236155 and parameters: {'n_estimators': 810, 'learning_rate': 0.15229834890666355, 'max_depth': 10, 'max_bin': 255, 'num_leaves': 138}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:45,219] Trial 372 finished with value: 0.7176199911900405 and parameters: {'n_estimators': 869, 'learning_rate': 0.15229445532958097, 'max_depth': 10, 'max_bin': 254, 'num_leaves': 135}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:47,008] Trial 373 finished with value: 0.7153272233206349 and parameters: {'n_estimators': 872, 'learning_rate': 0.14765133914109901, 'max_depth': 10, 'max_bin': 253, 'num_leaves': 134}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:48,757] Trial 374 finished with value: 0.7149165058295283 and parameters: {'n_estimators': 899, 'learning_rate': 0.1510420544658659, 'max_depth': 10, 'max_bin': 255, 'num_leaves': 134}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:50,394] Trial 375 finished with value: 0.7088191570301523 and parameters: {'n_estimators': 776, 'learning_rate': 0.1518037710210041, 'max_depth': 10, 'max_bin': 267, 'num_leaves': 150}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:52,161] Trial 376 finished with value: 0.7170114078388515 and parameters: {'n_estimators': 863, 'learning_rate': 0.1479369494502093, 'max_depth': 9, 'max_bin': 162, 'num_leaves': 165}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:53,562] Trial 377 finished with value: 0.699390603528212 and parameters: {'n_estimators': 831, 'learning_rate': 0.16236347822378377, 'max_depth': 8, 'max_bin': 248, 'num_leaves': 102}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:55,285] Trial 378 finished with value: 0.7108524782818973 and parameters: {'n_estimators': 755, 'learning_rate': 0.15232612939968598, 'max_depth': 10, 'max_bin': 256, 'num_leaves': 115}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:57,138] Trial 379 finished with value: 0.7126757536014109 and parameters: {'n_estimators': 818, 'learning_rate': 0.1479258669335146, 'max_depth': 10, 'max_bin': 265, 'num_leaves': 181}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:20:58,831] Trial 380 finished with value: 0.71169549605752 and parameters: {'n_estimators': 778, 'learning_rate': 0.1592134487796223, 'max_depth': 9, 'max_bin': 267, 'num_leaves': 212}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:00,465] Trial 381 finished with value: 0.7159772710551027 and parameters: {'n_estimators': 838, 'learning_rate': 0.15262753801653597, 'max_depth': 10, 'max_bin': 262, 'num_leaves': 187}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:01,920] Trial 382 finished with value: 0.7126570538229215 and parameters: {'n_estimators': 807, 'learning_rate': 0.1562768846871577, 'max_depth': 9, 'max_bin': 267, 'num_leaves': 159}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:03,440] Trial 383 finished with value: 0.7089431252769703 and parameters: {'n_estimators': 747, 'learning_rate': 0.14690663662068265, 'max_depth': 10, 'max_bin': 257, 'num_leaves': 217}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:04,934] Trial 384 finished with value: 0.7125610612499372 and parameters: {'n_estimators': 788, 'learning_rate': 0.164138271031988, 'max_depth': 9, 'max_bin': 263, 'num_leaves': 140}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:06,773] Trial 385 finished with value: 0.7155856188079108 and parameters: {'n_estimators': 765, 'learning_rate': 0.15190159500978967, 'max_depth': 10, 'max_bin': 253, 'num_leaves': 169}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:08,177] Trial 386 finished with value: 0.7114477649039121 and parameters: {'n_estimators': 848, 'learning_rate': 0.16068354713413108, 'max_depth': 10, 'max_bin': 259, 'num_leaves': 210}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:09,806] Trial 387 finished with value: 0.7143295691483075 and parameters: {'n_estimators': 820, 'learning_rate': 0.15624409168311315, 'max_depth': 10, 'max_bin': 267, 'num_leaves': 187}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:11,438] Trial 388 finished with value: 0.7149747869424855 and parameters: {'n_estimators': 801, 'learning_rate': 0.1457122826843023, 'max_depth': 9, 'max_bin': 262, 'num_leaves': 198}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:13,195] Trial 389 finished with value: 0.7115336980142415 and parameters: {'n_estimators': 772, 'learning_rate': 0.15095058093502198, 'max_depth': 10, 'max_bin': 268, 'num_leaves': 172}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:14,702] Trial 390 finished with value: 0.7114762099728822 and parameters: {'n_estimators': 832, 'learning_rate': 0.146423082093678, 'max_depth': 9, 'max_bin': 264, 'num_leaves': 220}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:16,228] Trial 391 finished with value: 0.7081740087732781 and parameters: {'n_estimators': 784, 'learning_rate': 0.1544454217356854, 'max_depth': 10, 'max_bin': 259, 'num_leaves': 243}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:18,055] Trial 392 finished with value: 0.7159535287842029 and parameters: {'n_estimators': 861, 'learning_rate': 0.16492512960994402, 'max_depth': 10, 'max_bin': 266, 'num_leaves': 188}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:19,520] Trial 393 finished with value: 0.7052420399369937 and parameters: {'n_estimators': 814, 'learning_rate': 0.15991704478092056, 'max_depth': 8, 'max_bin': 256, 'num_leaves': 158}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:21,021] Trial 394 finished with value: 0.7053559681714021 and parameters: {'n_estimators': 744, 'learning_rate': 0.15001665146011178, 'max_depth': 9, 'max_bin': 269, 'num_leaves': 205}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:22,728] Trial 395 finished with value: 0.718389020967466 and parameters: {'n_estimators': 877, 'learning_rate': 0.15665437064992876, 'max_depth': 10, 'max_bin': 261, 'num_leaves': 172}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:24,420] Trial 396 finished with value: 0.7126550421392264 and parameters: {'n_estimators': 884, 'learning_rate': 0.15846436259678026, 'max_depth': 10, 'max_bin': 250, 'num_leaves': 147}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:26,028] Trial 397 finished with value: 0.7133105153403976 and parameters: {'n_estimators': 867, 'learning_rate': 0.15456944743243944, 'max_depth': 9, 'max_bin': 259, 'num_leaves': 170}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:27,527] Trial 398 finished with value: 0.7110380495532913 and parameters: {'n_estimators': 848, 'learning_rate': 0.16304127226336726, 'max_depth': 10, 'max_bin': 261, 'num_leaves': 184}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:28,945] Trial 399 finished with value: 0.7094819092785648 and parameters: {'n_estimators': 829, 'learning_rate': 0.15377638924140732, 'max_depth': 10, 'max_bin': 263, 'num_leaves': 113}. Best is trial 344 with value: 0.7240465774291549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7240466\n",
      "\tBest params:\n",
      "\t\tn_estimators: 807\n",
      "\t\tlearning_rate: 0.1545739174645877\n",
      "\t\tmax_depth: 9\n",
      "\t\tmax_bin: 258\n",
      "\t\tnum_leaves: 176\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.655901    0.686027    0.679052    0.679859   \n",
      "1                    TP   28.000000   34.000000   39.000000   31.000000   \n",
      "2                    TN  305.000000  309.000000  308.000000  307.000000   \n",
      "3                    FP   11.000000    6.000000    7.000000    7.000000   \n",
      "4                    FN   38.000000   33.000000   28.000000   37.000000   \n",
      "5              Accuracy    0.871728    0.897906    0.908377    0.884817   \n",
      "6             Precision    0.717949    0.850000    0.847826    0.815789   \n",
      "7           Sensitivity    0.424242    0.507463    0.582090    0.455882   \n",
      "8           Specificity    0.965200    0.981000    0.977800    0.977700   \n",
      "9              F1 score    0.533333    0.635514    0.690265    0.584906   \n",
      "10  F1 score (weighted)    0.857863    0.887123    0.901341    0.871143   \n",
      "11     F1 score (macro)    0.729489    0.788077    0.818251    0.759018   \n",
      "12    Balanced Accuracy    0.694716    0.744208    0.779934    0.716795   \n",
      "13                  MCC    0.486260    0.606651    0.654228    0.554149   \n",
      "14                  NPV    0.889200    0.903500    0.916700    0.892400   \n",
      "15              ROC_AUC    0.694716    0.744208    0.779934    0.716795   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.729972    0.737187    0.704752    0.673579  \n",
      "1    34.000000   36.000000   33.000000   28.000000  \n",
      "2   312.000000  312.000000  307.000000  309.000000  \n",
      "3     4.000000    4.000000    7.000000    6.000000  \n",
      "4    32.000000   30.000000   35.000000   39.000000  \n",
      "5     0.905759    0.910995    0.890052    0.882199  \n",
      "6     0.894737    0.900000    0.825000    0.823529  \n",
      "7     0.515152    0.545455    0.485294    0.417910  \n",
      "8     0.987300    0.987300    0.977700    0.981000  \n",
      "9     0.653846    0.679245    0.611111    0.554455  \n",
      "10    0.895072    0.901837    0.878146    0.865886  \n",
      "11    0.799650    0.813787    0.773543    0.743291  \n",
      "12    0.751247    0.766398    0.731501    0.699431  \n",
      "13    0.634709    0.657860    0.578440    0.532705  \n",
      "14    0.907000    0.912300    0.897700    0.887900  \n",
      "15    0.751247    0.766398    0.731501    0.699431  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_lgbm_7)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_7_cat = np.where(((y_pred_lgbm_7 >= 2) | (y_pred_lgbm_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:21:30,862] Trial 400 finished with value: 0.7005412140414459 and parameters: {'n_estimators': 798, 'learning_rate': 0.157335854513238, 'max_depth': 9, 'max_bin': 257, 'num_leaves': 159}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:32,212] Trial 401 finished with value: 0.698126816574238 and parameters: {'n_estimators': 811, 'learning_rate': 0.16096959089078264, 'max_depth': 7, 'max_bin': 254, 'num_leaves': 198}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:33,873] Trial 402 finished with value: 0.7030422066727138 and parameters: {'n_estimators': 788, 'learning_rate': 0.15149208043126047, 'max_depth': 10, 'max_bin': 265, 'num_leaves': 177}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:35,506] Trial 403 finished with value: 0.7043171844761579 and parameters: {'n_estimators': 897, 'learning_rate': 0.1469042826817612, 'max_depth': 9, 'max_bin': 261, 'num_leaves': 141}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:36,979] Trial 404 finished with value: 0.7037631303166672 and parameters: {'n_estimators': 848, 'learning_rate': 0.15685213843312681, 'max_depth': 10, 'max_bin': 267, 'num_leaves': 207}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:38,924] Trial 405 finished with value: 0.7073216873789899 and parameters: {'n_estimators': 823, 'learning_rate': 0.1521395205420651, 'max_depth': 10, 'max_bin': 263, 'num_leaves': 189}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:40,434] Trial 406 finished with value: 0.6935345926621637 and parameters: {'n_estimators': 771, 'learning_rate': 0.16544722407988136, 'max_depth': 9, 'max_bin': 259, 'num_leaves': 168}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:42,128] Trial 407 finished with value: 0.7056148114003694 and parameters: {'n_estimators': 803, 'learning_rate': 0.16080359466036132, 'max_depth': 10, 'max_bin': 265, 'num_leaves': 128}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:43,700] Trial 408 finished with value: 0.697719637726352 and parameters: {'n_estimators': 833, 'learning_rate': 0.1556741902074105, 'max_depth': 9, 'max_bin': 269, 'num_leaves': 229}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:45,349] Trial 409 finished with value: 0.7018053520587471 and parameters: {'n_estimators': 872, 'learning_rate': 0.14692548020599894, 'max_depth': 10, 'max_bin': 260, 'num_leaves': 190}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:47,191] Trial 410 finished with value: 0.699976263037793 and parameters: {'n_estimators': 783, 'learning_rate': 0.15179422023895892, 'max_depth': 9, 'max_bin': 254, 'num_leaves': 211}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:48,962] Trial 411 finished with value: 0.7028923599266282 and parameters: {'n_estimators': 811, 'learning_rate': 0.15815950833171502, 'max_depth': 10, 'max_bin': 263, 'num_leaves': 152}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:50,593] Trial 412 finished with value: 0.7066385857183127 and parameters: {'n_estimators': 853, 'learning_rate': 0.14486665165214574, 'max_depth': 10, 'max_bin': 268, 'num_leaves': 175}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:52,456] Trial 413 finished with value: 0.7056046742477367 and parameters: {'n_estimators': 766, 'learning_rate': 0.0969863892333091, 'max_depth': 9, 'max_bin': 256, 'num_leaves': 198}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:53,925] Trial 414 finished with value: 0.6985644296384237 and parameters: {'n_estimators': 797, 'learning_rate': 0.14994489059019764, 'max_depth': 10, 'max_bin': 265, 'num_leaves': 221}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:55,670] Trial 415 finished with value: 0.703344326892294 and parameters: {'n_estimators': 836, 'learning_rate': 0.154221321323029, 'max_depth': 9, 'max_bin': 271, 'num_leaves': 242}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:57,126] Trial 416 finished with value: 0.6988836145499682 and parameters: {'n_estimators': 790, 'learning_rate': 0.16441875424714356, 'max_depth': 10, 'max_bin': 261, 'num_leaves': 85}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:21:58,856] Trial 417 finished with value: 0.7061818131301271 and parameters: {'n_estimators': 822, 'learning_rate': 0.1603039422940349, 'max_depth': 10, 'max_bin': 266, 'num_leaves': 178}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:01,289] Trial 418 finished with value: 0.7077650262797448 and parameters: {'n_estimators': 883, 'learning_rate': 0.07396882116785615, 'max_depth': 10, 'max_bin': 269, 'num_leaves': 160}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:03,006] Trial 419 finished with value: 0.7026102529488288 and parameters: {'n_estimators': 742, 'learning_rate': 0.1455192327427104, 'max_depth': 9, 'max_bin': 263, 'num_leaves': 207}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:04,520] Trial 420 finished with value: 0.7040783220299642 and parameters: {'n_estimators': 775, 'learning_rate': 0.1500169191262332, 'max_depth': 8, 'max_bin': 258, 'num_leaves': 191}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:06,189] Trial 421 finished with value: 0.7100720238794731 and parameters: {'n_estimators': 806, 'learning_rate': 0.15581423835367017, 'max_depth': 10, 'max_bin': 271, 'num_leaves': 170}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:07,625] Trial 422 finished with value: 0.6968904520572671 and parameters: {'n_estimators': 853, 'learning_rate': 0.16764179743429503, 'max_depth': 10, 'max_bin': 251, 'num_leaves': 229}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:09,361] Trial 423 finished with value: 0.7065810430183752 and parameters: {'n_estimators': 757, 'learning_rate': 0.15304132281626387, 'max_depth': 10, 'max_bin': 267, 'num_leaves': 147}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:10,861] Trial 424 finished with value: 0.6976726448414183 and parameters: {'n_estimators': 822, 'learning_rate': 0.15817232188682565, 'max_depth': 9, 'max_bin': 260, 'num_leaves': 184}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:12,804] Trial 425 finished with value: 0.7055818550793138 and parameters: {'n_estimators': 788, 'learning_rate': 0.1490747891577405, 'max_depth': 9, 'max_bin': 263, 'num_leaves': 205}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:14,427] Trial 426 finished with value: 0.6984347917890034 and parameters: {'n_estimators': 868, 'learning_rate': 0.16209450046297125, 'max_depth': 10, 'max_bin': 266, 'num_leaves': 242}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:16,034] Trial 427 finished with value: 0.7004308115452506 and parameters: {'n_estimators': 841, 'learning_rate': 0.15378947578087865, 'max_depth': 10, 'max_bin': 270, 'num_leaves': 216}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:17,636] Trial 428 finished with value: 0.7045146696810425 and parameters: {'n_estimators': 808, 'learning_rate': 0.14610185195178582, 'max_depth': 9, 'max_bin': 257, 'num_leaves': 194}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:19,307] Trial 429 finished with value: 0.7025615319271421 and parameters: {'n_estimators': 770, 'learning_rate': 0.1576485389035157, 'max_depth': 10, 'max_bin': 261, 'num_leaves': 163}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:21,223] Trial 430 finished with value: 0.7040308340908787 and parameters: {'n_estimators': 793, 'learning_rate': 0.10345712530528556, 'max_depth': 9, 'max_bin': 253, 'num_leaves': 124}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:23,170] Trial 431 finished with value: 0.7078784377786154 and parameters: {'n_estimators': 825, 'learning_rate': 0.1521466351983032, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 188}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:24,992] Trial 432 finished with value: 0.7071085120288012 and parameters: {'n_estimators': 779, 'learning_rate': 0.14904674735307782, 'max_depth': 10, 'max_bin': 271, 'num_leaves': 175}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:26,639] Trial 433 finished with value: 0.7035343444065948 and parameters: {'n_estimators': 757, 'learning_rate': 0.14423706106097214, 'max_depth': 9, 'max_bin': 258, 'num_leaves': 207}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:28,276] Trial 434 finished with value: 0.7076909978102716 and parameters: {'n_estimators': 841, 'learning_rate': 0.15659021678358867, 'max_depth': 10, 'max_bin': 266, 'num_leaves': 153}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:29,790] Trial 435 finished with value: 0.696492375315532 and parameters: {'n_estimators': 808, 'learning_rate': 0.1649307543409687, 'max_depth': 10, 'max_bin': 268, 'num_leaves': 228}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:31,566] Trial 436 finished with value: 0.7097673602729919 and parameters: {'n_estimators': 862, 'learning_rate': 0.1596618040335612, 'max_depth': 10, 'max_bin': 262, 'num_leaves': 93}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:32,971] Trial 437 finished with value: 0.6921224371774028 and parameters: {'n_estimators': 792, 'learning_rate': 0.15167404961945283, 'max_depth': 9, 'max_bin': 257, 'num_leaves': 198}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:34,542] Trial 438 finished with value: 0.6999256502091891 and parameters: {'n_estimators': 819, 'learning_rate': 0.14727757151338125, 'max_depth': 10, 'max_bin': 271, 'num_leaves': 141}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:36,007] Trial 439 finished with value: 0.7025263839438187 and parameters: {'n_estimators': 837, 'learning_rate': 0.15388202211878058, 'max_depth': 8, 'max_bin': 265, 'num_leaves': 178}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:37,678] Trial 440 finished with value: 0.6956774956310615 and parameters: {'n_estimators': 735, 'learning_rate': 0.16185246536270173, 'max_depth': 9, 'max_bin': 260, 'num_leaves': 218}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:39,394] Trial 441 finished with value: 0.7117701577900595 and parameters: {'n_estimators': 888, 'learning_rate': 0.15580208052687583, 'max_depth': 10, 'max_bin': 268, 'num_leaves': 244}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:41,146] Trial 442 finished with value: 0.7052831766619375 and parameters: {'n_estimators': 781, 'learning_rate': 0.14437760998638346, 'max_depth': 9, 'max_bin': 255, 'num_leaves': 164}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:42,767] Trial 443 finished with value: 0.7002396390260857 and parameters: {'n_estimators': 766, 'learning_rate': 0.15037081787898562, 'max_depth': 10, 'max_bin': 263, 'num_leaves': 188}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:47,144] Trial 444 finished with value: 0.7014194291361806 and parameters: {'n_estimators': 802, 'learning_rate': 0.019216086504352012, 'max_depth': 10, 'max_bin': 272, 'num_leaves': 203}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:48,620] Trial 445 finished with value: 0.6956298120595007 and parameters: {'n_estimators': 856, 'learning_rate': 0.15809778596781826, 'max_depth': 9, 'max_bin': 267, 'num_leaves': 175}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:51,486] Trial 446 finished with value: 0.7077804747205569 and parameters: {'n_estimators': 713, 'learning_rate': 0.05650178011066116, 'max_depth': 10, 'max_bin': 260, 'num_leaves': 218}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:53,067] Trial 447 finished with value: 0.7048084730508059 and parameters: {'n_estimators': 814, 'learning_rate': 0.15331766505205344, 'max_depth': 10, 'max_bin': 248, 'num_leaves': 155}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:54,422] Trial 448 finished with value: 0.6926200707693662 and parameters: {'n_estimators': 751, 'learning_rate': 0.16501584597622967, 'max_depth': 9, 'max_bin': 264, 'num_leaves': 197}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:56,040] Trial 449 finished with value: 0.7022280548244121 and parameters: {'n_estimators': 826, 'learning_rate': 0.14861084814808292, 'max_depth': 10, 'max_bin': 269, 'num_leaves': 227}. Best is trial 344 with value: 0.7240465774291549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.72404658\n",
      "\tBest params:\n",
      "\t\tn_estimators: 807\n",
      "\t\tlearning_rate: 0.1545739174645877\n",
      "\t\tmax_depth: 9\n",
      "\t\tmax_bin: 258\n",
      "\t\tnum_leaves: 176\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.655901    0.686027    0.679052    0.679859   \n",
      "1                    TP   28.000000   34.000000   39.000000   31.000000   \n",
      "2                    TN  305.000000  309.000000  308.000000  307.000000   \n",
      "3                    FP   11.000000    6.000000    7.000000    7.000000   \n",
      "4                    FN   38.000000   33.000000   28.000000   37.000000   \n",
      "5              Accuracy    0.871728    0.897906    0.908377    0.884817   \n",
      "6             Precision    0.717949    0.850000    0.847826    0.815789   \n",
      "7           Sensitivity    0.424242    0.507463    0.582090    0.455882   \n",
      "8           Specificity    0.965200    0.981000    0.977800    0.977700   \n",
      "9              F1 score    0.533333    0.635514    0.690265    0.584906   \n",
      "10  F1 score (weighted)    0.857863    0.887123    0.901341    0.871143   \n",
      "11     F1 score (macro)    0.729489    0.788077    0.818251    0.759018   \n",
      "12    Balanced Accuracy    0.694716    0.744208    0.779934    0.716795   \n",
      "13                  MCC    0.486260    0.606651    0.654228    0.554149   \n",
      "14                  NPV    0.889200    0.903500    0.916700    0.892400   \n",
      "15              ROC_AUC    0.694716    0.744208    0.779934    0.716795   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.729972    0.737187    0.704752    0.673579    0.668503  \n",
      "1    34.000000   36.000000   33.000000   28.000000   38.000000  \n",
      "2   312.000000  312.000000  307.000000  309.000000  306.000000  \n",
      "3     4.000000    4.000000    7.000000    6.000000    7.000000  \n",
      "4    32.000000   30.000000   35.000000   39.000000   31.000000  \n",
      "5     0.905759    0.910995    0.890052    0.882199    0.900524  \n",
      "6     0.894737    0.900000    0.825000    0.823529    0.844444  \n",
      "7     0.515152    0.545455    0.485294    0.417910    0.550725  \n",
      "8     0.987300    0.987300    0.977700    0.981000    0.977600  \n",
      "9     0.653846    0.679245    0.611111    0.554455    0.666667  \n",
      "10    0.895072    0.901837    0.878146    0.865886    0.891889  \n",
      "11    0.799650    0.813787    0.773543    0.743291    0.804103  \n",
      "12    0.751247    0.766398    0.731501    0.699431    0.764180  \n",
      "13    0.634709    0.657860    0.578440    0.532705    0.630530  \n",
      "14    0.907000    0.912300    0.897700    0.887900    0.908000  \n",
      "15    0.751247    0.766398    0.731501    0.699431    0.764180  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_lgbm_8)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_8_cat = np.where(((y_pred_lgbm_8 >= 2) | (y_pred_lgbm_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:22:57,831] Trial 450 finished with value: 0.695148786774822 and parameters: {'n_estimators': 785, 'learning_rate': 0.16774713941702515, 'max_depth': 10, 'max_bin': 257, 'num_leaves': 184}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:22:59,725] Trial 451 finished with value: 0.7007616782264723 and parameters: {'n_estimators': 805, 'learning_rate': 0.14484604939671258, 'max_depth': 10, 'max_bin': 272, 'num_leaves': 212}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:00,924] Trial 452 finished with value: 0.6918675864371658 and parameters: {'n_estimators': 840, 'learning_rate': 0.1608602240809884, 'max_depth': 9, 'max_bin': 262, 'num_leaves': 135}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:02,768] Trial 453 finished with value: 0.6998843468838608 and parameters: {'n_estimators': 764, 'learning_rate': 0.09290867743191256, 'max_depth': 8, 'max_bin': 265, 'num_leaves': 173}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:03,957] Trial 454 finished with value: 0.6767870144446262 and parameters: {'n_estimators': 869, 'learning_rate': 0.15689353628901456, 'max_depth': 4, 'max_bin': 268, 'num_leaves': 245}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:05,405] Trial 455 finished with value: 0.6909341424217589 and parameters: {'n_estimators': 795, 'learning_rate': 0.15046823698239561, 'max_depth': 9, 'max_bin': 254, 'num_leaves': 194}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:07,035] Trial 456 finished with value: 0.699679501645388 and parameters: {'n_estimators': 825, 'learning_rate': 0.15419672959966782, 'max_depth': 10, 'max_bin': 259, 'num_leaves': 109}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:08,559] Trial 457 finished with value: 0.6952548413700559 and parameters: {'n_estimators': 743, 'learning_rate': 0.16148868254616466, 'max_depth': 10, 'max_bin': 262, 'num_leaves': 154}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:14,701] Trial 458 finished with value: 0.5776271905808249 and parameters: {'n_estimators': 776, 'learning_rate': 0.0024480446142789297, 'max_depth': 9, 'max_bin': 272, 'num_leaves': 205}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:16,958] Trial 459 finished with value: 0.7028922983877919 and parameters: {'n_estimators': 848, 'learning_rate': 0.08451981701254588, 'max_depth': 10, 'max_bin': 266, 'num_leaves': 167}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:18,686] Trial 460 finished with value: 0.6945753460537138 and parameters: {'n_estimators': 803, 'learning_rate': 0.14758898571117463, 'max_depth': 10, 'max_bin': 269, 'num_leaves': 231}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:20,201] Trial 461 finished with value: 0.6954124496567269 and parameters: {'n_estimators': 880, 'learning_rate': 0.1532719189790668, 'max_depth': 9, 'max_bin': 251, 'num_leaves': 183}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:21,855] Trial 462 finished with value: 0.6974387840341045 and parameters: {'n_estimators': 817, 'learning_rate': 0.14398265379328376, 'max_depth': 10, 'max_bin': 260, 'num_leaves': 196}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:24,870] Trial 463 finished with value: 0.7006954063492002 and parameters: {'n_estimators': 770, 'learning_rate': 0.03690103792344282, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 212}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:26,472] Trial 464 finished with value: 0.6972446928461606 and parameters: {'n_estimators': 785, 'learning_rate': 0.15743618927715702, 'max_depth': 9, 'max_bin': 272, 'num_leaves': 141}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:28,146] Trial 465 finished with value: 0.6984431865304679 and parameters: {'n_estimators': 832, 'learning_rate': 0.14948733567341121, 'max_depth': 10, 'max_bin': 257, 'num_leaves': 169}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:29,548] Trial 466 finished with value: 0.694219565091859 and parameters: {'n_estimators': 850, 'learning_rate': 0.1547154020018304, 'max_depth': 9, 'max_bin': 267, 'num_leaves': 126}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:31,148] Trial 467 finished with value: 0.6969147255120569 and parameters: {'n_estimators': 811, 'learning_rate': 0.16108659176509998, 'max_depth': 10, 'max_bin': 262, 'num_leaves': 251}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:33,055] Trial 468 finished with value: 0.6979447923513213 and parameters: {'n_estimators': 756, 'learning_rate': 0.1513738546233277, 'max_depth': 10, 'max_bin': 269, 'num_leaves': 186}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:34,537] Trial 469 finished with value: 0.6911181751565941 and parameters: {'n_estimators': 794, 'learning_rate': 0.16631212689042854, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 155}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:36,167] Trial 470 finished with value: 0.699895870634187 and parameters: {'n_estimators': 900, 'learning_rate': 0.1435964656045288, 'max_depth': 9, 'max_bin': 255, 'num_leaves': 219}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:37,549] Trial 471 finished with value: 0.6883727194261081 and parameters: {'n_estimators': 866, 'learning_rate': 0.1576026674294558, 'max_depth': 7, 'max_bin': 259, 'num_leaves': 203}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:38,976] Trial 472 finished with value: 0.689725946834324 and parameters: {'n_estimators': 833, 'learning_rate': 0.1485116131916358, 'max_depth': 8, 'max_bin': 272, 'num_leaves': 183}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:40,328] Trial 473 finished with value: 0.6904093068060932 and parameters: {'n_estimators': 741, 'learning_rate': 0.1698315441044034, 'max_depth': 10, 'max_bin': 266, 'num_leaves': 164}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:42,709] Trial 474 finished with value: 0.7008117033156387 and parameters: {'n_estimators': 778, 'learning_rate': 0.05753893521174761, 'max_depth': 10, 'max_bin': 261, 'num_leaves': 232}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:44,229] Trial 475 finished with value: 0.6917251844274492 and parameters: {'n_estimators': 797, 'learning_rate': 0.15447422890411158, 'max_depth': 9, 'max_bin': 270, 'num_leaves': 196}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:46,167] Trial 476 finished with value: 0.7074414225256117 and parameters: {'n_estimators': 818, 'learning_rate': 0.1113743167959433, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 177}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:47,694] Trial 477 finished with value: 0.6955786844906902 and parameters: {'n_estimators': 762, 'learning_rate': 0.16240572717447246, 'max_depth': 9, 'max_bin': 258, 'num_leaves': 208}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:49,289] Trial 478 finished with value: 0.6972878867785732 and parameters: {'n_estimators': 842, 'learning_rate': 0.1471162633246163, 'max_depth': 10, 'max_bin': 268, 'num_leaves': 220}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:50,751] Trial 479 finished with value: 0.699195970215058 and parameters: {'n_estimators': 792, 'learning_rate': 0.15194717722789064, 'max_depth': 10, 'max_bin': 253, 'num_leaves': 189}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:52,486] Trial 480 finished with value: 0.7021068557094015 and parameters: {'n_estimators': 859, 'learning_rate': 0.10125484354036318, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 150}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:53,881] Trial 481 finished with value: 0.6963533211357485 and parameters: {'n_estimators': 807, 'learning_rate': 0.15830941855508318, 'max_depth': 9, 'max_bin': 273, 'num_leaves': 169}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:55,230] Trial 482 finished with value: 0.6958471788065105 and parameters: {'n_estimators': 780, 'learning_rate': 0.15485453164493113, 'max_depth': 9, 'max_bin': 262, 'num_leaves': 241}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:56,813] Trial 483 finished with value: 0.6939367951381504 and parameters: {'n_estimators': 826, 'learning_rate': 0.1442639860449995, 'max_depth': 10, 'max_bin': 266, 'num_leaves': 202}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:58,554] Trial 484 finished with value: 0.6964745233100814 and parameters: {'n_estimators': 881, 'learning_rate': 0.15022450633011203, 'max_depth': 10, 'max_bin': 260, 'num_leaves': 180}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:23:59,970] Trial 485 finished with value: 0.6946874435993966 and parameters: {'n_estimators': 811, 'learning_rate': 0.15858309512007532, 'max_depth': 9, 'max_bin': 256, 'num_leaves': 213}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:24:02,149] Trial 486 finished with value: 0.7010906834578962 and parameters: {'n_estimators': 732, 'learning_rate': 0.075265196161038, 'max_depth': 10, 'max_bin': 270, 'num_leaves': 140}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:24:03,684] Trial 487 finished with value: 0.703236376596025 and parameters: {'n_estimators': 771, 'learning_rate': 0.15253506378324297, 'max_depth': 10, 'max_bin': 264, 'num_leaves': 162}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:24:05,191] Trial 488 finished with value: 0.6937594283667663 and parameters: {'n_estimators': 844, 'learning_rate': 0.16369793700484286, 'max_depth': 9, 'max_bin': 273, 'num_leaves': 191}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:24:06,730] Trial 489 finished with value: 0.6997677172854087 and parameters: {'n_estimators': 793, 'learning_rate': 0.14714957879533877, 'max_depth': 10, 'max_bin': 267, 'num_leaves': 232}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:24:08,311] Trial 490 finished with value: 0.6959671191519907 and parameters: {'n_estimators': 755, 'learning_rate': 0.15669747017657235, 'max_depth': 10, 'max_bin': 262, 'num_leaves': 126}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:24:10,084] Trial 491 finished with value: 0.6960091336244048 and parameters: {'n_estimators': 718, 'learning_rate': 0.10354850533075707, 'max_depth': 9, 'max_bin': 259, 'num_leaves': 73}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:24:11,648] Trial 492 finished with value: 0.6921395330490482 and parameters: {'n_estimators': 826, 'learning_rate': 0.1435121994679688, 'max_depth': 8, 'max_bin': 269, 'num_leaves': 174}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:24:13,174] Trial 493 finished with value: 0.6892932623934607 and parameters: {'n_estimators': 858, 'learning_rate': 0.15044147451642356, 'max_depth': 10, 'max_bin': 265, 'num_leaves': 201}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:24:15,306] Trial 494 finished with value: 0.7043956399849275 and parameters: {'n_estimators': 810, 'learning_rate': 0.0987808921220048, 'max_depth': 9, 'max_bin': 256, 'num_leaves': 249}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:24:16,666] Trial 495 finished with value: 0.6902237735130914 and parameters: {'n_estimators': 791, 'learning_rate': 0.1623578340616174, 'max_depth': 10, 'max_bin': 274, 'num_leaves': 223}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:24:18,099] Trial 496 finished with value: 0.6988247238991294 and parameters: {'n_estimators': 774, 'learning_rate': 0.15488644505699983, 'max_depth': 10, 'max_bin': 263, 'num_leaves': 187}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:24:19,555] Trial 497 finished with value: 0.6907688203703144 and parameters: {'n_estimators': 829, 'learning_rate': 0.15922769171008821, 'max_depth': 9, 'max_bin': 268, 'num_leaves': 156}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:24:21,074] Trial 498 finished with value: 0.6908122438211504 and parameters: {'n_estimators': 871, 'learning_rate': 0.17321854590038305, 'max_depth': 10, 'max_bin': 249, 'num_leaves': 213}. Best is trial 344 with value: 0.7240465774291549.\n",
      "[I 2023-12-12 03:24:22,807] Trial 499 finished with value: 0.6955898864626386 and parameters: {'n_estimators': 800, 'learning_rate': 0.10689117427145303, 'max_depth': 10, 'max_bin': 259, 'num_leaves': 196}. Best is trial 344 with value: 0.7240465774291549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.724046577\n",
      "\tBest params:\n",
      "\t\tn_estimators: 807\n",
      "\t\tlearning_rate: 0.1545739174645877\n",
      "\t\tmax_depth: 9\n",
      "\t\tmax_bin: 258\n",
      "\t\tnum_leaves: 176\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.655901    0.686027    0.679052    0.679859   \n",
      "1                    TP   28.000000   34.000000   39.000000   31.000000   \n",
      "2                    TN  305.000000  309.000000  308.000000  307.000000   \n",
      "3                    FP   11.000000    6.000000    7.000000    7.000000   \n",
      "4                    FN   38.000000   33.000000   28.000000   37.000000   \n",
      "5              Accuracy    0.871728    0.897906    0.908377    0.884817   \n",
      "6             Precision    0.717949    0.850000    0.847826    0.815789   \n",
      "7           Sensitivity    0.424242    0.507463    0.582090    0.455882   \n",
      "8           Specificity    0.965200    0.981000    0.977800    0.977700   \n",
      "9              F1 score    0.533333    0.635514    0.690265    0.584906   \n",
      "10  F1 score (weighted)    0.857863    0.887123    0.901341    0.871143   \n",
      "11     F1 score (macro)    0.729489    0.788077    0.818251    0.759018   \n",
      "12    Balanced Accuracy    0.694716    0.744208    0.779934    0.716795   \n",
      "13                  MCC    0.486260    0.606651    0.654228    0.554149   \n",
      "14                  NPV    0.889200    0.903500    0.916700    0.892400   \n",
      "15              ROC_AUC    0.694716    0.744208    0.779934    0.716795   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.729972    0.737187    0.704752    0.673579    0.668503    0.693500  \n",
      "1    34.000000   36.000000   33.000000   28.000000   38.000000   34.000000  \n",
      "2   312.000000  312.000000  307.000000  309.000000  306.000000  308.000000  \n",
      "3     4.000000    4.000000    7.000000    6.000000    7.000000    7.000000  \n",
      "4    32.000000   30.000000   35.000000   39.000000   31.000000   33.000000  \n",
      "5     0.905759    0.910995    0.890052    0.882199    0.900524    0.895288  \n",
      "6     0.894737    0.900000    0.825000    0.823529    0.844444    0.829268  \n",
      "7     0.515152    0.545455    0.485294    0.417910    0.550725    0.507463  \n",
      "8     0.987300    0.987300    0.977700    0.981000    0.977600    0.977800  \n",
      "9     0.653846    0.679245    0.611111    0.554455    0.666667    0.629630  \n",
      "10    0.895072    0.901837    0.878146    0.865886    0.891889    0.884759  \n",
      "11    0.799650    0.813787    0.773543    0.743291    0.804103    0.784327  \n",
      "12    0.751247    0.766398    0.731501    0.699431    0.764180    0.742620  \n",
      "13    0.634709    0.657860    0.578440    0.532705    0.630530    0.596184  \n",
      "14    0.907000    0.912300    0.897700    0.887900    0.908000    0.903200  \n",
      "15    0.751247    0.766398    0.731501    0.699431    0.764180    0.742620  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_lgbm_9)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_9_cat = np.where(((y_pred_lgbm_9 >= 2) | (y_pred_lgbm_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.724363     0.030369\n",
      "1                    TP        17.300000     1.946507\n",
      "2                    TN       153.500000     2.549510\n",
      "3                    FP         4.100000     2.846050\n",
      "4                    FN        16.100000     2.078995\n",
      "5              Accuracy         0.894241     0.016703\n",
      "6             Precision         0.819274     0.107998\n",
      "7           Sensitivity         0.518206     0.059044\n",
      "8           Specificity         0.974020     0.017964\n",
      "9              F1 score         0.631035     0.056776\n",
      "10  F1 score (weighted)         0.884543     0.017641\n",
      "11     F1 score (macro)         0.784635     0.032939\n",
      "12    Balanced Accuracy         0.746116     0.029842\n",
      "13                  MCC         0.595762     0.070014\n",
      "14                  NPV         0.905180     0.011097\n",
      "15              ROC_AUC         0.746116     0.029842\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.655901</td>\n",
       "      <td>0.686027</td>\n",
       "      <td>0.679052</td>\n",
       "      <td>0.679859</td>\n",
       "      <td>0.729972</td>\n",
       "      <td>0.737187</td>\n",
       "      <td>0.704752</td>\n",
       "      <td>0.673579</td>\n",
       "      <td>0.668503</td>\n",
       "      <td>0.693500</td>\n",
       "      <td>0.690833</td>\n",
       "      <td>0.026205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>3.719319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>308.300000</td>\n",
       "      <td>2.311805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>1.955050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.600000</td>\n",
       "      <td>3.596294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.871728</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.908377</td>\n",
       "      <td>0.884817</td>\n",
       "      <td>0.905759</td>\n",
       "      <td>0.910995</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.882199</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.894764</td>\n",
       "      <td>0.012573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.834854</td>\n",
       "      <td>0.050118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.499167</td>\n",
       "      <td>0.054111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.965200</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.977700</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>0.977700</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>0.977600</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.979040</td>\n",
       "      <td>0.006186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.635514</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.554455</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.623897</td>\n",
       "      <td>0.052778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.857863</td>\n",
       "      <td>0.887123</td>\n",
       "      <td>0.901341</td>\n",
       "      <td>0.871143</td>\n",
       "      <td>0.895072</td>\n",
       "      <td>0.901837</td>\n",
       "      <td>0.878146</td>\n",
       "      <td>0.865886</td>\n",
       "      <td>0.891889</td>\n",
       "      <td>0.884759</td>\n",
       "      <td>0.883506</td>\n",
       "      <td>0.014990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.729489</td>\n",
       "      <td>0.788077</td>\n",
       "      <td>0.818251</td>\n",
       "      <td>0.759018</td>\n",
       "      <td>0.799650</td>\n",
       "      <td>0.813787</td>\n",
       "      <td>0.773543</td>\n",
       "      <td>0.743291</td>\n",
       "      <td>0.804103</td>\n",
       "      <td>0.784327</td>\n",
       "      <td>0.781354</td>\n",
       "      <td>0.029848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.694716</td>\n",
       "      <td>0.744208</td>\n",
       "      <td>0.779934</td>\n",
       "      <td>0.716795</td>\n",
       "      <td>0.751247</td>\n",
       "      <td>0.766398</td>\n",
       "      <td>0.731501</td>\n",
       "      <td>0.699431</td>\n",
       "      <td>0.764180</td>\n",
       "      <td>0.742620</td>\n",
       "      <td>0.739103</td>\n",
       "      <td>0.028531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.486260</td>\n",
       "      <td>0.606651</td>\n",
       "      <td>0.654228</td>\n",
       "      <td>0.554149</td>\n",
       "      <td>0.634709</td>\n",
       "      <td>0.657860</td>\n",
       "      <td>0.578440</td>\n",
       "      <td>0.532705</td>\n",
       "      <td>0.630530</td>\n",
       "      <td>0.596184</td>\n",
       "      <td>0.593172</td>\n",
       "      <td>0.055828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.889200</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.916700</td>\n",
       "      <td>0.892400</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>0.912300</td>\n",
       "      <td>0.897700</td>\n",
       "      <td>0.887900</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>0.901790</td>\n",
       "      <td>0.009774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.694716</td>\n",
       "      <td>0.744208</td>\n",
       "      <td>0.779934</td>\n",
       "      <td>0.716795</td>\n",
       "      <td>0.751247</td>\n",
       "      <td>0.766398</td>\n",
       "      <td>0.731501</td>\n",
       "      <td>0.699431</td>\n",
       "      <td>0.764180</td>\n",
       "      <td>0.742620</td>\n",
       "      <td>0.739103</td>\n",
       "      <td>0.028531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.655901    0.686027    0.679052    0.679859   \n",
       "1                    TP   28.000000   34.000000   39.000000   31.000000   \n",
       "2                    TN  305.000000  309.000000  308.000000  307.000000   \n",
       "3                    FP   11.000000    6.000000    7.000000    7.000000   \n",
       "4                    FN   38.000000   33.000000   28.000000   37.000000   \n",
       "5              Accuracy    0.871728    0.897906    0.908377    0.884817   \n",
       "6             Precision    0.717949    0.850000    0.847826    0.815789   \n",
       "7           Sensitivity    0.424242    0.507463    0.582090    0.455882   \n",
       "8           Specificity    0.965200    0.981000    0.977800    0.977700   \n",
       "9              F1 score    0.533333    0.635514    0.690265    0.584906   \n",
       "10  F1 score (weighted)    0.857863    0.887123    0.901341    0.871143   \n",
       "11     F1 score (macro)    0.729489    0.788077    0.818251    0.759018   \n",
       "12    Balanced Accuracy    0.694716    0.744208    0.779934    0.716795   \n",
       "13                  MCC    0.486260    0.606651    0.654228    0.554149   \n",
       "14                  NPV    0.889200    0.903500    0.916700    0.892400   \n",
       "15              ROC_AUC    0.694716    0.744208    0.779934    0.716795   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.729972    0.737187    0.704752    0.673579    0.668503    0.693500   \n",
       "1    34.000000   36.000000   33.000000   28.000000   38.000000   34.000000   \n",
       "2   312.000000  312.000000  307.000000  309.000000  306.000000  308.000000   \n",
       "3     4.000000    4.000000    7.000000    6.000000    7.000000    7.000000   \n",
       "4    32.000000   30.000000   35.000000   39.000000   31.000000   33.000000   \n",
       "5     0.905759    0.910995    0.890052    0.882199    0.900524    0.895288   \n",
       "6     0.894737    0.900000    0.825000    0.823529    0.844444    0.829268   \n",
       "7     0.515152    0.545455    0.485294    0.417910    0.550725    0.507463   \n",
       "8     0.987300    0.987300    0.977700    0.981000    0.977600    0.977800   \n",
       "9     0.653846    0.679245    0.611111    0.554455    0.666667    0.629630   \n",
       "10    0.895072    0.901837    0.878146    0.865886    0.891889    0.884759   \n",
       "11    0.799650    0.813787    0.773543    0.743291    0.804103    0.784327   \n",
       "12    0.751247    0.766398    0.731501    0.699431    0.764180    0.742620   \n",
       "13    0.634709    0.657860    0.578440    0.532705    0.630530    0.596184   \n",
       "14    0.907000    0.912300    0.897700    0.887900    0.908000    0.903200   \n",
       "15    0.751247    0.766398    0.731501    0.699431    0.764180    0.742620   \n",
       "\n",
       "           ave       std  \n",
       "0     0.690833  0.026205  \n",
       "1    33.500000  3.719319  \n",
       "2   308.300000  2.311805  \n",
       "3     6.600000  1.955050  \n",
       "4    33.600000  3.596294  \n",
       "5     0.894764  0.012573  \n",
       "6     0.834854  0.050118  \n",
       "7     0.499167  0.054111  \n",
       "8     0.979040  0.006186  \n",
       "9     0.623897  0.052778  \n",
       "10    0.883506  0.014990  \n",
       "11    0.781354  0.029848  \n",
       "12    0.739103  0.028531  \n",
       "13    0.593172  0.055828  \n",
       "14    0.901790  0.009774  \n",
       "15    0.739103  0.028531  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test['std'] = mat_met_lgbm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.716985</td>\n",
       "      <td>0.042865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.896859</td>\n",
       "      <td>0.018389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.822454</td>\n",
       "      <td>0.074273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.524852</td>\n",
       "      <td>0.091167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.975626</td>\n",
       "      <td>0.011284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.636594</td>\n",
       "      <td>0.079669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.886860</td>\n",
       "      <td>0.022057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.788218</td>\n",
       "      <td>0.044724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.750242</td>\n",
       "      <td>0.045981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.602478</td>\n",
       "      <td>0.080886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.906760</td>\n",
       "      <td>0.016057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.750242</td>\n",
       "      <td>0.045981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.716985     0.042865\n",
       "1              Accuracy         0.896859     0.018389\n",
       "2             Precision         0.822454     0.074273\n",
       "3           Sensitivity         0.524852     0.091167\n",
       "4           Specificity         0.975626     0.011284\n",
       "5              F1 score         0.636594     0.079669\n",
       "6   F1 score (weighted)         0.886860     0.022057\n",
       "7      F1 score (macro)         0.788218     0.044724\n",
       "8     Balanced Accuracy         0.750242     0.045981\n",
       "9                   MCC         0.602478     0.080886\n",
       "10                  NPV         0.906760     0.016057\n",
       "11              ROC_AUC         0.750242     0.045981"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_lgbm_cat = np.where(((y_pred_optimized_lgbm >= 2) | (y_pred_optimized_lgbm <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_lgbm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_lgbm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "lgbm_run0 = data_lgbm[['y_test_idx0', 'y_test0', 'y_pred_lgbm0']]\n",
    "lgbm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "lgbm_run0.reset_index(inplace=True, drop=True)\n",
    "lgbm_run1 = data_lgbm[['y_test_idx1', 'y_test1', 'y_pred_lgbm1']]\n",
    "lgbm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "lgbm_run1.reset_index(inplace=True, drop=True)\n",
    "lgbm_run2 = data_lgbm[['y_test_idx2', 'y_test2', 'y_pred_lgbm2']]\n",
    "lgbm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "lgbm_run2.reset_index(inplace=True, drop=True)\n",
    "lgbm_run3 = data_lgbm[['y_test_idx3', 'y_test3', 'y_pred_lgbm3']]\n",
    "lgbm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "lgbm_run3.reset_index(inplace=True, drop=True)\n",
    "lgbm_run4 = data_lgbm[['y_test_idx4', 'y_test4', 'y_pred_lgbm4']]\n",
    "lgbm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "lgbm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "lgbm_5preds = pd.concat([chembl_id, lgbm_run0, lgbm_run1, lgbm_run2, lgbm_run3, lgbm_run4], axis=1)\n",
    "lgbm_5preds = lgbm_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_lgbm0', 'y_pred_lgbm1', 'y_pred_lgbm2', 'y_pred_lgbm3', 'y_pred_lgbm4']]\n",
    "lgbm_5preds['y_pred_lgbm_ave'] = lgbm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "lgbm_5preds['y_pred_lgbm_std'] = lgbm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "\n",
    "mat_met_optimized_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db4ac315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model r2_score 0.7091 with a standard deviation of 0.0428\n",
      "LightGBM optimized model r2_score 0.7005 with a standard deviation of 0.0588\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#r2_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"r2\")\n",
    "r2_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"r2\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)r2_score %0.4f with a standard deviation of %0.4f\" % (r2_cv_lgbm_opt_testSet.mean(), r2_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(r2_cv_lgbm_opt), np.std(r2_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_lgbm_withSemiSel.joblib']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm_reg, \"OUTPUT/lgbm_reg_withSemiSel.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"OUTPUT/optimizedCV_lgbm_withSemiSel.joblib\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.708434     0.042340\n",
      "1                    TP        17.200000     3.359894\n",
      "2                    TN       153.200000     2.573368\n",
      "3                    FP         4.400000     2.796824\n",
      "4                    FN        16.200000     3.852849\n",
      "5              Accuracy         0.892147     0.021965\n",
      "6             Precision         0.807326     0.116356\n",
      "7           Sensitivity         0.516404     0.107853\n",
      "8           Specificity         0.972130     0.017653\n",
      "9              F1 score         0.622099     0.088261\n",
      "10  F1 score (weighted)         0.881826     0.026076\n",
      "11     F1 score (macro)         0.779566     0.050058\n",
      "12    Balanced Accuracy         0.744259     0.052788\n",
      "13                  MCC         0.586842     0.094223\n",
      "14                  NPV         0.904790     0.020676\n",
      "15              ROC_AUC         0.744259     0.052788\n",
      "CPU times: user 47.3 s, sys: 104 ms, total: 47.4 s\n",
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_reg.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores),np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:24:51,407] A new study created in memory with name: XGBRegressor\n",
      "[I 2023-12-12 03:24:55,160] Trial 0 finished with value: 0.7180022624250824 and parameters: {'n_estimators': 299, 'eta': 0.09860085191271734, 'max_depth': 6, 'alpha': 0.4611, 'lambda': 17.8283293919773, 'max_bin': 356}. Best is trial 0 with value: 0.7180022624250824.\n",
      "[I 2023-12-12 03:25:00,589] Trial 1 finished with value: 0.7119904749105851 and parameters: {'n_estimators': 357, 'eta': 0.04925858854823546, 'max_depth': 7, 'alpha': 0.5727, 'lambda': 38.72307887335977, 'max_bin': 394}. Best is trial 0 with value: 0.7180022624250824.\n",
      "[I 2023-12-12 03:25:15,743] Trial 2 finished with value: 0.7200563921766218 and parameters: {'n_estimators': 833, 'eta': 0.012463404946996723, 'max_depth': 9, 'alpha': 0.6918000000000001, 'lambda': 6.75910244369564, 'max_bin': 314}. Best is trial 2 with value: 0.7200563921766218.\n",
      "[I 2023-12-12 03:25:20,726] Trial 3 finished with value: 0.7087120586115938 and parameters: {'n_estimators': 182, 'eta': 0.04560666914239845, 'max_depth': 12, 'alpha': 0.6132000000000001, 'lambda': 34.4791842193365, 'max_bin': 280}. Best is trial 2 with value: 0.7200563921766218.\n",
      "[I 2023-12-12 03:25:26,425] Trial 4 finished with value: 0.7077082704257145 and parameters: {'n_estimators': 596, 'eta': 0.0431831697453917, 'max_depth': 5, 'alpha': 0.41490000000000005, 'lambda': 21.248638511385174, 'max_bin': 484}. Best is trial 2 with value: 0.7200563921766218.\n",
      "[I 2023-12-12 03:25:31,354] Trial 5 finished with value: 0.7180119587618442 and parameters: {'n_estimators': 665, 'eta': 0.07840321371734424, 'max_depth': 5, 'alpha': 0.28850000000000003, 'lambda': 14.0119939408705, 'max_bin': 485}. Best is trial 2 with value: 0.7200563921766218.\n",
      "[I 2023-12-12 03:25:34,583] Trial 6 finished with value: 0.6789753965430295 and parameters: {'n_estimators': 250, 'eta': 0.02933776410855722, 'max_depth': 6, 'alpha': 0.13190000000000002, 'lambda': 14.38265338964692, 'max_bin': 337}. Best is trial 2 with value: 0.7200563921766218.\n",
      "[I 2023-12-12 03:25:43,172] Trial 7 finished with value: 0.7098525216185033 and parameters: {'n_estimators': 878, 'eta': 0.03231828877512626, 'max_depth': 5, 'alpha': 0.6306, 'lambda': 24.265212893692212, 'max_bin': 270}. Best is trial 2 with value: 0.7200563921766218.\n",
      "[I 2023-12-12 03:25:47,626] Trial 8 finished with value: 0.7208980497337187 and parameters: {'n_estimators': 342, 'eta': 0.0836114628706292, 'max_depth': 9, 'alpha': 0.5901000000000001, 'lambda': 5.7069172340059655, 'max_bin': 457}. Best is trial 8 with value: 0.7208980497337187.\n",
      "[I 2023-12-12 03:26:02,537] Trial 9 finished with value: 0.713381982711186 and parameters: {'n_estimators': 609, 'eta': 0.012426283447602542, 'max_depth': 11, 'alpha': 0.7297, 'lambda': 21.623873618487472, 'max_bin': 493}. Best is trial 8 with value: 0.7208980497337187.\n",
      "[I 2023-12-12 03:26:08,254] Trial 10 finished with value: 0.7268400094639125 and parameters: {'n_estimators': 436, 'eta': 0.06947470295310976, 'max_depth': 9, 'alpha': 0.9946, 'lambda': 1.4203407131374348, 'max_bin': 415}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:26:13,181] Trial 11 finished with value: 0.7251676746343889 and parameters: {'n_estimators': 439, 'eta': 0.07079424701115528, 'max_depth': 9, 'alpha': 0.9873000000000001, 'lambda': 1.1460856799738428, 'max_bin': 417}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:26:14,937] Trial 12 finished with value: 0.7083431167129528 and parameters: {'n_estimators': 75, 'eta': 0.0672698088858118, 'max_depth': 10, 'alpha': 0.9968, 'lambda': 1.7936201433034122, 'max_bin': 410}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:26:21,413] Trial 13 finished with value: 0.7237833354381995 and parameters: {'n_estimators': 476, 'eta': 0.06394436690752942, 'max_depth': 8, 'alpha': 0.9721000000000001, 'lambda': 1.3879256471560417, 'max_bin': 428}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:26:27,561] Trial 14 finished with value: 0.7233454736073132 and parameters: {'n_estimators': 493, 'eta': 0.06494224072775766, 'max_depth': 10, 'alpha': 0.8514, 'lambda': 8.672110339093695, 'max_bin': 433}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:26:33,298] Trial 15 finished with value: 0.7190692278663275 and parameters: {'n_estimators': 443, 'eta': 0.06046540227186153, 'max_depth': 8, 'alpha': 0.8368, 'lambda': 9.9047798259111, 'max_bin': 383}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:26:37,999] Trial 16 finished with value: 0.7225485828131272 and parameters: {'n_estimators': 729, 'eta': 0.07898914179722977, 'max_depth': 10, 'alpha': 0.8400000000000001, 'lambda': 1.00619964610589, 'max_bin': 446}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:26:43,235] Trial 17 finished with value: 0.7235380473524216 and parameters: {'n_estimators': 424, 'eta': 0.09433189268144304, 'max_depth': 12, 'alpha': 0.9009, 'lambda': 5.5354960568276566, 'max_bin': 358}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:26:48,756] Trial 18 finished with value: 0.7232770417382134 and parameters: {'n_estimators': 583, 'eta': 0.07279229279231147, 'max_depth': 7, 'alpha': 0.0337, 'lambda': 11.388646406107913, 'max_bin': 411}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:26:52,282] Trial 19 finished with value: 0.7133772959512076 and parameters: {'n_estimators': 188, 'eta': 0.057256054660213336, 'max_depth': 9, 'alpha': 0.7609, 'lambda': 5.756507947987831, 'max_bin': 463}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:26:57,333] Trial 20 finished with value: 0.7171054239332892 and parameters: {'n_estimators': 529, 'eta': 0.08797361283798585, 'max_depth': 11, 'alpha': 0.3734, 'lambda': 3.5977893494624436, 'max_bin': 320}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:27:02,385] Trial 21 finished with value: 0.722385492991679 and parameters: {'n_estimators': 386, 'eta': 0.07103348651594762, 'max_depth': 8, 'alpha': 0.992, 'lambda': 1.0948705717794411, 'max_bin': 421}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:27:08,491] Trial 22 finished with value: 0.7231018604196221 and parameters: {'n_estimators': 508, 'eta': 0.05672707030597369, 'max_depth': 8, 'alpha': 0.9495, 'lambda': 3.586870044560178, 'max_bin': 395}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:27:13,336] Trial 23 finished with value: 0.7208487983138713 and parameters: {'n_estimators': 439, 'eta': 0.07484141388279555, 'max_depth': 7, 'alpha': 0.8934000000000001, 'lambda': 7.618377876314341, 'max_bin': 434}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:27:18,453] Trial 24 finished with value: 0.7153358843263774 and parameters: {'n_estimators': 717, 'eta': 0.08721784831800522, 'max_depth': 9, 'alpha': 0.7861, 'lambda': 2.958555653827011, 'max_bin': 369}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:27:24,736] Trial 25 finished with value: 0.7234725840501294 and parameters: {'n_estimators': 547, 'eta': 0.06484902931755562, 'max_depth': 8, 'alpha': 0.9249, 'lambda': 10.349494724297722, 'max_bin': 470}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:27:29,867] Trial 26 finished with value: 0.7201672208343679 and parameters: {'n_estimators': 291, 'eta': 0.07046411934742673, 'max_depth': 10, 'alpha': 0.9663, 'lambda': 4.731790033677672, 'max_bin': 438}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:27:36,530] Trial 27 finished with value: 0.7190169950889965 and parameters: {'n_estimators': 394, 'eta': 0.05718312332561519, 'max_depth': 11, 'alpha': 0.6865, 'lambda': 1.3175079383480508, 'max_bin': 404}. Best is trial 10 with value: 0.7268400094639125.\n",
      "[I 2023-12-12 03:27:42,842] Trial 28 finished with value: 0.7276046347525017 and parameters: {'n_estimators': 472, 'eta': 0.07791026994172356, 'max_depth': 9, 'alpha': 0.8350000000000001, 'lambda': 6.831439399506754, 'max_bin': 423}. Best is trial 28 with value: 0.7276046347525017.\n",
      "[I 2023-12-12 03:27:47,711] Trial 29 finished with value: 0.7246275043706376 and parameters: {'n_estimators': 292, 'eta': 0.09915245282952571, 'max_depth': 9, 'alpha': 0.5097, 'lambda': 7.343078638597176, 'max_bin': 371}. Best is trial 28 with value: 0.7276046347525017.\n",
      "[I 2023-12-12 03:27:52,373] Trial 30 finished with value: 0.720716347746851 and parameters: {'n_estimators': 241, 'eta': 0.07936868289726376, 'max_depth': 10, 'alpha': 0.8211, 'lambda': 4.52106181951718, 'max_bin': 342}. Best is trial 28 with value: 0.7276046347525017.\n",
      "[I 2023-12-12 03:27:56,973] Trial 31 finished with value: 0.7238938334371049 and parameters: {'n_estimators': 321, 'eta': 0.09414198867994691, 'max_depth': 9, 'alpha': 0.16790000000000002, 'lambda': 7.795199852217154, 'max_bin': 374}. Best is trial 28 with value: 0.7276046347525017.\n",
      "[I 2023-12-12 03:28:01,810] Trial 32 finished with value: 0.7211517695462278 and parameters: {'n_estimators': 379, 'eta': 0.09658676299126823, 'max_depth': 9, 'alpha': 0.5179, 'lambda': 12.074934966571501, 'max_bin': 382}. Best is trial 28 with value: 0.7276046347525017.\n",
      "[I 2023-12-12 03:28:06,195] Trial 33 finished with value: 0.7243882100112817 and parameters: {'n_estimators': 275, 'eta': 0.09812865559024882, 'max_depth': 9, 'alpha': 0.47840000000000005, 'lambda': 7.824318353079907, 'max_bin': 393}. Best is trial 28 with value: 0.7276046347525017.\n",
      "[I 2023-12-12 03:28:08,840] Trial 34 finished with value: 0.7192154359116675 and parameters: {'n_estimators': 189, 'eta': 0.08737574557330151, 'max_depth': 7, 'alpha': 0.2957, 'lambda': 4.15653251549805, 'max_bin': 416}. Best is trial 28 with value: 0.7276046347525017.\n",
      "[I 2023-12-12 03:28:10,813] Trial 35 finished with value: 0.7113049019679547 and parameters: {'n_estimators': 87, 'eta': 0.08402485541243371, 'max_depth': 10, 'alpha': 0.8872, 'lambda': 6.957736176641913, 'max_bin': 447}. Best is trial 28 with value: 0.7276046347525017.\n",
      "[I 2023-12-12 03:28:15,507] Trial 36 finished with value: 0.7237475107055588 and parameters: {'n_estimators': 330, 'eta': 0.0999668030527595, 'max_depth': 8, 'alpha': 0.6949000000000001, 'lambda': 9.51740968387615, 'max_bin': 360}. Best is trial 28 with value: 0.7276046347525017.\n",
      "[I 2023-12-12 03:28:17,908] Trial 37 finished with value: 0.7122111587081651 and parameters: {'n_estimators': 123, 'eta': 0.07632177874837501, 'max_depth': 9, 'alpha': 0.5136000000000001, 'lambda': 3.459535459172552, 'max_bin': 305}. Best is trial 28 with value: 0.7276046347525017.\n",
      "[I 2023-12-12 03:28:24,273] Trial 38 finished with value: 0.7224945401611957 and parameters: {'n_estimators': 629, 'eta': 0.09285425352258418, 'max_depth': 11, 'alpha': 0.4073, 'lambda': 15.482694341670078, 'max_bin': 399}. Best is trial 28 with value: 0.7276046347525017.\n",
      "[I 2023-12-12 03:28:31,164] Trial 39 finished with value: 0.727760073271562 and parameters: {'n_estimators': 545, 'eta': 0.08090815281931905, 'max_depth': 9, 'alpha': 0.6402, 'lambda': 12.960519579854427, 'max_bin': 339}. Best is trial 39 with value: 0.727760073271562.\n",
      "[I 2023-12-12 03:28:38,000] Trial 40 finished with value: 0.7240948422849007 and parameters: {'n_estimators': 571, 'eta': 0.08101921505474066, 'max_depth': 10, 'alpha': 0.6491, 'lambda': 12.97631881821306, 'max_bin': 299}. Best is trial 39 with value: 0.727760073271562.\n",
      "[I 2023-12-12 03:28:43,610] Trial 41 finished with value: 0.7217173313687301 and parameters: {'n_estimators': 433, 'eta': 0.07354836965178871, 'max_depth': 9, 'alpha': 0.5405, 'lambda': 6.704281817795467, 'max_bin': 340}. Best is trial 39 with value: 0.727760073271562.\n",
      "[I 2023-12-12 03:28:49,599] Trial 42 finished with value: 0.7249508968048215 and parameters: {'n_estimators': 663, 'eta': 0.08880179629189723, 'max_depth': 9, 'alpha': 0.7852, 'lambda': 9.146378041225308, 'max_bin': 329}. Best is trial 39 with value: 0.727760073271562.\n",
      "[I 2023-12-12 03:28:56,475] Trial 43 finished with value: 0.726641535010659 and parameters: {'n_estimators': 677, 'eta': 0.08262864556448257, 'max_depth': 8, 'alpha': 0.7783, 'lambda': 15.833732604590075, 'max_bin': 323}. Best is trial 39 with value: 0.727760073271562.\n",
      "[I 2023-12-12 03:29:03,518] Trial 44 finished with value: 0.7286044745456389 and parameters: {'n_estimators': 746, 'eta': 0.06819594114158128, 'max_depth': 8, 'alpha': 0.7329, 'lambda': 15.684999579174267, 'max_bin': 290}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:29:09,379] Trial 45 finished with value: 0.7198995553771389 and parameters: {'n_estimators': 798, 'eta': 0.08101097517385648, 'max_depth': 6, 'alpha': 0.7237, 'lambda': 16.47521621759575, 'max_bin': 287}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:29:16,881] Trial 46 finished with value: 0.7256438394534108 and parameters: {'n_estimators': 778, 'eta': 0.0763032181041246, 'max_depth': 8, 'alpha': 0.5914, 'lambda': 18.033327834405988, 'max_bin': 258}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:29:23,881] Trial 47 finished with value: 0.7241685345075937 and parameters: {'n_estimators': 669, 'eta': 0.06828888160664424, 'max_depth': 7, 'alpha': 0.6401, 'lambda': 13.488825631770844, 'max_bin': 276}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:29:30,616] Trial 48 finished with value: 0.722888613195855 and parameters: {'n_estimators': 852, 'eta': 0.08372150610919664, 'max_depth': 8, 'alpha': 0.7476, 'lambda': 17.72842929392619, 'max_bin': 292}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:29:37,096] Trial 49 finished with value: 0.7261297587375715 and parameters: {'n_estimators': 724, 'eta': 0.06829043244897016, 'max_depth': 7, 'alpha': 0.7942, 'lambda': 11.403051223555993, 'max_bin': 314}. Best is trial 44 with value: 0.7286044745456389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.7286\n",
      "\tBest params:\n",
      "\t\tn_estimators: 746\n",
      "\t\teta: 0.06819594114158128\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.7329\n",
      "\t\tlambda: 15.684999579174267\n",
      "\t\tmax_bin: 290\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBRegressor\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.693937\n",
      "1                    TP   31.000000\n",
      "2                    TN  310.000000\n",
      "3                    FP    6.000000\n",
      "4                    FN   35.000000\n",
      "5              Accuracy    0.892670\n",
      "6             Precision    0.837838\n",
      "7           Sensitivity    0.469697\n",
      "8           Specificity    0.981000\n",
      "9              F1 score    0.601942\n",
      "10  F1 score (weighted)    0.879915\n",
      "11     F1 score (macro)    0.769957\n",
      "12    Balanced Accuracy    0.725355\n",
      "13                  MCC    0.576105\n",
      "14                  NPV    0.898600\n",
      "15              ROC_AUC    0.725355\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_xgb_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_xgb_0_cat = np.where(((y_pred_xgb_0 >= 2) | (y_pred_xgb_0 <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:29:47,284] Trial 50 finished with value: 0.7219867709951128 and parameters: {'n_estimators': 759, 'eta': 0.05079226561429344, 'max_depth': 8, 'alpha': 0.8743000000000001, 'lambda': 14.08792108371411, 'max_bin': 349}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:29:54,444] Trial 51 finished with value: 0.7180995492864741 and parameters: {'n_estimators': 718, 'eta': 0.06929596985581109, 'max_depth': 7, 'alpha': 0.7983, 'lambda': 11.259003628724837, 'max_bin': 310}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:30:00,376] Trial 52 finished with value: 0.7122338989208157 and parameters: {'n_estimators': 642, 'eta': 0.07529082898853443, 'max_depth': 6, 'alpha': 0.6898000000000001, 'lambda': 12.185966744866013, 'max_bin': 324}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:30:08,171] Trial 53 finished with value: 0.716570402144791 and parameters: {'n_estimators': 700, 'eta': 0.06169534206265455, 'max_depth': 7, 'alpha': 0.728, 'lambda': 15.202421727919912, 'max_bin': 331}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:30:15,826] Trial 54 finished with value: 0.7192604406951033 and parameters: {'n_estimators': 807, 'eta': 0.06614349584082978, 'max_depth': 8, 'alpha': 0.7702, 'lambda': 10.653445498046011, 'max_bin': 254}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:30:21,645] Trial 55 finished with value: 0.7150274732178482 and parameters: {'n_estimators': 755, 'eta': 0.07347927519698455, 'max_depth': 6, 'alpha': 0.9336000000000001, 'lambda': 12.878255294958771, 'max_bin': 296}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:30:27,230] Trial 56 finished with value: 0.714966184365327 and parameters: {'n_estimators': 886, 'eta': 0.07968785411978845, 'max_depth': 7, 'alpha': 0.8223, 'lambda': 9.264603927444222, 'max_bin': 318}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:30:34,587] Trial 57 finished with value: 0.718891698849214 and parameters: {'n_estimators': 605, 'eta': 0.06858658005502562, 'max_depth': 8, 'alpha': 0.8612000000000001, 'lambda': 18.94396934130738, 'max_bin': 265}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:30:41,461] Trial 58 finished with value: 0.7159973265432222 and parameters: {'n_estimators': 491, 'eta': 0.06185592942564269, 'max_depth': 8, 'alpha': 0.6193000000000001, 'lambda': 14.875927714674035, 'max_bin': 311}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:30:47,655] Trial 59 finished with value: 0.7160722286780208 and parameters: {'n_estimators': 555, 'eta': 0.07803363350503201, 'max_depth': 7, 'alpha': 0.6665, 'lambda': 21.172426377981374, 'max_bin': 281}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:30:54,781] Trial 60 finished with value: 0.7199061409340434 and parameters: {'n_estimators': 470, 'eta': 0.06588634621456818, 'max_depth': 9, 'alpha': 0.5683, 'lambda': 10.750226379227186, 'max_bin': 347}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:31:03,174] Trial 61 finished with value: 0.7186846693021894 and parameters: {'n_estimators': 768, 'eta': 0.0720620151277821, 'max_depth': 8, 'alpha': 0.5869, 'lambda': 17.325014500478044, 'max_bin': 252}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:31:10,090] Trial 62 finished with value: 0.7162380712141108 and parameters: {'n_estimators': 842, 'eta': 0.07487342266054532, 'max_depth': 8, 'alpha': 0.7243, 'lambda': 15.989219400840737, 'max_bin': 264}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:31:18,318] Trial 63 finished with value: 0.7209333427864707 and parameters: {'n_estimators': 803, 'eta': 0.07632427386776468, 'max_depth': 9, 'alpha': 0.5947, 'lambda': 19.513991142590914, 'max_bin': 303}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:31:25,352] Trial 64 finished with value: 0.7209165539397772 and parameters: {'n_estimators': 520, 'eta': 0.08242976481783634, 'max_depth': 8, 'alpha': 0.8215, 'lambda': 13.227018689157006, 'max_bin': 275}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:31:33,606] Trial 65 finished with value: 0.7227619640699576 and parameters: {'n_estimators': 745, 'eta': 0.0713785643994256, 'max_depth': 10, 'alpha': 0.6647000000000001, 'lambda': 16.852553381167436, 'max_bin': 287}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:31:38,773] Trial 66 finished with value: 0.7089178141534909 and parameters: {'n_estimators': 695, 'eta': 0.07849166627424747, 'max_depth': 5, 'alpha': 0.5562, 'lambda': 14.457479628676582, 'max_bin': 330}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:31:45,116] Trial 67 finished with value: 0.7212563003891888 and parameters: {'n_estimators': 406, 'eta': 0.09056216287218531, 'max_depth': 9, 'alpha': 0.7036, 'lambda': 19.014878552679782, 'max_bin': 426}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:31:50,382] Trial 68 finished with value: 0.714743017022591 and parameters: {'n_estimators': 361, 'eta': 0.0822716746320465, 'max_depth': 7, 'alpha': 0.9257000000000001, 'lambda': 22.688431513480996, 'max_bin': 364}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:31:55,422] Trial 69 finished with value: 0.7092251759265206 and parameters: {'n_estimators': 680, 'eta': 0.08604592917043291, 'max_depth': 8, 'alpha': 0.6102000000000001, 'lambda': 5.626055920349541, 'max_bin': 386}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:32:01,082] Trial 70 finished with value: 0.7105649686339247 and parameters: {'n_estimators': 783, 'eta': 0.08419140641347979, 'max_depth': 9, 'alpha': 0.7649, 'lambda': 2.216936369103698, 'max_bin': 260}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:32:07,765] Trial 71 finished with value: 0.7193230097058155 and parameters: {'n_estimators': 483, 'eta': 0.07011152227707743, 'max_depth': 10, 'alpha': 0.9988, 'lambda': 4.764705152622021, 'max_bin': 441}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:32:14,305] Trial 72 finished with value: 0.716403193915814 and parameters: {'n_estimators': 463, 'eta': 0.06457005584707316, 'max_depth': 9, 'alpha': 0.9627, 'lambda': 2.1341720642843063, 'max_bin': 408}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:32:20,378] Trial 73 finished with value: 0.7104336030485788 and parameters: {'n_estimators': 416, 'eta': 0.07660073125435572, 'max_depth': 9, 'alpha': 0.9093, 'lambda': 3.3254402639957608, 'max_bin': 417}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:32:27,760] Trial 74 finished with value: 0.7225499504554832 and parameters: {'n_estimators': 642, 'eta': 0.06824175325995736, 'max_depth': 8, 'alpha': 0.8006000000000001, 'lambda': 8.640589599722773, 'max_bin': 447}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:32:33,557] Trial 75 finished with value: 0.7163320806523373 and parameters: {'n_estimators': 542, 'eta': 0.071975751801511, 'max_depth': 9, 'alpha': 0.8480000000000001, 'lambda': 2.5848229003726546, 'max_bin': 460}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:32:42,205] Trial 76 finished with value: 0.7192932203180138 and parameters: {'n_estimators': 735, 'eta': 0.07764943799028147, 'max_depth': 10, 'alpha': 0.9522, 'lambda': 11.971626826116188, 'max_bin': 430}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:32:48,665] Trial 77 finished with value: 0.717033181455847 and parameters: {'n_estimators': 450, 'eta': 0.05959434970023874, 'max_depth': 8, 'alpha': 0.8919, 'lambda': 6.5121995475564685, 'max_bin': 316}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:32:56,245] Trial 78 finished with value: 0.7197918041546351 and parameters: {'n_estimators': 587, 'eta': 0.08097733741370833, 'max_depth': 9, 'alpha': 0.9789, 'lambda': 15.975426546875557, 'max_bin': 352}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:33:01,746] Trial 79 finished with value: 0.7140858149315099 and parameters: {'n_estimators': 514, 'eta': 0.08571752666004648, 'max_depth': 8, 'alpha': 0.4773, 'lambda': 10.201706352191206, 'max_bin': 324}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:33:07,590] Trial 80 finished with value: 0.7097160631235265 and parameters: {'n_estimators': 360, 'eta': 0.07426161585495486, 'max_depth': 10, 'alpha': 0.756, 'lambda': 1.0722539226134469, 'max_bin': 421}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:33:13,913] Trial 81 finished with value: 0.716655035403226 and parameters: {'n_estimators': 701, 'eta': 0.089090133761133, 'max_depth': 9, 'alpha': 0.7909, 'lambda': 9.08976766407083, 'max_bin': 335}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:33:18,999] Trial 82 finished with value: 0.7095995356471636 and parameters: {'n_estimators': 616, 'eta': 0.09152931110356147, 'max_depth': 9, 'alpha': 0.8420000000000001, 'lambda': 4.808025987247808, 'max_bin': 303}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:33:26,167] Trial 83 finished with value: 0.7205084842779972 and parameters: {'n_estimators': 662, 'eta': 0.0872436898140981, 'max_depth': 9, 'alpha': 0.7133, 'lambda': 14.010248369201154, 'max_bin': 472}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:33:32,910] Trial 84 finished with value: 0.716161319975688 and parameters: {'n_estimators': 819, 'eta': 0.08049646095195116, 'max_depth': 9, 'alpha': 0.8651000000000001, 'lambda': 8.357966150310611, 'max_bin': 324}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:33:39,557] Trial 85 finished with value: 0.7203400995882133 and parameters: {'n_estimators': 782, 'eta': 0.06747108264611762, 'max_depth': 8, 'alpha': 0.7389, 'lambda': 6.205594852772002, 'max_bin': 403}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:33:45,778] Trial 86 finished with value: 0.716081011251359 and parameters: {'n_estimators': 865, 'eta': 0.08378879678488231, 'max_depth': 7, 'alpha': 0.7743, 'lambda': 12.032438513723289, 'max_bin': 343}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:33:52,268] Trial 87 finished with value: 0.7169882804286579 and parameters: {'n_estimators': 566, 'eta': 0.07304458634188336, 'max_depth': 9, 'alpha': 0.6791, 'lambda': 3.937017561520598, 'max_bin': 333}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:33:59,807] Trial 88 finished with value: 0.7207528010204564 and parameters: {'n_estimators': 721, 'eta': 0.0768319602023081, 'max_depth': 8, 'alpha': 0.8129000000000001, 'lambda': 10.118453286887037, 'max_bin': 453}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:34:06,412] Trial 89 finished with value: 0.7197463845537252 and parameters: {'n_estimators': 681, 'eta': 0.08955996112350127, 'max_depth': 10, 'alpha': 0.6501, 'lambda': 7.51496464465209, 'max_bin': 309}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:34:13,522] Trial 90 finished with value: 0.7202370524845965 and parameters: {'n_estimators': 654, 'eta': 0.06992327379141042, 'max_depth': 9, 'alpha': 0.9071, 'lambda': 14.921180032197126, 'max_bin': 294}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:34:18,963] Trial 91 finished with value: 0.7135704306894833 and parameters: {'n_estimators': 379, 'eta': 0.09497204250722288, 'max_depth': 9, 'alpha': 0.5118, 'lambda': 7.370030766179864, 'max_bin': 374}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:34:22,697] Trial 92 finished with value: 0.7048053415850536 and parameters: {'n_estimators': 207, 'eta': 0.09079477492419798, 'max_depth': 9, 'alpha': 0.4345, 'lambda': 2.846193757622647, 'max_bin': 363}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:34:25,249] Trial 93 finished with value: 0.7077245483158071 and parameters: {'n_estimators': 150, 'eta': 0.09626943724255155, 'max_depth': 8, 'alpha': 0.5307000000000001, 'lambda': 9.34497123762642, 'max_bin': 389}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:34:29,735] Trial 94 finished with value: 0.7179845644356919 and parameters: {'n_estimators': 237, 'eta': 0.07834480129168223, 'max_depth': 9, 'alpha': 0.3583, 'lambda': 11.08771368429849, 'max_bin': 498}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:34:34,864] Trial 95 finished with value: 0.7128415973892291 and parameters: {'n_estimators': 298, 'eta': 0.0938354332075156, 'max_depth': 9, 'alpha': 0.6229, 'lambda': 5.428899685417495, 'max_bin': 353}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:34:39,717] Trial 96 finished with value: 0.7156762528039686 and parameters: {'n_estimators': 454, 'eta': 0.0997362675492596, 'max_depth': 6, 'alpha': 0.9419000000000001, 'lambda': 8.585929010510872, 'max_bin': 287}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:34:43,946] Trial 97 finished with value: 0.7116256357416031 and parameters: {'n_estimators': 504, 'eta': 0.08546796947384881, 'max_depth': 8, 'alpha': 0.5536, 'lambda': 1.7620585663263388, 'max_bin': 379}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:34:50,047] Trial 98 finished with value: 0.7153001101012736 and parameters: {'n_estimators': 426, 'eta': 0.08802803592543115, 'max_depth': 10, 'alpha': 0.8756, 'lambda': 6.774711508394139, 'max_bin': 437}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:34:55,173] Trial 99 finished with value: 0.7161293652949323 and parameters: {'n_estimators': 269, 'eta': 0.07427190318058952, 'max_depth': 9, 'alpha': 0.015600000000000001, 'lambda': 13.942559745991302, 'max_bin': 338}. Best is trial 44 with value: 0.7286044745456389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7286\n",
      "\tBest params:\n",
      "\t\tn_estimators: 746\n",
      "\t\teta: 0.06819594114158128\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.7329\n",
      "\t\tlambda: 15.684999579174267\n",
      "\t\tmax_bin: 290\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.693937    0.727582\n",
      "1                    TP   31.000000   38.000000\n",
      "2                    TN  310.000000  307.000000\n",
      "3                    FP    6.000000    8.000000\n",
      "4                    FN   35.000000   29.000000\n",
      "5              Accuracy    0.892670    0.903141\n",
      "6             Precision    0.837838    0.826087\n",
      "7           Sensitivity    0.469697    0.567164\n",
      "8           Specificity    0.981000    0.974600\n",
      "9              F1 score    0.601942    0.672566\n",
      "10  F1 score (weighted)    0.879915    0.895703\n",
      "11     F1 score (macro)    0.769957    0.807865\n",
      "12    Balanced Accuracy    0.725355    0.770884\n",
      "13                  MCC    0.576105    0.633078\n",
      "14                  NPV    0.898600    0.913700\n",
      "15              ROC_AUC    0.725355    0.770884\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_xgb_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_1_cat = np.where(((y_pred_xgb_1 >= 2) | (y_pred_xgb_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:35:01,602] Trial 100 finished with value: 0.72333302745611 and parameters: {'n_estimators': 625, 'eta': 0.08179881378933063, 'max_depth': 7, 'alpha': 0.5881000000000001, 'lambda': 12.578943816077139, 'max_bin': 321}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:35:07,236] Trial 101 finished with value: 0.7209592068684154 and parameters: {'n_estimators': 743, 'eta': 0.09770268567996737, 'max_depth': 9, 'alpha': 0.46380000000000005, 'lambda': 11.36757783395324, 'max_bin': 392}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:35:11,403] Trial 102 finished with value: 0.7219662794932507 and parameters: {'n_estimators': 284, 'eta': 0.09700397282645454, 'max_depth': 9, 'alpha': 0.4929, 'lambda': 7.724735563748008, 'max_bin': 412}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:35:16,173] Trial 103 finished with value: 0.7221746802102078 and parameters: {'n_estimators': 347, 'eta': 0.09174061131318817, 'max_depth': 9, 'alpha': 0.42800000000000005, 'lambda': 9.825469518167118, 'max_bin': 401}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:35:20,448] Trial 104 finished with value: 0.7237717557492966 and parameters: {'n_estimators': 221, 'eta': 0.09258412822677864, 'max_depth': 10, 'alpha': 0.7452000000000001, 'lambda': 6.227838834476071, 'max_bin': 408}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:35:25,653] Trial 105 finished with value: 0.7210704255158744 and parameters: {'n_estimators': 266, 'eta': 0.09476728498547575, 'max_depth': 12, 'alpha': 0.4864, 'lambda': 3.1455576394038336, 'max_bin': 423}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:35:30,574] Trial 106 finished with value: 0.7238061984132953 and parameters: {'n_estimators': 534, 'eta': 0.09896515789404624, 'max_depth': 8, 'alpha': 0.8309000000000001, 'lambda': 8.079868578759104, 'max_bin': 395}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:35:35,997] Trial 107 finished with value: 0.7162109149530922 and parameters: {'n_estimators': 328, 'eta': 0.07982694317549055, 'max_depth': 9, 'alpha': 0.9731000000000001, 'lambda': 13.34802485342853, 'max_bin': 329}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:35:41,900] Trial 108 finished with value: 0.7227904260097255 and parameters: {'n_estimators': 703, 'eta': 0.06598612008004312, 'max_depth': 8, 'alpha': 0.3619, 'lambda': 5.051890173348836, 'max_bin': 416}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:35:47,327] Trial 109 finished with value: 0.7246617441051806 and parameters: {'n_estimators': 826, 'eta': 0.07106384446431711, 'max_depth': 9, 'alpha': 0.7763, 'lambda': 4.107947045456001, 'max_bin': 313}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:35:52,591] Trial 110 finished with value: 0.7200755057587312 and parameters: {'n_estimators': 827, 'eta': 0.07549725764698532, 'max_depth': 10, 'alpha': 0.7846000000000001, 'lambda': 1.9608779640358545, 'max_bin': 316}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:35:59,364] Trial 111 finished with value: 0.7224882146593844 and parameters: {'n_estimators': 780, 'eta': 0.07130986617028194, 'max_depth': 9, 'alpha': 0.8022, 'lambda': 16.964922210724698, 'max_bin': 312}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:36:04,706] Trial 112 finished with value: 0.7236331443067491 and parameters: {'n_estimators': 767, 'eta': 0.06310513241641948, 'max_depth': 9, 'alpha': 0.396, 'lambda': 3.5321475767507895, 'max_bin': 299}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:36:09,883] Trial 113 finished with value: 0.7259670427171458 and parameters: {'n_estimators': 844, 'eta': 0.078915042119392, 'max_depth': 9, 'alpha': 0.7153, 'lambda': 4.193113021102516, 'max_bin': 272}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:36:14,652] Trial 114 finished with value: 0.727761631962793 and parameters: {'n_estimators': 858, 'eta': 0.07905591672601331, 'max_depth': 9, 'alpha': 0.7120000000000001, 'lambda': 3.787403312300047, 'max_bin': 277}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:36:19,855] Trial 115 finished with value: 0.7210078414579771 and parameters: {'n_estimators': 899, 'eta': 0.07692262829693453, 'max_depth': 9, 'alpha': 0.7055, 'lambda': 4.431586142805786, 'max_bin': 273}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:36:24,938] Trial 116 finished with value: 0.7258624421342476 and parameters: {'n_estimators': 855, 'eta': 0.0730476434574145, 'max_depth': 8, 'alpha': 0.6746, 'lambda': 3.9044798874487965, 'max_bin': 281}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:36:29,573] Trial 117 finished with value: 0.724512824699133 and parameters: {'n_estimators': 850, 'eta': 0.07920801877604827, 'max_depth': 8, 'alpha': 0.6744, 'lambda': 2.7007309254779033, 'max_bin': 280}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:36:33,887] Trial 118 finished with value: 0.7231103666095434 and parameters: {'n_estimators': 400, 'eta': 0.08271857873933318, 'max_depth': 7, 'alpha': 0.64, 'lambda': 1.1659482168187298, 'max_bin': 264}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:36:38,653] Trial 119 finished with value: 0.7233281367966353 and parameters: {'n_estimators': 872, 'eta': 0.07376559102890091, 'max_depth': 8, 'alpha': 0.7244, 'lambda': 1.9774862423026167, 'max_bin': 268}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:36:44,450] Trial 120 finished with value: 0.7201567885326339 and parameters: {'n_estimators': 804, 'eta': 0.06895106947921417, 'max_depth': 8, 'alpha': 0.6559, 'lambda': 5.531774484104033, 'max_bin': 258}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:36:49,659] Trial 121 finished with value: 0.7251048348698337 and parameters: {'n_estimators': 838, 'eta': 0.07178085048937838, 'max_depth': 9, 'alpha': 0.7591, 'lambda': 4.153895160880279, 'max_bin': 278}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:36:55,034] Trial 122 finished with value: 0.7221088499965808 and parameters: {'n_estimators': 851, 'eta': 0.07516445435449091, 'max_depth': 9, 'alpha': 0.6882, 'lambda': 3.3835844741806036, 'max_bin': 282}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:37:00,654] Trial 123 finished with value: 0.7239252789037172 and parameters: {'n_estimators': 885, 'eta': 0.06667939086030772, 'max_depth': 8, 'alpha': 0.7542, 'lambda': 4.0900935084700265, 'max_bin': 271}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:37:05,640] Trial 124 finished with value: 0.7209534174304535 and parameters: {'n_estimators': 832, 'eta': 0.07176450333262636, 'max_depth': 9, 'alpha': 0.6994, 'lambda': 2.405150737423712, 'max_bin': 287}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:37:10,139] Trial 125 finished with value: 0.7210836680888114 and parameters: {'n_estimators': 862, 'eta': 0.0780984491429783, 'max_depth': 8, 'alpha': 0.7398, 'lambda': 4.923115705985252, 'max_bin': 259}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:37:16,187] Trial 126 finished with value: 0.725789266697936 and parameters: {'n_estimators': 792, 'eta': 0.08114026654136194, 'max_depth': 9, 'alpha': 0.7145, 'lambda': 5.984121121835738, 'max_bin': 278}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:37:22,749] Trial 127 finished with value: 0.7201518977337505 and parameters: {'n_estimators': 798, 'eta': 0.07999304811814388, 'max_depth': 9, 'alpha': 0.7156, 'lambda': 17.86136677665118, 'max_bin': 268}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:37:28,004] Trial 128 finished with value: 0.725154543890505 and parameters: {'n_estimators': 758, 'eta': 0.07324660900842314, 'max_depth': 9, 'alpha': 0.666, 'lambda': 3.8668173366541647, 'max_bin': 282}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:37:32,959] Trial 129 finished with value: 0.7255101149453509 and parameters: {'n_estimators': 753, 'eta': 0.07691569252909529, 'max_depth': 8, 'alpha': 0.6298, 'lambda': 6.001842895600938, 'max_bin': 285}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:37:38,707] Trial 130 finished with value: 0.7267687429171475 and parameters: {'n_estimators': 730, 'eta': 0.07672223837297376, 'max_depth': 8, 'alpha': 0.6428, 'lambda': 6.298640249898893, 'max_bin': 295}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:37:44,590] Trial 131 finished with value: 0.723634270620861 and parameters: {'n_estimators': 722, 'eta': 0.07666136238157681, 'max_depth': 8, 'alpha': 0.6285000000000001, 'lambda': 5.979927722853122, 'max_bin': 250}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:37:49,589] Trial 132 finished with value: 0.7221754219163201 and parameters: {'n_estimators': 783, 'eta': 0.08137098162255217, 'max_depth': 8, 'alpha': 0.6127, 'lambda': 6.387390940940433, 'max_bin': 292}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:37:54,453] Trial 133 finished with value: 0.7178805664788754 and parameters: {'n_estimators': 749, 'eta': 0.08442920064300953, 'max_depth': 8, 'alpha': 0.6477, 'lambda': 6.947430388296386, 'max_bin': 290}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:38:00,144] Trial 134 finished with value: 0.7215090175100044 and parameters: {'n_estimators': 813, 'eta': 0.07838768869998806, 'max_depth': 8, 'alpha': 0.5806, 'lambda': 5.344084275671516, 'max_bin': 284}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:38:04,616] Trial 135 finished with value: 0.7220381920561358 and parameters: {'n_estimators': 470, 'eta': 0.07573737890832215, 'max_depth': 7, 'alpha': 0.6839000000000001, 'lambda': 2.8557296777872865, 'max_bin': 276}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:38:11,310] Trial 136 finished with value: 0.7203207273451481 and parameters: {'n_estimators': 734, 'eta': 0.06962094904435047, 'max_depth': 8, 'alpha': 0.6008, 'lambda': 15.71240866994739, 'max_bin': 298}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:38:15,557] Trial 137 finished with value: 0.7182440137653296 and parameters: {'n_estimators': 781, 'eta': 0.08289949015238421, 'max_depth': 8, 'alpha': 0.7053, 'lambda': 1.6625040982842276, 'max_bin': 306}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:38:21,197] Trial 138 finished with value: 0.7169901872621905 and parameters: {'n_estimators': 442, 'eta': 0.08045437194189624, 'max_depth': 7, 'alpha': 0.7245, 'lambda': 14.837780339293381, 'max_bin': 274}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:38:26,258] Trial 139 finished with value: 0.7227975535361199 and parameters: {'n_estimators': 708, 'eta': 0.07443039852163633, 'max_depth': 8, 'alpha': 0.1696, 'lambda': 5.64781241232378, 'max_bin': 295}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:38:31,091] Trial 140 finished with value: 0.7243176472665777 and parameters: {'n_estimators': 793, 'eta': 0.08583533576684177, 'max_depth': 8, 'alpha': 0.6724, 'lambda': 4.7837848721960565, 'max_bin': 300}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:38:36,285] Trial 141 finished with value: 0.719941603296702 and parameters: {'n_estimators': 725, 'eta': 0.07347437029221451, 'max_depth': 9, 'alpha': 0.6609, 'lambda': 3.5080392379123877, 'max_bin': 281}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:38:40,897] Trial 142 finished with value: 0.720798062029903 and parameters: {'n_estimators': 754, 'eta': 0.07687909283487085, 'max_depth': 9, 'alpha': 0.6298, 'lambda': 4.023322343494657, 'max_bin': 286}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:38:46,716] Trial 143 finished with value: 0.7236634752749773 and parameters: {'n_estimators': 770, 'eta': 0.07296637576099607, 'max_depth': 9, 'alpha': 0.6931, 'lambda': 6.894425356752139, 'max_bin': 265}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:38:52,181] Trial 144 finished with value: 0.7222011841146917 and parameters: {'n_estimators': 686, 'eta': 0.06809325085655078, 'max_depth': 9, 'alpha': 0.9985, 'lambda': 1.131781943889802, 'max_bin': 271}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:38:57,495] Trial 145 finished with value: 0.7232292485452522 and parameters: {'n_estimators': 747, 'eta': 0.07877487954131099, 'max_depth': 9, 'alpha': 0.6704, 'lambda': 2.58648064710153, 'max_bin': 257}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:39:05,473] Trial 146 finished with value: 0.7207388881661059 and parameters: {'n_estimators': 766, 'eta': 0.08216991096007456, 'max_depth': 9, 'alpha': 0.736, 'lambda': 16.14009884861892, 'max_bin': 293}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:39:10,171] Trial 147 finished with value: 0.7226767257524663 and parameters: {'n_estimators': 899, 'eta': 0.07545430083873822, 'max_depth': 8, 'alpha': 0.6082000000000001, 'lambda': 4.7823194321706435, 'max_bin': 276}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:39:15,254] Trial 148 finished with value: 0.7201245753671432 and parameters: {'n_estimators': 818, 'eta': 0.06461654154937624, 'max_depth': 5, 'alpha': 0.5633, 'lambda': 6.012432444294738, 'max_bin': 289}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:39:20,987] Trial 149 finished with value: 0.7210103287305427 and parameters: {'n_estimators': 419, 'eta': 0.06987188818265502, 'max_depth': 8, 'alpha': 0.635, 'lambda': 8.098918854582235, 'max_bin': 306}. Best is trial 44 with value: 0.7286044745456389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7286\n",
      "\tBest params:\n",
      "\t\tn_estimators: 746\n",
      "\t\teta: 0.06819594114158128\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.7329\n",
      "\t\tlambda: 15.684999579174267\n",
      "\t\tmax_bin: 290\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.693937    0.727582    0.711964\n",
      "1                    TP   31.000000   38.000000   36.000000\n",
      "2                    TN  310.000000  307.000000  309.000000\n",
      "3                    FP    6.000000    8.000000    6.000000\n",
      "4                    FN   35.000000   29.000000   31.000000\n",
      "5              Accuracy    0.892670    0.903141    0.903141\n",
      "6             Precision    0.837838    0.826087    0.857143\n",
      "7           Sensitivity    0.469697    0.567164    0.537313\n",
      "8           Specificity    0.981000    0.974600    0.981000\n",
      "9              F1 score    0.601942    0.672566    0.660550\n",
      "10  F1 score (weighted)    0.879915    0.895703    0.893882\n",
      "11     F1 score (macro)    0.769957    0.807865    0.802031\n",
      "12    Balanced Accuracy    0.725355    0.770884    0.759133\n",
      "13                  MCC    0.576105    0.633078    0.630059\n",
      "14                  NPV    0.898600    0.913700    0.908800\n",
      "15              ROC_AUC    0.725355    0.770884    0.759133\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_xgb_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_2_cat = np.where(((y_pred_xgb_2 >= 2) | (y_pred_xgb_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:39:28,638] Trial 150 finished with value: 0.7247143257943325 and parameters: {'n_estimators': 737, 'eta': 0.07743448625073375, 'max_depth': 9, 'alpha': 0.7658, 'lambda': 3.045234084707255, 'max_bin': 432}. Best is trial 44 with value: 0.7286044745456389.\n",
      "[I 2023-12-12 03:39:35,230] Trial 151 finished with value: 0.7287886470404498 and parameters: {'n_estimators': 489, 'eta': 0.07282698524868998, 'max_depth': 9, 'alpha': 0.7542, 'lambda': 4.020492071163425, 'max_bin': 277}. Best is trial 151 with value: 0.7287886470404498.\n",
      "[I 2023-12-12 03:39:41,571] Trial 152 finished with value: 0.7244937199387421 and parameters: {'n_estimators': 444, 'eta': 0.0724068891257112, 'max_depth': 9, 'alpha': 0.8134, 'lambda': 3.6848065069081453, 'max_bin': 281}. Best is trial 151 with value: 0.7287886470404498.\n",
      "[I 2023-12-12 03:39:47,573] Trial 153 finished with value: 0.7269571491660732 and parameters: {'n_estimators': 494, 'eta': 0.06710344896888236, 'max_depth': 9, 'alpha': 0.7096, 'lambda': 4.505031443977398, 'max_bin': 268}. Best is trial 151 with value: 0.7287886470404498.\n",
      "[I 2023-12-12 03:39:53,904] Trial 154 finished with value: 0.7247782617830544 and parameters: {'n_estimators': 493, 'eta': 0.06704548371635195, 'max_depth': 9, 'alpha': 0.7153, 'lambda': 5.227456130324195, 'max_bin': 264}. Best is trial 151 with value: 0.7287886470404498.\n",
      "[I 2023-12-12 03:40:00,197] Trial 155 finished with value: 0.7267946131820937 and parameters: {'n_estimators': 508, 'eta': 0.06953974573829605, 'max_depth': 9, 'alpha': 0.787, 'lambda': 7.2733884648913465, 'max_bin': 262}. Best is trial 151 with value: 0.7287886470404498.\n",
      "[I 2023-12-12 03:40:06,885] Trial 156 finished with value: 0.7283871753776926 and parameters: {'n_estimators': 508, 'eta': 0.08003399996066023, 'max_depth': 9, 'alpha': 0.7815000000000001, 'lambda': 8.792217696734618, 'max_bin': 270}. Best is trial 151 with value: 0.7287886470404498.\n",
      "[I 2023-12-12 03:40:13,234] Trial 157 finished with value: 0.7251409464827306 and parameters: {'n_estimators': 494, 'eta': 0.08005719418844438, 'max_depth': 9, 'alpha': 0.797, 'lambda': 8.671081029904624, 'max_bin': 270}. Best is trial 151 with value: 0.7287886470404498.\n",
      "[I 2023-12-12 03:40:20,806] Trial 158 finished with value: 0.729867125574399 and parameters: {'n_estimators': 513, 'eta': 0.06312616806587949, 'max_depth': 9, 'alpha': 0.7501, 'lambda': 7.756488696005851, 'max_bin': 261}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:40:27,668] Trial 159 finished with value: 0.7220252154087878 and parameters: {'n_estimators': 519, 'eta': 0.06347082077445056, 'max_depth': 10, 'alpha': 0.7783, 'lambda': 7.402845919294002, 'max_bin': 254}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:40:34,140] Trial 160 finished with value: 0.7258287677989423 and parameters: {'n_estimators': 539, 'eta': 0.06748321732772661, 'max_depth': 9, 'alpha': 0.7496, 'lambda': 10.914651039093798, 'max_bin': 261}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:40:42,222] Trial 161 finished with value: 0.7287303623127093 and parameters: {'n_estimators': 552, 'eta': 0.06622118664740581, 'max_depth': 9, 'alpha': 0.7475, 'lambda': 9.611175425540852, 'max_bin': 268}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:40:50,798] Trial 162 finished with value: 0.7259032713969134 and parameters: {'n_estimators': 564, 'eta': 0.06575141053175847, 'max_depth': 9, 'alpha': 0.7479, 'lambda': 10.748904649176705, 'max_bin': 263}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:40:58,767] Trial 163 finished with value: 0.7284546248690893 and parameters: {'n_estimators': 559, 'eta': 0.06560785986880227, 'max_depth': 9, 'alpha': 0.8420000000000001, 'lambda': 9.528498170689636, 'max_bin': 269}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:41:06,281] Trial 164 finished with value: 0.7240462788131989 and parameters: {'n_estimators': 559, 'eta': 0.06520868220459966, 'max_depth': 9, 'alpha': 0.8317, 'lambda': 9.861592168393347, 'max_bin': 255}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:41:14,601] Trial 165 finished with value: 0.7278309748578399 and parameters: {'n_estimators': 582, 'eta': 0.062040759292982986, 'max_depth': 9, 'alpha': 0.7909, 'lambda': 9.574963456985119, 'max_bin': 266}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:41:22,487] Trial 166 finished with value: 0.7286512807720411 and parameters: {'n_estimators': 596, 'eta': 0.06297512919270683, 'max_depth': 9, 'alpha': 0.8084, 'lambda': 9.29826198885224, 'max_bin': 268}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:41:30,098] Trial 167 finished with value: 0.7277710389422107 and parameters: {'n_estimators': 592, 'eta': 0.062458691632192906, 'max_depth': 9, 'alpha': 0.809, 'lambda': 9.567447484687323, 'max_bin': 269}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:41:38,597] Trial 168 finished with value: 0.7297965019107493 and parameters: {'n_estimators': 580, 'eta': 0.059529886622706904, 'max_depth': 9, 'alpha': 0.8415, 'lambda': 9.135693747200584, 'max_bin': 270}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:41:46,833] Trial 169 finished with value: 0.7268653778224026 and parameters: {'n_estimators': 592, 'eta': 0.06006700734786532, 'max_depth': 9, 'alpha': 0.8589, 'lambda': 9.164029085454205, 'max_bin': 266}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:41:54,915] Trial 170 finished with value: 0.7281002827865702 and parameters: {'n_estimators': 595, 'eta': 0.059833526553452164, 'max_depth': 9, 'alpha': 0.8526, 'lambda': 8.775195577675431, 'max_bin': 268}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:42:02,854] Trial 171 finished with value: 0.7218124143572927 and parameters: {'n_estimators': 594, 'eta': 0.059829760210019046, 'max_depth': 9, 'alpha': 0.8553000000000001, 'lambda': 9.554160242246079, 'max_bin': 268}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:42:11,210] Trial 172 finished with value: 0.7293326249946953 and parameters: {'n_estimators': 577, 'eta': 0.06096047994298054, 'max_depth': 9, 'alpha': 0.8469, 'lambda': 8.67259658334465, 'max_bin': 267}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:42:19,392] Trial 173 finished with value: 0.7280332745335478 and parameters: {'n_estimators': 583, 'eta': 0.06161807786925906, 'max_depth': 9, 'alpha': 0.8476, 'lambda': 9.508215720705136, 'max_bin': 268}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:42:27,588] Trial 174 finished with value: 0.7277891400587637 and parameters: {'n_estimators': 603, 'eta': 0.061020387415652085, 'max_depth': 9, 'alpha': 0.8477, 'lambda': 8.881677766131189, 'max_bin': 265}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:42:34,767] Trial 175 finished with value: 0.7261393425561836 and parameters: {'n_estimators': 549, 'eta': 0.06112822640819243, 'max_depth': 9, 'alpha': 0.8334, 'lambda': 8.65288400158948, 'max_bin': 250}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:42:42,244] Trial 176 finished with value: 0.726755438905067 and parameters: {'n_estimators': 610, 'eta': 0.0624787293957127, 'max_depth': 9, 'alpha': 0.8783000000000001, 'lambda': 9.634780744616124, 'max_bin': 272}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:42:50,582] Trial 177 finished with value: 0.7256424484326403 and parameters: {'n_estimators': 573, 'eta': 0.05646083366376018, 'max_depth': 9, 'alpha': 0.8174, 'lambda': 10.174813221470485, 'max_bin': 258}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:42:58,891] Trial 178 finished with value: 0.726315727414643 and parameters: {'n_estimators': 637, 'eta': 0.058092983566181214, 'max_depth': 9, 'alpha': 0.8478, 'lambda': 8.136250856252051, 'max_bin': 268}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:43:07,014] Trial 179 finished with value: 0.7267692802146867 and parameters: {'n_estimators': 578, 'eta': 0.06193803424640266, 'max_depth': 9, 'alpha': 0.9064000000000001, 'lambda': 9.252117407571701, 'max_bin': 274}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:43:14,866] Trial 180 finished with value: 0.725194493001775 and parameters: {'n_estimators': 530, 'eta': 0.0634090817576726, 'max_depth': 9, 'alpha': 0.8047000000000001, 'lambda': 10.143635757661052, 'max_bin': 260}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:43:23,375] Trial 181 finished with value: 0.7263906854077303 and parameters: {'n_estimators': 599, 'eta': 0.0596278354872592, 'max_depth': 9, 'alpha': 0.8688, 'lambda': 8.697391535469395, 'max_bin': 266}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:43:31,911] Trial 182 finished with value: 0.7276170899284539 and parameters: {'n_estimators': 587, 'eta': 0.058098353678068204, 'max_depth': 9, 'alpha': 0.8504, 'lambda': 11.623193935130319, 'max_bin': 266}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:43:40,253] Trial 183 finished with value: 0.727181417755071 and parameters: {'n_estimators': 580, 'eta': 0.05772705424783106, 'max_depth': 9, 'alpha': 0.8381000000000001, 'lambda': 11.74223912593962, 'max_bin': 270}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:43:48,473] Trial 184 finished with value: 0.7282860214111628 and parameters: {'n_estimators': 579, 'eta': 0.0575636307723013, 'max_depth': 9, 'alpha': 0.8906000000000001, 'lambda': 11.97222824006606, 'max_bin': 277}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:43:56,938] Trial 185 finished with value: 0.72703144806473 and parameters: {'n_estimators': 557, 'eta': 0.054088329302731185, 'max_depth': 9, 'alpha': 0.8881, 'lambda': 10.62848824736343, 'max_bin': 276}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:44:04,796] Trial 186 finished with value: 0.7271215832444317 and parameters: {'n_estimators': 615, 'eta': 0.06134832483525343, 'max_depth': 9, 'alpha': 0.8216, 'lambda': 12.177666664809411, 'max_bin': 254}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:44:12,446] Trial 187 finished with value: 0.7221863392049966 and parameters: {'n_estimators': 584, 'eta': 0.06368565856296382, 'max_depth': 10, 'alpha': 0.8488, 'lambda': 10.612858045178422, 'max_bin': 262}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:44:20,428] Trial 188 finished with value: 0.7244254414872974 and parameters: {'n_estimators': 544, 'eta': 0.05593314725181063, 'max_depth': 9, 'alpha': 0.89, 'lambda': 11.244757505256931, 'max_bin': 273}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:44:28,657] Trial 189 finished with value: 0.7267886272323827 and parameters: {'n_estimators': 615, 'eta': 0.05861107970662238, 'max_depth': 9, 'alpha': 0.9158000000000001, 'lambda': 9.036250641631534, 'max_bin': 276}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:44:36,473] Trial 190 finished with value: 0.7254616591991181 and parameters: {'n_estimators': 525, 'eta': 0.061146948985611946, 'max_depth': 9, 'alpha': 0.8133, 'lambda': 8.091986979338495, 'max_bin': 263}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:44:44,567] Trial 191 finished with value: 0.7275032032074977 and parameters: {'n_estimators': 572, 'eta': 0.05914148528384488, 'max_depth': 9, 'alpha': 0.8385, 'lambda': 12.14025083714711, 'max_bin': 271}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:44:52,683] Trial 192 finished with value: 0.7239782521367357 and parameters: {'n_estimators': 566, 'eta': 0.06278050652511925, 'max_depth': 9, 'alpha': 0.8648, 'lambda': 12.849521905485451, 'max_bin': 269}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:45:01,141] Trial 193 finished with value: 0.7273433647680572 and parameters: {'n_estimators': 599, 'eta': 0.05980253983507691, 'max_depth': 9, 'alpha': 0.8444, 'lambda': 11.61988565942024, 'max_bin': 258}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:45:09,711] Trial 194 finished with value: 0.7245399450253418 and parameters: {'n_estimators': 580, 'eta': 0.055127430927666796, 'max_depth': 9, 'alpha': 0.7976000000000001, 'lambda': 9.81154177745299, 'max_bin': 279}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:45:17,528] Trial 195 finished with value: 0.7258444997964614 and parameters: {'n_estimators': 549, 'eta': 0.06448071698402344, 'max_depth': 9, 'alpha': 0.8336, 'lambda': 7.794833361305533, 'max_bin': 273}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:45:26,495] Trial 196 finished with value: 0.728600693351504 and parameters: {'n_estimators': 649, 'eta': 0.057691468921818666, 'max_depth': 9, 'alpha': 0.8778, 'lambda': 10.230922926001252, 'max_bin': 265}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:45:34,745] Trial 197 finished with value: 0.7273726239158131 and parameters: {'n_estimators': 624, 'eta': 0.06134001179630928, 'max_depth': 9, 'alpha': 0.8756, 'lambda': 9.17395563932223, 'max_bin': 265}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:45:43,220] Trial 198 finished with value: 0.7261005611186893 and parameters: {'n_estimators': 605, 'eta': 0.057196613574238026, 'max_depth': 9, 'alpha': 0.7632, 'lambda': 10.259954053734196, 'max_bin': 258}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:45:51,816] Trial 199 finished with value: 0.7277162268900683 and parameters: {'n_estimators': 630, 'eta': 0.05373312761187163, 'max_depth': 9, 'alpha': 0.7851, 'lambda': 9.780436025079247, 'max_bin': 278}. Best is trial 158 with value: 0.729867125574399.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7299\n",
      "\tBest params:\n",
      "\t\tn_estimators: 513\n",
      "\t\teta: 0.06312616806587949\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.7501\n",
      "\t\tlambda: 7.756488696005851\n",
      "\t\tmax_bin: 261\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.693937    0.727582    0.711964    0.721309\n",
      "1                    TP   31.000000   38.000000   36.000000   35.000000\n",
      "2                    TN  310.000000  307.000000  309.000000  309.000000\n",
      "3                    FP    6.000000    8.000000    6.000000    5.000000\n",
      "4                    FN   35.000000   29.000000   31.000000   33.000000\n",
      "5              Accuracy    0.892670    0.903141    0.903141    0.900524\n",
      "6             Precision    0.837838    0.826087    0.857143    0.875000\n",
      "7           Sensitivity    0.469697    0.567164    0.537313    0.514706\n",
      "8           Specificity    0.981000    0.974600    0.981000    0.984100\n",
      "9              F1 score    0.601942    0.672566    0.660550    0.648148\n",
      "10  F1 score (weighted)    0.879915    0.895703    0.893882    0.889751\n",
      "11     F1 score (macro)    0.769957    0.807865    0.802031    0.795111\n",
      "12    Balanced Accuracy    0.725355    0.770884    0.759133    0.749391\n",
      "13                  MCC    0.576105    0.633078    0.630059    0.623142\n",
      "14                  NPV    0.898600    0.913700    0.908800    0.903500\n",
      "15              ROC_AUC    0.725355    0.770884    0.759133    0.749391\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_xgb_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_3_cat = np.where(((y_pred_xgb_3 >= 2) | (y_pred_xgb_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:46:00,498] Trial 200 finished with value: 0.7154171504632924 and parameters: {'n_estimators': 654, 'eta': 0.053754620187371226, 'max_depth': 9, 'alpha': 0.7802, 'lambda': 11.079764727720397, 'max_bin': 277}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:46:07,405] Trial 201 finished with value: 0.715077203101958 and parameters: {'n_estimators': 633, 'eta': 0.05818973539153795, 'max_depth': 9, 'alpha': 0.8041, 'lambda': 8.517976906601346, 'max_bin': 269}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:46:14,747] Trial 202 finished with value: 0.7160622098330995 and parameters: {'n_estimators': 592, 'eta': 0.052407449344608434, 'max_depth': 9, 'alpha': 0.8573000000000001, 'lambda': 9.82607162741118, 'max_bin': 262}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:46:21,471] Trial 203 finished with value: 0.7115992565981448 and parameters: {'n_estimators': 478, 'eta': 0.06311597528231047, 'max_depth': 9, 'alpha': 0.8226, 'lambda': 9.131078357250534, 'max_bin': 266}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:46:27,823] Trial 204 finished with value: 0.7124666651932705 and parameters: {'n_estimators': 532, 'eta': 0.056056760017434454, 'max_depth': 9, 'alpha': 0.894, 'lambda': 10.46863843358492, 'max_bin': 279}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:46:34,045] Trial 205 finished with value: 0.7148370734738485 and parameters: {'n_estimators': 566, 'eta': 0.05810702494202101, 'max_depth': 9, 'alpha': 0.7721, 'lambda': 7.845674942271901, 'max_bin': 275}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:46:39,672] Trial 206 finished with value: 0.7125478900103699 and parameters: {'n_estimators': 513, 'eta': 0.06487467351382527, 'max_depth': 9, 'alpha': 0.7946000000000001, 'lambda': 8.524097668152049, 'max_bin': 284}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:46:46,544] Trial 207 finished with value: 0.7151528447365715 and parameters: {'n_estimators': 647, 'eta': 0.06085119128023425, 'max_depth': 9, 'alpha': 0.8705, 'lambda': 9.585081078089507, 'max_bin': 255}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:46:54,147] Trial 208 finished with value: 0.7143021195560182 and parameters: {'n_estimators': 609, 'eta': 0.051067432431140015, 'max_depth': 9, 'alpha': 0.8186, 'lambda': 11.046734096172806, 'max_bin': 266}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:47:01,454] Trial 209 finished with value: 0.7130383925913719 and parameters: {'n_estimators': 592, 'eta': 0.055665805056921494, 'max_depth': 9, 'alpha': 0.8480000000000001, 'lambda': 13.037033189580445, 'max_bin': 478}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:47:07,852] Trial 210 finished with value: 0.7160585733925755 and parameters: {'n_estimators': 546, 'eta': 0.05967963785884266, 'max_depth': 9, 'alpha': 0.7907000000000001, 'lambda': 7.028524718785579, 'max_bin': 272}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:47:14,599] Trial 211 finished with value: 0.713364515856067 and parameters: {'n_estimators': 580, 'eta': 0.05845148220694055, 'max_depth': 9, 'alpha': 0.8304, 'lambda': 11.98368995144533, 'max_bin': 271}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:47:21,397] Trial 212 finished with value: 0.7161199381664061 and parameters: {'n_estimators': 569, 'eta': 0.061956191670721715, 'max_depth': 9, 'alpha': 0.8448, 'lambda': 12.365081430416604, 'max_bin': 260}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:47:28,399] Trial 213 finished with value: 0.7152469669785979 and parameters: {'n_estimators': 627, 'eta': 0.05954144772950666, 'max_depth': 9, 'alpha': 0.9249, 'lambda': 10.350461529943049, 'max_bin': 270}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:47:34,623] Trial 214 finished with value: 0.716483110940703 and parameters: {'n_estimators': 554, 'eta': 0.06411644393931881, 'max_depth': 9, 'alpha': 0.8827, 'lambda': 9.466182748374857, 'max_bin': 263}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:47:41,631] Trial 215 finished with value: 0.716492076903561 and parameters: {'n_estimators': 604, 'eta': 0.05417707724575087, 'max_depth': 9, 'alpha': 0.808, 'lambda': 8.608495402931887, 'max_bin': 279}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:47:48,713] Trial 216 finished with value: 0.7181556269017031 and parameters: {'n_estimators': 575, 'eta': 0.056417201904423786, 'max_depth': 9, 'alpha': 0.7548, 'lambda': 11.352201260240484, 'max_bin': 274}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:47:55,292] Trial 217 finished with value: 0.717032102062754 and parameters: {'n_estimators': 523, 'eta': 0.06553535557991402, 'max_depth': 9, 'alpha': 0.8683000000000001, 'lambda': 13.417261960511471, 'max_bin': 267}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:48:02,013] Trial 218 finished with value: 0.7153769876471481 and parameters: {'n_estimators': 592, 'eta': 0.061190588029133956, 'max_depth': 9, 'alpha': 0.8307, 'lambda': 7.631344705831228, 'max_bin': 261}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:48:09,567] Trial 219 finished with value: 0.7138039840270156 and parameters: {'n_estimators': 555, 'eta': 0.05766065235522512, 'max_depth': 9, 'alpha': 0.7731, 'lambda': 10.12308180726237, 'max_bin': 284}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:48:16,692] Trial 220 finished with value: 0.7134210165847115 and parameters: {'n_estimators': 571, 'eta': 0.06317638319659712, 'max_depth': 9, 'alpha': 0.7393000000000001, 'lambda': 11.64772919947958, 'max_bin': 254}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:48:23,118] Trial 221 finished with value: 0.7171516218139442 and parameters: {'n_estimators': 633, 'eta': 0.06036797051199304, 'max_depth': 9, 'alpha': 0.862, 'lambda': 9.167752699588856, 'max_bin': 264}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:48:29,721] Trial 222 finished with value: 0.7121216965579261 and parameters: {'n_estimators': 621, 'eta': 0.06161111891531317, 'max_depth': 9, 'alpha': 0.8844000000000001, 'lambda': 9.218744855730483, 'max_bin': 269}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:48:35,202] Trial 223 finished with value: 0.711133442731781 and parameters: {'n_estimators': 620, 'eta': 0.06601517579034943, 'max_depth': 9, 'alpha': 0.9084000000000001, 'lambda': 8.779130517262372, 'max_bin': 276}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:48:41,799] Trial 224 finished with value: 0.7140867038403608 and parameters: {'n_estimators': 506, 'eta': 0.05954897921973395, 'max_depth': 9, 'alpha': 0.8477, 'lambda': 8.10143086903686, 'max_bin': 266}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:48:48,344] Trial 225 finished with value: 0.7128996758618918 and parameters: {'n_estimators': 591, 'eta': 0.0624123104180521, 'max_depth': 9, 'alpha': 0.8136, 'lambda': 10.567971839298426, 'max_bin': 271}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:48:55,216] Trial 226 finished with value: 0.7137671210615644 and parameters: {'n_estimators': 540, 'eta': 0.05830644449226932, 'max_depth': 9, 'alpha': 0.8738, 'lambda': 9.616193824329605, 'max_bin': 259}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:49:01,527] Trial 227 finished with value: 0.7127253410562501 and parameters: {'n_estimators': 601, 'eta': 0.06394485491858269, 'max_depth': 9, 'alpha': 0.7877000000000001, 'lambda': 12.54265860177192, 'max_bin': 265}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:49:07,788] Trial 228 finished with value: 0.7139348204706183 and parameters: {'n_estimators': 459, 'eta': 0.061556011720419204, 'max_depth': 9, 'alpha': 0.8314, 'lambda': 9.890578881221067, 'max_bin': 279}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:49:14,630] Trial 229 finished with value: 0.7167990827863444 and parameters: {'n_estimators': 578, 'eta': 0.053180392690906526, 'max_depth': 9, 'alpha': 0.8559, 'lambda': 7.301407443082645, 'max_bin': 271}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:49:21,526] Trial 230 finished with value: 0.7140316887916428 and parameters: {'n_estimators': 560, 'eta': 0.06603897483095922, 'max_depth': 9, 'alpha': 0.8934000000000001, 'lambda': 13.776672229883271, 'max_bin': 275}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:49:28,053] Trial 231 finished with value: 0.7138868926712618 and parameters: {'n_estimators': 599, 'eta': 0.06013021757851737, 'max_depth': 9, 'alpha': 0.8465, 'lambda': 11.456726647953936, 'max_bin': 257}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:49:34,863] Trial 232 finished with value: 0.7161336411230261 and parameters: {'n_estimators': 619, 'eta': 0.05707228529470489, 'max_depth': 9, 'alpha': 0.8155, 'lambda': 11.786939586935004, 'max_bin': 263}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:49:42,061] Trial 233 finished with value: 0.7142911325776701 and parameters: {'n_estimators': 607, 'eta': 0.059693621941979254, 'max_depth': 9, 'alpha': 0.8346, 'lambda': 8.958106864967283, 'max_bin': 258}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:49:48,727] Trial 234 finished with value: 0.7165950769001287 and parameters: {'n_estimators': 643, 'eta': 0.05593766045559572, 'max_depth': 9, 'alpha': 0.7626000000000001, 'lambda': 8.22850385959244, 'max_bin': 251}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:49:55,099] Trial 235 finished with value: 0.7161555070178809 and parameters: {'n_estimators': 481, 'eta': 0.06285043029129647, 'max_depth': 9, 'alpha': 0.8652000000000001, 'lambda': 10.905424130541622, 'max_bin': 267}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:50:02,822] Trial 236 finished with value: 0.7121765391141632 and parameters: {'n_estimators': 664, 'eta': 0.04692418693520859, 'max_depth': 9, 'alpha': 0.8023, 'lambda': 10.383781359006825, 'max_bin': 262}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:50:09,608] Trial 237 finished with value: 0.7138835085868687 and parameters: {'n_estimators': 583, 'eta': 0.059065073122905966, 'max_depth': 9, 'alpha': 0.8496, 'lambda': 14.249731011098214, 'max_bin': 272}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:50:16,617] Trial 238 finished with value: 0.7136897935292044 and parameters: {'n_estimators': 534, 'eta': 0.06104969308564044, 'max_depth': 9, 'alpha': 0.8745, 'lambda': 9.75058662260728, 'max_bin': 267}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:50:23,533] Trial 239 finished with value: 0.7149548203822537 and parameters: {'n_estimators': 562, 'eta': 0.06438672711531736, 'max_depth': 9, 'alpha': 0.7833, 'lambda': 12.830183921761563, 'max_bin': 281}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:50:29,164] Trial 240 finished with value: 0.7147639242474192 and parameters: {'n_estimators': 590, 'eta': 0.06724775246944005, 'max_depth': 9, 'alpha': 0.7352000000000001, 'lambda': 8.843064707287839, 'max_bin': 257}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:50:36,290] Trial 241 finished with value: 0.7152117045027129 and parameters: {'n_estimators': 576, 'eta': 0.05728540211303002, 'max_depth': 9, 'alpha': 0.8316, 'lambda': 11.791225270995849, 'max_bin': 271}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:50:43,209] Trial 242 finished with value: 0.7141950429781818 and parameters: {'n_estimators': 585, 'eta': 0.05823329625211392, 'max_depth': 9, 'alpha': 0.8415, 'lambda': 12.076593351191038, 'max_bin': 268}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:50:49,868] Trial 243 finished with value: 0.7148611976437149 and parameters: {'n_estimators': 607, 'eta': 0.05974557959396362, 'max_depth': 9, 'alpha': 0.8222, 'lambda': 10.919432902269078, 'max_bin': 275}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:50:57,462] Trial 244 finished with value: 0.7179239631697897 and parameters: {'n_estimators': 629, 'eta': 0.055211526819306106, 'max_depth': 9, 'alpha': 0.8034, 'lambda': 11.424283245004613, 'max_bin': 263}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:51:03,080] Trial 245 finished with value: 0.7157310633332573 and parameters: {'n_estimators': 563, 'eta': 0.06230450873493125, 'max_depth': 9, 'alpha': 0.8976000000000001, 'lambda': 9.633228184752463, 'max_bin': 272}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:51:09,431] Trial 246 finished with value: 0.7167438445600569 and parameters: {'n_estimators': 546, 'eta': 0.05766513230184203, 'max_depth': 9, 'alpha': 0.8448, 'lambda': 8.089674921775034, 'max_bin': 266}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:51:16,477] Trial 247 finished with value: 0.7141155150133647 and parameters: {'n_estimators': 601, 'eta': 0.05496223777194487, 'max_depth': 9, 'alpha': 0.8621000000000001, 'lambda': 10.366517610989359, 'max_bin': 278}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:51:24,013] Trial 248 finished with value: 0.7134368517419704 and parameters: {'n_estimators': 574, 'eta': 0.0522190254058172, 'max_depth': 9, 'alpha': 0.8315, 'lambda': 9.240601839176044, 'max_bin': 261}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:51:30,637] Trial 249 finished with value: 0.7169526322525261 and parameters: {'n_estimators': 516, 'eta': 0.0613107646325164, 'max_depth': 9, 'alpha': 0.767, 'lambda': 7.024609569968247, 'max_bin': 270}. Best is trial 158 with value: 0.729867125574399.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7299\n",
      "\tBest params:\n",
      "\t\tn_estimators: 513\n",
      "\t\teta: 0.06312616806587949\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.7501\n",
      "\t\tlambda: 7.756488696005851\n",
      "\t\tmax_bin: 261\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.693937    0.727582    0.711964    0.721309   \n",
      "1                    TP   31.000000   38.000000   36.000000   35.000000   \n",
      "2                    TN  310.000000  307.000000  309.000000  309.000000   \n",
      "3                    FP    6.000000    8.000000    6.000000    5.000000   \n",
      "4                    FN   35.000000   29.000000   31.000000   33.000000   \n",
      "5              Accuracy    0.892670    0.903141    0.903141    0.900524   \n",
      "6             Precision    0.837838    0.826087    0.857143    0.875000   \n",
      "7           Sensitivity    0.469697    0.567164    0.537313    0.514706   \n",
      "8           Specificity    0.981000    0.974600    0.981000    0.984100   \n",
      "9              F1 score    0.601942    0.672566    0.660550    0.648148   \n",
      "10  F1 score (weighted)    0.879915    0.895703    0.893882    0.889751   \n",
      "11     F1 score (macro)    0.769957    0.807865    0.802031    0.795111   \n",
      "12    Balanced Accuracy    0.725355    0.770884    0.759133    0.749391   \n",
      "13                  MCC    0.576105    0.633078    0.630059    0.623142   \n",
      "14                  NPV    0.898600    0.913700    0.908800    0.903500   \n",
      "15              ROC_AUC    0.725355    0.770884    0.759133    0.749391   \n",
      "\n",
      "          Set4  \n",
      "0     0.733245  \n",
      "1    36.000000  \n",
      "2   306.000000  \n",
      "3    10.000000  \n",
      "4    30.000000  \n",
      "5     0.895288  \n",
      "6     0.782609  \n",
      "7     0.545455  \n",
      "8     0.968400  \n",
      "9     0.642857  \n",
      "10    0.887545  \n",
      "11    0.790754  \n",
      "12    0.756904  \n",
      "13    0.596855  \n",
      "14    0.910700  \n",
      "15    0.756904  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_xgb_4)\n",
    "# now convert the resuls to binary with cutoff 6.4\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_4_cat = np.where(((y_pred_xgb_4 >= 2) | (y_pred_xgb_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:51:38,278] Trial 250 finished with value: 0.709967803068117 and parameters: {'n_estimators': 620, 'eta': 0.06487026021772355, 'max_depth': 9, 'alpha': 0.8089000000000001, 'lambda': 12.36818388681485, 'max_bin': 255}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:51:46,682] Trial 251 finished with value: 0.7146986863783968 and parameters: {'n_estimators': 587, 'eta': 0.058945103043923676, 'max_depth': 10, 'alpha': 0.8825000000000001, 'lambda': 13.274489835151492, 'max_bin': 276}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:51:53,543] Trial 252 finished with value: 0.7127423013581806 and parameters: {'n_estimators': 551, 'eta': 0.0681260278386191, 'max_depth': 9, 'alpha': 0.791, 'lambda': 11.272036234628542, 'max_bin': 267}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:52:01,078] Trial 253 finished with value: 0.7177416197580178 and parameters: {'n_estimators': 490, 'eta': 0.06276659576083637, 'max_depth': 9, 'alpha': 0.8560000000000001, 'lambda': 8.519996971983273, 'max_bin': 283}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:52:11,121] Trial 254 finished with value: 0.7019784532064473 and parameters: {'n_estimators': 529, 'eta': 0.015055268912880684, 'max_depth': 9, 'alpha': 0.7481, 'lambda': 10.341179187314681, 'max_bin': 260}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:52:19,481] Trial 255 finished with value: 0.7168627926783898 and parameters: {'n_estimators': 600, 'eta': 0.056612416874397185, 'max_depth': 9, 'alpha': 0.8239000000000001, 'lambda': 19.9046337440996, 'max_bin': 272}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:52:26,627] Trial 256 finished with value: 0.7136326908112733 and parameters: {'n_estimators': 565, 'eta': 0.06057396262837727, 'max_depth': 9, 'alpha': 0.9306000000000001, 'lambda': 8.002936190235138, 'max_bin': 266}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:52:33,885] Trial 257 finished with value: 0.7117359671012116 and parameters: {'n_estimators': 643, 'eta': 0.06373110054289667, 'max_depth': 9, 'alpha': 0.8764000000000001, 'lambda': 9.377296376698212, 'max_bin': 275}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:52:41,801] Trial 258 finished with value: 0.7144981422808515 and parameters: {'n_estimators': 580, 'eta': 0.0582895361462651, 'max_depth': 9, 'alpha': 0.7823, 'lambda': 11.927347178464432, 'max_bin': 262}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:52:48,955] Trial 259 finished with value: 0.7149841178250076 and parameters: {'n_estimators': 613, 'eta': 0.06642145659796668, 'max_depth': 9, 'alpha': 0.8432000000000001, 'lambda': 10.010384635270817, 'max_bin': 286}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:52:56,621] Trial 260 finished with value: 0.7137047185761926 and parameters: {'n_estimators': 593, 'eta': 0.053110734833246215, 'max_depth': 9, 'alpha': 0.9094000000000001, 'lambda': 10.838636092348587, 'max_bin': 270}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:53:03,147] Trial 261 finished with value: 0.7113036792886376 and parameters: {'n_estimators': 503, 'eta': 0.06130566415568413, 'max_depth': 9, 'alpha': 0.8122, 'lambda': 8.686558036973748, 'max_bin': 256}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:53:10,608] Trial 262 finished with value: 0.71512408997498 and parameters: {'n_estimators': 472, 'eta': 0.0499612640338796, 'max_depth': 9, 'alpha': 0.8677, 'lambda': 7.900708046555013, 'max_bin': 279}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:53:20,600] Trial 263 finished with value: 0.720360823254487 and parameters: {'n_estimators': 540, 'eta': 0.05574338194777578, 'max_depth': 9, 'alpha': 0.763, 'lambda': 33.32318867871066, 'max_bin': 264}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:53:27,001] Trial 264 finished with value: 0.7106682923100461 and parameters: {'n_estimators': 575, 'eta': 0.0648993171034356, 'max_depth': 9, 'alpha': 0.7313000000000001, 'lambda': 6.801333287447122, 'max_bin': 269}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:53:34,545] Trial 265 finished with value: 0.7148319848107328 and parameters: {'n_estimators': 630, 'eta': 0.060039851588116935, 'max_depth': 9, 'alpha': 0.8291000000000001, 'lambda': 13.891464642111812, 'max_bin': 370}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:53:41,683] Trial 266 finished with value: 0.7139799916215833 and parameters: {'n_estimators': 557, 'eta': 0.06936187289745749, 'max_depth': 9, 'alpha': 0.8480000000000001, 'lambda': 12.77060327940684, 'max_bin': 252}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:53:50,159] Trial 267 finished with value: 0.716786880827922 and parameters: {'n_estimators': 611, 'eta': 0.06314021467002777, 'max_depth': 10, 'alpha': 0.7846000000000001, 'lambda': 15.182961106559066, 'max_bin': 274}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:53:57,375] Trial 268 finished with value: 0.711713209944356 and parameters: {'n_estimators': 593, 'eta': 0.057236307556663736, 'max_depth': 9, 'alpha': 0.8877, 'lambda': 9.178143021649458, 'max_bin': 260}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:54:05,726] Trial 269 finished with value: 0.7146613870810599 and parameters: {'n_estimators': 527, 'eta': 0.0543524717048073, 'max_depth': 9, 'alpha': 0.8116, 'lambda': 10.090936623740886, 'max_bin': 279}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:54:13,824] Trial 270 finished with value: 0.7125366718508959 and parameters: {'n_estimators': 570, 'eta': 0.05917783199537729, 'max_depth': 9, 'alpha': 0.8552000000000001, 'lambda': 11.648806401677932, 'max_bin': 266}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:54:21,089] Trial 271 finished with value: 0.713378807721433 and parameters: {'n_estimators': 655, 'eta': 0.06154159847122785, 'max_depth': 9, 'alpha': 0.7505000000000001, 'lambda': 7.447978162989518, 'max_bin': 271}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:54:27,287] Trial 272 finished with value: 0.7114762199456404 and parameters: {'n_estimators': 551, 'eta': 0.06549330816770366, 'max_depth': 9, 'alpha': 0.8012, 'lambda': 10.872378154356703, 'max_bin': 359}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:54:32,747] Trial 273 finished with value: 0.7109334136881815 and parameters: {'n_estimators': 586, 'eta': 0.0838333928216955, 'max_depth': 9, 'alpha': 0.0915, 'lambda': 8.847763593257541, 'max_bin': 258}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:54:39,617] Trial 274 finished with value: 0.7145340364695112 and parameters: {'n_estimators': 509, 'eta': 0.06711813531909096, 'max_depth': 9, 'alpha': 0.8340000000000001, 'lambda': 9.60095841527016, 'max_bin': 283}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:54:47,031] Trial 275 finished with value: 0.7130877309066191 and parameters: {'n_estimators': 608, 'eta': 0.06301611006520683, 'max_depth': 9, 'alpha': 0.7708, 'lambda': 14.649300148734218, 'max_bin': 288}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:54:56,582] Trial 276 finished with value: 0.7179007248575526 and parameters: {'n_estimators': 620, 'eta': 0.05973561980973884, 'max_depth': 9, 'alpha': 0.8708, 'lambda': 18.18633999214562, 'max_bin': 488}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:55:03,036] Trial 277 finished with value: 0.7137962240505626 and parameters: {'n_estimators': 569, 'eta': 0.08065095236351141, 'max_depth': 9, 'alpha': 0.8229000000000001, 'lambda': 12.47686795479584, 'max_bin': 265}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:55:10,399] Trial 278 finished with value: 0.7131890492379218 and parameters: {'n_estimators': 539, 'eta': 0.057851534747858216, 'max_depth': 9, 'alpha': 0.9021, 'lambda': 10.840491106076774, 'max_bin': 275}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:55:18,126] Trial 279 finished with value: 0.7173466033316036 and parameters: {'n_estimators': 678, 'eta': 0.061959557495361306, 'max_depth': 9, 'alpha': 0.8445, 'lambda': 8.383263689805373, 'max_bin': 269}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:55:26,727] Trial 280 finished with value: 0.7142533578551893 and parameters: {'n_estimators': 586, 'eta': 0.043527901971846014, 'max_depth': 9, 'alpha': 0.7337, 'lambda': 9.525918095328603, 'max_bin': 381}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:55:36,862] Trial 281 finished with value: 0.7173163924969992 and parameters: {'n_estimators': 629, 'eta': 0.03751998138596316, 'max_depth': 9, 'alpha': 0.7967000000000001, 'lambda': 11.635687486724063, 'max_bin': 261}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:55:42,863] Trial 282 finished with value: 0.7141002908635794 and parameters: {'n_estimators': 558, 'eta': 0.07136544455221583, 'max_depth': 10, 'alpha': 0.8613000000000001, 'lambda': 7.566658897235377, 'max_bin': 277}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:55:49,059] Trial 283 finished with value: 0.713455952924164 and parameters: {'n_estimators': 463, 'eta': 0.06840019768954354, 'max_depth': 9, 'alpha': 0.8841, 'lambda': 10.168441735171205, 'max_bin': 271}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:55:56,315] Trial 284 finished with value: 0.7120845342799361 and parameters: {'n_estimators': 598, 'eta': 0.05170863760253434, 'max_depth': 9, 'alpha': 0.7732, 'lambda': 6.644292148177332, 'max_bin': 265}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:56:04,573] Trial 285 finished with value: 0.7114985477214837 and parameters: {'n_estimators': 521, 'eta': 0.05582843017986697, 'max_depth': 9, 'alpha': 0.8176, 'lambda': 8.91181749827123, 'max_bin': 252}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:56:10,404] Trial 286 finished with value: 0.7138174193541096 and parameters: {'n_estimators': 577, 'eta': 0.07887100993381206, 'max_depth': 9, 'alpha': 0.8411000000000001, 'lambda': 13.081805062550142, 'max_bin': 258}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:56:17,624] Trial 287 finished with value: 0.7160213392492933 and parameters: {'n_estimators': 487, 'eta': 0.06401618520438132, 'max_depth': 9, 'alpha': 0.7941, 'lambda': 10.41823066367798, 'max_bin': 280}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:56:26,336] Trial 288 finished with value: 0.719340813466219 and parameters: {'n_estimators': 599, 'eta': 0.06068511946341029, 'max_depth': 9, 'alpha': 0.8703000000000001, 'lambda': 16.448750245939845, 'max_bin': 273}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:56:34,634] Trial 289 finished with value: 0.715947720085962 and parameters: {'n_estimators': 637, 'eta': 0.057955574447426286, 'max_depth': 9, 'alpha': 0.7492000000000001, 'lambda': 11.124287505788345, 'max_bin': 264}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:56:42,636] Trial 290 finished with value: 0.712165445019553 and parameters: {'n_estimators': 553, 'eta': 0.05416769207471363, 'max_depth': 9, 'alpha': 0.8240000000000001, 'lambda': 8.174775915632898, 'max_bin': 345}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:56:49,600] Trial 291 finished with value: 0.712802890515466 and parameters: {'n_estimators': 610, 'eta': 0.06408928101081864, 'max_depth': 9, 'alpha': 0.9114000000000001, 'lambda': 9.534497725328803, 'max_bin': 268}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:56:58,039] Trial 292 finished with value: 0.7146991102290936 and parameters: {'n_estimators': 573, 'eta': 0.05941102482963318, 'max_depth': 9, 'alpha': 0.8064, 'lambda': 11.686620863701581, 'max_bin': 275}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:57:05,518] Trial 293 finished with value: 0.7146714961019192 and parameters: {'n_estimators': 541, 'eta': 0.07010190196430519, 'max_depth': 9, 'alpha': 0.8534, 'lambda': 17.21062981312623, 'max_bin': 261}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:57:11,553] Trial 294 finished with value: 0.7108262674003865 and parameters: {'n_estimators': 587, 'eta': 0.062416046318635225, 'max_depth': 9, 'alpha': 0.7197, 'lambda': 7.482512295242568, 'max_bin': 250}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:57:18,030] Trial 295 finished with value: 0.7135014259732076 and parameters: {'n_estimators': 496, 'eta': 0.0663050246518954, 'max_depth': 9, 'alpha': 0.7775000000000001, 'lambda': 8.77808192673194, 'max_bin': 467}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:57:24,313] Trial 296 finished with value: 0.7131741319858563 and parameters: {'n_estimators': 619, 'eta': 0.08261562227609226, 'max_depth': 9, 'alpha': 0.8906000000000001, 'lambda': 9.916913843436275, 'max_bin': 282}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:57:33,459] Trial 297 finished with value: 0.7153102287460193 and parameters: {'n_estimators': 566, 'eta': 0.056817063649144246, 'max_depth': 10, 'alpha': 0.8314, 'lambda': 13.647333827294887, 'max_bin': 269}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:57:42,179] Trial 298 finished with value: 0.7163400539304907 and parameters: {'n_estimators': 654, 'eta': 0.060680882807342226, 'max_depth': 9, 'alpha': 0.7572, 'lambda': 12.01121233196937, 'max_bin': 291}. Best is trial 158 with value: 0.729867125574399.\n",
      "[I 2023-12-12 03:57:48,305] Trial 299 finished with value: 0.7129992704782434 and parameters: {'n_estimators': 600, 'eta': 0.07521748505493682, 'max_depth': 9, 'alpha': 0.8539, 'lambda': 6.264287352695403, 'max_bin': 264}. Best is trial 158 with value: 0.729867125574399.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7299\n",
      "\tBest params:\n",
      "\t\tn_estimators: 513\n",
      "\t\teta: 0.06312616806587949\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.7501\n",
      "\t\tlambda: 7.756488696005851\n",
      "\t\tmax_bin: 261\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.693937    0.727582    0.711964    0.721309   \n",
      "1                    TP   31.000000   38.000000   36.000000   35.000000   \n",
      "2                    TN  310.000000  307.000000  309.000000  309.000000   \n",
      "3                    FP    6.000000    8.000000    6.000000    5.000000   \n",
      "4                    FN   35.000000   29.000000   31.000000   33.000000   \n",
      "5              Accuracy    0.892670    0.903141    0.903141    0.900524   \n",
      "6             Precision    0.837838    0.826087    0.857143    0.875000   \n",
      "7           Sensitivity    0.469697    0.567164    0.537313    0.514706   \n",
      "8           Specificity    0.981000    0.974600    0.981000    0.984100   \n",
      "9              F1 score    0.601942    0.672566    0.660550    0.648148   \n",
      "10  F1 score (weighted)    0.879915    0.895703    0.893882    0.889751   \n",
      "11     F1 score (macro)    0.769957    0.807865    0.802031    0.795111   \n",
      "12    Balanced Accuracy    0.725355    0.770884    0.759133    0.749391   \n",
      "13                  MCC    0.576105    0.633078    0.630059    0.623142   \n",
      "14                  NPV    0.898600    0.913700    0.908800    0.903500   \n",
      "15              ROC_AUC    0.725355    0.770884    0.759133    0.749391   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.733245    0.752080  \n",
      "1    36.000000   38.000000  \n",
      "2   306.000000  312.000000  \n",
      "3    10.000000    4.000000  \n",
      "4    30.000000   28.000000  \n",
      "5     0.895288    0.916230  \n",
      "6     0.782609    0.904762  \n",
      "7     0.545455    0.575758  \n",
      "8     0.968400    0.987300  \n",
      "9     0.642857    0.703704  \n",
      "10    0.887545    0.908455  \n",
      "11    0.790754    0.827462  \n",
      "12    0.756904    0.781550  \n",
      "13    0.596855    0.680513  \n",
      "14    0.910700    0.917600  \n",
      "15    0.756904    0.781550  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_xgb_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_5_cat = np.where(((y_pred_xgb_5 >= 2) | (y_pred_xgb_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 03:57:57,003] Trial 300 finished with value: 0.7349724231277704 and parameters: {'n_estimators': 526, 'eta': 0.0589379251251031, 'max_depth': 9, 'alpha': 0.8034, 'lambda': 10.568086373228688, 'max_bin': 272}. Best is trial 300 with value: 0.7349724231277704.\n",
      "[I 2023-12-12 03:58:03,888] Trial 301 finished with value: 0.7315791255403261 and parameters: {'n_estimators': 526, 'eta': 0.06550080264890723, 'max_depth': 9, 'alpha': 0.7897000000000001, 'lambda': 10.813260586688532, 'max_bin': 451}. Best is trial 300 with value: 0.7349724231277704.\n",
      "[I 2023-12-12 03:58:12,254] Trial 302 finished with value: 0.7350160843949177 and parameters: {'n_estimators': 522, 'eta': 0.06548122295463962, 'max_depth': 9, 'alpha': 0.7833, 'lambda': 9.136967604816826, 'max_bin': 445}. Best is trial 302 with value: 0.7350160843949177.\n",
      "[I 2023-12-12 03:58:19,630] Trial 303 finished with value: 0.7345277675717985 and parameters: {'n_estimators': 506, 'eta': 0.06809651749114885, 'max_depth': 9, 'alpha': 0.7893, 'lambda': 10.536405015060199, 'max_bin': 441}. Best is trial 302 with value: 0.7350160843949177.\n",
      "[I 2023-12-12 03:58:26,423] Trial 304 finished with value: 0.7324931321566955 and parameters: {'n_estimators': 504, 'eta': 0.06796969709052872, 'max_depth': 9, 'alpha': 0.772, 'lambda': 10.372315844539068, 'max_bin': 446}. Best is trial 302 with value: 0.7350160843949177.\n",
      "[I 2023-12-12 03:58:34,198] Trial 305 finished with value: 0.7315519773904883 and parameters: {'n_estimators': 507, 'eta': 0.06841024590070091, 'max_depth': 9, 'alpha': 0.7627, 'lambda': 10.501235710062534, 'max_bin': 454}. Best is trial 302 with value: 0.7350160843949177.\n",
      "[I 2023-12-12 03:58:42,176] Trial 306 finished with value: 0.7349954398205303 and parameters: {'n_estimators': 517, 'eta': 0.06875475912504612, 'max_depth': 9, 'alpha': 0.7646000000000001, 'lambda': 10.577378735476161, 'max_bin': 454}. Best is trial 302 with value: 0.7350160843949177.\n",
      "[I 2023-12-12 03:58:50,308] Trial 307 finished with value: 0.7362892660389271 and parameters: {'n_estimators': 515, 'eta': 0.06817891080023505, 'max_depth': 9, 'alpha': 0.76, 'lambda': 10.756603385522187, 'max_bin': 455}. Best is trial 307 with value: 0.7362892660389271.\n",
      "[I 2023-12-12 03:58:57,723] Trial 308 finished with value: 0.7339154566679893 and parameters: {'n_estimators': 504, 'eta': 0.06700435315903612, 'max_depth': 9, 'alpha': 0.7599, 'lambda': 10.911554791285662, 'max_bin': 450}. Best is trial 307 with value: 0.7362892660389271.\n",
      "[I 2023-12-12 03:59:05,504] Trial 309 finished with value: 0.7359420948000666 and parameters: {'n_estimators': 517, 'eta': 0.06885557894504912, 'max_depth': 9, 'alpha': 0.7593000000000001, 'lambda': 10.484479530151061, 'max_bin': 455}. Best is trial 307 with value: 0.7362892660389271.\n",
      "[I 2023-12-12 03:59:13,032] Trial 310 finished with value: 0.7351336320802272 and parameters: {'n_estimators': 508, 'eta': 0.06827855610730899, 'max_depth': 9, 'alpha': 0.7609, 'lambda': 10.613191037716756, 'max_bin': 454}. Best is trial 307 with value: 0.7362892660389271.\n",
      "[I 2023-12-12 03:59:20,410] Trial 311 finished with value: 0.735480625056933 and parameters: {'n_estimators': 503, 'eta': 0.07007831847263568, 'max_depth': 9, 'alpha': 0.7564000000000001, 'lambda': 10.597987609429701, 'max_bin': 455}. Best is trial 307 with value: 0.7362892660389271.\n",
      "[I 2023-12-12 03:59:27,266] Trial 312 finished with value: 0.7331395085375702 and parameters: {'n_estimators': 506, 'eta': 0.06951773705905025, 'max_depth': 9, 'alpha': 0.7418, 'lambda': 10.190216503128346, 'max_bin': 452}. Best is trial 307 with value: 0.7362892660389271.\n",
      "[I 2023-12-12 03:59:34,092] Trial 313 finished with value: 0.7368898298258583 and parameters: {'n_estimators': 505, 'eta': 0.07026222679422633, 'max_depth': 9, 'alpha': 0.7439, 'lambda': 10.698292887930286, 'max_bin': 448}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 03:59:40,934] Trial 314 finished with value: 0.731976575054027 and parameters: {'n_estimators': 507, 'eta': 0.07073151943712858, 'max_depth': 9, 'alpha': 0.7345, 'lambda': 10.800662555000372, 'max_bin': 455}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 03:59:48,198] Trial 315 finished with value: 0.734895587660868 and parameters: {'n_estimators': 498, 'eta': 0.0703735319669983, 'max_depth': 9, 'alpha': 0.7433000000000001, 'lambda': 10.820828966424338, 'max_bin': 453}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 03:59:55,263] Trial 316 finished with value: 0.7362764787583033 and parameters: {'n_estimators': 500, 'eta': 0.06986638319172239, 'max_depth': 9, 'alpha': 0.7347, 'lambda': 10.820065659124362, 'max_bin': 453}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:00:03,237] Trial 317 finished with value: 0.7322332042249299 and parameters: {'n_estimators': 498, 'eta': 0.07090810339468469, 'max_depth': 12, 'alpha': 0.7332000000000001, 'lambda': 10.725922015507859, 'max_bin': 452}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:00:11,223] Trial 318 finished with value: 0.7318355620208512 and parameters: {'n_estimators': 502, 'eta': 0.07064920183252679, 'max_depth': 10, 'alpha': 0.7354, 'lambda': 10.804327489147159, 'max_bin': 453}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:00:19,175] Trial 319 finished with value: 0.7321126988885596 and parameters: {'n_estimators': 500, 'eta': 0.07062785581027677, 'max_depth': 10, 'alpha': 0.7343000000000001, 'lambda': 10.818931340803248, 'max_bin': 452}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:00:27,262] Trial 320 finished with value: 0.7324096631558037 and parameters: {'n_estimators': 495, 'eta': 0.07023576788931887, 'max_depth': 11, 'alpha': 0.7296, 'lambda': 10.937190420602922, 'max_bin': 452}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:00:34,092] Trial 321 finished with value: 0.7315579274574804 and parameters: {'n_estimators': 484, 'eta': 0.07066926370722319, 'max_depth': 12, 'alpha': 0.7287, 'lambda': 10.830337442007519, 'max_bin': 455}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:00:42,162] Trial 322 finished with value: 0.728739140935047 and parameters: {'n_estimators': 479, 'eta': 0.07073617949407929, 'max_depth': 12, 'alpha': 0.6987, 'lambda': 10.732427457442943, 'max_bin': 453}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:00:50,253] Trial 323 finished with value: 0.7307154767404618 and parameters: {'n_estimators': 503, 'eta': 0.06960369248024977, 'max_depth': 12, 'alpha': 0.733, 'lambda': 10.76757588048691, 'max_bin': 444}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:00:58,152] Trial 324 finished with value: 0.7325932959218674 and parameters: {'n_estimators': 502, 'eta': 0.06988730588095027, 'max_depth': 12, 'alpha': 0.7274, 'lambda': 11.031324679141125, 'max_bin': 444}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:01:05,459] Trial 325 finished with value: 0.7317749201467348 and parameters: {'n_estimators': 502, 'eta': 0.0697715620095714, 'max_depth': 12, 'alpha': 0.7245, 'lambda': 11.175113042405833, 'max_bin': 444}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:01:13,163] Trial 326 finished with value: 0.7326879954147552 and parameters: {'n_estimators': 500, 'eta': 0.06988197701202147, 'max_depth': 12, 'alpha': 0.7239, 'lambda': 10.977772405462337, 'max_bin': 443}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:01:20,480] Trial 327 finished with value: 0.7321381547757071 and parameters: {'n_estimators': 497, 'eta': 0.07040537153622349, 'max_depth': 12, 'alpha': 0.7181000000000001, 'lambda': 11.287193316749342, 'max_bin': 453}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:01:28,339] Trial 328 finished with value: 0.7315809730377187 and parameters: {'n_estimators': 476, 'eta': 0.07132543252213851, 'max_depth': 12, 'alpha': 0.7176, 'lambda': 11.172001763946799, 'max_bin': 459}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:01:35,591] Trial 329 finished with value: 0.7320316580261601 and parameters: {'n_estimators': 464, 'eta': 0.07168398730242928, 'max_depth': 11, 'alpha': 0.7063, 'lambda': 11.412431947063679, 'max_bin': 449}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:01:43,438] Trial 330 finished with value: 0.7310077007322221 and parameters: {'n_estimators': 462, 'eta': 0.07192621008345232, 'max_depth': 12, 'alpha': 0.7010000000000001, 'lambda': 11.350166738948783, 'max_bin': 459}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:01:51,765] Trial 331 finished with value: 0.7310728735471341 and parameters: {'n_estimators': 495, 'eta': 0.07167190953216827, 'max_depth': 11, 'alpha': 0.7167, 'lambda': 12.520700873744813, 'max_bin': 448}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:01:59,900] Trial 332 finished with value: 0.7316460941572036 and parameters: {'n_estimators': 475, 'eta': 0.06948125809486982, 'max_depth': 12, 'alpha': 0.7295, 'lambda': 11.484611598061674, 'max_bin': 463}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:02:07,976] Trial 333 finished with value: 0.7332623111905112 and parameters: {'n_estimators': 444, 'eta': 0.06939415087454479, 'max_depth': 11, 'alpha': 0.6912, 'lambda': 11.342812516452957, 'max_bin': 439}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:02:15,490] Trial 334 finished with value: 0.7279770365764767 and parameters: {'n_estimators': 442, 'eta': 0.0695452430707351, 'max_depth': 11, 'alpha': 0.6966, 'lambda': 12.418456556842761, 'max_bin': 439}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:02:23,440] Trial 335 finished with value: 0.7318985764791126 and parameters: {'n_estimators': 499, 'eta': 0.07323317872307411, 'max_depth': 11, 'alpha': 0.6833, 'lambda': 11.292400721772394, 'max_bin': 444}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:02:31,150] Trial 336 finished with value: 0.7303970998730039 and parameters: {'n_estimators': 515, 'eta': 0.07322872490122623, 'max_depth': 11, 'alpha': 0.6857000000000001, 'lambda': 10.616770951926394, 'max_bin': 448}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:02:39,002] Trial 337 finished with value: 0.7290743722289267 and parameters: {'n_estimators': 455, 'eta': 0.07295153238262472, 'max_depth': 11, 'alpha': 0.7398, 'lambda': 11.505518612997852, 'max_bin': 437}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:02:46,767] Trial 338 finished with value: 0.7308534057127639 and parameters: {'n_estimators': 496, 'eta': 0.07133661025894462, 'max_depth': 11, 'alpha': 0.7032, 'lambda': 12.177372891743968, 'max_bin': 449}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:02:55,554] Trial 339 finished with value: 0.7335834270591441 and parameters: {'n_estimators': 515, 'eta': 0.06821030970267637, 'max_depth': 11, 'alpha': 0.7422000000000001, 'lambda': 10.987823635377417, 'max_bin': 457}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:03:04,110] Trial 340 finished with value: 0.7361942838746176 and parameters: {'n_estimators': 517, 'eta': 0.0681029977643073, 'max_depth': 11, 'alpha': 0.6858000000000001, 'lambda': 10.345194924396266, 'max_bin': 464}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:03:12,518] Trial 341 finished with value: 0.7319942161348367 and parameters: {'n_estimators': 519, 'eta': 0.06815102625246527, 'max_depth': 11, 'alpha': 0.7463000000000001, 'lambda': 10.439988681396896, 'max_bin': 467}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:03:20,452] Trial 342 finished with value: 0.735672930872312 and parameters: {'n_estimators': 518, 'eta': 0.0680732908772039, 'max_depth': 11, 'alpha': 0.75, 'lambda': 10.182039318878571, 'max_bin': 465}. Best is trial 313 with value: 0.7368898298258583.\n",
      "[I 2023-12-12 04:03:28,404] Trial 343 finished with value: 0.7370498919707076 and parameters: {'n_estimators': 485, 'eta': 0.06825622478419201, 'max_depth': 11, 'alpha': 0.7107, 'lambda': 10.268016721097911, 'max_bin': 461}. Best is trial 343 with value: 0.7370498919707076.\n",
      "[I 2023-12-12 04:03:35,788] Trial 344 finished with value: 0.733770515185719 and parameters: {'n_estimators': 484, 'eta': 0.06784707442281165, 'max_depth': 11, 'alpha': 0.7572, 'lambda': 10.127217086167816, 'max_bin': 474}. Best is trial 343 with value: 0.7370498919707076.\n",
      "[I 2023-12-12 04:03:43,077] Trial 345 finished with value: 0.7343270138145688 and parameters: {'n_estimators': 481, 'eta': 0.06790436936552859, 'max_depth': 11, 'alpha': 0.7542, 'lambda': 10.11899983001766, 'max_bin': 463}. Best is trial 343 with value: 0.7370498919707076.\n",
      "[I 2023-12-12 04:03:50,315] Trial 346 finished with value: 0.7359919761863043 and parameters: {'n_estimators': 482, 'eta': 0.06782324331917193, 'max_depth': 11, 'alpha': 0.759, 'lambda': 10.024630927010232, 'max_bin': 474}. Best is trial 343 with value: 0.7370498919707076.\n",
      "[I 2023-12-12 04:03:58,713] Trial 347 finished with value: 0.7354463886108661 and parameters: {'n_estimators': 477, 'eta': 0.06798572623499473, 'max_depth': 11, 'alpha': 0.7653000000000001, 'lambda': 10.079490945469104, 'max_bin': 474}. Best is trial 343 with value: 0.7370498919707076.\n",
      "[I 2023-12-12 04:04:06,289] Trial 348 finished with value: 0.7342825976111411 and parameters: {'n_estimators': 432, 'eta': 0.06767685665225102, 'max_depth': 11, 'alpha': 0.7545000000000001, 'lambda': 10.075474365202778, 'max_bin': 475}. Best is trial 343 with value: 0.7370498919707076.\n",
      "[I 2023-12-12 04:04:13,895] Trial 349 finished with value: 0.7339522225253633 and parameters: {'n_estimators': 446, 'eta': 0.06782680471945234, 'max_depth': 11, 'alpha': 0.7538, 'lambda': 10.03931787360134, 'max_bin': 476}. Best is trial 343 with value: 0.7370498919707076.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7370\n",
      "\tBest params:\n",
      "\t\tn_estimators: 485\n",
      "\t\teta: 0.06825622478419201\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.7107\n",
      "\t\tlambda: 10.268016721097911\n",
      "\t\tmax_bin: 461\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.693937    0.727582    0.711964    0.721309   \n",
      "1                    TP   31.000000   38.000000   36.000000   35.000000   \n",
      "2                    TN  310.000000  307.000000  309.000000  309.000000   \n",
      "3                    FP    6.000000    8.000000    6.000000    5.000000   \n",
      "4                    FN   35.000000   29.000000   31.000000   33.000000   \n",
      "5              Accuracy    0.892670    0.903141    0.903141    0.900524   \n",
      "6             Precision    0.837838    0.826087    0.857143    0.875000   \n",
      "7           Sensitivity    0.469697    0.567164    0.537313    0.514706   \n",
      "8           Specificity    0.981000    0.974600    0.981000    0.984100   \n",
      "9              F1 score    0.601942    0.672566    0.660550    0.648148   \n",
      "10  F1 score (weighted)    0.879915    0.895703    0.893882    0.889751   \n",
      "11     F1 score (macro)    0.769957    0.807865    0.802031    0.795111   \n",
      "12    Balanced Accuracy    0.725355    0.770884    0.759133    0.749391   \n",
      "13                  MCC    0.576105    0.633078    0.630059    0.623142   \n",
      "14                  NPV    0.898600    0.913700    0.908800    0.903500   \n",
      "15              ROC_AUC    0.725355    0.770884    0.759133    0.749391   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.733245    0.752080    0.742097  \n",
      "1    36.000000   38.000000   39.000000  \n",
      "2   306.000000  312.000000  308.000000  \n",
      "3    10.000000    4.000000    6.000000  \n",
      "4    30.000000   28.000000   29.000000  \n",
      "5     0.895288    0.916230    0.908377  \n",
      "6     0.782609    0.904762    0.866667  \n",
      "7     0.545455    0.575758    0.573529  \n",
      "8     0.968400    0.987300    0.980900  \n",
      "9     0.642857    0.703704    0.690265  \n",
      "10    0.887545    0.908455    0.900671  \n",
      "11    0.790754    0.827462    0.818251  \n",
      "12    0.756904    0.781550    0.777211  \n",
      "13    0.596855    0.680513    0.657867  \n",
      "14    0.910700    0.917600    0.913900  \n",
      "15    0.756904    0.781550    0.777211  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_xgb_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_6_cat = np.where(((y_pred_xgb_6 >= 2) | (y_pred_xgb_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:04:23,207] Trial 350 finished with value: 0.7322799040254235 and parameters: {'n_estimators': 435, 'eta': 0.06773949343068497, 'max_depth': 11, 'alpha': 0.7593000000000001, 'lambda': 9.87175758354655, 'max_bin': 477}. Best is trial 343 with value: 0.7370498919707076.\n",
      "[I 2023-12-12 04:04:31,635] Trial 351 finished with value: 0.7364146541725695 and parameters: {'n_estimators': 406, 'eta': 0.06759877918630167, 'max_depth': 11, 'alpha': 0.757, 'lambda': 10.107936422462892, 'max_bin': 474}. Best is trial 343 with value: 0.7370498919707076.\n",
      "[I 2023-12-12 04:04:39,923] Trial 352 finished with value: 0.7342900524428772 and parameters: {'n_estimators': 408, 'eta': 0.06763755723526828, 'max_depth': 11, 'alpha': 0.7587, 'lambda': 9.786423309844903, 'max_bin': 471}. Best is trial 343 with value: 0.7370498919707076.\n",
      "[I 2023-12-12 04:04:47,607] Trial 353 finished with value: 0.734482576354641 and parameters: {'n_estimators': 404, 'eta': 0.06708020895810748, 'max_depth': 11, 'alpha': 0.7605000000000001, 'lambda': 9.947188983599863, 'max_bin': 475}. Best is trial 343 with value: 0.7370498919707076.\n",
      "[I 2023-12-12 04:04:55,360] Trial 354 finished with value: 0.7345265202669965 and parameters: {'n_estimators': 394, 'eta': 0.06712097095825532, 'max_depth': 11, 'alpha': 0.7594000000000001, 'lambda': 10.034174943931133, 'max_bin': 473}. Best is trial 343 with value: 0.7370498919707076.\n",
      "[I 2023-12-12 04:05:03,401] Trial 355 finished with value: 0.7366853460956543 and parameters: {'n_estimators': 392, 'eta': 0.06802607953641668, 'max_depth': 11, 'alpha': 0.7617, 'lambda': 9.989454359458318, 'max_bin': 474}. Best is trial 343 with value: 0.7370498919707076.\n",
      "[I 2023-12-12 04:05:10,778] Trial 356 finished with value: 0.7343866031489109 and parameters: {'n_estimators': 382, 'eta': 0.06675248548455996, 'max_depth': 11, 'alpha': 0.7675000000000001, 'lambda': 9.910111946193501, 'max_bin': 471}. Best is trial 343 with value: 0.7370498919707076.\n",
      "[I 2023-12-12 04:05:18,716] Trial 357 finished with value: 0.7388703292180938 and parameters: {'n_estimators': 383, 'eta': 0.06679005365939406, 'max_depth': 11, 'alpha': 0.7597, 'lambda': 9.968258042127214, 'max_bin': 481}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:05:26,610] Trial 358 finished with value: 0.7360505879866752 and parameters: {'n_estimators': 381, 'eta': 0.06659266111954233, 'max_depth': 11, 'alpha': 0.7698, 'lambda': 9.746262930227719, 'max_bin': 482}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:05:34,429] Trial 359 finished with value: 0.7333572350734097 and parameters: {'n_estimators': 376, 'eta': 0.06670811665441685, 'max_depth': 11, 'alpha': 0.772, 'lambda': 9.609207423637326, 'max_bin': 482}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:05:42,133] Trial 360 finished with value: 0.7351025691841343 and parameters: {'n_estimators': 393, 'eta': 0.0662140319489087, 'max_depth': 11, 'alpha': 0.7732, 'lambda': 9.677492978777655, 'max_bin': 470}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:05:49,928] Trial 361 finished with value: 0.7372367218064143 and parameters: {'n_estimators': 393, 'eta': 0.06655069740134463, 'max_depth': 11, 'alpha': 0.7774000000000001, 'lambda': 9.47136639740218, 'max_bin': 464}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:05:58,009] Trial 362 finished with value: 0.7371182642324422 and parameters: {'n_estimators': 384, 'eta': 0.06638992150491284, 'max_depth': 11, 'alpha': 0.7779, 'lambda': 9.14389103559828, 'max_bin': 482}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:06:05,217] Trial 363 finished with value: 0.7339923755003868 and parameters: {'n_estimators': 355, 'eta': 0.06585983030453432, 'max_depth': 11, 'alpha': 0.7785000000000001, 'lambda': 9.33349875863628, 'max_bin': 491}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:06:13,926] Trial 364 finished with value: 0.7345125634792966 and parameters: {'n_estimators': 397, 'eta': 0.06572365880696214, 'max_depth': 11, 'alpha': 0.7763, 'lambda': 22.573974795482016, 'max_bin': 482}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:06:21,729] Trial 365 finished with value: 0.7372480705927578 and parameters: {'n_estimators': 372, 'eta': 0.0654295684903872, 'max_depth': 11, 'alpha': 0.3013, 'lambda': 8.919814465287795, 'max_bin': 483}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:06:30,676] Trial 366 finished with value: 0.7365685521538191 and parameters: {'n_estimators': 365, 'eta': 0.06589122560052017, 'max_depth': 11, 'alpha': 0.24200000000000002, 'lambda': 20.969528091034555, 'max_bin': 468}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:06:38,832] Trial 367 finished with value: 0.733051625957578 and parameters: {'n_estimators': 335, 'eta': 0.06555283899574006, 'max_depth': 11, 'alpha': 0.27290000000000003, 'lambda': 21.45343301952318, 'max_bin': 465}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:06:46,427] Trial 368 finished with value: 0.7334628418005209 and parameters: {'n_estimators': 366, 'eta': 0.06863678487453306, 'max_depth': 11, 'alpha': 0.30160000000000003, 'lambda': 8.671390729481605, 'max_bin': 482}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:06:53,713] Trial 369 finished with value: 0.7382051147250417 and parameters: {'n_estimators': 371, 'eta': 0.06524179579983541, 'max_depth': 11, 'alpha': 0.3073, 'lambda': 9.003236915311627, 'max_bin': 487}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:07:02,383] Trial 370 finished with value: 0.7345204319109876 and parameters: {'n_estimators': 370, 'eta': 0.06550065617952523, 'max_depth': 11, 'alpha': 0.233, 'lambda': 18.478217474355304, 'max_bin': 489}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:07:09,482] Trial 371 finished with value: 0.7354603609586469 and parameters: {'n_estimators': 316, 'eta': 0.06514970381537295, 'max_depth': 11, 'alpha': 0.23270000000000002, 'lambda': 9.084925302373888, 'max_bin': 485}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:07:18,306] Trial 372 finished with value: 0.7342561801284655 and parameters: {'n_estimators': 385, 'eta': 0.06550799996182968, 'max_depth': 11, 'alpha': 0.2093, 'lambda': 19.167798808840487, 'max_bin': 484}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:07:25,729] Trial 373 finished with value: 0.7335835330466224 and parameters: {'n_estimators': 312, 'eta': 0.06487344817380979, 'max_depth': 11, 'alpha': 0.3164, 'lambda': 20.314472399485897, 'max_bin': 498}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:07:33,892] Trial 374 finished with value: 0.7352086623876717 and parameters: {'n_estimators': 348, 'eta': 0.06627446553139185, 'max_depth': 11, 'alpha': 0.21480000000000002, 'lambda': 23.401350160444757, 'max_bin': 468}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:07:41,938] Trial 375 finished with value: 0.7351115294673806 and parameters: {'n_estimators': 345, 'eta': 0.0659494874584675, 'max_depth': 11, 'alpha': 0.22210000000000002, 'lambda': 22.940440462651758, 'max_bin': 468}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:07:50,151] Trial 376 finished with value: 0.734919498913148 and parameters: {'n_estimators': 340, 'eta': 0.06514739644132451, 'max_depth': 11, 'alpha': 0.23650000000000002, 'lambda': 23.713324162811766, 'max_bin': 468}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:07:58,471] Trial 377 finished with value: 0.7339887698504174 and parameters: {'n_estimators': 352, 'eta': 0.06652294127716112, 'max_depth': 11, 'alpha': 0.18810000000000002, 'lambda': 22.517183346804078, 'max_bin': 461}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:08:06,411] Trial 378 finished with value: 0.7332796239976246 and parameters: {'n_estimators': 326, 'eta': 0.06476559729646808, 'max_depth': 11, 'alpha': 0.26480000000000004, 'lambda': 24.178409974099726, 'max_bin': 479}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:08:14,629] Trial 379 finished with value: 0.7344277814626279 and parameters: {'n_estimators': 353, 'eta': 0.07335494606294188, 'max_depth': 11, 'alpha': 0.33080000000000004, 'lambda': 21.10000774052358, 'max_bin': 494}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:08:23,516] Trial 380 finished with value: 0.7339348224616482 and parameters: {'n_estimators': 375, 'eta': 0.06652691341463733, 'max_depth': 11, 'alpha': 0.202, 'lambda': 22.732963440849396, 'max_bin': 469}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:08:30,967] Trial 381 finished with value: 0.7357474049136774 and parameters: {'n_estimators': 304, 'eta': 0.06817413989166979, 'max_depth': 11, 'alpha': 0.232, 'lambda': 20.859052951942687, 'max_bin': 464}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:08:38,271] Trial 382 finished with value: 0.7338815375201024 and parameters: {'n_estimators': 306, 'eta': 0.06868122963098672, 'max_depth': 11, 'alpha': 0.2388, 'lambda': 19.928011647408095, 'max_bin': 464}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:08:45,017] Trial 383 finished with value: 0.7357925028802816 and parameters: {'n_estimators': 280, 'eta': 0.07442192372687277, 'max_depth': 11, 'alpha': 0.1474, 'lambda': 22.15795722281567, 'max_bin': 487}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:08:53,108] Trial 384 finished with value: 0.7360303470965674 and parameters: {'n_estimators': 337, 'eta': 0.07488504534277397, 'max_depth': 11, 'alpha': 0.13340000000000002, 'lambda': 20.52428371465634, 'max_bin': 484}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:09:00,038] Trial 385 finished with value: 0.7338903377941113 and parameters: {'n_estimators': 283, 'eta': 0.07503252194117159, 'max_depth': 11, 'alpha': 0.1593, 'lambda': 21.2106704883622, 'max_bin': 487}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:09:07,491] Trial 386 finished with value: 0.7326450660795886 and parameters: {'n_estimators': 311, 'eta': 0.07363240486960254, 'max_depth': 11, 'alpha': 0.10640000000000001, 'lambda': 20.326897752572144, 'max_bin': 494}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:09:16,945] Trial 387 finished with value: 0.7358334051487213 and parameters: {'n_estimators': 418, 'eta': 0.07202485564177374, 'max_depth': 11, 'alpha': 0.1414, 'lambda': 20.81244109069094, 'max_bin': 483}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:09:23,394] Trial 388 finished with value: 0.7307440656629127 and parameters: {'n_estimators': 264, 'eta': 0.07126109066068417, 'max_depth': 11, 'alpha': 0.1376, 'lambda': 20.88231454658575, 'max_bin': 480}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:09:32,236] Trial 389 finished with value: 0.7346685197463974 and parameters: {'n_estimators': 415, 'eta': 0.07544116112113233, 'max_depth': 11, 'alpha': 0.1114, 'lambda': 21.604520609005043, 'max_bin': 484}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:09:40,189] Trial 390 finished with value: 0.7329047793659587 and parameters: {'n_estimators': 321, 'eta': 0.07450200923977826, 'max_depth': 11, 'alpha': 0.14830000000000002, 'lambda': 21.907832476534665, 'max_bin': 478}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:09:48,489] Trial 391 finished with value: 0.7341019313933832 and parameters: {'n_estimators': 356, 'eta': 0.07203742198333092, 'max_depth': 11, 'alpha': 0.0709, 'lambda': 20.412047117222922, 'max_bin': 490}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:09:56,983] Trial 392 finished with value: 0.7306437649240058 and parameters: {'n_estimators': 376, 'eta': 0.0729757995309043, 'max_depth': 11, 'alpha': 0.17400000000000002, 'lambda': 19.539719361779696, 'max_bin': 487}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:10:03,987] Trial 393 finished with value: 0.7319556771133348 and parameters: {'n_estimators': 292, 'eta': 0.07228254389299699, 'max_depth': 11, 'alpha': 0.2597, 'lambda': 21.76225195382883, 'max_bin': 484}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:10:11,907] Trial 394 finished with value: 0.736720905436177 and parameters: {'n_estimators': 334, 'eta': 0.06930218686337652, 'max_depth': 11, 'alpha': 0.18000000000000002, 'lambda': 20.881941790910155, 'max_bin': 461}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:10:21,014] Trial 395 finished with value: 0.7349383497077853 and parameters: {'n_estimators': 414, 'eta': 0.06959167450018315, 'max_depth': 11, 'alpha': 0.1356, 'lambda': 20.777075137264205, 'max_bin': 461}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:10:28,698] Trial 396 finished with value: 0.7321802921427426 and parameters: {'n_estimators': 332, 'eta': 0.07451370293779071, 'max_depth': 11, 'alpha': 0.1645, 'lambda': 19.011364180244207, 'max_bin': 479}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:10:36,881] Trial 397 finished with value: 0.7330777066602983 and parameters: {'n_estimators': 367, 'eta': 0.06897277033665176, 'max_depth': 11, 'alpha': 0.18510000000000001, 'lambda': 21.918418984999683, 'max_bin': 473}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:10:45,653] Trial 398 finished with value: 0.7349791755565431 and parameters: {'n_estimators': 422, 'eta': 0.07230082384921095, 'max_depth': 11, 'alpha': 0.1241, 'lambda': 20.5850481545358, 'max_bin': 461}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:10:53,665] Trial 399 finished with value: 0.7320340508844247 and parameters: {'n_estimators': 385, 'eta': 0.075945333458663, 'max_depth': 11, 'alpha': 0.0745, 'lambda': 19.926882519804458, 'max_bin': 495}. Best is trial 357 with value: 0.7388703292180938.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7389\n",
      "\tBest params:\n",
      "\t\tn_estimators: 383\n",
      "\t\teta: 0.06679005365939406\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.7597\n",
      "\t\tlambda: 9.968258042127214\n",
      "\t\tmax_bin: 481\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.693937    0.727582    0.711964    0.721309   \n",
      "1                    TP   31.000000   38.000000   36.000000   35.000000   \n",
      "2                    TN  310.000000  307.000000  309.000000  309.000000   \n",
      "3                    FP    6.000000    8.000000    6.000000    5.000000   \n",
      "4                    FN   35.000000   29.000000   31.000000   33.000000   \n",
      "5              Accuracy    0.892670    0.903141    0.903141    0.900524   \n",
      "6             Precision    0.837838    0.826087    0.857143    0.875000   \n",
      "7           Sensitivity    0.469697    0.567164    0.537313    0.514706   \n",
      "8           Specificity    0.981000    0.974600    0.981000    0.984100   \n",
      "9              F1 score    0.601942    0.672566    0.660550    0.648148   \n",
      "10  F1 score (weighted)    0.879915    0.895703    0.893882    0.889751   \n",
      "11     F1 score (macro)    0.769957    0.807865    0.802031    0.795111   \n",
      "12    Balanced Accuracy    0.725355    0.770884    0.759133    0.749391   \n",
      "13                  MCC    0.576105    0.633078    0.630059    0.623142   \n",
      "14                  NPV    0.898600    0.913700    0.908800    0.903500   \n",
      "15              ROC_AUC    0.725355    0.770884    0.759133    0.749391   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.733245    0.752080    0.742097    0.705249  \n",
      "1    36.000000   38.000000   39.000000   29.000000  \n",
      "2   306.000000  312.000000  308.000000  310.000000  \n",
      "3    10.000000    4.000000    6.000000    5.000000  \n",
      "4    30.000000   28.000000   29.000000   38.000000  \n",
      "5     0.895288    0.916230    0.908377    0.887435  \n",
      "6     0.782609    0.904762    0.866667    0.852941  \n",
      "7     0.545455    0.575758    0.573529    0.432836  \n",
      "8     0.968400    0.987300    0.980900    0.984100  \n",
      "9     0.642857    0.703704    0.690265    0.574257  \n",
      "10    0.887545    0.908455    0.900671    0.871847  \n",
      "11    0.790754    0.827462    0.818251    0.754700  \n",
      "12    0.756904    0.781550    0.777211    0.708481  \n",
      "13    0.596855    0.680513    0.657867    0.556879  \n",
      "14    0.910700    0.917600    0.913900    0.890800  \n",
      "15    0.756904    0.781550    0.777211    0.708481  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_xgb_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_7_cat = np.where(((y_pred_xgb_7 >= 2) | (y_pred_xgb_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:11:02,244] Trial 400 finished with value: 0.7198436825505918 and parameters: {'n_estimators': 363, 'eta': 0.06934773623879986, 'max_depth': 11, 'alpha': 0.1826, 'lambda': 21.113141966388607, 'max_bin': 485}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:11:09,579] Trial 401 finished with value: 0.720704711534769 and parameters: {'n_estimators': 403, 'eta': 0.07379404321222299, 'max_depth': 11, 'alpha': 0.34240000000000004, 'lambda': 19.453661196833423, 'max_bin': 475}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:11:15,649] Trial 402 finished with value: 0.7220750658859335 and parameters: {'n_estimators': 242, 'eta': 0.07197111295311884, 'max_depth': 11, 'alpha': 0.2878, 'lambda': 22.229875021701172, 'max_bin': 464}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:11:22,548] Trial 403 finished with value: 0.7214886794968892 and parameters: {'n_estimators': 293, 'eta': 0.06854390340185863, 'max_depth': 11, 'alpha': 0.25930000000000003, 'lambda': 20.83205780937618, 'max_bin': 480}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:11:32,272] Trial 404 finished with value: 0.7147564289551344 and parameters: {'n_estimators': 386, 'eta': 0.029486357314735295, 'max_depth': 11, 'alpha': 0.1399, 'lambda': 17.88777827809723, 'max_bin': 489}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:11:40,231] Trial 405 finished with value: 0.7215876425766866 and parameters: {'n_estimators': 320, 'eta': 0.049178369147267816, 'max_depth': 11, 'alpha': 0.40540000000000004, 'lambda': 19.62137407889735, 'max_bin': 500}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:11:46,733] Trial 406 finished with value: 0.7213058214676147 and parameters: {'n_estimators': 260, 'eta': 0.06870984855201631, 'max_depth': 11, 'alpha': 0.1557, 'lambda': 21.749878987135183, 'max_bin': 461}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:11:54,160] Trial 407 finished with value: 0.722570888651147 and parameters: {'n_estimators': 339, 'eta': 0.07090921587356439, 'max_depth': 11, 'alpha': 0.3831, 'lambda': 20.049350297854602, 'max_bin': 470}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:12:01,699] Trial 408 finished with value: 0.7225707757954076 and parameters: {'n_estimators': 401, 'eta': 0.0771030647856572, 'max_depth': 11, 'alpha': 0.11610000000000001, 'lambda': 21.285594986574047, 'max_bin': 475}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:12:10,151] Trial 409 finished with value: 0.7219723552151721 and parameters: {'n_estimators': 365, 'eta': 0.06390114731360759, 'max_depth': 11, 'alpha': 0.2492, 'lambda': 20.581328998638508, 'max_bin': 459}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:12:18,564] Trial 410 finished with value: 0.7227860014732814 and parameters: {'n_estimators': 386, 'eta': 0.06754864839405635, 'max_depth': 11, 'alpha': 0.22440000000000002, 'lambda': 22.156815644902423, 'max_bin': 484}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:12:25,368] Trial 411 finished with value: 0.7198300294851409 and parameters: {'n_estimators': 303, 'eta': 0.08867570415548075, 'max_depth': 11, 'alpha': 0.2932, 'lambda': 24.949414919080716, 'max_bin': 465}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:12:33,947] Trial 412 finished with value: 0.7259915008635394 and parameters: {'n_estimators': 421, 'eta': 0.07251069285668549, 'max_depth': 11, 'alpha': 0.1888, 'lambda': 19.054721942462812, 'max_bin': 472}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:12:40,693] Trial 413 finished with value: 0.718070740272918 and parameters: {'n_estimators': 343, 'eta': 0.06979960757260922, 'max_depth': 11, 'alpha': 0.4323, 'lambda': 8.275267254722497, 'max_bin': 480}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:12:49,112] Trial 414 finished with value: 0.724245435386875 and parameters: {'n_estimators': 371, 'eta': 0.06723481486742515, 'max_depth': 11, 'alpha': 0.1985, 'lambda': 18.732646667207636, 'max_bin': 488}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:12:56,172] Trial 415 finished with value: 0.7198651236736366 and parameters: {'n_estimators': 328, 'eta': 0.0643760584988845, 'max_depth': 11, 'alpha': 0.0944, 'lambda': 8.331254744005681, 'max_bin': 466}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:13:02,648] Trial 416 finished with value: 0.7166664214757512 and parameters: {'n_estimators': 398, 'eta': 0.07446289056054879, 'max_depth': 11, 'alpha': 0.27690000000000003, 'lambda': 9.076701353382894, 'max_bin': 477}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:13:12,669] Trial 417 finished with value: 0.6680435799447001 and parameters: {'n_estimators': 359, 'eta': 0.010246291364209438, 'max_depth': 11, 'alpha': 0.17320000000000002, 'lambda': 23.09037417918508, 'max_bin': 492}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:13:20,063] Trial 418 finished with value: 0.695456644086372 and parameters: {'n_estimators': 280, 'eta': 0.015531668062185616, 'max_depth': 11, 'alpha': 0.1532, 'lambda': 8.349779185074958, 'max_bin': 458}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:13:27,062] Trial 419 finished with value: 0.721006107979423 and parameters: {'n_estimators': 427, 'eta': 0.06947878782799645, 'max_depth': 11, 'alpha': 0.1254, 'lambda': 7.34884202102162, 'max_bin': 471}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:13:34,641] Trial 420 finished with value: 0.7197764413879055 and parameters: {'n_estimators': 387, 'eta': 0.0714731214880136, 'max_depth': 11, 'alpha': 0.24760000000000001, 'lambda': 16.26909539726998, 'max_bin': 482}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:13:40,691] Trial 421 finished with value: 0.7187770917694012 and parameters: {'n_estimators': 314, 'eta': 0.06748874536954934, 'max_depth': 11, 'alpha': 0.31520000000000004, 'lambda': 9.039629170050839, 'max_bin': 465}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:13:49,435] Trial 422 finished with value: 0.7243408422859622 and parameters: {'n_estimators': 377, 'eta': 0.06452161889206397, 'max_depth': 11, 'alpha': 0.1433, 'lambda': 17.57458818243744, 'max_bin': 477}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:13:56,645] Trial 423 finished with value: 0.7207527253804684 and parameters: {'n_estimators': 409, 'eta': 0.0905463202726937, 'max_depth': 11, 'alpha': 0.3553, 'lambda': 24.42910746405903, 'max_bin': 459}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:14:05,832] Trial 424 finished with value: 0.7223565264054064 and parameters: {'n_estimators': 461, 'eta': 0.06676401619619435, 'max_depth': 11, 'alpha': 0.1948, 'lambda': 21.204741175022026, 'max_bin': 485}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:14:12,968] Trial 425 finished with value: 0.7221939790443409 and parameters: {'n_estimators': 335, 'eta': 0.06924892912072864, 'max_depth': 11, 'alpha': 0.2195, 'lambda': 22.02299490124705, 'max_bin': 473}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:14:14,840] Trial 426 finished with value: 0.6921529802314149 and parameters: {'n_estimators': 69, 'eta': 0.09231727567955922, 'max_depth': 11, 'alpha': 0.1663, 'lambda': 9.01782780939575, 'max_bin': 493}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:14:22,649] Trial 427 finished with value: 0.7209855105086318 and parameters: {'n_estimators': 362, 'eta': 0.07387345075252857, 'max_depth': 11, 'alpha': 0.0514, 'lambda': 15.253029807353595, 'max_bin': 469}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:14:33,986] Trial 428 finished with value: 0.5470517164783226 and parameters: {'n_estimators': 394, 'eta': 0.004241375590544431, 'max_depth': 11, 'alpha': 0.12890000000000001, 'lambda': 25.145573501685632, 'max_bin': 480}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:14:42,449] Trial 429 finished with value: 0.7253211835369026 and parameters: {'n_estimators': 418, 'eta': 0.07123658435132472, 'max_depth': 11, 'alpha': 0.2426, 'lambda': 20.378184448102843, 'max_bin': 463}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:14:48,843] Trial 430 finished with value: 0.7202996054928492 and parameters: {'n_estimators': 298, 'eta': 0.0641902197147584, 'max_depth': 11, 'alpha': 0.536, 'lambda': 7.944648070295181, 'max_bin': 487}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:14:55,178] Trial 431 finished with value: 0.7210972931935375 and parameters: {'n_estimators': 351, 'eta': 0.06746417593206164, 'max_depth': 11, 'alpha': 0.7109000000000001, 'lambda': 1.5444828500645063, 'max_bin': 458}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:15:01,814] Trial 432 finished with value: 0.7176518019155432 and parameters: {'n_estimators': 379, 'eta': 0.07654389750519512, 'max_depth': 11, 'alpha': 0.275, 'lambda': 9.162926302632929, 'max_bin': 474}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:15:10,317] Trial 433 finished with value: 0.7234222723943338 and parameters: {'n_estimators': 474, 'eta': 0.07051011215939497, 'max_depth': 11, 'alpha': 0.21150000000000002, 'lambda': 25.85011751659672, 'max_bin': 466}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:15:16,758] Trial 434 finished with value: 0.720392157422896 and parameters: {'n_estimators': 277, 'eta': 0.09992895381747535, 'max_depth': 11, 'alpha': 0.1044, 'lambda': 18.730270762466837, 'max_bin': 479}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:15:22,849] Trial 435 finished with value: 0.7210137288516603 and parameters: {'n_estimators': 331, 'eta': 0.08587205757808711, 'max_depth': 11, 'alpha': 0.7437, 'lambda': 5.5134043205586645, 'max_bin': 484}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:15:30,581] Trial 436 finished with value: 0.7240511911139501 and parameters: {'n_estimators': 401, 'eta': 0.06871443549560313, 'max_depth': 11, 'alpha': 0.7748, 'lambda': 19.72351304212073, 'max_bin': 470}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:15:39,125] Trial 437 finished with value: 0.7239810380785503 and parameters: {'n_estimators': 365, 'eta': 0.07288706212535385, 'max_depth': 11, 'alpha': 0.30610000000000004, 'lambda': 33.43357997145417, 'max_bin': 490}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:15:48,328] Trial 438 finished with value: 0.7266413900132681 and parameters: {'n_estimators': 445, 'eta': 0.0658279737367918, 'max_depth': 11, 'alpha': 0.1568, 'lambda': 22.727013955740468, 'max_bin': 461}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:15:51,865] Trial 439 finished with value: 0.7010211439210059 and parameters: {'n_estimators': 349, 'eta': 0.06400069768695846, 'max_depth': 5, 'alpha': 0.745, 'lambda': 7.759609068866656, 'max_bin': 476}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:15:58,864] Trial 440 finished with value: 0.7233526751042898 and parameters: {'n_estimators': 315, 'eta': 0.06708069600085627, 'max_depth': 10, 'alpha': 0.7122, 'lambda': 29.142806843539194, 'max_bin': 457}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:16:03,663] Trial 441 finished with value: 0.7138477860904952 and parameters: {'n_estimators': 426, 'eta': 0.09646606505020813, 'max_depth': 6, 'alpha': 0.782, 'lambda': 17.04978698484503, 'max_bin': 481}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:16:06,231] Trial 442 finished with value: 0.7043009536058207 and parameters: {'n_estimators': 99, 'eta': 0.07512390598938429, 'max_depth': 11, 'alpha': 0.7408, 'lambda': 13.990538372238849, 'max_bin': 496}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:16:11,951] Trial 443 finished with value: 0.7182719731326127 and parameters: {'n_estimators': 225, 'eta': 0.06950345563753671, 'max_depth': 11, 'alpha': 0.2069, 'lambda': 21.119168912898925, 'max_bin': 466}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:16:19,405] Trial 444 finished with value: 0.721734390160212 and parameters: {'n_estimators': 462, 'eta': 0.07214675521275307, 'max_depth': 11, 'alpha': 0.7691, 'lambda': 18.24072209918753, 'max_bin': 486}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:16:28,468] Trial 445 finished with value: 0.7252771303678796 and parameters: {'n_estimators': 386, 'eta': 0.06856855155037907, 'max_depth': 11, 'alpha': 0.45520000000000005, 'lambda': 39.13130552632106, 'max_bin': 472}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:16:37,289] Trial 446 finished with value: 0.7206347130879839 and parameters: {'n_estimators': 364, 'eta': 0.041628237492767894, 'max_depth': 11, 'alpha': 0.2349, 'lambda': 15.443086036885536, 'max_bin': 477}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:16:46,001] Trial 447 finished with value: 0.7242048402451491 and parameters: {'n_estimators': 409, 'eta': 0.0661375995802967, 'max_depth': 11, 'alpha': 0.1456, 'lambda': 20.04902504274027, 'max_bin': 464}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:16:53,317] Trial 448 finished with value: 0.7106064081926944 and parameters: {'n_estimators': 301, 'eta': 0.025128497599935185, 'max_depth': 11, 'alpha': 0.7197, 'lambda': 9.049380200502192, 'max_bin': 491}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:17:00,081] Trial 449 finished with value: 0.7168789946766407 and parameters: {'n_estimators': 474, 'eta': 0.07093694254965263, 'max_depth': 11, 'alpha': 0.7509, 'lambda': 9.574953835590696, 'max_bin': 458}. Best is trial 357 with value: 0.7388703292180938.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7389\n",
      "\tBest params:\n",
      "\t\tn_estimators: 383\n",
      "\t\teta: 0.06679005365939406\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.7597\n",
      "\t\tlambda: 9.968258042127214\n",
      "\t\tmax_bin: 481\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.693937    0.727582    0.711964    0.721309   \n",
      "1                    TP   31.000000   38.000000   36.000000   35.000000   \n",
      "2                    TN  310.000000  307.000000  309.000000  309.000000   \n",
      "3                    FP    6.000000    8.000000    6.000000    5.000000   \n",
      "4                    FN   35.000000   29.000000   31.000000   33.000000   \n",
      "5              Accuracy    0.892670    0.903141    0.903141    0.900524   \n",
      "6             Precision    0.837838    0.826087    0.857143    0.875000   \n",
      "7           Sensitivity    0.469697    0.567164    0.537313    0.514706   \n",
      "8           Specificity    0.981000    0.974600    0.981000    0.984100   \n",
      "9              F1 score    0.601942    0.672566    0.660550    0.648148   \n",
      "10  F1 score (weighted)    0.879915    0.895703    0.893882    0.889751   \n",
      "11     F1 score (macro)    0.769957    0.807865    0.802031    0.795111   \n",
      "12    Balanced Accuracy    0.725355    0.770884    0.759133    0.749391   \n",
      "13                  MCC    0.576105    0.633078    0.630059    0.623142   \n",
      "14                  NPV    0.898600    0.913700    0.908800    0.903500   \n",
      "15              ROC_AUC    0.725355    0.770884    0.759133    0.749391   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.733245    0.752080    0.742097    0.705249    0.687504  \n",
      "1    36.000000   38.000000   39.000000   29.000000   36.000000  \n",
      "2   306.000000  312.000000  308.000000  310.000000  306.000000  \n",
      "3    10.000000    4.000000    6.000000    5.000000    7.000000  \n",
      "4    30.000000   28.000000   29.000000   38.000000   33.000000  \n",
      "5     0.895288    0.916230    0.908377    0.887435    0.895288  \n",
      "6     0.782609    0.904762    0.866667    0.852941    0.837209  \n",
      "7     0.545455    0.575758    0.573529    0.432836    0.521739  \n",
      "8     0.968400    0.987300    0.980900    0.984100    0.977600  \n",
      "9     0.642857    0.703704    0.690265    0.574257    0.642857  \n",
      "10    0.887545    0.908455    0.900671    0.871847    0.885222  \n",
      "11    0.790754    0.827462    0.818251    0.754700    0.790754  \n",
      "12    0.756904    0.781550    0.777211    0.708481    0.749687  \n",
      "13    0.596855    0.680513    0.657867    0.556879    0.607840  \n",
      "14    0.910700    0.917600    0.913900    0.890800    0.902700  \n",
      "15    0.756904    0.781550    0.777211    0.708481    0.749687  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_xgb_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_8_cat = np.where(((y_pred_xgb_8 >= 2) | (y_pred_xgb_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:17:08,888] Trial 450 finished with value: 0.7321412573299535 and parameters: {'n_estimators': 344, 'eta': 0.06412937317548088, 'max_depth': 11, 'alpha': 0.7907000000000001, 'lambda': 21.6649568569377, 'max_bin': 470}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:17:16,075] Trial 451 finished with value: 0.7326929881779665 and parameters: {'n_estimators': 376, 'eta': 0.06752464770030592, 'max_depth': 11, 'alpha': 0.1811, 'lambda': 13.045842402419382, 'max_bin': 483}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:17:22,604] Trial 452 finished with value: 0.7315954742633803 and parameters: {'n_estimators': 328, 'eta': 0.0739007267675924, 'max_depth': 11, 'alpha': 0.7704000000000001, 'lambda': 7.039678061439359, 'max_bin': 475}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:17:31,691] Trial 453 finished with value: 0.7274879784095473 and parameters: {'n_estimators': 391, 'eta': 0.07673232139250251, 'max_depth': 11, 'alpha': 0.08120000000000001, 'lambda': 35.59237916343611, 'max_bin': 463}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:17:39,131] Trial 454 finished with value: 0.7344599898925399 and parameters: {'n_estimators': 448, 'eta': 0.07029117604588499, 'max_depth': 10, 'alpha': 0.7419, 'lambda': 12.399564748985199, 'max_bin': 479}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:17:50,353] Trial 455 finished with value: 0.7305412916521422 and parameters: {'n_estimators': 531, 'eta': 0.047880325574342755, 'max_depth': 11, 'alpha': 0.1126, 'lambda': 20.860958619502007, 'max_bin': 488}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:17:58,230] Trial 456 finished with value: 0.730928773759882 and parameters: {'n_estimators': 375, 'eta': 0.06532217066986491, 'max_depth': 11, 'alpha': 0.28650000000000003, 'lambda': 16.31183263009544, 'max_bin': 469}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:18:03,691] Trial 457 finished with value: 0.7192067248159153 and parameters: {'n_estimators': 482, 'eta': 0.06885884284780924, 'max_depth': 11, 'alpha': 0.2605, 'lambda': 1.0526680024321386, 'max_bin': 458}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:18:09,875] Trial 458 finished with value: 0.7252966244304506 and parameters: {'n_estimators': 409, 'eta': 0.0718910853808641, 'max_depth': 11, 'alpha': 0.7235, 'lambda': 8.15750321146212, 'max_bin': 481}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:18:17,135] Trial 459 finished with value: 0.7377957743402972 and parameters: {'n_estimators': 317, 'eta': 0.06729327847557325, 'max_depth': 11, 'alpha': 0.7826000000000001, 'lambda': 9.679731620464983, 'max_bin': 472}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:18:23,168] Trial 460 finished with value: 0.7361040792441733 and parameters: {'n_estimators': 321, 'eta': 0.07816633809751884, 'max_depth': 10, 'alpha': 0.783, 'lambda': 8.405518603860497, 'max_bin': 465}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:18:28,522] Trial 461 finished with value: 0.7314945273507387 and parameters: {'n_estimators': 280, 'eta': 0.07633053489582818, 'max_depth': 10, 'alpha': 0.7864, 'lambda': 6.346909736649806, 'max_bin': 466}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:18:34,349] Trial 462 finished with value: 0.7366105723880952 and parameters: {'n_estimators': 347, 'eta': 0.0781705699395704, 'max_depth': 10, 'alpha': 0.7868, 'lambda': 8.454477148946378, 'max_bin': 462}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:18:40,425] Trial 463 finished with value: 0.7311427217533855 and parameters: {'n_estimators': 343, 'eta': 0.07958882188142824, 'max_depth': 10, 'alpha': 0.793, 'lambda': 7.988660686552861, 'max_bin': 463}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:18:46,343] Trial 464 finished with value: 0.7276930906858092 and parameters: {'n_estimators': 302, 'eta': 0.07748723288996577, 'max_depth': 10, 'alpha': 0.7852, 'lambda': 7.300415802484336, 'max_bin': 471}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:18:52,063] Trial 465 finished with value: 0.732765874170059 and parameters: {'n_estimators': 322, 'eta': 0.07839948944205968, 'max_depth': 10, 'alpha': 0.3251, 'lambda': 8.58372138452238, 'max_bin': 468}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:18:59,114] Trial 466 finished with value: 0.7324132063799972 and parameters: {'n_estimators': 357, 'eta': 0.08026201330191389, 'max_depth': 10, 'alpha': 0.7961, 'lambda': 17.452158824023208, 'max_bin': 460}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:19:06,650] Trial 467 finished with value: 0.7276958644790711 and parameters: {'n_estimators': 339, 'eta': 0.07696236501510628, 'max_depth': 10, 'alpha': 0.12890000000000001, 'lambda': 38.36718558929397, 'max_bin': 465}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:19:12,103] Trial 468 finished with value: 0.7288315105765346 and parameters: {'n_estimators': 354, 'eta': 0.09404174729969478, 'max_depth': 11, 'alpha': 0.7693, 'lambda': 6.526035383486219, 'max_bin': 474}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:19:18,962] Trial 469 finished with value: 0.7296554796582162 and parameters: {'n_estimators': 318, 'eta': 0.08215471251696908, 'max_depth': 11, 'alpha': 0.3749, 'lambda': 19.460877706090372, 'max_bin': 461}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:19:25,612] Trial 470 finished with value: 0.7330950329541632 and parameters: {'n_estimators': 292, 'eta': 0.07445728042573797, 'max_depth': 11, 'alpha': 0.7512000000000001, 'lambda': 22.42636841619152, 'max_bin': 456}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:19:32,736] Trial 471 finished with value: 0.7324681777864142 and parameters: {'n_estimators': 330, 'eta': 0.07811672416696097, 'max_depth': 11, 'alpha': 0.7758, 'lambda': 23.26702131078075, 'max_bin': 477}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:19:39,450] Trial 472 finished with value: 0.7314865054166011 and parameters: {'n_estimators': 370, 'eta': 0.0862885650927847, 'max_depth': 11, 'alpha': 0.7952, 'lambda': 8.379360229409754, 'max_bin': 469}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:19:46,043] Trial 473 finished with value: 0.7303403773087325 and parameters: {'n_estimators': 393, 'eta': 0.0738957613948189, 'max_depth': 11, 'alpha': 0.7020000000000001, 'lambda': 9.084709201562355, 'max_bin': 472}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:19:52,721] Trial 474 finished with value: 0.7326785491908018 and parameters: {'n_estimators': 351, 'eta': 0.07530856764347749, 'max_depth': 11, 'alpha': 0.729, 'lambda': 9.635896679135211, 'max_bin': 465}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:19:58,356] Trial 475 finished with value: 0.7280119940552956 and parameters: {'n_estimators': 258, 'eta': 0.07232036871115899, 'max_depth': 11, 'alpha': 0.5089, 'lambda': 7.4056978299812855, 'max_bin': 480}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:20:05,248] Trial 476 finished with value: 0.7292147441729546 and parameters: {'n_estimators': 377, 'eta': 0.0842308569077592, 'max_depth': 11, 'alpha': 0.1721, 'lambda': 20.294393761760478, 'max_bin': 457}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:20:13,046] Trial 477 finished with value: 0.7316989994284532 and parameters: {'n_estimators': 336, 'eta': 0.06626135414020139, 'max_depth': 11, 'alpha': 0.677, 'lambda': 26.945032393972298, 'max_bin': 492}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:20:20,236] Trial 478 finished with value: 0.723913927665937 and parameters: {'n_estimators': 301, 'eta': 0.038298918817518644, 'max_depth': 11, 'alpha': 0.7535000000000001, 'lambda': 21.632044370395516, 'max_bin': 500}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:20:27,623] Trial 479 finished with value: 0.7347106845454969 and parameters: {'n_estimators': 391, 'eta': 0.07914593376746146, 'max_depth': 11, 'alpha': 0.7767000000000001, 'lambda': 18.753582719363752, 'max_bin': 467}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:20:34,543] Trial 480 finished with value: 0.7299908165408395 and parameters: {'n_estimators': 361, 'eta': 0.06313031837250463, 'max_depth': 11, 'alpha': 0.14070000000000002, 'lambda': 8.382130856558003, 'max_bin': 475}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:20:40,980] Trial 481 finished with value: 0.7255641123957195 and parameters: {'n_estimators': 431, 'eta': 0.08105913405239333, 'max_depth': 11, 'alpha': 0.7979, 'lambda': 14.490847365610001, 'max_bin': 461}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:20:48,612] Trial 482 finished with value: 0.7344861344646438 and parameters: {'n_estimators': 413, 'eta': 0.06793394317764706, 'max_depth': 11, 'alpha': 0.3423, 'lambda': 9.82170544535007, 'max_bin': 485}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:20:55,313] Trial 483 finished with value: 0.7329924228056708 and parameters: {'n_estimators': 314, 'eta': 0.07106368096610743, 'max_depth': 10, 'alpha': 0.7451, 'lambda': 21.972176100299432, 'max_bin': 471}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:21:01,639] Trial 484 finished with value: 0.7307530944432243 and parameters: {'n_estimators': 271, 'eta': 0.04885600309302591, 'max_depth': 11, 'alpha': 0.7679, 'lambda': 9.41427105711999, 'max_bin': 481}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:21:10,435] Trial 485 finished with value: 0.729967242427268 and parameters: {'n_estimators': 375, 'eta': 0.05050367568087433, 'max_depth': 11, 'alpha': 0.7140000000000001, 'lambda': 20.866235459397874, 'max_bin': 462}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:21:18,799] Trial 486 finished with value: 0.7327165685251115 and parameters: {'n_estimators': 402, 'eta': 0.06923361940019233, 'max_depth': 11, 'alpha': 0.735, 'lambda': 23.81260806100211, 'max_bin': 450}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:21:24,692] Trial 487 finished with value: 0.7308868129451047 and parameters: {'n_estimators': 349, 'eta': 0.08831935122786354, 'max_depth': 11, 'alpha': 0.7822, 'lambda': 8.782055591461983, 'max_bin': 488}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:21:32,015] Trial 488 finished with value: 0.7316630678823528 and parameters: {'n_estimators': 330, 'eta': 0.06383433414001868, 'max_depth': 11, 'alpha': 0.1622, 'lambda': 15.603410157729204, 'max_bin': 457}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:21:39,149] Trial 489 finished with value: 0.7292542696506604 and parameters: {'n_estimators': 380, 'eta': 0.07544274674640877, 'max_depth': 11, 'alpha': 0.7543000000000001, 'lambda': 19.552629063259708, 'max_bin': 477}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:21:45,867] Trial 490 finished with value: 0.7317740501926226 and parameters: {'n_estimators': 364, 'eta': 0.0667353542582299, 'max_depth': 10, 'alpha': 0.8059000000000001, 'lambda': 6.916203067172592, 'max_bin': 466}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:21:51,005] Trial 491 finished with value: 0.7265772941699419 and parameters: {'n_estimators': 310, 'eta': 0.07263381395717912, 'max_depth': 12, 'alpha': 0.7652, 'lambda': 1.9090474401899975, 'max_bin': 472}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:21:57,033] Trial 492 finished with value: 0.7294977768860834 and parameters: {'n_estimators': 285, 'eta': 0.06987712154718574, 'max_depth': 11, 'alpha': 0.736, 'lambda': 5.08690427864277, 'max_bin': 485}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:22:04,192] Trial 493 finished with value: 0.7369453120602538 and parameters: {'n_estimators': 344, 'eta': 0.06575622714565098, 'max_depth': 11, 'alpha': 0.1131, 'lambda': 11.995063779229648, 'max_bin': 468}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:22:12,472] Trial 494 finished with value: 0.7336456698384581 and parameters: {'n_estimators': 340, 'eta': 0.06450167882938374, 'max_depth': 11, 'alpha': 0.12000000000000001, 'lambda': 22.686577678288657, 'max_bin': 494}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:22:19,909] Trial 495 finished with value: 0.732920192178823 and parameters: {'n_estimators': 395, 'eta': 0.06637859679596109, 'max_depth': 11, 'alpha': 0.1298, 'lambda': 11.768704458723025, 'max_bin': 478}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:22:26,428] Trial 496 finished with value: 0.7294774048620871 and parameters: {'n_estimators': 355, 'eta': 0.07847392566167084, 'max_depth': 11, 'alpha': 0.0983, 'lambda': 13.1643150919555, 'max_bin': 471}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:22:33,378] Trial 497 finished with value: 0.7313576596547262 and parameters: {'n_estimators': 327, 'eta': 0.06292589911998532, 'max_depth': 11, 'alpha': 0.06670000000000001, 'lambda': 12.279379907762463, 'max_bin': 456}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:22:39,921] Trial 498 finished with value: 0.7285908048722911 and parameters: {'n_estimators': 367, 'eta': 0.08275076024837437, 'max_depth': 11, 'alpha': 0.020900000000000002, 'lambda': 9.993719498778342, 'max_bin': 481}. Best is trial 357 with value: 0.7388703292180938.\n",
      "[I 2023-12-12 04:22:47,709] Trial 499 finished with value: 0.7287109489933066 and parameters: {'n_estimators': 340, 'eta': 0.06511622231624613, 'max_depth': 11, 'alpha': 0.0879, 'lambda': 20.194765287120468, 'max_bin': 449}. Best is trial 357 with value: 0.7388703292180938.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7389\n",
      "\tBest params:\n",
      "\t\tn_estimators: 383\n",
      "\t\teta: 0.06679005365939406\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.7597\n",
      "\t\tlambda: 9.968258042127214\n",
      "\t\tmax_bin: 481\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.693937    0.727582    0.711964    0.721309   \n",
      "1                    TP   31.000000   38.000000   36.000000   35.000000   \n",
      "2                    TN  310.000000  307.000000  309.000000  309.000000   \n",
      "3                    FP    6.000000    8.000000    6.000000    5.000000   \n",
      "4                    FN   35.000000   29.000000   31.000000   33.000000   \n",
      "5              Accuracy    0.892670    0.903141    0.903141    0.900524   \n",
      "6             Precision    0.837838    0.826087    0.857143    0.875000   \n",
      "7           Sensitivity    0.469697    0.567164    0.537313    0.514706   \n",
      "8           Specificity    0.981000    0.974600    0.981000    0.984100   \n",
      "9              F1 score    0.601942    0.672566    0.660550    0.648148   \n",
      "10  F1 score (weighted)    0.879915    0.895703    0.893882    0.889751   \n",
      "11     F1 score (macro)    0.769957    0.807865    0.802031    0.795111   \n",
      "12    Balanced Accuracy    0.725355    0.770884    0.759133    0.749391   \n",
      "13                  MCC    0.576105    0.633078    0.630059    0.623142   \n",
      "14                  NPV    0.898600    0.913700    0.908800    0.903500   \n",
      "15              ROC_AUC    0.725355    0.770884    0.759133    0.749391   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.733245    0.752080    0.742097    0.705249    0.687504    0.722362  \n",
      "1    36.000000   38.000000   39.000000   29.000000   36.000000   36.000000  \n",
      "2   306.000000  312.000000  308.000000  310.000000  306.000000  306.000000  \n",
      "3    10.000000    4.000000    6.000000    5.000000    7.000000    9.000000  \n",
      "4    30.000000   28.000000   29.000000   38.000000   33.000000   31.000000  \n",
      "5     0.895288    0.916230    0.908377    0.887435    0.895288    0.895288  \n",
      "6     0.782609    0.904762    0.866667    0.852941    0.837209    0.800000  \n",
      "7     0.545455    0.575758    0.573529    0.432836    0.521739    0.537313  \n",
      "8     0.968400    0.987300    0.980900    0.984100    0.977600    0.971400  \n",
      "9     0.642857    0.703704    0.690265    0.574257    0.642857    0.642857  \n",
      "10    0.887545    0.908455    0.900671    0.871847    0.885222    0.886770  \n",
      "11    0.790754    0.827462    0.818251    0.754700    0.790754    0.790754  \n",
      "12    0.756904    0.781550    0.777211    0.708481    0.749687    0.754371  \n",
      "13    0.596855    0.680513    0.657867    0.556879    0.607840    0.600163  \n",
      "14    0.910700    0.917600    0.913900    0.890800    0.902700    0.908000  \n",
      "15    0.756904    0.781550    0.777211    0.708481    0.749687    0.754371  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_xgb_9)\n",
    "# now convert the resuls to binary with cutoff 6.9\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_9_cat = np.where(((y_pred_xgb_9 >= 2) | (y_pred_xgb_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwgAAAHJCAYAAAAhLh4vAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAACKsElEQVR4nOzdd1xV9f8H8Ne5g71FGTIEBXJrajkwFEsbflPUXA3NTMuWNvWb5viWlfXTyrLSTCszFypqmWbukaNSVFJT3ICCbGTcyz2/P+ieuN7BvXAX+Ho+Hj2SMz/3zeXe8/5MQRRFEURERERERABkji4AERERERE5DyYIREREREQkYYJAREREREQSJghERERERCRhgkBERERERBImCEREREREJGGCQEREREREEiYIREREREQkYYJAREREREQSJghE9VyvXr0gCIJN7zF69GgIgoALFy7Y9D7mWrp0KQRBwNKlSx1dFKtoaK/Hluzxficiut0xQSCqpSNHjuDJJ59EdHQ03N3d4ePjg7Zt2+K1117D1atXrXYfZ3s4t4edO3dCEATMmDHD0UUxm/Yhf/To0UaP0b6uXr16WfXeM2bMgCAI2Llzp1Wvaw/a93f1/zw9PdG2bVv897//RX5+vk3ua4vfAxFRQ6FwdAGI6htRFDF58mTMmTMHCoUC9913Hx555BFUVFRg//79+PDDD7FgwQJ88803GDJkiM3L8+233+LmzZs2vce7776LyZMno2nTpja9j7mSkpLQtWtXhISEOLooVtHQXk9tDBgwAB06dAAAZGVlYePGjXj33XexZs0aHDp0CH5+fg4tHxHR7YQJApGFZs2ahTlz5qBZs2bYtGkTWrdurbM/OTkZjz32GIYPH46tW7ciMTHRpuWJiIiw6fUBICQkxKkeXn19feHr6+voYlhNQ3s9tTFw4ECd1pcPP/wQd999N9LS0jB//nxMmzbNcYUjIrrNsIsRkQXOnz+Pt99+G0qlEhs2bNBLDgBg8ODBmDdvHiorK/Hss89Co9FI+6r3Nd+0aRO6d+8OT09P+Pv7Y8iQIfj77791riUIAr755hsAQFRUlNQFo1mzZtIxhvpkV++ic+TIEdx///3w8/ODn58fBg8ejMuXLwMA/v77bwwdOhSNGzeGu7s7evfujdTUVL3XZKibU7NmzfS6hlT/r/rD3pkzZzB58mR07twZjRs3hqurKyIjI/H000/j0qVLevfq3bs3AGDmzJk619R2oTHVZ//IkSMYNGgQmjRpIt3n2WefRUZGhsnX9eWXX6Jt27Zwc3NDUFAQnn76aZt1b7mVsdfz559/YtiwYYiMjISrqysaNWqEdu3a4aWXXoJKpQJQ9XuYOXMmAKB379468aouIyMDEyZMQLNmzeDi4oLGjRsjKSkJhw8fNlmeH3/8Effccw98fHwgCALy8vLg4eGB5s2bQxRFg6+nf//+EAQBv//+e61j4uXlhVGjRgEADh48WOPxGo0GCxYsQJcuXeDl5QVPT0907twZCxYsMPg3CAC7du3SiVd96tJGRGRLbEEgssCSJUugVqvxyCOPoG3btkaPGzt2LGbNmoUzZ85g165d0gOv1tq1a7F582YkJSWhV69eOHr0KJKTk7Fjxw7s378fcXFxAIDp06dj/fr1OHbsGF566SWpm4W53S0OHz6M999/HwkJCRg7diyOHz+OtWvX4sSJE1i3bh3i4+PRqlUrPPHEE7h06RKSk5Nx7733Ij09HV5eXiavPXHiRIMP0Bs3bsQff/wBDw8Pndf7xRdfoHfv3ujevTtcXFxw4sQJLF68GBs2bMDvv/+OsLAwAFU1yQDwzTffICEhQaefePXEyJCUlBQ88sgjEAQBQ4YMQUREBI4cOYIvvvgCKSkp2Lt3L6Kjo/XOe/3117Flyxb85z//Qd++fbFjxw589dVX0u/PEY4ePYpu3bpBJpPh4YcfRlRUFAoLC3H27Fl8/vnneOedd6BUKjFx4kSsX78eu3btwqhRowzGKD09HfHx8cjMzESfPn0wYsQIXL58GatXr8aPP/6I1atXY8CAAXrnrV69Gj///DMefPBBPPPMMzh//jz8/f0xfPhwLFmyBNu2bcN9992nc87ly5exefNmdOrUCZ06dapTDIwlIIaMHDkSK1euREREBMaOHQtBELBu3To899xz2L17N1asWAEA6NChA6ZPn46ZM2ciMjJSJ5HlmAQion+IRGS23r17iwDEhQsX1njsiBEjRADi//73P2nbkiVLRAAiAHHjxo06x3/00UciADExMVFn+6hRo0QA4vnz5w3eJyEhQbz1T3nHjh3SfZYtW6azb8yYMSIA0dfXV3z77bd19r3zzjsiAPGjjz6yqAxaW7duFRUKhdiiRQsxOztb2n7lyhWxrKxM7/iffvpJlMlk4vjx4w2Wf/r06Qbvo43jkiVLpG1FRUViQECAKJfLxX379ukcP3v2bBGAeO+99xp8XREREeLFixel7SqVSuzZs6cIQPztt99MvuZby9S+fXtx+vTpBv/T3i8hIaHG1zNp0iQRgLhu3Tq9e+Xm5oqVlZXSz9OnTxcBiDt27DBYtvvuu08EIL733ns62/fs2SPKZDLR399fLCws1CuPIAji5s2b9a535MgREYA4ePBgvX3Tpk0z+29EFP/9HVR/7aIoiiUlJWLr1q1FAOLMmTOl7Ybe799//70IQOzcubNYXFwsbS8uLhbvvPNOg38Hhn4PRERUhS0IRBbIysoCAISHh9d4rPYYQ11bEhMT0b9/f51tzz//PObPn4/t27fj4sWLiIyMrHN5e/bsiUcffVRn26hRo/D111/D398fkydP1tn32GOP4c0338TRo0ctvteJEycwZMgQ+Pr64qeffkJgYKC0z9jg5gceeACtWrXC1q1bLb7frdavX4/c3Fw8+uij6N69u86+V199FV9++SW2bdtmMLZvvfWWzlgOhUKBJ598Env27MHhw4dx9913m12OY8eO4dixY3V7MYDUDaZ6S4yWv7+/2de5cuUKfvnlF0RGRuKVV17R2RcfH4/hw4dj+fLlWLduHZ544gmd/Q8//DDuv/9+vWt26tQJXbp0wYYNG3Dt2jUEBQUBACorK7F48WJ4e3tj5MiRZpcRqPr9abuwXbt2DRs3bsTVq1fRvHlzvPDCCybP/frrrwFUDab39PSUtnt6euK9995D3759sXjxYr2/BSIiMoxjEIgsIP7T5cGcedi1xxg6NiEhQW+bXC5HfHw8gKq+59ZgqItHaGgogKquFnK53OC+K1euWHSfzMxMPPTQQygvL8e6desQExOjs18URSxbtgz33nsvGjduDIVCIfX7PnHihFWmhdXG7NbuXACgVCqlmBuKbefOnfW2aRO8vLw8i8oxatQoiKJo8L8dO3aYfZ3hw4dDLpdj4MCBGDVqFL799lucO3fOorIA/77enj17QqHQrxO69957AQB//PGH3j5TidGECROgUqmkh3OgqntZRkYGHnvsMZ0HdXOkpKRg5syZmDlzJr755hv4+Pjgtddew6FDh2pMiP7880/IZDKDf1e9e/eGXC43+PqIiMgwJghEFtDO5KMd5GuK9iHb0Ow/2hrXWwUHBwMACgoKaltEHYZmxtE+JJrapx0Aa46SkhL0798fly9fxpIlS9CzZ0+9Y15++WU8/vjjSEtLQ79+/fDKK69g+vTpmD59OiIjI1FRUWH2/YzRxkwbw1tpfw+GYmsqFpWVlXUuW2106dIFe/bsQWJiIlavXo1Ro0ahRYsWaNmyJVauXGn2deoSF2PnAMCwYcMQEBCAr776Skqcv/zySwDAM888Y3b5tJYsWSIlUjdv3kRaWhrmzJmDgICAGs8tKChAQEAAlEql3j6FQoHAwEAUFhZaXCYiotsVuxgRWSA+Ph47duzAtm3bMHbsWKPHVVZWSrXFPXr00Nt/7do1g+dpuzDVlykvNRoNRowYgT/++APvvPMORowYoXfM9evX8cknn6BNmzbYv38/vL29dfb/8MMPVimLNmbaGN4qMzNT57j6oFu3bti0aRPKy8vx+++/4+eff8b8+fMxYsQING7c2KwpdOsSF1MtZe7u7hg9ejTmzp2LX375BbGxsdi6dSu6du2Kdu3amfPyrMbX1xe5ublQqVR6SYJarUZOTg58fHzsWiYiovqMLQhEFhg9ejTkcjnWrl2LtLQ0o8d9/fXXyMjIQFxcnMFuD4ZmxqmsrMTevXsBAB07dpS2a7sBOaom25SJEydi48aNGDNmDP773/8aPCY9PR0ajQZ9+/bVSw6uXLmC9PR0vXNq85q1MTO0mrBarZZie+edd5p9TWfh6uqK7t27Y9asWfjkk08giiLWr18v7TcVL21c9u7dC7Varbdfm8jWJi7PPvssBEHAl19+iUWLFkGj0WD8+PEWX6euOnbsCI1Gg927d+vt2717NyorK/Ven0wmc8q/KSIiZ8AEgcgC0dHR+O9//wuVSoX//Oc/BpOE9evX46WXXoJcLseCBQsgk+n/mW3fvh2bNm3S2fbpp5/i3Llz6N27t84g2kaNGgEwr1uTPX300UeYP38++vTpgy+++MLocdppN/fu3avzQFZcXIynn37a4ENrbV7zwIEDERAQgB9++AG//fabXlnT09Nx77332mVhOWvYs2ePwW4/2tYnNzc3aZupeIWFheG+++7DhQsX8NFHH+nsO3jwIJYvXw5/f38kJSVZXMYWLVrgvvvuw4YNG7Bw4UL4+flh2LBhFl+nrsaMGQMAmDJlis6q4jdv3pQG4j/11FM65zRq1Mjp/qaIiJwFuxgRWWjGjBkoKSnB3Llz0b59e/Tr1w+tW7eGSqXC/v37cfDgQbi7u+OHH34w2gXk4YcfRlJSEpKSktCiRQscO3YMP/30EwICArBgwQKdY/v06YMPPvgATz/9NAYPHgwvLy/4+fnh+eeft8fLNSgrKwuvvPIKBEFA27Zt8c477+gd06FDBwwcOBDBwcEYPnw4VqxYgQ4dOqBv374oKCjAL7/8Ajc3N3To0EFv1qS4uDg0bdoUK1asgFKpREREBARBwOOPP250dicvLy98/fXXeOSRR5CQkIBHHnkEERER+P3337F161YEBwdLfeTrg//7v//D1q1b0atXL0RHR8PLywsnT57E5s2b4efnh3HjxknH9u7dGzKZDFOmTMHx48elQb1Tp04FAHzxxRfo0aMHXnvtNWzduhWdO3eW1kGQyWRYsmSJXuuOuZ599lls3boVOTk5ePHFF+Hu7l73F2+hkSNHIiUlBatWrULr1q0xcOBACIKA9evX4/z58xg6dKjeDEZ9+vTBihUrMGDAAHTs2BEKhQL33HMP7rnnHruXn4jI6ThmdlWi+u/gwYPiE088ITZr1kx0c3MTPT09xdatW4uvvPKKePnyZYPnVJ/vftOmTWLXrl1FDw8P0dfXVxw0aJB4+vRpg+f93//9n3jHHXeILi4uIgAxMjJS2mdqHQRD6wicP39eBCCOGjXK4L1gYH74W9dB0F7D1H/Vr19SUiL+97//FZs3by66urqKYWFh4oQJE8ScnByD5RdFUTx06JCYmJgo+vj4iIIg6Mzzb2jdgOrnDRw4UAwMDBSVSqUYHh4uPvPMM+LVq1f1jjW1vkNNazHcSlsmY3Gtfk1z1kHYsmWLOHr0aLFly5aij4+P6OHhIcbGxoovvPCCeOHCBb1rf/fdd2L79u1FNzc36XdQ3ZUrV8RnnnlGjIiIEJVKpdioUSNxwIAB4qFDh4y+FkPxvZVarRYDAwNFAOLJkydrPP5WxtZBMMbY+6WyslL87LPPxE6dOonu7u6iu7u7eOedd4qffvqpzpoRWteuXRNHjBghNmnSRJTJZBb9romIGjpBFC1YqpKI6mTp0qV48sknsWTJEp0VXInqq3PnziEmJgbx8fEGxwAQEVH9wzEIRERUax988AFEUXRolzciIrIujkEgIiKLXLx4Ed999x3+/vtvfPfdd+jYsSOGDBni6GIREZGVMEEgIiKLnD9/HtOmTYOnpyf69euHzz//3OBsXUREVD9xDAIREREREUlY5UNERERERBImCEREREREJGGCQEREREREEiYIREREREQk4SxGVpCXlwe1Wm316zZu3BjZ2dlWvy7pYpztg3G2H8baPhhn+7F2rBUKBfz9/a12PaKGhgmCFajVaqhUKqteUxAE6dqcaMp2GGf7YJzth7G2D8bZfhhrIvtjFyMiIiIiIpIwQSAiIiIiIgkTBCIiIiIikjBBICIiIiIiCQcpExERETlAaWkprl27BlEUOQCbbM7DwwPBwcFmHcsEgYiIiMjOSktLcfXqVXh7e0MmY4cOsr2SkhLk5+fDz8+vxmP5jiQiIiKys2vXrjE5ILvy8PBAXl6eWcfyXUlERERkZ6IoMjkguxIEweyubHxnEhEREdkZxxyQM2OCQERETosPUURE9scEgYiInEpJRSXm7bqMQUtOYsDXJzBoyUnM23UZJRWVJs9jMkHkPDp16oQvv/yyzsfU1YoVK9CiRQub3sManK2cTBCIiMhplFRUYtyqM0g+loOsogrklKiRVVSB5NQcjFt1Ri9JqG0yQYYxyaKaXL16FRMnTkTbtm3RtGlT3HnnnXjzzTeRm5tr8bW2bNmCxx9/3GplM5RwDBgwAAcOHLDaPW61ceNGBAcH48qVKwb3d+/eHf/9739tdn9b4TSnRES3OVEUIQiCU9xr4YEMXMwtg6bqYHioyyH889Cac60US7b/jQk9mkIQBJRUVGLiurO4nFdedfw/fs4twMmz1/FRUgt4usht8zoEAZWFhdAUFwP1/KG6pKIS3xzOxP4LRajUaCCXydC9mTdGdQmxWfwMMfreUPBRpSb2+hu+cOECHnzwQTRv3hxffvklIiIicPr0acycORO//vorNm/eDH9/f7OvFxgYaMPSVnF3d4e7u7vNrn///fcjICAAK1euxCuvvKKz7+DBgzh79iwWLlxos/vbCv/qiIhuE9UfIkoqKrHwQAb2pBdCrdFAIZMhPsob47s3tfpDoaF79Yz2wbhuoTr3EkURe9ILpYf9Ltf+QmzeZd2LnQW+SAG0z0IdRaCjgXsKAnDkyj50jfS1ymsQIUJAtQcwAcj18kZ5cRFQj/ODikoNNp28AaFUje7VtgvHgJQtCvyndSO4yG3X2aCiUoM/rhThYl45NKIImSAg0t8Vd4Z5S/eVR0cD0dE2K0N9VVJRic/3XsHuc3lQa0QoZALuae6PZ+PDbJbYTZ48GS4uLli1apX00B0WFoY2bdrg7rvvxuzZs/HBBx9IxxcXF+OZZ57Bzz//DG9vb7z00ksYO3astL9Tp04YN24cxo8fDwAoLCzEzJkzsXnzZpSVlaFDhw6YNWsW2rRpI53z888/4//+7/9w6tQpeHp6omvXrli6dCkGDhyIy5cvY9q0aZg2bRoA4Pr161ixYgWmTp2Ks2fP4uzZs+jevTv27duHmJgY6Zqff/45vvrqKxw5cgSCIOD06dOYMWMGDhw4AA8PD/Tq1Qv/+9//0KhRI72YKJVKDBkyBCtWrMDLL7+sk6j98MMPaN++Pdq0aYPPP/8cK1aswMWLF+Hn54e+ffvirbfegpeXl8FYv/DCCygoKMC3334rbZs6dSpOnDiB9evXA6j6zPz000/xzTff4Pr164iOjsYrr7yC//znP2b/To1hFyMiogbMUBec93+9iLErT+t141mTegP3L0zFnO2XrNZFx1iXoTXHqroMZRdXSOXr/1UqsooqAAAyUYOowkwAQKVMrvOfWiaHSqj679Z90jGCHOfz1YBcbuQ/WbX/y27ZVvWfCgL2Xy7GD8duYPnRG/jh2A3sv1wMFQQIcjkEhRyC0evXj/+OZNzEjXIRagPxu1Eu4vuj/75ma99bBQEpf+Xj+PVyFKiAIrWAAhVw/Ho5Uv7K//eeMvu0btUnJRWVGLP8JFb/eQ2ZhRXILlYhs7ACq49ew5jlJ23SxS4vLw87duzAk08+qVcjHxQUhMGDByMlJUWnm9pnn32GVq1a4ddff8VLL72EadOmYefOnQavL4oiRo4cievXr2P58uXYtm0b2rZtiyFDhkhz9//yyy948sknce+99+LXX3/FmjVr0KFDBwDAkiVLEBoaijfeeAPHjx/H8ePH9e7RokULtG/fHsnJyTrb165di0GDBkEQBFy7dg0DBw5EmzZt8Msvv2DlypXIzs7G008/bTQ2jz76KC5evIj9+/dL20pKSpCSkoKRI0cCAGQyGd555x3s2rUL8+fPx969ezFr1izjATfDu+++ixUrVmDOnDnYvXs3nnnmGUyYMEGnHLXFFgQiogZK+3Auddn5R8pJ432FKzXA+hM3cCyjBAuHxta6JlLbarApLRelKo3efhHA+dwyDFmaBrVG1KuEDyzNh7JSjXKFC9a06PVvk4EFGnsqMerRVtJc89oy7TpXgIJSFcqrPUMJAFwVAnzdFLinuS8e6xSEievP4aJvGTTVGiFkAhCpcMOiYXEIjAyDKjOzXvfb/3TJSWT5Vpg8Rvua6/J+MOTzXZeRXJgD/XdH1T1zogIxKSHcbt3f6pPP917BhRtlerHTiMCF3DJ8vvcKXk2MtOo909PTIYqiTs17dTExMcjPz0dOTg4aN24MALjrrrvw4osvAgCaN2+OQ4cO4csvv0SvXr30zt+7dy/++usvpKWlwdXVFQCk1oSNGzfiiSeewLx58zBw4EC88cYb0nna1gV/f3/I5XJ4eXkhKCjI6OsYPHgwFi9ejMmTJwMAzp07h2PHjuHTTz8FUJVotG3bFm+++aZ0zscff4wOHTrg3LlzaN68ud414+Li0KlTJ/zwww/o0aMHAGDDhg3QaDQYNGgQAEitJAAQGRmJyZMn4/XXX8ecOXOMltWUkpISfPHFF0hOTkaXLl0AAM2aNcPBgwfx7bffonv37jVcwTQmCEREDYz2gfXL/Rl6yUF1XhU30TL3Alwq1Xr7hAxgS9Ff+E9ry/sIl6k1+GJ/BrKLVLjzn20KsRIBZYVQaMyr2ZSLVaXO9GhUq+QAAHJKVBiw5CTkALpH+eDPqyW4lFdusEeQCKBMLaKsWIU1x3Lw86lcFJdr9I7VPoAt3J+BOZFhtSqXsxBFEWqNsXfHvzQicDGvDAsPZGBSQrhF1zf1cF+9O5mhe+5NL8SkBLNvd1vZfS7PZOz2nMuzeoJQE+3nTvXfeefOnXWO6dy5s9H++MeOHUNJSQni4uJ0tpeVleHChQsAgJMnT9Z5UHNSUhJmzpyJI0eOoHPnzlizZg3atGkj3Tc1NRX79u1Ds2bN9M69cOGCwQQBAEaOHIlp06bhvffeg5eXF5YvX44HH3wQvr5VNQx79+7FRx99hDNnzqCoqAiVlZUoKytDSUkJPD09LX4dZ86cQVlZGR555BGd7SqVCm3btrX4erdigkBE1ABoa8d3pxegsOwoytUaaKo93YYVXUfT4mydc0JKbsBTVWr0msWnr6PSvcjisvx+sQCeWTfhYfGZ+s77htT6XBHAjZKq5Gf9CfNnWBEBFJUbf3AWAaxJzYaH50k81t4XHsr62VtXEAQozFzJ19wHdnPHtpiTnKg1ol0H0NcXVbEz3WqlskHsoqKiIAgCzpw5gwcffFBv/9mzZ+Hn52ewn745NBoNgoKCsG7dOr192odsNze3Wl27uqCgIPTo0QNr165F586dsW7dOjzxxBM65ejbt680juHWc41JSkrCtGnTsH79enTv3h0HDx6UWjouX76MkSNHYtSoUZg8eTL8/f1x8OBBTJw4EWq1fgUNAIOrbKtUKp1yAsDy5csRHBysc5y2BaYumCAQEdVz2q5EF3LLDNaOu6orEJ+RCrmB2vtiFw+c9o8weF0/dwVGdImy+CFj0/V05AbpfumJAPJdvVCqMP8LvkKuQJmi7l90tqARgW8PXMCuU9bvemNr1R/i80tVNZ/wj5oe2I11aVuTegPrTtzAf1o1wnPxVYmCOcmJXCYwOTCgKnam46KwQewCAgKQkJCAJUuWYPz48TrjEK5du4bk5GQ88sgjOvf9/fffda7x+++/G+2i1K5dO1y/fh0KhQIREYY/k1q1aoXdu3djxIgRBvcrlUpUVtbcSjlkyBDMmjULSUlJuHDhApKSknTKsWnTJkREREBhwQxaXl5eePjhh/HDDz/g4sWLiIyMlLobHT16FGq1GjNnzpQe/FNSUkxer1GjRjh16pTOthMnTkCpVAKo6tbk6uqKK1eu1Lk7kSFMEIiI6jnt1KAiAL+yIniqSiGDCEEUIRNFBN3MhVxTiSIXT6RXq5GvFOS44BOMUqXhh/ZgbxcoW7c2uxzaLganD1TihpvhWrGGpLZdbxzJ2EO8Oao/sBtKFHSmqL2FobEtPaN9kJyaA2OV4V0jDc/uQsA9zf2x+ug1g7GTCVX7beG9997DQw89hGHDhmHKlCk605wGBwfrzfd/6NAhzJ8/Hw8++CB27tyJDRs24Pvvvzd47YSEBHTu3BmjRo3CtGnT0KJFC2RlZeHXX3/FAw88gA4dOuDVV1/F4MGD0axZMyQlJUGtVuPXX3/FCy+8AAAIDw/Hb7/9hqSkJLi4uBhtzXjooYfw+uuv4/XXX0ePHj0QEvLv5+KYMWOwbNkyjB8/Hs899xwCAgJw/vx5rF+/HnPnzoVcbrwyYOTIkXj44Ydx5swZTJgwQfobadasGdRqNb766iv07dsXhw4dwjfffGMy1vHx8fjss8+wcuVKdOnSBatXr8apU6ek7kNeXl6YMGEC3nrrLWg0Gtx9990oLi7GoUOH4OnpieHDh5u8fk2YIBAR1XPavty+5cV46Lzx2SvSGjXDWT/z+83HR3lL/9Y+EN76YHhrlxKZICC3pOEnB1r1ra+8qYd4UwQAd0d4Yd6uy9LvWi4IuKe5L8Z1C4WHUmZyTIHW+dwyfLrnMt7o0wzjuoXi0KWqKU4N2XYmH0/eFYIm3s7ZiuRIz8aH4fClAlzILdNJEmQC0CzAHc/G22Z8THR0NLZu3YoPPvgATz/9NPLy8tCkSRM88MADePXVV/XWQHj22WeRmpqK//u//4OnpydmzpyJxMREg9cWBAE//PADZs+ejYkTJ+LGjRto0qQJunbtKg167tGjB7766ivMnTsX8+fPh7e3N7p27Spd44033sCrr76Ku+66C+Xl5bh+/brBe3l7e6Nv377YsGEDPv74Y519wcHB2LRpE2bNmoVhw4ahoqICYWFhSExMNNjtp7quXbuiRYsWSE9Px7Bhw6Ttbdu2xaxZszB//ny888476Nq1K9588008//zzRq+VmJiIl19+GbNmzUJ5eTlGjBiBoUOH4q+//pKOmTx5MgIDA/HJJ5/g4sWL8PX1Rdu2bTFx4kST5TSHINbn6RecRHZ2tk6/MGsQBAEhISHIrOczZDg7xtk+GGfbEUURA74+gZwSNSILMxF/NRUVciUKXL2ggQAIgAYylCjdcDjoDmhk5neFCXCXo1QtAqIIDQBVpQgXuQAfVzkSWvjh0TubYFJKeq0eOBuSxp5KrB/T2qm7w2gTOVM19rUlAPBzl6OgrNLsawd5KXFPc1/crKjEj3/lGT3Ox1WOtWPaoEVkmFU/P5RKpfTQ6Sjp6enw9vau+UAjtOsg7DmXB5VGhFImoKeN10GwtjZt2mDy5Ml47LHHHF2U20ZRURGizVhXhC0IRET1WPW+3O7qqprYTM9G2Nu0fZ2vnVuq35e3TC2iTK3G6mM5WH0sp873aAgEAU6dHGQXV+Dx70+hsNz6c+MDVeNL8gy8V0y5Vqwy6/1TWF7ZIGaMsgVPFzleTYzEq4mR9W4w982bN3Ho0CFkZ2frzVpEzqF+Tr1ARESSntE+kAmAu7pqPvubFgwEprorU2lssjCVNZRUVOKx7/+yKDlwtsfMTWk3UFx++3Rbq436lBwAwHfffYfx48dj3Lhx0hz+5FyYIBAR1XPjuoUi3M9VakEoddKZfxqq4n+67zijhQcyTE7ZaoizdQK8qdJg0Gf7nDYJI8uNHz8ep0+fxv/+9z9HF4WMYIJA1MCY20eXYwEaDk8XOTo29YKHugwAEwR70w5Udka7zxU4ughWcTa7GAv3O2cSRtQQcQwCUQNgaHGintE+GNctVGewmrHjnu4aAi9XfhzUZ79dLEKXf1oQbjJBsDtnXNRLFEVUNpCKAI0I7DlfgIkJHItAZA98IqBacbYvwtuR9ndgbF7z5NQcHL5UhEXD4uChlOGmSmPwuNXHcrA2NQeBnkppykJTM2BY8rvn+8Q+tKvSSl2MlPU7QRAAPNw6AKmZN3ExT38axwg/V4gALuWV63WH8VICvWIC8PvlYlRUalBQpobaDlMsOeOiXpaslFwfqCudLwkjaqiYIJBZRFHETZXGrFrq24GjvqQMtQB4KAWcz616MJRrKhFakgP5P0uwIx8Y9/4FKOWAWgOIImB4fcqqY49cBTKOncb0fs3grvz3waJUpcGaY9fxx5US6b53hnliSPsmOscZP9YLz/S5A5WF+ezaZCNR+ZlQVlYN5CyTm04Q3BVC1fSlDiIAGNAmAMcyDCcAzfzd8HzPqprihQcysDe9EGqNCIVMQPw/nzm37pMLQM9bEtzqn1vVr9EjyhuPdw7GS+vO4oKROfgtIROqBoo7o57RPlhzLMfpxhXURnGFGjdVmtvu+4bIEbgOghXU53UQTD3oVn8YraisRGFZpV5NnEwAIv3dpJUx6xtL4mxuNx5bMWcF1C5ZaYjNu1zne/m7K9C/dSO4yGWoqNRg08kbyC9V6zxkCALg5/bvcSJEqCpFg8cCgEwQ4OEiQ6S/K+4M84aLvOHUbDqD3y4WIO3aTVTIFFgV28focTKhqnbe0MO5PWgTgC+HxgIwngDc+jdl6rPK3ITd0CJvAxafwE1V7ZsYqr8eZ/wM1H5unM8tc3RRrCLS3xVfDYurc6wbwjoIRLVh7joITBCswJEJQm1qss150DX5MCqK8FCXQfZPuWQC8J/WAXi2e1OLyuEMBEFAcFAQsq5dMxnnm6pKTFp/DpfzynXiIROAcD9XzBvYHB5KM76wRLHqyboWPt9/FRtP5FatmFtRAt/yYt3XIopol3MOMlGD6x7+0Ah1ewBv5KnAY3cGYfkf15BtYmXcIC8FStUiSlUaqCpr/jgRBCDQQ4kn7w6Gq4JJgrWUqzVYcjALvysa4W9fw/20jT2cV1RqcLOiEuVq0aYLnnkoZXioVYDFCYAtzdt12aLFw2T/FNFVLkOAlyt6RHrh6W4hDk0OaopdSUUlPtt7BZvScu3S3crWHmkfiEkJ4XW6BhMEul0xQbAjeycIdanJNvbgf2tLwLxdl5F8LMfgw0LbnHNol31WZ5u3qxyPdGgMwelm0K6BAHh7eaOouMjk3H6/XSxAWtZNg4cIAtAqyANdI30NnltRqcEfV4pwMa8cGlGETBBqVYu+8uh1FJsxl3m2hz+2Rt5l9nUdQSYAg9vV/UuedJVUVGLRgUzsv1SM0nIVSv+pGfdwkUEpk5lVOy+KIgYtOYlrxdb9TJMB+Hl8W6cbDC99JtbQmqJNrr54JEZ6DaGhoWa38lo7AarN94AlC6YJcL7pTrVCvF2Q/GTrOl2DCQIBwAsvvICCggJ8++23ji6K3XAl5QbK1IDUI5eLa+zqs/BAhnRuQGkB/MuLpH2yfCB5bQ4evTMIlw6nI+qmCgIA3/JiafAjAIQXXQcAqGUKiP983+WpgWXHciETBDQLcMNdEd5Q1oPaYQECBBcXCEoXiCa+Ds8VVKJCbvzP5VxBJbq6KPW2q9QarD+Zh7xSNUQI0C5BlJ9dgUvFBUhqG2henEQRFYIcKnnV+RUyJXLcfVEp6P6uNYKAvwKa1Xw9B9NOCzkpwdElaVg8XeSY1Cscc0JCkJHx75SQNT2c3rqvpplvAtzl6Bnti1/O5JvdPSfQS+l0yQFQFbOFQ2N1ujrJhKpKj6KKSmg0MNj1yZyHfVt1S6zt98Cy36/VWMng4yrDPc398FNartMmCGqNhoOVHeSFF17AypUrpZ/9/f3RoUMHvPXWW2jdum5Jm9acOXOwefNm7Nixw+gxU6ZMwfbt23Hw4EG9fZmZmejYsSO++uor9O/f3ypluh0536c1mVT9AV8QNZCL/349XL1RjK/2XsKLPY1PA7f/71zINGr4lZeg38VDkIm3fLnfkKOirDHaXb6OmxXGv/ivejXGzrCOBrvLyAQgUuaGhUOcs09udYIgIDAkBCoTtYCiKGJ96QncMNHNxk0hYNjgNvB0kUszCy08kIFNJ2+gNNzwdWUCcCPS/Fr0zRUnkVVUYdax9YEzTgvZkAiCIL2nLYmxOTPfuCjkeKNPJF5PjMCAr08gx8TfBlD1Xk9obriFzRl4usgxKSEckxL0k6navkfrWplT3a1lqP49UJ1GBC7mlWHhgQzpc6X6uXvSC012IfNQyrBmdGs8sfy0VbuaWbs1Qi6T8XPDgRITE/Hxxx8DAK5fv4733nsPjz32GP7880+7lWHkyJFYvHgxfvvtN3Tt2lVn34oVKxAQEIB+/frZrTwNkVMkCFu2bMGGDRuQn5+PsLAwjB49Gi1btjR47GeffYZdu3bpbQ8LC8PcuXMBANu2bcPu3btx+XLVYM3o6GiMGDECLVq0kI5ftWoV1qxZo3MNX19fLFq0yFovyya0H/CeFaV48MIBuFTqdgPwSpej/FITg+eKEPHgcd0H/wJXLxQr3aWffVzlkDUNQ3a6gPzSqi/9UoUrCly9pA94jSDDRZ9go33pDX1J1WclFZVSLIwpU4t4YNFx+LkrIBcElKk0NTbjG6tFN/ZA0jPax6K+0s7OGaeFpCqm3mvVZ+wxdxrNZv5u0sxDzu7W92Rt36OWPMRrVf/bN9X6YOpBXyP+uzha9XPjo7yh0ph+7C9TazDiuzTklVp3xWLt20ghA3zc5Mi9WbfrO+uMUbcLFxcXBAUFAQCCgoLwwgsv4OGHH0ZOTg4CAwMBVNXiv/XWW9i5cydkMhnuvvtuvP3224iIqJpHb9++fZg1axZOnz4NhUKBuLg4fPHFF9i3bx8+/PBDAECTJlXPMp988gmGDx+uU4a2bduiXbt2WL58ucEE4ZFHHoFMJsPEiROxd+9eXL9+HU2bNsWTTz6JcePGGX1tnTp1wrhx4zB+/HhpW+/evfHAAw/g9ddfBwAUFhZi5syZ2Lx5M8rKytChQwfMmjULbdq0qUtYnY7DE4T9+/dj6dKlGDt2LOLi4rBt2zbMnj0b8+bNk95o1T355JN49NFHpZ8rKyvx2muv6bxB0tLS0KNHD8TFxUGpVCIlJQVvv/025s6di4CAAOm48PBwTJs2TfpZ5uTzRWvnOgeAxqV5eskBUDWYtqJSY7BvuwABsuozeCjdsS2iM8qqLaokE4D9lxQoCWks9V+ujfrchUQUxaq+3L9lYk96IfJLVag0IxSVGphsZTBEW4tuzhSyj3UKws9/5aLIRMtOfcIveec1rlsojlwuNjoFafWH/ZoS1xaN3PD5I87fmmhtNT3Eaz8fDSUCXSO98OfVEr1JEbRrm9T0oJ9TotIbQ5acegM1fcVpROBGHR/ea7p+7+a+2JleaPKzUgYYjV0zf9d6k2xaQhRFQG3Z94fVKBS1ToSLi4uxZs0aREVFSc9XN2/eRFJSErp27YqUlBQoFArMnTsXw4cPlxKGUaNG4bHHHsMXX3wBlUqFP/74A4IgYMCAAfjrr7+wY8cOrF69GgDg42P4u2LkyJGYNWsWZs+eDS8vLwBVz5Tnz5/HyJEjodFoEBISgkWLFiEgIACHDx/Gq6++iqCgIAwYMKBWr1cURYwcORL+/v5Yvnw5fHx88M0332DIkCE4cOAA/P39a3VdZ+TwBGHTpk1ITExEnz5V0/KNHj0ax44dw9atWzFy5Ei94z08PODh4SH9fOjQIZSUlKB3797SthdffFHnnGeeeQYHDx7E8ePHkZDw7xOrTCaDn5+flV+R7VSvrXOrrOpqctEnGAdCdLPWkgh/vNwrQjqnuryIy1ibegMigEpBptcKoBFRY3cBc9WXLiTahODTPZex5XQ+ylQau/W9lcsEowuYVV/oTBRFTFx/rsEkB3IBeLpriKOLQUYY6pdvbApSY8mEACAq4PZMDqpX5hiTV6rC9aJyTEpJ1/vbX38i1+A5GhG4lF9e4+xfhiYTEwGzKjos4e0iwN/DBVcKys1q2dSIwP4LxVDWkKkEeinRvZk3tp7OR5laO9BegftifTGhR9OG+X5Sq3Hzu+8ccmuPxx8HlPpj6Iz55Zdf0KxZMwBVyUBQUBC+//57qZJ1/fr1kMlkmDdvnvT9/8knnyAmJgb79u1Dhw4dUFhYiL59+yIqKgoAEBsbK13f09MTcrlcaqUwZvDgwZgxYwY2btyIESNGAACWL1+Ozp07Iy4uDgDwxhtvSMdHRkbi8OHDSElJqXWCsHfvXvz1119IS0uDq2tV5aq2NWHjxo144oknanVdZ+TQBEGtViM9PR0DBw7U2d6uXTucPn3arGts374dbdu2NTkbQXl5OdRqtZRhamVlZWH8+PFQKBSIiYnBiBEjTL4hVSqVzmxFgiDA3d1d+rc1aa9363V7Rvti9bFsuKmrEoRShSsqZboflmv/KsT6UycAAG4KGfre4Y8xd4XguyNZ2H2xBGqZfT5cFXLBrFYZRyQRJRWV+HJ/BvaeL4BaPInc4nKDX6q21jXSCwsPZBrtinAhrxx9v0iFIKDBdC0Cqh4AvN3M/0Ii8xn77LCUl6sCL/eKwMu9TP+NerkqsGhYHBbuz8Ce8wVQV4pQyAX0jPLFuO4NdxHFmuJcU9erMrWIx5efQnG5ZRUSGhF1at2tK7kABHoqkNDcH+O6/7Ng3f4M7ErPR3aRqsaxC2qNiF4tfE12YevV3A+TeoXjjT7/vvdCQkKQlZXFhRadQI8ePTBnzhwAQH5+PpYsWYLhw4djy5YtCA8Px7Fjx3D+/Hnp4V+rrKwMFy5cQO/evTF8+HAMGzYMCQkJuOeeezBgwIAaE4Jb+fr64sEHH8Ty5csxYsQIFBcXY9OmTXj77belY5YuXYrvv/8eV65cQWlpKVQqVZ26Ah07dgwlJSVSAnLra2tIHJogFBYWQqPRwNdXd/Car68v8vPzazw/Ly8PR48e1WsxuNX333+PgIAAtG3bVtoWExOD5557DqGhocjPz8fatWsxdepUzJ071+i0Y+vWrdMZtxAVFYX333/fplOlBQcH6/z8VlIg1h3/RWpBKJO7GDxP+8F7U6XB+uM3sP74DZuV0RCZANzfJhQhIYZriYvL1fhwy2ls++saVJUilHIB97YMwqv94gzOdGLNJKK4XI1RC/bh7PVihz90n7hejpLySpNfqiKqlk9oKGQC8GDbpkbfG2Qdt3522NqcyKrJEepDq6E1BQcHS62Q7/70F9b/eRU3KyrNeugvKnfcg76XqwKeLnJkF5uu/ZcJQBNvVyjkMtzXMggv943VS+7nRIZhxoaT+PbAhRpHI7u6KDB9UCccy9L/DJYJQIsmXnhr0J0Gvwfs/Z62K4WiqibfQfe2hIeHh840me3bt0fz5s2xbNkyTJkyBRqNBu3bt8eCBQv0ztV2Hf/kk0/w9NNPY/v27Vi/fj3effddrF69Gp07d7aoLI8++igGDx6M9PR07N+/HwCkSueUlBS89dZbmDFjBrp06QJPT0989tln+OOPP4xer/rkDlrqal2/NBoNgoKCsG7dOr1zb32Wre8c3sUIMFwDY84XzM6dO+Hp6Ym77jI+53tKSgr27duHGTNmwMXl34fpjh07Sv+OiIhAbGwsXnjhBezatcvotFhJSUk6+7RlzM7O1nkDWYMgCAgODjZYY9LIUwH3f1oQyhSGEwRH81TKkNTSC5mZmXr7Sioq8fTK03q15t8euIBdp7Kw6J9VMnVq+f+plYyP8sX4OtZKzt15GWevFdt0QShznb1e4rB7ywQgyNsFd4d74WhGCS7kldd8khXu2SzADY+29zX43qC6M/XZQdZzU6XBd0fzseVEBirUGuSXqh3SCllb3q4yJI9uhcFLTyKz0PjsaEHeLkge3Ur6vivOy0GxgeO2nMioscJFJgDdI7xQlJuNBYOaG211KsrNRlG182zxnlYoFA5fB6E6QRAs6ubjTAShqrdAaWkpgKpeICkpKWjcuLHJdR7atm2Ltm3b4qWXXsIDDzyAtWvXonPnznBxcYGmhi56WvHx8YiMjMSKFSuwd+9eDBgwQOot8ttvv6FLly4YM2aMdHxNtfyBgYG4du2a9HNRUREuXbok/dyuXTtcv34dCoVCGnDdUDk0QfDx8YFMJtNrLSgoKKgxExNFETt27EDPnj2hMJL9btiwAevWrcO0adMQGRlp8npubm6IiIgw+dCiVCqhNPIHbKsvYlEU9a7dM8oHJcdNtyA4WlGFBi+tO2twKr8v9181ObvHl/uvYly3UCP98rNx5HKRRVME3mpPeoFTJAeO5iITIIrA3guFkAkCogNccTG/vMZ+yrWdsrCpn7u06qyHUsaHVxsz9NlB1qGdwvRCbpnTrhVQE3Vl1fsjPsr0jFXxUVUDRE29l0RRhMqMAQ6R/m54ulsIRFGEh1KGiQlhmJgQZnBqWWP34Xva8SoqKqSH6IKCAixevBglJSXStKKDBw/GZ599hieeeAJvvPEGQkJCcPXqVfz444947rnnoFKp8N1336Ffv34IDg7G2bNnkZ6ejqFDhwKomkDm4sWLOH78OEJDQ+Hl5SX197+VIAgYMWIEvvjiC+Tn52P69OnSvqioKKxatQrbt29HZGQkVq9ejaNHj5p8sI+Pj8eKFSvQr18/+Pr64r333tPpKp2QkIDOnTtj1KhRmDZtGlq0aIGsrCz8+uuveOCBB9ChQ4e6htdpOHTaHoVCgejoaKSmpupsT01N1evfdau0tDRkZWUhMTHR4P4NGzYgOTkZ//3vf9G8efMay6JSqXD16lWnHoFeUlGJebsuY3d6IVz/aUEod9IWBODfqfxutfuc8Qd0jQj8mJaLz/aaTiJuva72S+PW/9/KnMGDt4uyShFZRRXIKVHjerEKF/LKEebriodaBsDQGEiZULWIUm2+nge2DsC+yYmY1Cu8wfZJB2xXUUDOZeGBjHqdHAD/TjM8rlsoIv3dILul0d7QjFXGmDPdrbtSZrRi53bqktYQaMd+tm3bFvfffz+OHj2Kr776Cj169ABQ1QUpJSVFmlY0Pj4eL730EsrKyuDt7Q13d3f8/fffGDNmDLp164ZXX30VY8aMwahRowAA/fv3R2JiIgYNGoSWLVsa7M5T3fDhw1FYWIgWLVrg7rvvlraPGjUKDz30EMaNG4f7778fubm5ePLJJ01e66WXXkK3bt3w6KOPYuTIkXjggQekAdlA1Xv1hx9+QLdu3TBx4kR069YN48ePx6VLl5yqRcoaBNHB32j79+/H/Pnz8fTTTyM2Nhbbtm3Dr7/+irlz56Jx48ZYvnw5cnNz8fzzz+ucN3/+fGRlZeGdd97Ru2ZKSgpWrlyJF198EXfccYe03c3NDW5ubgCAb7/9Fp07d0ZgYCAKCgqQnJyMv/76Cx9++KHFv+Ts7GydwcvWoB2UlZmZCY1GozvTjShi+JlfIddUYn3ze1Di4l7zBR0kxNsFyU/+u7picbkaDyw8XmNTvFxmesaNEG8XfPvoHVh4IAO7zhWgsEyNcrUoTcrkIhfg4yrHPc19Mb677qwXg5Y0rAXHrEkmAIPbBWJct1As3J+Bved1Z7DZfa4A14ote69HB7hh4bA4tIgMQ6aJBenqK1utlltb1T87GlqsnUXS1ycs/jtwJtq/c+06DNr3cE0zVpkyb9dlky0R1e9nKVu8p5VKpcMf6NLT0012wSGyhaKiIp0xJMY4fAxC9+7dUVRUhOTkZOTl5SE8PBxTpkyR/nDz8vKQk5Ojc87Nmzdx8OBBjB492uA1t27dCrVaLS2cpjVkyBCpCSs3Nxcff/wxCgsL4ePjg5iYGLzzzjsO/8DQKqmoxIwNJ7HlRAZUlRqUVGikmSuUmkrINVVzVZcrnLvP4q3z/G9KyzWrn25NrdVZRRUYvOSk3mJk2u+OMrWIMrUaa1JvIDn1Bh5qFYCX7gmDp4scPaN9sPpYjoGr0r/zs4djUq9wTOr176BTURSx42y+2ddSyID+rRrhufgGOi0hrLtabk1ut8G/zup6UTmu1+PkAAAi/HTXEtCuJD3xHstX3tayZO0MInJ+Dm9BaAis3YIgPXTc8kELABBF3J2Vhhb5V6CWKbAyro/V7msLMgF4uHWAwUV/7C3S3xVfDavqutbvy1SHz2BkLSHeLhABk60iMgFo5KGEXADyy9QoUxt/8Y09lVg/prXBh4SaWl9kAhDk5YIeUd46LTcNtVZ73q7LegtSadW11hSoXetEQ421MyipqMSgJSccOvuQNQxsE4DXE6vG5VmzBcwaLRGGsAWByHrqTQsC6Vt4IEOqkfSsKIVC/LeWPOhmLlrkXwEAFDtx1yItjWh80R97u5hXjoUHMjDxnjD4eygsXvXYWak1IhKa+2LtcdPN+xPvCYMgCDU+5Gv7JhtiatVcmQAMattIWqTPmpy19tzc1XJrw56tE2SehQcy6n1yAAAHL1bNQ2Tt95i2JWJSgvP+zRKReZggOJmSikr8mJYLDYAW+Vdwd+ZJg8dlu/vht5DaL/Zxu9pzrgCTEsJrXMmzPpHLBIzvHorfr5hu3td+Wdf0kN8z2vCy9kDN3QjGd29qrZdl9779NT3QGJpppaYB7zWtJm5qX/WKguqqD9SvS+sEWW73uQJHF8EkN4VgsnVQS/u+tOV7jMkBUf3GBMGJaNcHuPnPWIOIwqppxFRyBTTCvw+0N9x8sCuso842Ms/1YhUGLTkJLxdZrafrdDY9o3zg6SLHwqGxZjXv16WvsCX3qYuaaja/fCTG4EJKtbmPqSSkuFyNRb9l6uyPr9Z9qqaZWwy1xhi759NdQ+DlqpCSBlu2TpDlRFFEpZN32VLKBJgzt5L2fcn3mGMxiSJnxgTBiSw8kIFL/yxWJRM1aFKaBwDYEnkXClwd108xwF2OPjH+SDbShaU+0aCqr74AQCETqmrSHF2oOlDIgHHdqx7ozW3er+tDvj26EZiq2TyfW4aHF5+An7vSaIuCodp+Qw/qhpKQ1cdy8POpXLgpZMi9qb/41ZrUG1h34gb+06oRukZ6Y8PJGybmkNf9uzV1z9XHciAT/p2B69YB+Lcy1DrBbh22Y85Uno6klAHFFeZ1f/JykaG4XF3nFjCqG0EQoNFodObZJ7IlS/6emSA4EW1tjn9ZIbpmpUGuqUS5wgUFLl4GjxcAuJrZpFwXLgo5JvUKBwQY7ZpS34io+vJr3sgNZRoB5RVqCBDh46ZAUUUlCkpND+T1UMqgEUWbx74m/VsF1GpecWs95NfmvJq63NxUabDpn252xpSpq9ZwqN5XGoBOzbxMEODjKkNRuQaVogi5IOCe5r5SQmEsCQGAonKNyb7mlRpg/YkbiPR3RbifKy7nl+v9XcgEYMe5Auw9f1JKZEzdE6hKgLQzcNVEWwt8a4uEUi5Dvza5eKy9LzyUfPCwpp7RPlhzLMdopYJSBgR4Kmv8/LA2H1cZ3JRys2dXSs8tw/jVf0New9+vqfFIVHdBQUG4evUqvL29mSSQXdy8eRMBAQFmHcsEwUlU78/sU1GCgNKqvq5ZHgGAkQ/oQE8l5DLBpnP6V++TbqxrijU08VLinmhfJKca//K1NhFVX5Shfu5IaO6L8f/UxGvXVlCVqPRqj7XdcL4cGgtRFDF+9d968dC2TlSKos2SKW05nosPq/O17PEAUFJRiRkpJ7DlZCZUlbpdeQDgy/0Z2J1egIJSFWqoONeh7Sv92d4rOJZxU+/h+3qx7vHa1oFlj7Y02b3CXJfzy/Fw6wDcFeGNvemFqKjUoKBMDbUGUGsgDYTXJjI3KyqtNpNX10gvoy0S3x64gF2n3DiQ2cq0n4GGFknzcZXhu0dborGXC4rL1VUrLf/TIlwb7koZVjzeEiO++0vqdmqIh1KGNaNbY+Syv8y+tvbvJjrADdklqlqNR6K6c3d3R9OmTXHt2jWuEk124eHhAV9fX7OO5TSnVmCtaU61s8v4lRUhsigLGkGGc76huKk0PFtRsLeLyQGndVX9YVj7kGFoGruCMrXJLzBz+LjK8d2jd+Cx7/9yyCwhMgEI96tayt3QdKxyAQj8J4mp3qXF2LR+j3UKwrLfr0nbzZle9FYCgCHtA/WuZYs+/7ZSUlGJz/Zewaa0XKhvCaoAwFMpoFQtmrU2hikeShnKVBqzH74VQlV3M2v83QR7KbF2TBuIooh5u65gbarhaU+t3eIX7qtE+6be2JRmeJYwa0yzSvqkv/nzhRAhA8RK9Pznc8FDKZMS7pKKSgxYfKJWn43VZx4b8PUJ5JiYcU07LfHgpWkWVxYFeSnh4SI3Oh7pSydJMBvqNKdEzowJghVYK0EwtRLlraqveGt0zQQzyYSqhXM6NPXCwYtFZj+EaruKWFJuUzyUMpSpNU7ZhUn7sG7qYctY1xntdktXcDa0HkF96g+srd0+n1tm83vJBOs87Nfl/koZUFFpeuC7vQfG37qSOVmPIAgIDg7GuUtX8eX+qwYHui88kGHxZ+OtD+c1fW4Ee7tg7ZOta/U53NhTie8fuwOLfst06goIJghE9scuRk7E3C481WebMTTg9MZNw03GhngoZXioVYDOl4G5D6HaY8Z1C8XhS0V1ak4HUOdWCFsSUfOMHsZiZs70ooYY6v9bX5IDoKqr1gU7JAeAY5MD7f3N6Rpl72JykKltaWeeMzbb1kcDmxvtknQrGYAgbxe9h3NzpyWuTRdQuUyAl6uCaxcQkR6OinEi2of9we0CEeLjgmAfNwR5K9GiUdX/G3sqEeLtgsHtAvHl0FhpAKJ2wGnyk62rmprbBUJm5me8r1vVl0P1miJLvyA8XeRYNCyuwQ+I1D5s1da4bqGI9Hcz63fTEPr/7kkvrNczRDUEHGRqWx9u0U8OgH/7+C/7/RoWDo3FkPY1fyY39lIi+cnWep/Hxj43qlcUabs9lVRUwkUuQCYAbgoZ3JXGb2roM4bvFSLSYguCk9E+7L/cq6r5OisrS3oo1c7wsvBABh7//pTBedsFQcBjnYKw5VRejdMkAtarYfR0keOhVgFYfSynTtdxZnV92Lq1taf6gNbqzFmPwNmZs4gY2V7XCMMzoJF1bPvrmhnrCIRLXRNNtQTc09zwwMGapiUGYHCgerlagwj/f8ZV3TLLVkP4jCEi22KC4MQEQdCpsb6p0phcPEo71ePE9edQZOZUMDU99FqSPDzdNQTJxwwP0KzvrFWjf+v0otqEz5n7/9aGIAg1TqFItvdnRglKKirr9XvJWYmiCFUNo+urV8DUdYFCY92A5u26bLAVQ4T+LFsN6TOGiGyLCYITqmouzsSBS3+hvEINuUxAz2gfqCpFk83ZCw9kAAAumtHfFTD+0FtSUYkv91fN0mGolcIYQRDgppRZfSyBvQZ2ervIEOCptFttmyAIdll0zFHuae7boFuU6oPL+eVYeCCDMxnZgCAIUMrNX0fAWquQ3/oZUdNqyAcvFv/TdanhfcYQke0wQXAyxuY1T07NgfDP1IyGaJuzRRg/pjpDD72mpqSs3kph6ItMW25zkgMBVXN8e7jIcLNCY/QcmQAMatsIL/eKMGsGIJkAiGLtkgntHOYe/yyg5Yjatob2xT2uWyh+PpXrkGlrqcq/3VwcXZKG6d6WQfj2wAWz1xGwdoWAOV35OFCdiGqDCYKTMbbSqkZEjU++qkpN1dO3CcZmyqhpSsrqrRSGaiO15a5JpJ8Lvhp+Bzxd5FIXG0PTtAqoSmDGd28KURRrnMlDO2c4AIvnAxcALHusFQI9lQDQYGv07c3TRY5lj7bE49+fMms8DNkGHxBt59V+cdh1KqtW3Yas8fsQBAGKGlbgFQTgo91XDE7Dyi5GRGRMw552ph6qy+quCrmsxi8LYzNlmPOAr62NNKSmcgsABrZpJCUHwL9dbLQzNwV5KeGmqJqBw0UOZBaWY8DiE3j46xPYda4AXi5ygzN5RPq5Yly3UAhCVXN+z2gfs2dxAqryru+OZOmXmQ9UddbYq2oe/uYBrrW+Bn8LdcOZjGzHy1WBRcPiqmae83bRm2nOHg/gpj7vBABlKg2Sj+Ugq6gCOSVqZBVVIDk1B+NWnUFJBRN3IjKMCYITqcvML9rmbFNfFqZmyjA3MTE01ac55Q70VOK13uEGvzA9XeR4umsIPFzkqFCL0pzypeqqFoYbJWpcL1ahqLwSHkoB7kpBemgURSCzqAKPLfsL83ZdrmoJsWA6Ua295w0nPlR3ni5y3KzD6sFRdUguzCEXgCZeCjzU0h/erg3rI7EhTJfr7G6dZtpQBYwtmZoG1dtVhqLyyhrHrRER3YpdjJyIOc3FhlSvRQdg8UwZliQmxhbvqqnchs7Tzt29J70QeTcralxoSgRQXCHqbStTiygrVumMk6g+GDDXjGurK9kNw1ZEUYS6htlejGnm74p5A1vgxXVncbEWC/HVNMDdXSEg5ak28HKt+iic+M84nJQTufV+DQdOZWl/jvj8MDX4efe5AhQaGQPE8SlEZAoTBCdj6Wq7QNWDQGFFJR7//hR6Rvvgo4HNsez3a2YPtDU3MTFVG2nuap9axgZj18Wt4yS0YwmKy9V4YNFxVJq4kULObhi2IggCFDXM9lKddpGnvnH+eC6+KTxd5PhqWByeWXUa53LNSxK0SXP7pl7YcPKG0fdl/9aNpOQAqHrYUsrt34oQ6KkwOWDf0mu5uSjRPcILT3cLYT/z24Chwc+iKGLH2XyT53F8ChEZwwTByYzrFopDl4osqi1Va4AbJWoAurMNTUoIN/vD35zExFRtpKVzfBsbjF1Xt9aKlVRUYtFvmXCRCSg18uJkAtAzynDXK7KO+ChfJKdmG31/eShleKhVAJ7uGiIt+Fedp4scXwyNw/0LU00mejIBCPJy0VlE6lhGiUUtavZeATrY2wXJo1vhpkqDQUtO1GnWp2BvF6x9sjVCQ0ORmZlZp5W/qX7S/u3UtmWXiAjgGASn4+kiR8emtV/99Na+peZ++Jvqt6+QVQ0wNjXorvpgY3MG69VlMHZNtLVi2laK5GM5KDXSB14mAC2aeGFcd3bDsKXx3UPRoomXwX7SUf6uSHmqDSYlhMPLVWH0PevpIsd/WjUyeg/ttLjV+4Bb+r40p7udm0JAiLcLGnkooKjjJ6i2dU07YH/Zoy3h42qglQ9VU/G6K43/PVe/FhFgegAzx6cQkSlsQXBCv10sqtP5telbaqwfa48ob4zv3tSsbgrmzvFdl8HY5tDWitXUSuGhlKF/q0Z4a9CdKMrNZm2rDXm6yLF2Qg/MWvsH9qQX1HqNiefim5psERjfvanBe5s7da05ta5+7lUzgWmn6TW2doi2TO88GInxq8/qTfVqqBVDO+uTsbU4ABicFpjjDciQuqzeTES3N0HkU1GdZWdnQ6VSWeVaoihiwNcnkPNPl6HaauypxPoxrWtdm2jrfqnmLHxWG9o1ESYlhNd4jxBvF6wd0wYhISHsjmFjgiDoxLku7y/t4HZbLWY3b9flGtfcuHUtkJKKSiyUVh/XL1Nty2woTjVd69ZYk23Ulzjb+u/FHmwRa6VSicaNG1vlWkQNEVsQnExtZzK6VV37ltq6m4Ilg7HdFAL83ZW4O9ILf14twaW8coN9xKvXilmywijZX13eX9ZejfZWtal19XSRY1KvcEzqZbhMtS2zoeNs/fqpYeH7hYhqgwmCE4qP8saa1Bu1Pr8+9C019hBWnfaB7ItHYqSZZrS1YbvPFaCgTI2KShEuchl83eW4J9pXp1aMA/QaPlv8/kxNG2lOrWtNZbJmmfn+JUOMJQJ8vxCRuZggOKHx3Zti3YkbJmdraeKlhKeLvN72Lb31IayiUoPSf6Z49HCRQSmTGXwg+7c2LFxnOj9DX3xdI72x/oThRKs+JFHkOKx1pfqm+royao0GCpkMPetZVyIich5MEJyQp4sc/2ndCOuPG3+4TWheVVten/uWGnsIM/eBrPp0frcqqajEn1eLjZ4bUW1hOSJTmByQszO2rkz1aa/rw3cCETkPJghO6vn4MKRdL8fZ68VGWwgaUi1n9bJb43UsPJCByybWkujQ1JNfmETUIBibse3WxSOJiMzFdRCclHZayCHtGps1f3t9Tg5soaZ1Fg5eNN66QERUn5j6vNNOe01EZAm2IDgxL1cFJvUKx8SEsHrfQmBPlsxgxJiSJfieIWfDzzsisgUmCPUEP9jNZ85UsZzBiMzFwZ/kzPh5R0S2wC5G1CD1jPaBzMj3IWcwInNpB38mH8tBVlEFckrUyCqqQHJqDsatOoOSisqaL0JkIyUVlZi36zIKyowvrMnPOyKqDSYI1CCN6xaKSH83vSShvkwDS87BnMGfRI5QPXnVThF9K37eEVFtOUUXoy1btmDDhg3Iz89HWFgYRo8ejZYtWxo89rPPPsOuXbv0toeFhWHu3LnSz7/99htWrlyJa9euISgoCCNGjMBdd91V6/tS/VLXxa6IAPMGf05KsGuRiAAYT161PJQyPNQqgJ93RFQrDk8Q9u/fj6VLl2Ls2LGIi4vDtm3bMHv2bMybNw+BgYF6xz/55JN49NFHpZ8rKyvx2muvoWvXrtK2M2fO4KOPPsKwYcNw11134dChQ5g3bx5mzZqFmJiYWt2X6p+GNA0s2R8Hf5Izq2mmNl83Bac2JaJac3gXo02bNiExMRF9+vSRavEDAwOxdetWg8d7eHjAz89P+u/cuXMoKSlB7969pWN+/PFHtGvXDklJSWjatCmSkpLQpk0b/Pjjj7W+L9VvfIAjS3HwJzkrS5JXIqLacGgLglqtRnp6OgYOHKizvV27djh9+rRZ19i+fTvatm2Lxo0bS9vOnDmDhx56SOe49u3b46effqrTfVUqFVQqlfSzIAhwd3eX/m1NplYJJuthnO2jvsa5Z7QvklOzdRYr1JIJwD3Rvk73muprrOsbR8ZZEAQo5aaTV4VcgKyGBLe+4HuayP4cmiAUFhZCo9HA19dXZ7uvry/y8/NrPD8vLw9Hjx7Fiy++qLM9Pz8ffn5+Otv8/Pyka9b2vuvWrcOaNWukn6OiovD+++/rJCfWFhwcbLNr078YZ/uob3GePqgxjmXtM7iieYsmXnhr0J3wcnV4T02D6lus6ytHxblfm1x8e+CC0eT1/jahCAkJsX/BbIjvaSL7cYpvNkO1AubUFOzcuROenp56g48NMdRP2NL7JiUloX///nrHZmdnQ602Ps1cbQiCgODgYGRlZbGZ2IYYZ/uoz3FeMKg5Fu7PwJ7zBVBXilDIBfSM8sW47qEoys1GkaMLeIv6HOv6xNFxfqy9L3adcsPFvDK95LVZgBsebe+LzMxMu5fLFmwRa4VCYdPKPaL6zqEJgo+PD2QymV6tfUFBgV7t/q1EUcSOHTvQs2dPKBS6L6N6a4Gha9b2vkqlEkql0mh5bEEU2Y/UHhhn+6iPcfZQyjAxIczgiubO/FrqY6zrI0fF2UMpMzlTm4dS1uB+/3xPE9mPQxMEhUKB6OhopKam6rQCpKamokuXLibPTUtLQ1ZWFhITE/X2xcbG4vjx4zq1/ampqYiNja3zfYno9sU+0GRNdZ0BizO1EZGtOLyLUf/+/TF//nxER0cjNjYW27ZtQ05ODu677z4AwPLly5Gbm4vnn39e57zt27cjJiYGERERetd88MEHMX36dKxfvx5dunTB4cOHcfz4ccyaNcvs+xIREVlbSUUlFh7IwJ70Qqg1GihkMvS0wvosTA6IyJocniB0794dRUVFSE5ORl5eHsLDwzFlyhSpb2BeXh5ycnJ0zrl58yYOHjyI0aNHG7xmXFwcJk6ciBUrVmDlypUIDg7GxIkTpTUQzLkvERGRNWlXP751gbPk1BwcuVyMhUNjuagZETkFQWSHvjrLzs7Wmf7UGgRBQEhICDIzM9nn0oYYZ/tgnO2HsbaP2sR53q7LSD6WY3CBM5kADG4XyMXNDLDFe1qpVLJCkMiEhjFJMhERkZMztfqxRgT2phfatTxERMYwQSAiIrIxrn5MRPUJEwQiIiIbEwQBihpWNpbLhHozjS4RNWwOH6RMRER0O+gZ7YPk1Byjqx/3jPax2SxHRESWYIJARERkB+O6heLI5WLDqx/7u+GxTkGc5YiInAK7GBEREdmBp4scC4fGYnC7QIR4u6CxpxIh3i4Y3C4QXw6NxbLfr+klB0DVAOaLeWVYeCDDIeUmotsPWxCIiIjsxNTqx+bMcjQpwT7lJKLbG1sQiIiIHODWAcmc5YiInAUTBCIiIgerzSxHRES2wgSBiIjICfSM9oHMyPO/dpYjIiJ7YIJARETkBMZ1C0Wkv5tekqCd5Whct1DHFIyIbjscpExEROQEtLMcLTyQgb3phVBrRChkAuK5DgIR2RkTBCIiIidhapYjIiJ7YRcjIiIiJ8TkgIgchQkCERERERFJmCAQEREREZGECQIREREREUmYIBARERERkYQJAhERERERSZggEBERERGRhAkCERERERFJmCAQEREREZGECQIREREREUmYIBARERERkYQJAhERERERSZgg1DOiKDq6CERERETUgCkcXQCqWUlFJRYeyMCe9EKoNRooZDL0jPbBuG6h8HSRO7p4RERERNSAMEFwciUVlRi36gwu5pZBU217cmoOjlwuxsKhsUwSiIiIiMhq2MXIyX25P0MvOQAAjQhczCvDwgMZDikXERERETVMtU4Qrl69il9++QVr165Ffn4+ACA3NxcVFRXWKhsB2Hu+QC850NKIwN70QruWh4iIiIgaNou7GGk0Gnz55ZfYuXOntK1Dhw7w8/PDwoULERUVhWHDhlmzjLctURShrjQ9KFmtESGKIgRBsFOpiIiIiKghs7gFYe3atdi7dy8ef/xx/N///Z/Ovo4dO+Lo0aPWKtttTxAEKOSmH/zlMoHJARERERFZjcUJws6dOzF48GD0798foaGhOvuaNGmC69evW61wBMRH+UJm5PlfJgA9o33sWyAiIiIiatAs7mKUm5uL2NhYg/uUSiXKysosLsSWLVuwYcMG5OfnIywsDKNHj0bLli2NHq9SqbBmzRrs2bMH+fn5aNSoEZKSkpCYmAgAmDFjBtLS0vTO69ixI6ZMmQIAWLVqFdasWaOz39fXF4sWLbK4/LY0vnsojlwuwsW8Mmiq9TaSCUAzfzeM6xZq/GQiIiIiIgtZnCD4+voabSXIyMhAQECARdfbv38/li5dirFjxyIuLg7btm3D7NmzMW/ePAQGBho8Z968eSgoKMAzzzyD4OBgFBYWorKyUtr/6quvQq1WSz8XFRXhtddeQ7du3XSuEx4ejmnTpkk/y2TON6mTp4scC4fGYuGBDOxNL4RaI0IhExDPdRCIiIiIyAYsThA6duyItWvXSgOTgaq+8jdv3sTmzZvRqVMni663adMmJCYmok+fPgCA0aNH49ixY9i6dStGjhypd/zRo0eRlpaGTz/9FF5eXgCqujZVp92utW/fPri6uqJr164622UymfQanJmnixyTEsIxKQEckExERERENmVxgjB06FD8+eefmDRpElq3bg0A+OGHH3D58mXI5XIMGTLE7Gup1Wqkp6dj4MCBOtvbtWuH06dPGzznyJEjaN68OVJSUrB79264ubmhU6dOGD58OFxcXAyes337dnTv3h1ubm4627OysjB+/HgoFArExMRgxIgRCAoKMlpelUoFlUol/SwIAtzd3aV/W5P2erdel8mBdRmLM1kX42w/jLV9MM72w1gT2Z/FCYKfnx/effddrFq1Cn/++SdkMhkuXryIO++8E8OGDdOrvTelsLAQGo0Gvr6+Ott9fX2ltRVude3aNZw6dQpKpRKvvfYaCgsLsXjxYhQXF2PChAl6x589exaXL1/Gs88+q7M9JiYGzz33HEJDQ5Gfn4+1a9di6tSpmDt3Lry9vQ3ee926dTrjFqKiovD++++jcePGZr9mSwUHB9vs2vQvxtk+GGf7Yaztg3G2H8aayH4sThCAqiRh3LhxViuEoVoBYzUFolg1UvfFF1+Eh4cHgKqa/blz52Ls2LF6rQjbt29HeHg4WrRoobO9Y8eO0r8jIiIQGxuLF154Abt27UL//v0N3jspKUlnn7aM2dnZOmMerEEQBAQHByMrK0t6zWR9jLN9MM72w1jbB+NsP7aItUKhsGnlHlF9V6sEwVp8fHwgk8n0WgsKCgr0WhW0/Pz8EBAQICUHANC0aVOIoogbN24gJCRE2l5eXo59+/aZtXCbm5sbIiIikJmZafQYpVIJpVJpcJ+tviBEUeSXjx0wzvbBONsPY20fjLP9MNZE9mNxgrBgwQKT+wVB0OvOY/TmCgWio6ORmpqKu+66S9qempqKLl26GDznjjvuwG+//YaysjJpTEFmZiYEQUCjRo10jj1w4ADUajV69uxZY1lUKhWuXr1qcnpVIiIiIqKGzuIE4eTJk3rbiouLUVZWBg8PD3h6elp0vf79+2P+/PmIjo5GbGwstm3bhpycHNx3330AgOXLlyM3NxfPP/88ACA+Ph7JyclYsGABhg4disLCQixbtgy9e/c22L2oS5cuBscUfPvtt+jcuTMCAwNRUFCA5ORklJaWIiEhwaLyExERERE1JBYnCJ999pnB7SdOnMBXX32Fl19+2aLrde/eHUVFRUhOTkZeXh7Cw8MxZcoUqW9gXl4ecnJypOPd3NwwdepUfP3115g8eTK8vb3RrVs3DB8+XOe6GRkZOHXqFKZOnWrwvrm5ufj4449RWFgIHx8fxMTE4J133mGfRCIiIiK6rQmiFTv0/fzzzzh48CCmT59urUvWC9nZ2TrTn1qDIAgICQlBZmYm+1zaEONsH4yz/TDW9sE4248tYq1UKlkhSGSCVZcODgsLw9mzZ615SSIiogaJiQUROSurzmKUlpYGHx8fa16SiIiowSipqMTCAxnYk14ItUYDhUyGntE+GNctFJ4uckcXj4gIQC0ShOoLhWmpVCpcvHgRR48excMPP2yVghERETUkJRWVGLfqDC7mlkFTbXtyag6OXC7GwqGxTBKIyClYnCCsXr1a/yIKBZo0aYKhQ4cyQSAiIjJg4YEMveQAADQicDGvDAsPZGBSQrhDykZEVJ3FCcLKlSttUQ4iIqIGbU96oV5yoKURgb3phZjEmbaJyAlYdZAyERER6RNFEWqNsfSgilrDlYKJyDkwQSAiIrIxQRCgkJn+ypXLBAiCYKcSEREZZ1YXo2HDhpl9QUEQsGLFiloXiIiIqCHqGe2D5NQcaAw0EsiEqv1ERM7ArARh8ODBrNUgIiKqg3HdQnHkcjEu5pXpJAkyAWjm74Zx3UIdVzgiomrMShCGDh1q63IQERE1aJ4uciwcGouFBzKwN70Qao0IhUxAPNdBICInY9WF0oiIiMg4Txc5JiWEY1JC1cBlts4TkTOqdYJw6dIlXL16FRUVFXr7EhI4TxsREZEpTA6IyFlZnCCUl5djzpw5OHHihNFjmCAQEREREdVPFk9zmpycjOvXr2PGjBkAgFdeeQVTp07F3XffjZCQELz//vvWLiMREREREdmJxQnC4cOHMWDAAMTFxQEAAgMD0bZtW7z88suIiorC1q1brV5IIiIiIiKyD4sThOzsbDRt2hSyfxZ8qT4GoWfPnjh8+LD1SkdERERERHZlcYLg6emJ8vJyAICvry8yMzOlfWq1WtpHRERERET1j8UJQkREBDIyMgAArVu3xrp163Dq1CmcPXsWycnJiIyMtHohiYiIiIjIPixOEHr37o2ysjIAwIgRI1BeXo7p06fjzTffRHZ2Np544gmrF5KIiIiIiOzDrGlOly5disTERERERKB79+7S9iZNmuDjjz/GiRMnIAgC4uLi4OXlZbPCEhERERGRbZmVIGzevBmbN29GdHQ0EhMT0aNHD3h4eAAA3Nzc0LlzZ5sWkoiIiIiI7MOsLkYff/wxBgwYgPz8fHz11VcYP348Pv30U6Slpdm6fEREREREZEdmtSAEBwdj5MiRGD58OI4dO4YdO3bgwIED2LNnD5o0aYLExEQkJCQgICDA1uUlIiIiIiIbMitB0JLJZOjYsSM6duyI4uJi7NmzBzt37sSKFSuwatUqtGvXDomJibj77rttVV4iIiIiIrIhixKE6ry8vPDAAw/ggQcewMWLF7Flyxb8+uuvOHbsGFasWGHNMhIRERERkZ3UOkHQSk9Px44dO/Dbb78BAHx8fOpcKCIiIiIicoxaJQhFRUXYs2cPduzYgUuXLkEmk6F9+/ZITExEp06drF1GIiIiIiKyE7MTBFEU8eeff2Lnzp34/fffoVarERQUhOHDh6NXr17w9/e3ZTmJiIiIiMgOzEoQli9fjt27dyMvLw8uLi7o1q0bEhMT0apVK1uXj4iIiIiI7MisBCElJQXR0dEYNGgQ4uPjpUXSiIiIiIioYTErQZgzZw4iIyNtXRYiIiIiInIws1ZSZnJARERERHR7qPM0p9awZcsWbNiwAfn5+QgLC8Po0aPRsmVLo8erVCqsWbMGe/bsQX5+Pho1aoSkpCQkJiYCAHbu3IkFCxbonbds2TK4uLjU+r5ERERERA2dwxOE/fv3Y+nSpRg7dizi4uKwbds2zJ49G/PmzUNgYKDBc+bNm4eCggI888wzCA4ORmFhISorK3WOcXd3x8cff6yzrXpyUJv7EhERERE1dGZ1MbKlTZs2ITExEX369JFq8QMDA7F161aDxx89ehRpaWmYMmUK2rVrhyZNmqBFixaIi4vTOU4QBPj5+en8V5f7EhERERHdDhzagqBWq5Geno6BAwfqbG/Xrh1Onz5t8JwjR46gefPmSElJwe7du+Hm5oZOnTph+PDhOi0EZWVlmDBhAjQaDZo1a4Zhw4YhKiqq1vclIiIiIrod1DpBuHnzJs6cOYOioiJ07NgRXl5eFl+jsLAQGo0Gvr6+Ott9fX2Rn59v8Jxr167h1KlTUCqVeO2111BYWIjFixejuLgYEyZMAACEhoZiwoQJiIiIQGlpKX766SdMmzYNH3zwAUJCQmp1X6Bq7INKpZJ+FgQB7u7u0r+tSXs9a1+XdDHO9sE42w9jbR+Ms/0w1kT2V6sEYc2aNUhJSUFFRQUA4N1334WXlxdmzZqFdu3a6dXM18TQH72xDwJRFAEAL774orQeg0qlwty5czF27Fi4uLggNjYWsbGx0jlxcXF44403sHnzZowZM6ZW9wWAdevWYc2aNdLPUVFReP/999G4ceMaXmHtBQcH2+za9C/G2T4YZ/thrO2DcbYfxprIfixOELZs2YI1a9agb9++6NixI9577z1p35133olDhw6ZnSD4+PhAJpPp1doXFBTo1e5r+fn5ISAgQGextqZNm0IURdy4cQMhISF658hkMjRv3hxZWVm1vi8AJCUloX///tLP2mQiOzsbarXa5Gu1lCAICA4ORlZWlpQUkfUxzvbBONsPY20fjLP92CLWCoXCppV7RPWdxQnCzz//jP79++Oxxx6DRqPR2RcSEoLMzEzzb65QIDo6Gqmpqbjrrruk7ampqejSpYvBc+644w789ttvKCsrg5ubGwAgMzMTgiCgUaNGBs8RRREXL15EeHh4re8LAEqlEkql0ug9bEEURX752AHjbB+Ms/0w1vbBONsPY01kPxbPYnT9+nW0b9/e4D53d3fcvHnTouv1798fv/76K7Zv344rV65g6dKlyMnJwX333QcAWL58OT799FPp+Pj4eHh7e2PBggW4cuUK0tLSsGzZMvTu3VsapLx69WocPXoU165dw4ULF/D555/jwoUL6Nu3r9n3JSIiIiK6HVncguDh4YGCggKD+65fvw4fHx+Lrte9e3cUFRUhOTkZeXl5CA8Px5QpU6Smv7y8POTk5EjHu7m5YerUqfj6668xefJkeHt7o1u3bhg+fLh0TElJCRYuXIj8/Hx4eHggKioKM2fORIsWLcy+LxERERHR7cjiBKFNmzZISUlB586dpRp7QRBQWVmJX375xWjrgin9+vVDv379DO577rnn9LY1bdoU06ZNM3q90aNHY/To0XW6LxERERHR7cjiBGHYsGGYMmUKXn75Zan//s8//4wLFy4gJycHkyZNsnohiYiIiIjIPiwegxAcHIz//e9/aNq0KbZs2QIA2L17N7y9vTFz5kwEBgZavZBERERERGQftVoHISwsDG+++SZUKhWKiorg5eWls4oxERERERHVTxa3IPz+++/S9KZKpRIBAQFMDoiIiIiIGgiLWxDmzJkDX19f3HPPPejVqxfCwsJsUS4iIiIiInIAixOEyZMnY+fOndi8eTM2btyIFi1aoHfv3ujRowfc3d1tUUYiIiIiIrITixOEjh07omPHjigpKcHevXuxa9cuLFq0CN988w3uuusu9O7dG23atLFFWYmIiIiIyMZqNUgZADw9PaV1BK5cuYKdO3di165d2LdvH1asWGHNMhIRERERkZ1YPEj5VqIo4saNG8jJycHNmzchiqI1ykVERERERA5Q6xaErKwsqdUgNzcXAQEB6N+/P3r37m3N8hERERERkR1ZnCDs2LEDO3fuxKlTp6BQKNC5c2f07t0b7dq1g0xW5wYJIiIiIiJyIIsThC+++ALNmjXDk08+ifj4eHh5edmiXERERERE5AC1WgchMjLSFmUhIiIiIiIHs7hPEJMDIiIiIqKGy6wWhDVr1iAxMREBAQFYs2ZNjccPGTKkzgUjIiIiIiL7MytBWL16NTp06ICAgACsXr26xuOZIBARERER1U9mJQgrV640+G8iIiIiImpYOC8pERERERFJLE4Qhg0bhrNnzxrcl56ejmHDhtW5UERERERE5BhWbUHQaDQQBMGalyQiIiIiIjuyaoKQnp4ODw8Pa16SiIiIiIjsyKxByj/99BN++ukn6ecPPvgASqVS55iKigoUFBSga9eu1i0hERERERHZjVkJgo+PD8LCwgAA2dnZCAoK0mspUCqViIiIwIMPPmj9UhIRERERkV2YlSDEx8cjPj4eADBz5kyMHTsWTZs2tWnBiIiIiIjI/sxKEKqbPn26LcpBREREREROwOJByjt27MCqVasM7lu1ahV27dpV50IREREREZFjWJwgbN68GV5eXgb3+fj4YPPmzXUuFBEREREROYbFCUJWVhbCw8MN7gsLC0NmZmadC0VERERERI5Rq3UQbt68aXS7RqOpU4GIiIiIiMhxLE4QIiIisG/fPoP79u7di4iIiDoXioiIiIiIHMPiBOH+++/HwYMH8emnn+Lvv/9Gbm4u/v77b3z22Wc4ePAg7r//fluUk4iIiIiI7MDiaU7j4+Nx9epVrF+/Hnv27JG2y2QyDB48GD179rRqAYmIiIiIyH4sThAAYNiwYejduzdSU1NRWFgIHx8ftG/fHo0bN7Z2+YiIiIiIyI5qlSAAQJMmTXDvvfdapRBbtmzBhg0bkJ+fj7CwMIwePRotW7Y0erxKpcKaNWuwZ88e5Ofno1GjRkhKSkJiYiIAYNu2bdi9ezcuX74MAIiOjsaIESPQokUL6RqrVq3CmjVrdK7r6+uLRYsWWeU1ERERERHVR7VKEFQqFXbu3ImTJ0+iuLgYTz31FEJCQnD48GFEREQgKCjI7Gvt378fS5cuxdixYxEXF4dt27Zh9uzZmDdvHgIDAw2eM2/ePBQUFOCZZ55BcHAwCgsLUVlZKe1PS0tDjx49EBcXB6VSiZSUFLz99tuYO3cuAgICpOPCw8Mxbdo06WeZrFaTOhERERERNRgWJwiFhYWYOXMmrly5Aj8/P+Tn56O0tBQAcPjwYRw7dgxjx441+3qbNm1CYmIi+vTpAwAYPXo0jh07hq1bt2LkyJF6xx89ehRpaWn49NNPpQXbmjRponPMiy++qPPzM888g4MHD+L48eNISEiQtstkMvj5+ZldViIiIiKihs7iBGHZsmW4efMm3n33XURGRuo8xLdu3RopKSlmX0utViM9PR0DBw7U2d6uXTucPn3a4DlHjhxB8+bNkZKSgt27d8PNzQ2dOnXC8OHD4eLiYvCc8vJyqNVqvRWgs7KyMH78eCgUCsTExGDEiBEmWz9UKhVUKpX0syAIcHd3l/5tTdrrWfu6pItxtg/G2X4Ya/tgnO2HsSayP4sThD/++AOPPvoooqOj9RZFa9SoEW7cuGH2tQoLC6HRaODr66uz3dfXF/n5+QbPuXbtGk6dOgWlUonXXnsNhYWFWLx4MYqLizFhwgSD53z//fcICAhA27ZtpW0xMTF47rnnEBoaivz8fKxduxZTp07F3Llz4e3tbfA669at0xm3EBUVhffff9+mg7ODg4Ntdm36F+NsH4yz/TDW9sE42w9jTWQ/FicIpaWlRh+I1Wp1rVZSNlQrYKymQBRFAFXdiDw8PABU1ezPnTsXY8eO1WtFSElJwb59+zBjxgydfR07dpT+HRERgdjYWLzwwgvYtWsX+vfvb/DeSUlJOvu0ZczOzoZarTbnpZpNEAQEBwcjKytLes1kfYyzfTDO9sNY2wfjbD+2iLVCoeDMi0QmWJwgNGnSBGfOnEGbNm309p09exahoaFmX8vHxwcymUyvtaCgoECvVUHLz88PAQEBUnIAAE2bNoUoirhx4wZCQkKk7Rs2bMC6deswbdo0REZGmiyLm5sbIiIikJmZafQYpVIJpVJpcJ+tviBEUeSXjx0wzvbBONsPY20fjLP9MNZE9mPxtD3x8fFISUnB4cOHpT9UQRBw9uxZbN682aKF0hQKBaKjo5GamqqzPTU1FXFxcQbPueOOO5CXl4eysjJpW2ZmJgRBQKNGjaRtGzZsQHJyMv773/+iefPmNZZFpVLh6tWr8Pf3N7v8REREREQNjcUtCAMGDMDp06fx4YcfwtPTEwDwzjvvoKioCB06dMCDDz5o0fX69++P+fPnIzo6GrGxsdi2bRtycnJw3333AQCWL1+O3NxcPP/88wCqEpTk5GQsWLAAQ4cORWFhIZYtW4bevXtLXYhSUlKwcuVKvPjii2jSpInUQuHm5gY3NzcAwLfffovOnTsjMDAQBQUFSE5ORmlpqc4sR0REREREtxuLEwSFQoEpU6Zg//79+OOPP1BQUABvb2906tQJ3bt3t3gtge7du6OoqAjJycnIy8tDeHg4pkyZIvUNzMvLQ05OjnS8m5sbpk6diq+//hqTJ0+Gt7c3unXrhuHDh0vHbN26FWq1GnPnztW515AhQzB06FAAQG5uLj7++GNpJeiYmBi888477JNIRERERLc1QWSHvjrLzs7Wmf7UGgRBQEhICDIzM9nn0oYYZ/tgnO2HsbYPxtl+bBFrpVLJCkEiE7h0MBERERERSczqYjRz5kyMHTsWTZs2xcyZM00eKwgCvLy8EBcXh759+xqd9YeIiIiIiJyPxWMQRFE0uZqhKIq4du0aDh8+jMuXL+OZZ56pUwGJiIiIiMh+zEoQpk+fLv17xowZZl14+/btWL58ea0KRUREREREjmGzMQgtW7bEnXfeaavLExERERGRDVjcxQgANBoN9u/fj5MnT6KoqAje3t5o3bo1unXrBrlcDgAICQnBhAkTrFpYIiIiIiKyLYsThMLCQsyePRvnz5+HTCaDt7c3ioqKsH37dmzcuBFvvvkmfHx8bFFWIiIiIiKyMYsThG+++QYZGRl44YUXpIXRtC0KixYtwjfffIMXXnjBFmUlIiIiIiIbszhB+P333zF8+HDEx8dL22QyGeLj41FQUIDVq1dbtYBERERERGQ/Fg9SFkURYWFhBveFh4dzRUkiIiIionrM4gShbdu2OH78uMF9qampaN26dZ0LRUREREREjmFWF6Pi4mLp30OGDMGHH34IjUaD+Ph4+Pn5IT8/H3v27MGhQ4fw6quv2qywRERERERkW2YlCE899ZTetk2bNmHTpk1629944w2sXLmy7iUjIiIiIiK7MytBGDx4MARBsHVZiIiIiIjIwcxKEIYOHWrrchARERERkROo1UrKoiiiqKgIgiDAy8uLrQtERERERA2ERQnCmTNnsH79epw4cQLl5eUAAFdXV7Rp0wZJSUmIiYmxSSGJiIiIiMg+zE4QtmzZgqVLlwIAoqOj0bhxYwBAdnY2/vzzT/z5558YPXo0+vXrZ5OCEhERERGR7ZmVIJw5cwZLlixBx44dMXbsWDRq1Ehn/40bN7Bo0SIsXboUzZs3R4sWLWxSWCIiIiIisi2zFkrbtGkTYmJi8Nprr+klBwDQqFEjvP7662jRogU2bNhg9UISEREREZF9mJUgnDp1Cv369YNMZvxwmUyGvn374tSpU1YrHBERERER2ZdZCUJxcTECAwNrPK5x48Y6qy4TEREREVH9YlaC4O3tjezs7BqPy8nJgbe3d50LRUREREREjmFWghAXF4etW7dCo9EYPUaj0eDnn3/GHXfcYbXCERERERGRfZmVIPTv3x9///03PvzwQ+Tl5entz83NxYcffohz587hP//5j9ULSURERERE9mHWNKexsbEYNWoUvvnmG0yYMAHNmzdHkyZNAADXr1/HuXPnIIoiRo8ezSlOiYiIiIjqMbMXSnvggQcQFRWF9evX4+TJk/j7778BAC4uLmjfvj2SkpIQFxdns4ISEREREZHtmZ0gAMAdd9yByZMnQ6PRoKioCEDVAGZT058SEREREVH9YVGCoCWTyeDr62vtshARERERkYOx6p+IiIiIiCRMEIiIiIiISMIEgYiIiIiIJEwQiIiIiIhIUqtByta2ZcsWbNiwAfn5+QgLC8Po0aPRsmVLo8erVCqsWbMGe/bsQX5+Pho1aoSkpCQkJiZKx/z2229YuXIlrl27hqCgIIwYMQJ33XVXne5LRERERNTQOTxB2L9/P5YuXYqxY8ciLi4O27Ztw+zZszFv3jwEBgYaPGfevHkoKCjAM888g+DgYBQWFqKyslLaf+bMGXz00UcYNmwY7rrrLhw6dAjz5s3DrFmzEBMTU+v7EhERERE1dA7vYrRp0yYkJiaiT58+Ui1+YGAgtm7davD4o0ePIi0tDVOmTEG7du3QpEkTtGjRQmeRth9//BHt2rVDUlISmjZtiqSkJLRp0wY//vhjre9LRERERHQ7cGgLglqtRnp6OgYOHKizvV27djh9+rTBc44cOYLmzZsjJSUFu3fvhpubGzp16oThw4fDxcUFQFULwkMPPaRzXvv27fHTTz/V+r5AVdcmlUol/SwIAtzd3aV/W5P2eta+LulinO2DcbYfxto+GGf7YayJ7M+hCUJhYSE0Go3eomu+vr7Iz883eM61a9dw6tQpKJVKvPbaaygsLMTixYtRXFyMCRMmAADy8/Ph5+enc56fn590zdrcFwDWrVuHNWvWSD9HRUXh/fffR+PGjc17wbUQHBxss2vTvxhn+2Cc7Yextg/G2X4YayL7cfgYBMBwrYCxmgJRFAEAL774Ijw8PABU1ezPnTsXY8eOlVoRDJ136zUtuS8AJCUloX///nrHZmdnQ61WGz2vNgRBQHBwMLKysqTXTNbHONsH42w/jLV9MM72Y4tYKxQKm1buEdV3Dk0QfHx8IJPJ9GrtCwoK9Gr3tfz8/BAQECAlBwDQtGlTiKKIGzduICQkRKe1wNA1a3NfAFAqlVAqlQb32eoLQhRFfvnYAeNsH4yz/TDW9sE42w9jTWQ/Dh2krFAoEB0djdTUVJ3tqampOoOOq7vjjjuQl5eHsrIyaVtmZiYEQUCjRo0AALGxsTh+/LjeNWNjY2t9XyIiIiKi24HDZzHq378/fv31V2zfvh1XrlzB0qVLkZOTg/vuuw8AsHz5cnz66afS8fHx8fD29saCBQtw5coVpKWlYdmyZejdu7fUvejBBx/EsWPHsH79ely9ehXr16/H8ePHdQYu13RfIiIiIqLbkcPHIHTv3h1FRUVITk5GXl4ewsPDMWXKFKlvYF5eHnJycqTj3dzcMHXqVHz99deYPHkyvL290a1bNwwfPlw6Ji4uDhMnTsSKFSuwcuVKBAcHY+LEidIaCObcl4iIiIjodiSI7NBXZ9nZ2TrTn1qDIAgICQlBZmYm+1zaEONsH4yz/TDW9sE4248tYq1UKlkhSGSCw7sYERERERGR82CCQEREREREEiYIREREREQkYYJAREREREQSJghERERERCRhgkBERERERBImCEREREREJGGCQEREREREEiYIREREREQkYYJAREREREQSJghERERERCRhgkBERERERBImCEREREREJGGCQEREREREEiYIREREREQkYYJAREREREQSJghERERERCRhgkBERERERBImCEREREREJGGCQEREREREEiYIREREREQkYYJAREREREQSJghERERERCRhgkBERERERBImCEREREREJGGCQEREREREEiYIREREREQkYYJAREREREQSJghERERERCRhgkBERERERBImCEREREREJGGCQEREREREEiYIREREREQkUTi6AACwZcsWbNiwAfn5+QgLC8Po0aPRsmVLg8eePHkSM2fO1Ns+b948NG3aFAAwY8YMpKWl6R3TsWNHTJkyBQCwatUqrFmzRme/r68vFi1aVNeXQ0RERERUbzk8Qdi/fz+WLl2KsWPHIi4uDtu2bcPs2bMxb948BAYGGj3vo48+goeHh/Szj4+P9O9XX30VarVa+rmoqAivvfYaunXrpnON8PBwTJs2TfpZJmODChERERHd3hyeIGzatAmJiYno06cPAGD06NE4duwYtm7dipEjRxo9z9fXF56engb3eXl56fy8b98+uLq6omvXrjrbZTIZ/Pz86vYCiIiIiIgaEIcmCGq1Gunp6Rg4cKDO9nbt2uH06dMmz3399dehUqkQFhaGQYMGoU2bNkaP3b59O7p37w43Nzed7VlZWRg/fjwUCgViYmIwYsQIBAUFGb2OSqWCSqWSfhYEAe7u7tK/rUl7PWtfl3QxzvbBONsPY20fjLP9MNZE9ufQBKGwsBAajQa+vr462319fZGfn2/wHH9/f4wbNw7R0dFQq9XYvXs3/ve//2H69Olo1aqV3vFnz57F5cuX8eyzz+psj4mJwXPPPYfQ0FDk5+dj7dq1mDp1KubOnQtvb2+D9163bp3OuIWoqCi8//77aNy4sYWv3HzBwcE2uzb9i3G2D8bZfhhr+2Cc7YexJrIfh3cxAgzXChirKQgNDUVoaKj0c2xsLHJycrBx40aDCcL27dsRHh6OFi1a6Gzv2LGj9O+IiAjExsbihRdewK5du9C/f3+D905KStLZpy1jdna2zpgHaxAEAcHBwcjKyoIoila9Nv2LcbYPxtl+GGv7YJztxxaxVigUNq3cI6rvHJog+Pj4QCaT6bUWFBQU6LUqmBIbG4s9e/bobS8vL8e+ffswbNiwGq/h5uaGiIgIZGZmGj1GqVRCqVQa3GerLwhRFPnlYweMs30wzvbDWNsH42w/jDWR/Th02h6FQoHo6GikpqbqbE9NTUVcXJzZ1zl//rzBwcYHDhyAWq1Gz549a7yGSqXC1atX4e/vb/Z9iYiIiIgaGod3Merfvz/mz5+P6OhoxMbGYtu2bcjJycF9990HAFi+fDlyc3Px/PPPAwB+/PFHNG7cGOHh4VCr1dizZw8OHjyIV155Re/a27dvR5cuXQyOKfj222/RuXNnBAYGoqCgAMnJySgtLUVCQoJtXzARERERkRNzeILQvXt3FBUVITk5GXl5eQgPD8eUKVOkvoF5eXnIycmRjler1fjuu++Qm5sLFxcXhIeHY/Lkybjzzjt1rpuRkYFTp05h6tSpBu+bm5uLjz/+GIWFhfDx8UFMTAzeeecd9kkkIiIiotuaILJDX51lZ2frTH9qDYIgICQkBJmZmexzaUOMs30wzvbDWNsH42w/toi1UqlkhSCRCVw6mIiIiIiIJEwQiIiIiIhIwgSBiIiIiIgkTBCIiIiIiEjCBIGIiIiIiCRMEIiIiIiISMIEgYiIiIiIJEwQiIiIiIhIwgSBiIiIiIgkTBCIiIiIiEjCBIGIiIiIiCRMEIiIiIiISMIEgYiIiIiIJEwQiIiIiIhIwgSBiIiIiIgkTBCIiIiIiEjCBIGIiIiIiCRMEIiIiIiISMIEgYiIiIiIJEwQiIiIiIhIwgSBiIiIiIgkTBCIiIiIiEjCBIGIiIiIiCRMEIiIiIiISMIEgYiIiIiIJEwQiIiIiIhIwgSBiIiIiIgkTBCIiIiIiEjCBIGIiIiIiCRMEIiIiIiISMIEgYiIiIiIJEwQiIiIiIhIonB0AQBgy5Yt2LBhA/Lz8xEWFobRo0ejZcuWBo89efIkZs6cqbd93rx5aNq0KQBg586dWLBggd4xy5Ytg4uLS63uS0RERER0O3B4grB//34sXboUY8eORVxcHLZt24bZs2dj3rx5CAwMNHreRx99BA8PD+lnHx8fnf3u7u74+OOPdbZVTw5qe18iIiIioobM4V2MNm3ahMTERPTp00eqxQ8MDMTWrVtNnufr6ws/Pz/pP5lM96UIgqCz38/Pzyr3JSIiIiJqyBzagqBWq5Geno6BAwfqbG/Xrh1Onz5t8tzXX38dKpUKYWFhGDRoENq0aaOzv6ysDBMmTIBGo0GzZs0wbNgwREVF1fm+REREREQNmUMThMLCQmg0Gvj6+ups9/X1RX5+vsFz/P39MW7cOERHR0OtVmP37t343//+h+nTp6NVq1YAgNDQUEyYMAEREREoLS3FTz/9hGnTpuGDDz5ASEhIre4LACqVCiqVSvpZEAS4u7tL/7Ym7fWsfV3SxTjbB+NsP4y1fTDO9sNYE9mfw8cgAIb/6I19EISGhiI0NFT6OTY2Fjk5Odi4caOUIMTGxiI2NlY6Ji4uDm+88QY2b96MMWPG1Oq+ALBu3TqsWbNG+jkqKgrvv/8+GjdubOLV1U1wcLDNrk3/Ypztg3G2H8baPmwdZ1EU+WD8D76niezHoQmCj48PZDKZXq19QUGBXu2+KbGxsdizZ4/R/TKZDM2bN0dWVlad7puUlIT+/ftLP2s/tLOzs6FWq80urzkEQUBwcDCysrIgiqJVr03/Ypztg3G2H8baPmwZ55KKSny5PwN7zxdAXSlCIRcQH+WL8d1D4ekit+q96gNbxFqhUNi0co+ovnNogqBQKBAdHY3U1FTcdddd0vbU1FR06dLF7OucP39ebxBydaIo4uLFiwgPD6/TfZVKJZRKpdF72IIoivyStwPG2T4YZ/thrO3D2nEuqajEuFVncDG3DJpq25NTs3HkchEWDo29LZMEgO9pIntyeBej/v37Y/78+YiOjkZsbCy2bduGnJwc3HfffQCA5cuXIzc3F88//zwA4Mcff0Tjxo0RHh4OtVqNPXv24ODBg3jllVeka65evRoxMTEICQmRxiBcuHABTz31lNn3JSIisreFBzL0kgMA0IjAxbwyLDyQgUkJ4Q4pGxHdPhyeIHTv3h1FRUVITk5GXl4ewsPDMWXKFKnpLy8vDzk5OdLxarUa3333HXJzc+Hi4oLw8HBMnjwZd955p3RMSUkJFi5ciPz8fHh4eCAqKgozZ85EixYtzL4vERGRve1JL9RLDrQ0IrA3vRCTEuxaJCK6DQki2+vqLDs7W2d2I2sQBAEhISHIzMxkk6oNMc72wTjbD2NtH7aIsyiKGPD1CeSUGB/T1thTifVjWt9WA5dtEWulUskKQSITHL5QGhEREVU9CCtkpr+W5TLhtkoOiMgxmCAQERE5iZ7RPpAZef6XCVX7iYhsjQkCERGRkxjXLRSR/m56SYJMAJr5u2Fct1DDJxIRWZHDBykTERFRFU8XORYOjcXCAxnYm14ItUaEQiYgPtoH47rdnusgEJH9MUEgIiJyIp4uckxKCMekBK6kTESOwS5GRERETorJARE5AhMEIiIiIiKSMEEgIiIiIiIJEwQiIiIiIpIwQSAiIiIiIgkTBCIiIiIikjBBICIiIiIiCRMEIiIiIiKSMEEgIiIiIiIJEwQiIiIiIpIoHF2AhkChsF0YbXlt+hfjbB+Ms/0w1vbBONuPNWPN3xuRaYIoiqKjC0FERERERM6BXYycVGlpKd544w2UlpY6uigNGuNsH4yz/TDW9sE42w9jTWR/TBCclCiKOH/+PNjAY1uMs30wzvbDWNsH42w/jDWR/TFBICIiIiIiCRMEIiIiIiKSMEFwUkqlEkOGDIFSqXR0URo0xtk+GGf7Yaztg3G2H8aayP44ixEREREREUnYgkBERERERBImCEREREREJGGCQEREREREEiYIREREREQkUTi6AKRvy5Yt2LBhA/Lz8xEWFobRo0ejZcuWji5WvZGWloYNGzbg/PnzyMvLw6uvvoq77rpL2i+KIlavXo1ff/0VxcXFiImJwVNPPYXw8HDpGJVKhe+++w779u1DRUUF2rRpg7Fjx6JRo0aOeElOad26dTh06BCuXr0KFxcXxMbG4rHHHkNoaKh0DGNtHVu3bsXWrVuRnZ0NAAgLC8OQIUPQsWNHAIyzraxbtw4//PADHnzwQYwePRoAY20Nq1atwpo1a3S2+fr6YtGiRQAYYyJnwBYEJ7N//34sXboUgwYNwvvvv4+WLVti9uzZyMnJcXTR6o3y8nI0a9YMY8aMMbg/JSUFP/74I8aMGYN3330Xfn5+ePvtt1FaWiods3TpUhw6dAgvvfQSZs2ahbKyMrz33nvQaDT2ehlOLy0tDf369cM777yDqVOnQqPR4O2330ZZWZl0DGNtHQEBARg5ciTeffddvPvuu2jTpg3mzJmDy5cvA2CcbeHs2bPYtm0bIiMjdbYz1tYRHh6OhQsXSv/93//9n7SPMSZyAiI5lSlTpogLFy7U2TZx4kTx+++/d1CJ6rdHHnlEPHjwoPSzRqMRn376aXHdunXStoqKCnHUqFHi1q1bRVEUxZKSEnH48OHivn37pGNu3LghDh06VPzzzz/tVfR6p6CgQHzkkUfEkydPiqLIWNva6NGjxV9//ZVxtoHS0lLxxRdfFI8dOyZOnz5dXLJkiSiKfE9by8qVK8VXX33V4D7GmMg5sAXBiajVaqSnp6N9+/Y629u1a4fTp087qFQNy/Xr15Gfn68TY6VSiVatWkkxTk9PR2VlJdq1aycdExAQgIiICJw5c8buZa4vbt68CQDw8vICwFjbikajwb59+1BeXo7Y2FjG2Qa++uordOzYUSdeAN/T1pSVlYXx48fjueeew0cffYRr164BYIyJnAXHIDiRwsJCaDQa+Pr66mz39fVFfn6+YwrVwGjjaCjG2m5c+fn5UCgU0oNu9WP4ezBMFEV88803uOOOOxAREQGAsba2S5cu4c0334RKpYKbmxteffVVhIWFSQ9NjLN17Nu3D+fPn8e7776rt4/vaeuIiYnBc889h9DQUOTn52Pt2rWYOnUq5s6dyxgTOQkmCE5IEASztlHt3RpP0YwFxc055na1ePFiXLp0CbNmzdLbx1hbR2hoKD744AOUlJTg4MGD+OyzzzBz5kxpP+Ncdzk5OVi6dCnefPNNuLi4GD2Osa4b7eB6AIiIiEBsbCxeeOEF7Nq1CzExMQAYYyJHYxcjJ+Lj4wOZTKZXA1JQUKBXm0K14+fnBwB6MS4sLJRi7OfnB7VajeLiYr1jtOfTv77++mv8/vvvmD59us4MIoy1dSkUCgQHB6N58+YYOXIkmjVrhp9++olxtqL09HQUFBRg8uTJGD58OIYPH460tDRs3rwZw4cPl+LJWFuXm5sbIiIikJmZyfczkZNgguBEFAoFoqOjkZqaqrM9NTUVcXFxDipVw9KkSRP4+fnpxFitViMtLU2KcXR0NORyuc4xeXl5uHTpEmJjY+1eZmcliiIWL16MgwcP4q233kKTJk109jPWtiWKIlQqFeNsRW3btsWHH36IOXPmSP81b94c8fHxmDNnDoKCghhrG1CpVLh69Sr8/f35fiZyEuxi5GT69++P+fPnIzo6GrGxsdi2bRtycnJw3333Obpo9UZZWRmysrKkn69fv44LFy7Ay8sLgYGBePDBB7Fu3TqEhIQgODgY69atg6urK+Lj4wEAHh4eSExMxHfffQdvb294eXnhu+++Q0REhN6gxdvZ4sWLsXfvXrz++utwd3eXavw8PDzg4uICQRAYaytZvnw5OnbsiEaNGqGsrAz79u3DyZMn8eabbzLOVuTu7i6NodFydXWFt7e3tJ2xrrtvv/0WnTt3RmBgIAoKCpCcnIzS0lIkJCTw/UzkJASRnfacjnahtLy8PISHh2PUqFFo1aqVo4tVb5w8eVKnb7ZWQkICnnvuOWkRnm3btqGkpAQtWrTAU089pfNgUFFRgWXLlmHv3r06i/AEBgba86U4taFDhxrcPmHCBPTq1QsAGGsr+fzzz3HixAnk5eXBw8MDkZGRGDBggPQwxDjbzowZM9CsWTO9hdIY69r76KOP8Ndff6GwsBA+Pj6IiYnB8OHDERYWBoAxJnIGTBCIiIiIiEjCMQhERERERCRhgkBERERERBImCEREREREJGGCQEREREREEiYIREREREQkYYJAREREREQSJghERERERCThSspE1CAZW8jtVtOnT0fr1q31ts+YMUPn/5aoy7lERESOxgSBiBqkt99+W+fn5ORknDx5Em+99ZbOdu3qrbcaO3aszcpGRETkzJggEFGDFBsbq/Ozj48PBEHQ236r8vJyuLq6Gk0ciIiIGjomCER025oxYwaKiorw1FNPYfny5bhw4QI6d+6MiRMnGuwmtHr1avz555/IzMyERqNBcHAw+vXrh969e0MQBMe8CCIiIitjgkBEt7W8vDzMnz8fAwYMwIgRI0w+6GdnZ+Pee+9FYGAgAODvv//G119/jdzcXAwZMsReRSYiIrIpJghEdFsrLi7Gyy+/jDZt2tR47IQJE6R/azQatG7dGqIoYvPmzRg8eDBbEYiIqEFggkBEtzVPT0+zkgMAOHHiBNatW4ezZ8+itLRUZ19BQQH8/PxsUEIiIiL7YoJARLc1f39/s447e/Ys3n77bbRu3Rrjx49Ho0aNoFAocPjwYaxduxYVFRU2LikREZF9MEEgotuaud2C9u3bB7lcjjfeeAMuLi7S9sOHD9uqaERERA7BlZSJiMwgCALkcjlksn8/NisqKrB7924HloqIiMj62IJARGSGO++8E5s2bcInn3yCe++9F0VFRdi4cSOUSqWji0ZERGRVbEEgIjJDmzZt8Oyzz+LSpUt4//33sWLFCnTt2hUDBgxwdNGIiIisShBFUXR0IYiIiIiIyDmwBYGIiIiIiCRMEIiIiIiISMIEgYiIiIiIJEwQiIiIiIhIwgSBiIiIiIgkTBCIiIiIiEjCBIGIiIiIiCRMEIiIiIiISMIEgYiIiIiIJEwQiIiIiIhI8v/t17EAAAAAwCB/632jKIsEAQAAmCAAAAAL78glze3eNLgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.743639</td>\n",
       "      <td>0.035142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>2.913570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>152.800000</td>\n",
       "      <td>2.973961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>3.119829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3.299832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.896335</td>\n",
       "      <td>0.022048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.804092</td>\n",
       "      <td>0.104836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.551885</td>\n",
       "      <td>0.093192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.969550</td>\n",
       "      <td>0.019660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.648577</td>\n",
       "      <td>0.076970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.888249</td>\n",
       "      <td>0.024021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.793859</td>\n",
       "      <td>0.044544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.760728</td>\n",
       "      <td>0.046563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.608453</td>\n",
       "      <td>0.085759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.910920</td>\n",
       "      <td>0.018086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.760728</td>\n",
       "      <td>0.046563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.743639     0.035142\n",
       "1                    TP        18.400000     2.913570\n",
       "2                    TN       152.800000     2.973961\n",
       "3                    FP         4.800000     3.119829\n",
       "4                    FN        15.000000     3.299832\n",
       "5              Accuracy         0.896335     0.022048\n",
       "6             Precision         0.804092     0.104836\n",
       "7           Sensitivity         0.551885     0.093192\n",
       "8           Specificity         0.969550     0.019660\n",
       "9              F1 score         0.648577     0.076970\n",
       "10  F1 score (weighted)         0.888249     0.024021\n",
       "11     F1 score (macro)         0.793859     0.044544\n",
       "12    Balanced Accuracy         0.760728     0.046563\n",
       "13                  MCC         0.608453     0.085759\n",
       "14                  NPV         0.910920     0.018086\n",
       "15              ROC_AUC         0.760728     0.046563"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.693937</td>\n",
       "      <td>0.727582</td>\n",
       "      <td>0.711964</td>\n",
       "      <td>0.721309</td>\n",
       "      <td>0.733245</td>\n",
       "      <td>0.752080</td>\n",
       "      <td>0.742097</td>\n",
       "      <td>0.705249</td>\n",
       "      <td>0.687504</td>\n",
       "      <td>0.722362</td>\n",
       "      <td>0.719733</td>\n",
       "      <td>0.020478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>35.400000</td>\n",
       "      <td>3.134042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>308.300000</td>\n",
       "      <td>2.057507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>1.897367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.700000</td>\n",
       "      <td>3.093003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.916230</td>\n",
       "      <td>0.908377</td>\n",
       "      <td>0.887435</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.899738</td>\n",
       "      <td>0.008374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.844026</td>\n",
       "      <td>0.035792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.469697</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.537313</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.432836</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.537313</td>\n",
       "      <td>0.527551</td>\n",
       "      <td>0.045989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>0.974600</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>0.984100</td>\n",
       "      <td>0.968400</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>0.980900</td>\n",
       "      <td>0.984100</td>\n",
       "      <td>0.977600</td>\n",
       "      <td>0.971400</td>\n",
       "      <td>0.979040</td>\n",
       "      <td>0.005999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.672566</td>\n",
       "      <td>0.660550</td>\n",
       "      <td>0.648148</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.574257</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.648000</td>\n",
       "      <td>0.038426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.879915</td>\n",
       "      <td>0.895703</td>\n",
       "      <td>0.893882</td>\n",
       "      <td>0.889751</td>\n",
       "      <td>0.887545</td>\n",
       "      <td>0.908455</td>\n",
       "      <td>0.900671</td>\n",
       "      <td>0.871847</td>\n",
       "      <td>0.885222</td>\n",
       "      <td>0.886770</td>\n",
       "      <td>0.889976</td>\n",
       "      <td>0.010396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.769957</td>\n",
       "      <td>0.807865</td>\n",
       "      <td>0.802031</td>\n",
       "      <td>0.795111</td>\n",
       "      <td>0.790754</td>\n",
       "      <td>0.827462</td>\n",
       "      <td>0.818251</td>\n",
       "      <td>0.754700</td>\n",
       "      <td>0.790754</td>\n",
       "      <td>0.790754</td>\n",
       "      <td>0.794764</td>\n",
       "      <td>0.021380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.725355</td>\n",
       "      <td>0.770884</td>\n",
       "      <td>0.759133</td>\n",
       "      <td>0.749391</td>\n",
       "      <td>0.756904</td>\n",
       "      <td>0.781550</td>\n",
       "      <td>0.777211</td>\n",
       "      <td>0.708481</td>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.754371</td>\n",
       "      <td>0.753297</td>\n",
       "      <td>0.022472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.576105</td>\n",
       "      <td>0.633078</td>\n",
       "      <td>0.630059</td>\n",
       "      <td>0.623142</td>\n",
       "      <td>0.596855</td>\n",
       "      <td>0.680513</td>\n",
       "      <td>0.657867</td>\n",
       "      <td>0.556879</td>\n",
       "      <td>0.607840</td>\n",
       "      <td>0.600163</td>\n",
       "      <td>0.616250</td>\n",
       "      <td>0.036856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.898600</td>\n",
       "      <td>0.913700</td>\n",
       "      <td>0.908800</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.910700</td>\n",
       "      <td>0.917600</td>\n",
       "      <td>0.913900</td>\n",
       "      <td>0.890800</td>\n",
       "      <td>0.902700</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.906830</td>\n",
       "      <td>0.008071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.725355</td>\n",
       "      <td>0.770884</td>\n",
       "      <td>0.759133</td>\n",
       "      <td>0.749391</td>\n",
       "      <td>0.756904</td>\n",
       "      <td>0.781550</td>\n",
       "      <td>0.777211</td>\n",
       "      <td>0.708481</td>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.754371</td>\n",
       "      <td>0.753297</td>\n",
       "      <td>0.022472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.693937    0.727582    0.711964    0.721309   \n",
       "1                    TP   31.000000   38.000000   36.000000   35.000000   \n",
       "2                    TN  310.000000  307.000000  309.000000  309.000000   \n",
       "3                    FP    6.000000    8.000000    6.000000    5.000000   \n",
       "4                    FN   35.000000   29.000000   31.000000   33.000000   \n",
       "5              Accuracy    0.892670    0.903141    0.903141    0.900524   \n",
       "6             Precision    0.837838    0.826087    0.857143    0.875000   \n",
       "7           Sensitivity    0.469697    0.567164    0.537313    0.514706   \n",
       "8           Specificity    0.981000    0.974600    0.981000    0.984100   \n",
       "9              F1 score    0.601942    0.672566    0.660550    0.648148   \n",
       "10  F1 score (weighted)    0.879915    0.895703    0.893882    0.889751   \n",
       "11     F1 score (macro)    0.769957    0.807865    0.802031    0.795111   \n",
       "12    Balanced Accuracy    0.725355    0.770884    0.759133    0.749391   \n",
       "13                  MCC    0.576105    0.633078    0.630059    0.623142   \n",
       "14                  NPV    0.898600    0.913700    0.908800    0.903500   \n",
       "15              ROC_AUC    0.725355    0.770884    0.759133    0.749391   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.733245    0.752080    0.742097    0.705249    0.687504    0.722362   \n",
       "1    36.000000   38.000000   39.000000   29.000000   36.000000   36.000000   \n",
       "2   306.000000  312.000000  308.000000  310.000000  306.000000  306.000000   \n",
       "3    10.000000    4.000000    6.000000    5.000000    7.000000    9.000000   \n",
       "4    30.000000   28.000000   29.000000   38.000000   33.000000   31.000000   \n",
       "5     0.895288    0.916230    0.908377    0.887435    0.895288    0.895288   \n",
       "6     0.782609    0.904762    0.866667    0.852941    0.837209    0.800000   \n",
       "7     0.545455    0.575758    0.573529    0.432836    0.521739    0.537313   \n",
       "8     0.968400    0.987300    0.980900    0.984100    0.977600    0.971400   \n",
       "9     0.642857    0.703704    0.690265    0.574257    0.642857    0.642857   \n",
       "10    0.887545    0.908455    0.900671    0.871847    0.885222    0.886770   \n",
       "11    0.790754    0.827462    0.818251    0.754700    0.790754    0.790754   \n",
       "12    0.756904    0.781550    0.777211    0.708481    0.749687    0.754371   \n",
       "13    0.596855    0.680513    0.657867    0.556879    0.607840    0.600163   \n",
       "14    0.910700    0.917600    0.913900    0.890800    0.902700    0.908000   \n",
       "15    0.756904    0.781550    0.777211    0.708481    0.749687    0.754371   \n",
       "\n",
       "           ave       std  \n",
       "0     0.719733  0.020478  \n",
       "1    35.400000  3.134042  \n",
       "2   308.300000  2.057507  \n",
       "3     6.600000  1.897367  \n",
       "4    31.700000  3.093003  \n",
       "5     0.899738  0.008374  \n",
       "6     0.844026  0.035792  \n",
       "7     0.527551  0.045989  \n",
       "8     0.979040  0.005999  \n",
       "9     0.648000  0.038426  \n",
       "10    0.889976  0.010396  \n",
       "11    0.794764  0.021380  \n",
       "12    0.753297  0.022472  \n",
       "13    0.616250  0.036856  \n",
       "14    0.906830  0.008071  \n",
       "15    0.753297  0.022472  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test['std'] = mat_met_xgb_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.742834</td>\n",
       "      <td>0.042351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.900419</td>\n",
       "      <td>0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.819497</td>\n",
       "      <td>0.083098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.554858</td>\n",
       "      <td>0.088047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.973598</td>\n",
       "      <td>0.012987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.658198</td>\n",
       "      <td>0.079042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.892125</td>\n",
       "      <td>0.023131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.799932</td>\n",
       "      <td>0.045042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.764226</td>\n",
       "      <td>0.045567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.620367</td>\n",
       "      <td>0.084828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.911928</td>\n",
       "      <td>0.016022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.764226</td>\n",
       "      <td>0.045567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.742834     0.042351\n",
       "1              Accuracy         0.900419     0.020250\n",
       "2             Precision         0.819497     0.083098\n",
       "3           Sensitivity         0.554858     0.088047\n",
       "4           Specificity         0.973598     0.012987\n",
       "5              F1 score         0.658198     0.079042\n",
       "6   F1 score (weighted)         0.892125     0.023131\n",
       "7      F1 score (macro)         0.799932     0.045042\n",
       "8     Balanced Accuracy         0.764226     0.045567\n",
       "9                   MCC         0.620367     0.084828\n",
       "10                  NPV         0.911928     0.016022\n",
       "11              ROC_AUC         0.764226     0.045567"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"rmse\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_xgb_cat = np.where(((y_pred_optimized_xgb >= 2) | (y_pred_optimized_xgb <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_xgb_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_xgb))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "xgb_run0 = data_xgb[['y_test_idx0', 'y_test0', 'y_pred_xgb0']]\n",
    "xgb_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "xgb_run0.reset_index(inplace=True, drop=True)\n",
    "xgb_run1 = data_xgb[['y_test_idx1', 'y_test1', 'y_pred_xgb1']]\n",
    "xgb_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "xgb_run1.reset_index(inplace=True, drop=True)\n",
    "xgb_run2 = data_xgb[['y_test_idx2', 'y_test2', 'y_pred_xgb2']]\n",
    "xgb_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "xgb_run2.reset_index(inplace=True, drop=True)\n",
    "xgb_run3 = data_xgb[['y_test_idx3', 'y_test3', 'y_pred_xgb3']]\n",
    "xgb_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "xgb_run3.reset_index(inplace=True, drop=True)\n",
    "xgb_run4 = data_xgb[['y_test_idx4', 'y_test4', 'y_pred_xgb4']]\n",
    "xgb_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "xgb_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "xgb_5preds = pd.concat([chembl_id,xgb_run0, xgb_run1, xgb_run2, xgb_run3, xgb_run4], axis=1)\n",
    "xgb_5preds = xgb_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_xgb0', 'y_pred_xgb1', 'y_pred_xgb2', 'y_pred_xgb3', 'y_pred_xgb4']]\n",
    "xgb_5preds['y_pred_xgb_ave'] = xgb_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "xgb_5preds['y_pred_xgb_std'] = xgb_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "\n",
    "mat_met_optimized_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "02aaad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b19aca7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt  \n",
    "# retrieve performance metrics\n",
    "results = optimized_xgb_0.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "    \n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "pyplot.show()\n",
    "\n",
    " # plot classification error\n",
    "#fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "#ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "#ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "#ax.legend()\n",
    "    \n",
    "#pyplot.ylabel('Classification Error')\n",
    "#pyplot.title('XGBoost Classification Error')\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost baseline model r2_score 0.6987 with a standard deviation of 0.0646\n",
      "XGBoost optimized model r2_score 0.7382 with a standard deviation of 0.0398\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_reg, X, Y, cv=10, scoring=\"r2\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_xgb_withSemiSel.joblib']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_reg, \"OUTPUT/xgb_reg_withSemiSel.joblib\")\n",
    "joblib.dump(optimizedCV_xgb, \"OUTPUT/optimizedCV_xgb_withSemiSel.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.695705     0.041277\n",
      "1                    TP        20.300000     2.496664\n",
      "2                    TN       152.700000     1.702939\n",
      "3                    FP         4.900000     1.728840\n",
      "4                    FN        13.100000     2.766867\n",
      "5              Accuracy         0.905759     0.015803\n",
      "6             Precision         0.807717     0.059895\n",
      "7           Sensitivity         0.608429     0.079052\n",
      "8           Specificity         0.968920     0.010900\n",
      "9              F1 score         0.691233     0.059250\n",
      "10  F1 score (weighted)         0.900057     0.018075\n",
      "11     F1 score (macro)         0.817799     0.034021\n",
      "12    Balanced Accuracy         0.788676     0.039138\n",
      "13                  MCC         0.647764     0.064140\n",
      "14                  NPV         0.921210     0.015509\n",
      "15              ROC_AUC         0.788676     0.039138\n",
      "CPU times: user 1.43 s, sys: 2.89 s, total: 4.32 s\n",
      "Wall time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    \n",
    "    knn_reg.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:24:00,907] A new study created in memory with name: KNNregressor\n",
      "[I 2023-12-12 04:24:01,080] Trial 0 finished with value: 0.608296411925902 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 64}. Best is trial 0 with value: 0.608296411925902.\n",
      "[I 2023-12-12 04:24:01,345] Trial 1 finished with value: 0.5795275096575174 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 0 with value: 0.608296411925902.\n",
      "[I 2023-12-12 04:24:01,518] Trial 2 finished with value: 0.5370403383257483 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 91}. Best is trial 0 with value: 0.608296411925902.\n",
      "[I 2023-12-12 04:24:01,692] Trial 3 finished with value: 0.5268870643283033 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 0 with value: 0.608296411925902.\n",
      "[I 2023-12-12 04:24:01,866] Trial 4 finished with value: 0.5652625066651128 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 59}. Best is trial 0 with value: 0.608296411925902.\n",
      "[I 2023-12-12 04:24:02,039] Trial 5 finished with value: 0.5781035265441605 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 38}. Best is trial 0 with value: 0.608296411925902.\n",
      "[I 2023-12-12 04:24:02,212] Trial 6 finished with value: 0.5609033173777224 and parameters: {'n_neighbors': 17, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 59}. Best is trial 0 with value: 0.608296411925902.\n",
      "[I 2023-12-12 04:24:02,479] Trial 7 finished with value: 0.6478781122393482 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 7 with value: 0.6478781122393482.\n",
      "[I 2023-12-12 04:24:02,651] Trial 8 finished with value: 0.5857313126216646 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 70}. Best is trial 7 with value: 0.6478781122393482.\n",
      "[I 2023-12-12 04:24:02,826] Trial 9 finished with value: 0.5268870643283033 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 7 with value: 0.6478781122393482.\n",
      "[I 2023-12-12 04:24:03,099] Trial 10 finished with value: 0.655139457544483 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 10 with value: 0.655139457544483.\n",
      "[I 2023-12-12 04:24:03,376] Trial 11 finished with value: 0.6653425265868093 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 11 with value: 0.6653425265868093.\n",
      "[I 2023-12-12 04:24:03,647] Trial 12 finished with value: 0.676985384097619 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 12 with value: 0.676985384097619.\n",
      "[I 2023-12-12 04:24:03,935] Trial 13 finished with value: 0.6841798943647579 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:04,223] Trial 14 finished with value: 0.6233614332699637 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:04,505] Trial 15 finished with value: 0.6233614332699637 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:04,762] Trial 16 finished with value: 0.6841798943647579 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:05,028] Trial 17 finished with value: 0.6841798943647579 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:05,296] Trial 18 finished with value: 0.6233614332699637 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:05,563] Trial 19 finished with value: 0.6432444382067772 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:05,741] Trial 20 finished with value: 0.6841798943647579 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 49}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:05,985] Trial 21 finished with value: 0.6841798943647579 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:06,274] Trial 22 finished with value: 0.5862888647104578 and parameters: {'n_neighbors': 14, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:06,565] Trial 23 finished with value: 0.6432444382067772 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:06,852] Trial 24 finished with value: 0.598555435015166 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:07,141] Trial 25 finished with value: 0.6653425265868093 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:07,431] Trial 26 finished with value: 0.6841798943647579 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:07,724] Trial 27 finished with value: 0.5331802062194264 and parameters: {'n_neighbors': 20, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:07,907] Trial 28 finished with value: 0.6432444382067772 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 65}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:08,172] Trial 29 finished with value: 0.6117149531611376 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:08,354] Trial 30 finished with value: 0.6653425265868093 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 66}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:08,536] Trial 31 finished with value: 0.6841798943647579 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 49}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:08,719] Trial 32 finished with value: 0.6841798943647579 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 45}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:08,902] Trial 33 finished with value: 0.655139457544483 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 38}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:09,085] Trial 34 finished with value: 0.631931830478664 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 53}. Best is trial 13 with value: 0.6841798943647579.\n",
      "[I 2023-12-12 04:24:09,268] Trial 35 finished with value: 0.6857663970176795 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 48}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:09,453] Trial 36 finished with value: 0.5593647323646054 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:09,637] Trial 37 finished with value: 0.6857663970176795 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:09,820] Trial 38 finished with value: 0.6857663970176795 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:10,004] Trial 39 finished with value: 0.6857663970176795 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 21}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:10,188] Trial 40 finished with value: 0.6797076908066533 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:10,372] Trial 41 finished with value: 0.6857663970176795 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:10,555] Trial 42 finished with value: 0.6634775743141601 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:10,739] Trial 43 finished with value: 0.6857663970176795 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:10,922] Trial 44 finished with value: 0.6376597841582216 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:11,107] Trial 45 finished with value: 0.5926599199704625 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:11,292] Trial 46 finished with value: 0.5424736510339361 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:11,476] Trial 47 finished with value: 0.6857663970176795 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:11,659] Trial 48 finished with value: 0.6708620222398212 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:11,843] Trial 49 finished with value: 0.617176647451153 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 35 with value: 0.6857663970176795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6858\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 48\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNregressor\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.686112\n",
      "1                    TP   35.000000\n",
      "2                    TN  308.000000\n",
      "3                    FP    8.000000\n",
      "4                    FN   31.000000\n",
      "5              Accuracy    0.897906\n",
      "6             Precision    0.813953\n",
      "7           Sensitivity    0.530303\n",
      "8           Specificity    0.974700\n",
      "9              F1 score    0.642202\n",
      "10  F1 score (weighted)    0.888927\n",
      "11     F1 score (macro)    0.791330\n",
      "12    Balanced Accuracy    0.752493\n",
      "13                  MCC    0.604034\n",
      "14                  NPV    0.908600\n",
      "15              ROC_AUC    0.752493\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_knn_0)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_knn_0_cat = np.where(((y_pred_knn_0 >= 2) | (y_pred_knn_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_knn_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:24:12,063] Trial 50 finished with value: 0.6492104650724556 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:12,242] Trial 51 finished with value: 0.6714087377783267 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:12,421] Trial 52 finished with value: 0.6811408323628914 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:12,600] Trial 53 finished with value: 0.6811408323628914 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:12,779] Trial 54 finished with value: 0.6653834549465555 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:12,958] Trial 55 finished with value: 0.6653834549465555 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:13,137] Trial 56 finished with value: 0.6811408323628914 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:13,317] Trial 57 finished with value: 0.6341720437543771 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:13,496] Trial 58 finished with value: 0.6492104650724556 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:13,675] Trial 59 finished with value: 0.6714087377783267 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:13,855] Trial 60 finished with value: 0.6572212369272424 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 61}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:14,034] Trial 61 finished with value: 0.6811408323628914 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:14,213] Trial 62 finished with value: 0.6714087377783267 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:14,392] Trial 63 finished with value: 0.6653834549465555 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:14,572] Trial 64 finished with value: 0.6714087377783267 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:14,752] Trial 65 finished with value: 0.6811408323628914 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:14,932] Trial 66 finished with value: 0.6412938868486847 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:15,113] Trial 67 finished with value: 0.6572212369272424 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:15,293] Trial 68 finished with value: 0.6653834549465555 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:15,473] Trial 69 finished with value: 0.6811408323628914 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:15,653] Trial 70 finished with value: 0.6341720437543771 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:15,833] Trial 71 finished with value: 0.6846221091080682 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 87}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:16,004] Trial 72 finished with value: 0.6846221091080682 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 93}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:16,171] Trial 73 finished with value: 0.6714087377783267 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 87}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:16,343] Trial 74 finished with value: 0.6811408323628914 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 80}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:16,515] Trial 75 finished with value: 0.5537759247180677 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 68}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:16,683] Trial 76 finished with value: 0.6653834549465555 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 76}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:16,852] Trial 77 finished with value: 0.6572212369272424 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:17,022] Trial 78 finished with value: 0.6846221091080682 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:17,189] Trial 79 finished with value: 0.6714087377783267 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 62}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:17,360] Trial 80 finished with value: 0.6492104650724556 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:17,532] Trial 81 finished with value: 0.6846221091080682 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:17,704] Trial 82 finished with value: 0.6811408323628914 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 91}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:17,876] Trial 83 finished with value: 0.6846221091080682 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:18,046] Trial 84 finished with value: 0.6714087377783267 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 91}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:18,217] Trial 85 finished with value: 0.6811408323628914 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:18,384] Trial 86 finished with value: 0.6653834549465555 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 57}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:18,555] Trial 87 finished with value: 0.6846221091080682 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 87}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:18,727] Trial 88 finished with value: 0.6714087377783267 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:18,899] Trial 89 finished with value: 0.6572212369272424 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:19,071] Trial 90 finished with value: 0.6653834549465555 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:19,242] Trial 91 finished with value: 0.6846221091080682 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:19,414] Trial 92 finished with value: 0.6811408323628914 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:19,587] Trial 93 finished with value: 0.6846221091080682 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 21}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:19,760] Trial 94 finished with value: 0.6714087377783267 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:19,933] Trial 95 finished with value: 0.6811408323628914 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 52}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:20,106] Trial 96 finished with value: 0.5873177084575941 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:20,279] Trial 97 finished with value: 0.6846221091080682 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:20,452] Trial 98 finished with value: 0.5321086554032719 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:20,624] Trial 99 finished with value: 0.6811408323628914 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 36}. Best is trial 35 with value: 0.6857663970176795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6858\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 48\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.686112    0.708065\n",
      "1                    TP   35.000000   41.000000\n",
      "2                    TN  308.000000  303.000000\n",
      "3                    FP    8.000000   12.000000\n",
      "4                    FN   31.000000   26.000000\n",
      "5              Accuracy    0.897906    0.900524\n",
      "6             Precision    0.813953    0.773585\n",
      "7           Sensitivity    0.530303    0.611940\n",
      "8           Specificity    0.974700    0.961900\n",
      "9              F1 score    0.642202    0.683333\n",
      "10  F1 score (weighted)    0.888927    0.895802\n",
      "11     F1 score (macro)    0.791330    0.812164\n",
      "12    Balanced Accuracy    0.752493    0.786923\n",
      "13                  MCC    0.604034    0.631323\n",
      "14                  NPV    0.908600    0.921000\n",
      "15              ROC_AUC    0.752493    0.786923\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_knn_1)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet1_cat = np.where(((Y_testSet1>=2) | (Y_testSet1<=-2)), 1, 0) \n",
    "y_pred_knn_1_cat = np.where(((y_pred_knn_1 >= 2) | (y_pred_knn_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_knn_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:24:20,847] Trial 100 finished with value: 0.6693023095293273 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:21,025] Trial 101 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:21,202] Trial 102 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:21,380] Trial 103 finished with value: 0.673436194439127 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 93}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:21,557] Trial 104 finished with value: 0.6793840112695253 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:21,734] Trial 105 finished with value: 0.673436194439127 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 89}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:21,911] Trial 106 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 21}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:22,087] Trial 107 finished with value: 0.6793840112695253 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:22,263] Trial 108 finished with value: 0.6693023095293273 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:22,435] Trial 109 finished with value: 0.660709519943955 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:22,612] Trial 110 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 81}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:22,788] Trial 111 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 79}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:22,965] Trial 112 finished with value: 0.6793840112695253 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 81}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:23,142] Trial 113 finished with value: 0.673436194439127 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 21}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:23,320] Trial 114 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 82}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:23,497] Trial 115 finished with value: 0.6793840112695253 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:23,776] Trial 116 finished with value: 0.6811641264660537 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:23,955] Trial 117 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 78}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:24,141] Trial 118 finished with value: 0.673436194439127 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 75}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:24,327] Trial 119 finished with value: 0.6793840112695253 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:24,512] Trial 120 finished with value: 0.673436194439127 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:24,697] Trial 121 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:24,884] Trial 122 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 85}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:25,070] Trial 123 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:25,256] Trial 124 finished with value: 0.6181613268984275 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 81}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:25,443] Trial 125 finished with value: 0.6793840112695253 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 83}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:25,630] Trial 126 finished with value: 0.571315464205695 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 77}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:25,816] Trial 127 finished with value: 0.673436194439127 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:26,002] Trial 128 finished with value: 0.6793840112695253 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 66}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:26,188] Trial 129 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 63}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:26,373] Trial 130 finished with value: 0.660709519943955 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 84}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:26,560] Trial 131 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 78}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:26,747] Trial 132 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:26,935] Trial 133 finished with value: 0.5222088853472807 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 82}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:27,121] Trial 134 finished with value: 0.6793840112695253 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 79}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:27,307] Trial 135 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:27,494] Trial 136 finished with value: 0.6693023095293273 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:27,761] Trial 137 finished with value: 0.6830978463030732 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:27,941] Trial 138 finished with value: 0.6793840112695253 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:28,127] Trial 139 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 56}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:28,313] Trial 140 finished with value: 0.6535346898999128 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:28,501] Trial 141 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:28,688] Trial 142 finished with value: 0.6793840112695253 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:28,875] Trial 143 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:29,063] Trial 144 finished with value: 0.6793840112695253 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:29,250] Trial 145 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:29,437] Trial 146 finished with value: 0.673436194439127 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 74}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:29,624] Trial 147 finished with value: 0.6793840112695253 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 59}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:29,811] Trial 148 finished with value: 0.6851440505964455 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:29,999] Trial 149 finished with value: 0.6693023095293273 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 85}. Best is trial 35 with value: 0.6857663970176795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6858\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 7\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 48\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.686112    0.708065    0.676224\n",
      "1                    TP   35.000000   41.000000   38.000000\n",
      "2                    TN  308.000000  303.000000  305.000000\n",
      "3                    FP    8.000000   12.000000   10.000000\n",
      "4                    FN   31.000000   26.000000   29.000000\n",
      "5              Accuracy    0.897906    0.900524    0.897906\n",
      "6             Precision    0.813953    0.773585    0.791667\n",
      "7           Sensitivity    0.530303    0.611940    0.567164\n",
      "8           Specificity    0.974700    0.961900    0.968300\n",
      "9              F1 score    0.642202    0.683333    0.660870\n",
      "10  F1 score (weighted)    0.888927    0.895802    0.890966\n",
      "11     F1 score (macro)    0.791330    0.812164    0.800389\n",
      "12    Balanced Accuracy    0.752493    0.786923    0.767709\n",
      "13                  MCC    0.604034    0.631323    0.614316\n",
      "14                  NPV    0.908600    0.921000    0.913200\n",
      "15              ROC_AUC    0.752493    0.786923    0.767709\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_knn_2)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet2_cat = np.where(((Y_testSet2>=2) | (Y_testSet2<=-2)), 1, 0) \n",
    "y_pred_knn_2_cat = np.where(((y_pred_knn_2 >= 2) | (y_pred_knn_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_knn_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:24:30,235] Trial 150 finished with value: 0.6834646991295641 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 35 with value: 0.6857663970176795.\n",
      "[I 2023-12-12 04:24:30,422] Trial 151 finished with value: 0.6892591466250312 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 85}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:30,609] Trial 152 finished with value: 0.6892591466250312 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 76}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:30,796] Trial 153 finished with value: 0.6834646991295641 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 80}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:30,983] Trial 154 finished with value: 0.6892591466250312 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 78}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:31,171] Trial 155 finished with value: 0.6756267979517936 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 76}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:31,359] Trial 156 finished with value: 0.6892591466250312 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 89}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:31,547] Trial 157 finished with value: 0.6834646991295641 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 85}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:31,735] Trial 158 finished with value: 0.6687774595735108 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 90}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:31,922] Trial 159 finished with value: 0.6756267979517936 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:32,110] Trial 160 finished with value: 0.6892591466250312 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 86}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:32,298] Trial 161 finished with value: 0.6892591466250312 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 88}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:32,487] Trial 162 finished with value: 0.6834646991295641 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 90}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:32,675] Trial 163 finished with value: 0.6892591466250312 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 86}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:32,864] Trial 164 finished with value: 0.6834646991295641 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 87}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:33,052] Trial 165 finished with value: 0.6892591466250312 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 87}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:33,240] Trial 166 finished with value: 0.6834646991295641 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 88}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:33,429] Trial 167 finished with value: 0.6756267979517936 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 89}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:33,617] Trial 168 finished with value: 0.674529825397699 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 92}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:33,897] Trial 169 finished with value: 0.6859082787086288 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:34,168] Trial 170 finished with value: 0.6859082787086288 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:34,439] Trial 171 finished with value: 0.6859082787086288 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:34,713] Trial 172 finished with value: 0.6809975028576668 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:35,007] Trial 173 finished with value: 0.6859082787086288 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:35,302] Trial 174 finished with value: 0.6859082787086288 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:35,598] Trial 175 finished with value: 0.6805367599421835 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:35,891] Trial 176 finished with value: 0.6859082787086288 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:36,186] Trial 177 finished with value: 0.6859082787086288 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:36,484] Trial 178 finished with value: 0.6809975028576668 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:36,788] Trial 179 finished with value: 0.6809975028576668 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:37,111] Trial 180 finished with value: 0.6859082787086288 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:37,402] Trial 181 finished with value: 0.6859082787086288 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:37,673] Trial 182 finished with value: 0.6859082787086288 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:37,946] Trial 183 finished with value: 0.6859082787086288 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:38,253] Trial 184 finished with value: 0.6859082787086288 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:38,558] Trial 185 finished with value: 0.6859082787086288 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 151 with value: 0.6892591466250312.\n",
      "[I 2023-12-12 04:24:38,853] Trial 186 finished with value: 0.6920912338492775 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 186 with value: 0.6920912338492775.\n",
      "[I 2023-12-12 04:24:39,150] Trial 187 finished with value: 0.6920912338492775 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 186 with value: 0.6920912338492775.\n",
      "[I 2023-12-12 04:24:39,446] Trial 188 finished with value: 0.6920912338492775 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 186 with value: 0.6920912338492775.\n",
      "[I 2023-12-12 04:24:39,742] Trial 189 finished with value: 0.6920912338492775 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 186 with value: 0.6920912338492775.\n",
      "[I 2023-12-12 04:24:40,037] Trial 190 finished with value: 0.6920912338492775 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 186 with value: 0.6920912338492775.\n",
      "[I 2023-12-12 04:24:40,333] Trial 191 finished with value: 0.6920912338492775 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 186 with value: 0.6920912338492775.\n",
      "[I 2023-12-12 04:24:40,620] Trial 192 finished with value: 0.6920912338492775 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 186 with value: 0.6920912338492775.\n",
      "[I 2023-12-12 04:24:40,916] Trial 193 finished with value: 0.6920912338492775 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 186 with value: 0.6920912338492775.\n",
      "[I 2023-12-12 04:24:41,212] Trial 194 finished with value: 0.6920912338492775 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 186 with value: 0.6920912338492775.\n",
      "[I 2023-12-12 04:24:41,508] Trial 195 finished with value: 0.6920912338492775 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 186 with value: 0.6920912338492775.\n",
      "[I 2023-12-12 04:24:41,803] Trial 196 finished with value: 0.6920912338492775 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 186 with value: 0.6920912338492775.\n",
      "[I 2023-12-12 04:24:42,099] Trial 197 finished with value: 0.6920912338492775 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 186 with value: 0.6920912338492775.\n",
      "[I 2023-12-12 04:24:42,395] Trial 198 finished with value: 0.6920912338492775 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 186 with value: 0.6920912338492775.\n",
      "[I 2023-12-12 04:24:42,690] Trial 199 finished with value: 0.6666827742289918 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 186 with value: 0.6920912338492775.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6921\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 83\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.686112    0.708065    0.676224    0.691604\n",
      "1                    TP   35.000000   41.000000   38.000000   38.000000\n",
      "2                    TN  308.000000  303.000000  305.000000  308.000000\n",
      "3                    FP    8.000000   12.000000   10.000000    6.000000\n",
      "4                    FN   31.000000   26.000000   29.000000   30.000000\n",
      "5              Accuracy    0.897906    0.900524    0.897906    0.905759\n",
      "6             Precision    0.813953    0.773585    0.791667    0.863636\n",
      "7           Sensitivity    0.530303    0.611940    0.567164    0.558824\n",
      "8           Specificity    0.974700    0.961900    0.968300    0.980900\n",
      "9              F1 score    0.642202    0.683333    0.660870    0.678571\n",
      "10  F1 score (weighted)    0.888927    0.895802    0.890966    0.897396\n",
      "11     F1 score (macro)    0.791330    0.812164    0.800389    0.811678\n",
      "12    Balanced Accuracy    0.752493    0.786923    0.767709    0.769858\n",
      "13                  MCC    0.604034    0.631323    0.614316    0.646695\n",
      "14                  NPV    0.908600    0.921000    0.913200    0.911200\n",
      "15              ROC_AUC    0.752493    0.786923    0.767709    0.769858\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_knn_3)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet3_cat = np.where(((Y_testSet3>=2) | (Y_testSet3<=-2)), 1, 0) \n",
    "y_pred_knn_3_cat = np.where(((y_pred_knn_3 >= 2) | (y_pred_knn_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_knn_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:24:43,068] Trial 200 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:43,352] Trial 201 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:43,664] Trial 202 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:43,959] Trial 203 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:44,254] Trial 204 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:44,548] Trial 205 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:44,843] Trial 206 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:45,140] Trial 207 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:45,439] Trial 208 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:45,739] Trial 209 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:46,035] Trial 210 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:46,329] Trial 211 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:46,629] Trial 212 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:46,928] Trial 213 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:47,228] Trial 214 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:47,530] Trial 215 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:47,817] Trial 216 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:48,111] Trial 217 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:48,405] Trial 218 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:48,700] Trial 219 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:48,995] Trial 220 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:49,291] Trial 221 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:49,586] Trial 222 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:49,881] Trial 223 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:50,179] Trial 224 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:50,479] Trial 225 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:50,789] Trial 226 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:51,082] Trial 227 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:51,377] Trial 228 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:51,661] Trial 229 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:51,956] Trial 230 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:52,253] Trial 231 finished with value: 0.6225312645965875 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:52,573] Trial 232 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:52,857] Trial 233 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:53,153] Trial 234 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:53,448] Trial 235 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:53,744] Trial 236 finished with value: 0.6954795365491225 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:54,040] Trial 237 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:54,334] Trial 238 finished with value: 0.6929604796065849 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:54,630] Trial 239 finished with value: 0.5873635423504154 and parameters: {'n_neighbors': 14, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:54,926] Trial 240 finished with value: 0.6954795365491225 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:55,222] Trial 241 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:55,518] Trial 242 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:55,813] Trial 243 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:56,109] Trial 244 finished with value: 0.6929604796065849 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:56,405] Trial 245 finished with value: 0.6954795365491225 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:56,700] Trial 246 finished with value: 0.6929604796065849 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:56,994] Trial 247 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:57,289] Trial 248 finished with value: 0.6966794648849804 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:57,585] Trial 249 finished with value: 0.6929604796065849 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6966794648849804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6967\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 95\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686112    0.708065    0.676224    0.691604   \n",
      "1                    TP   35.000000   41.000000   38.000000   38.000000   \n",
      "2                    TN  308.000000  303.000000  305.000000  308.000000   \n",
      "3                    FP    8.000000   12.000000   10.000000    6.000000   \n",
      "4                    FN   31.000000   26.000000   29.000000   30.000000   \n",
      "5              Accuracy    0.897906    0.900524    0.897906    0.905759   \n",
      "6             Precision    0.813953    0.773585    0.791667    0.863636   \n",
      "7           Sensitivity    0.530303    0.611940    0.567164    0.558824   \n",
      "8           Specificity    0.974700    0.961900    0.968300    0.980900   \n",
      "9              F1 score    0.642202    0.683333    0.660870    0.678571   \n",
      "10  F1 score (weighted)    0.888927    0.895802    0.890966    0.897396   \n",
      "11     F1 score (macro)    0.791330    0.812164    0.800389    0.811678   \n",
      "12    Balanced Accuracy    0.752493    0.786923    0.767709    0.769858   \n",
      "13                  MCC    0.604034    0.631323    0.614316    0.646695   \n",
      "14                  NPV    0.908600    0.921000    0.913200    0.911200   \n",
      "15              ROC_AUC    0.752493    0.786923    0.767709    0.769858   \n",
      "\n",
      "          Set4  \n",
      "0     0.707325  \n",
      "1    36.000000  \n",
      "2   310.000000  \n",
      "3     6.000000  \n",
      "4    30.000000  \n",
      "5     0.905759  \n",
      "6     0.857143  \n",
      "7     0.545455  \n",
      "8     0.981000  \n",
      "9     0.666667  \n",
      "10    0.897012  \n",
      "11    0.805894  \n",
      "12    0.763234  \n",
      "13    0.636243  \n",
      "14    0.911800  \n",
      "15    0.763234  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_knn_4)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet4_cat = np.where(((Y_testSet4>=2) | (Y_testSet4<=-2)), 1, 0) \n",
    "y_pred_knn_4_cat = np.where(((y_pred_knn_4 >= 2) | (y_pred_knn_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_knn_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:24:57,964] Trial 250 finished with value: 0.693388883442161 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:58,254] Trial 251 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:58,544] Trial 252 finished with value: 0.68749113119697 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:58,837] Trial 253 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:59,135] Trial 254 finished with value: 0.693388883442161 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:59,421] Trial 255 finished with value: 0.6292004224656699 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:24:59,715] Trial 256 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:00,008] Trial 257 finished with value: 0.68749113119697 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:00,298] Trial 258 finished with value: 0.693388883442161 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:00,599] Trial 259 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:00,891] Trial 260 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:01,185] Trial 261 finished with value: 0.693388883442161 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:01,478] Trial 262 finished with value: 0.68749113119697 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:01,774] Trial 263 finished with value: 0.6123029365535714 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:02,065] Trial 264 finished with value: 0.6500219571064261 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:02,359] Trial 265 finished with value: 0.693388883442161 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:02,652] Trial 266 finished with value: 0.68749113119697 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:02,949] Trial 267 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:03,240] Trial 268 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:03,529] Trial 269 finished with value: 0.693388883442161 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:03,838] Trial 270 finished with value: 0.68749113119697 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:04,105] Trial 271 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:04,374] Trial 272 finished with value: 0.5854231985510978 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:04,642] Trial 273 finished with value: 0.693388883442161 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:04,909] Trial 274 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:05,178] Trial 275 finished with value: 0.68749113119697 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:05,356] Trial 276 finished with value: 0.6766719645475183 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 94}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:05,623] Trial 277 finished with value: 0.68749113119697 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:05,900] Trial 278 finished with value: 0.6391222992268496 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:06,178] Trial 279 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:06,465] Trial 280 finished with value: 0.693388883442161 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:06,765] Trial 281 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:07,047] Trial 282 finished with value: 0.68749113119697 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:07,337] Trial 283 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:07,623] Trial 284 finished with value: 0.693388883442161 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:07,888] Trial 285 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:08,155] Trial 286 finished with value: 0.6441815785561841 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:08,441] Trial 287 finished with value: 0.693388883442161 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:08,720] Trial 288 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:08,986] Trial 289 finished with value: 0.68749113119697 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:09,252] Trial 290 finished with value: 0.693388883442161 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:09,519] Trial 291 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:09,795] Trial 292 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:10,087] Trial 293 finished with value: 0.68749113119697 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:10,365] Trial 294 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:10,652] Trial 295 finished with value: 0.693388883442161 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:10,919] Trial 296 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:11,178] Trial 297 finished with value: 0.6730430618392026 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:11,452] Trial 298 finished with value: 0.693388883442161 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:11,733] Trial 299 finished with value: 0.6895112460018012 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 200 with value: 0.6966794648849804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6967\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 95\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686112    0.708065    0.676224    0.691604   \n",
      "1                    TP   35.000000   41.000000   38.000000   38.000000   \n",
      "2                    TN  308.000000  303.000000  305.000000  308.000000   \n",
      "3                    FP    8.000000   12.000000   10.000000    6.000000   \n",
      "4                    FN   31.000000   26.000000   29.000000   30.000000   \n",
      "5              Accuracy    0.897906    0.900524    0.897906    0.905759   \n",
      "6             Precision    0.813953    0.773585    0.791667    0.863636   \n",
      "7           Sensitivity    0.530303    0.611940    0.567164    0.558824   \n",
      "8           Specificity    0.974700    0.961900    0.968300    0.980900   \n",
      "9              F1 score    0.642202    0.683333    0.660870    0.678571   \n",
      "10  F1 score (weighted)    0.888927    0.895802    0.890966    0.897396   \n",
      "11     F1 score (macro)    0.791330    0.812164    0.800389    0.811678   \n",
      "12    Balanced Accuracy    0.752493    0.786923    0.767709    0.769858   \n",
      "13                  MCC    0.604034    0.631323    0.614316    0.646695   \n",
      "14                  NPV    0.908600    0.921000    0.913200    0.911200   \n",
      "15              ROC_AUC    0.752493    0.786923    0.767709    0.769858   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.707325    0.746283  \n",
      "1    36.000000   38.000000  \n",
      "2   310.000000  310.000000  \n",
      "3     6.000000    6.000000  \n",
      "4    30.000000   28.000000  \n",
      "5     0.905759    0.910995  \n",
      "6     0.857143    0.863636  \n",
      "7     0.545455    0.575758  \n",
      "8     0.981000    0.981000  \n",
      "9     0.666667    0.690909  \n",
      "10    0.897012    0.903591  \n",
      "11    0.805894    0.819461  \n",
      "12    0.763234    0.778385  \n",
      "13    0.636243    0.659336  \n",
      "14    0.911800    0.917200  \n",
      "15    0.763234    0.778385  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_knn_5)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet5_cat = np.where(((Y_testSet5>=2) | (Y_testSet5<=-2)), 1, 0) \n",
    "y_pred_knn_5_cat = np.where(((y_pred_knn_5 >= 2) | (y_pred_knn_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_knn_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:25:12,091] Trial 300 finished with value: 0.6877460051987053 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:12,363] Trial 301 finished with value: 0.6937498194858222 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:12,638] Trial 302 finished with value: 0.6665525993075837 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 200 with value: 0.6966794648849804.\n",
      "[I 2023-12-12 04:25:12,908] Trial 303 finished with value: 0.6969358291068557 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 303 with value: 0.6969358291068557.\n",
      "[I 2023-12-12 04:25:13,203] Trial 304 finished with value: 0.6969358291068557 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 303 with value: 0.6969358291068557.\n",
      "[I 2023-12-12 04:25:13,500] Trial 305 finished with value: 0.6969358291068557 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 303 with value: 0.6969358291068557.\n",
      "[I 2023-12-12 04:25:13,797] Trial 306 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:14,094] Trial 307 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:14,391] Trial 308 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:14,680] Trial 309 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:14,951] Trial 310 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:15,222] Trial 311 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:15,503] Trial 312 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:15,798] Trial 313 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:16,095] Trial 314 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:16,392] Trial 315 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:16,685] Trial 316 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:16,982] Trial 317 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:17,279] Trial 318 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:17,461] Trial 319 finished with value: 0.6872633825966901 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:17,741] Trial 320 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:18,012] Trial 321 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:18,301] Trial 322 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:18,571] Trial 323 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:18,842] Trial 324 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:19,113] Trial 325 finished with value: 0.6665525993075837 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:19,397] Trial 326 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:19,673] Trial 327 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:19,970] Trial 328 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:20,266] Trial 329 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:20,564] Trial 330 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:20,870] Trial 331 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:21,167] Trial 332 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:21,462] Trial 333 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:21,760] Trial 334 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:22,067] Trial 335 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:22,384] Trial 336 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:22,683] Trial 337 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:22,956] Trial 338 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:23,227] Trial 339 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:23,498] Trial 340 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:23,791] Trial 341 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:23,974] Trial 342 finished with value: 0.6872633825966901 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:24,244] Trial 343 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:24,529] Trial 344 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:24,809] Trial 345 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:25,093] Trial 346 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:25,396] Trial 347 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:25,690] Trial 348 finished with value: 0.6665525993075837 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 306 with value: 0.6975294518047729.\n",
      "[I 2023-12-12 04:25:25,941] Trial 349 finished with value: 0.6975294518047729 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 306 with value: 0.6975294518047729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6975\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686112    0.708065    0.676224    0.691604   \n",
      "1                    TP   35.000000   41.000000   38.000000   38.000000   \n",
      "2                    TN  308.000000  303.000000  305.000000  308.000000   \n",
      "3                    FP    8.000000   12.000000   10.000000    6.000000   \n",
      "4                    FN   31.000000   26.000000   29.000000   30.000000   \n",
      "5              Accuracy    0.897906    0.900524    0.897906    0.905759   \n",
      "6             Precision    0.813953    0.773585    0.791667    0.863636   \n",
      "7           Sensitivity    0.530303    0.611940    0.567164    0.558824   \n",
      "8           Specificity    0.974700    0.961900    0.968300    0.980900   \n",
      "9              F1 score    0.642202    0.683333    0.660870    0.678571   \n",
      "10  F1 score (weighted)    0.888927    0.895802    0.890966    0.897396   \n",
      "11     F1 score (macro)    0.791330    0.812164    0.800389    0.811678   \n",
      "12    Balanced Accuracy    0.752493    0.786923    0.767709    0.769858   \n",
      "13                  MCC    0.604034    0.631323    0.614316    0.646695   \n",
      "14                  NPV    0.908600    0.921000    0.913200    0.911200   \n",
      "15              ROC_AUC    0.752493    0.786923    0.767709    0.769858   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.707325    0.746283    0.675420  \n",
      "1    36.000000   38.000000   41.000000  \n",
      "2   310.000000  310.000000  300.000000  \n",
      "3     6.000000    6.000000   14.000000  \n",
      "4    30.000000   28.000000   27.000000  \n",
      "5     0.905759    0.910995    0.892670  \n",
      "6     0.857143    0.863636    0.745455  \n",
      "7     0.545455    0.575758    0.602941  \n",
      "8     0.981000    0.981000    0.955400  \n",
      "9     0.666667    0.690909    0.666667  \n",
      "10    0.897012    0.903591    0.888087  \n",
      "11    0.805894    0.819461    0.801352  \n",
      "12    0.763234    0.778385    0.779178  \n",
      "13    0.636243    0.659336    0.608380  \n",
      "14    0.911800    0.917200    0.917400  \n",
      "15    0.763234    0.778385    0.779178  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_knn_6)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet6_cat = np.where(((Y_testSet6>=2) | (Y_testSet6<=-2)), 1, 0) \n",
    "y_pred_knn_6_cat = np.where(((y_pred_knn_6 >= 2) | (y_pred_knn_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_knn_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:25:26,287] Trial 350 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:26,569] Trial 351 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:26,852] Trial 352 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:27,139] Trial 353 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:27,431] Trial 354 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:27,730] Trial 355 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:28,028] Trial 356 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:28,326] Trial 357 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:28,623] Trial 358 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:28,902] Trial 359 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:29,185] Trial 360 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:29,488] Trial 361 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:29,801] Trial 362 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:30,084] Trial 363 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:30,358] Trial 364 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:30,534] Trial 365 finished with value: 0.6946669754247883 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:30,819] Trial 366 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:31,091] Trial 367 finished with value: 0.5623506505101943 and parameters: {'n_neighbors': 17, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:31,370] Trial 368 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:31,642] Trial 369 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:31,931] Trial 370 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:32,229] Trial 371 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:32,514] Trial 372 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:32,794] Trial 373 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:33,067] Trial 374 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:33,338] Trial 375 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:33,610] Trial 376 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:33,883] Trial 377 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:34,155] Trial 378 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:34,426] Trial 379 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:34,700] Trial 380 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:34,977] Trial 381 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:35,248] Trial 382 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:35,520] Trial 383 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:35,799] Trial 384 finished with value: 0.6722087376623771 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:36,098] Trial 385 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:36,397] Trial 386 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:36,690] Trial 387 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:36,867] Trial 388 finished with value: 0.5022014873894104 and parameters: {'n_neighbors': 24, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 97}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:37,149] Trial 389 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:37,447] Trial 390 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:37,743] Trial 391 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:38,042] Trial 392 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:38,341] Trial 393 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:38,655] Trial 394 finished with value: 0.5756901958466561 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:38,949] Trial 395 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:39,250] Trial 396 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:39,554] Trial 397 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:39,838] Trial 398 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 350 with value: 0.7032168146333427.\n",
      "[I 2023-12-12 04:25:40,124] Trial 399 finished with value: 0.7032168146333427 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 350 with value: 0.7032168146333427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7032\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 98\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686112    0.708065    0.676224    0.691604   \n",
      "1                    TP   35.000000   41.000000   38.000000   38.000000   \n",
      "2                    TN  308.000000  303.000000  305.000000  308.000000   \n",
      "3                    FP    8.000000   12.000000   10.000000    6.000000   \n",
      "4                    FN   31.000000   26.000000   29.000000   30.000000   \n",
      "5              Accuracy    0.897906    0.900524    0.897906    0.905759   \n",
      "6             Precision    0.813953    0.773585    0.791667    0.863636   \n",
      "7           Sensitivity    0.530303    0.611940    0.567164    0.558824   \n",
      "8           Specificity    0.974700    0.961900    0.968300    0.980900   \n",
      "9              F1 score    0.642202    0.683333    0.660870    0.678571   \n",
      "10  F1 score (weighted)    0.888927    0.895802    0.890966    0.897396   \n",
      "11     F1 score (macro)    0.791330    0.812164    0.800389    0.811678   \n",
      "12    Balanced Accuracy    0.752493    0.786923    0.767709    0.769858   \n",
      "13                  MCC    0.604034    0.631323    0.614316    0.646695   \n",
      "14                  NPV    0.908600    0.921000    0.913200    0.911200   \n",
      "15              ROC_AUC    0.752493    0.786923    0.767709    0.769858   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.707325    0.746283    0.675420    0.681963  \n",
      "1    36.000000   38.000000   41.000000   39.000000  \n",
      "2   310.000000  310.000000  300.000000  304.000000  \n",
      "3     6.000000    6.000000   14.000000   11.000000  \n",
      "4    30.000000   28.000000   27.000000   28.000000  \n",
      "5     0.905759    0.910995    0.892670    0.897906  \n",
      "6     0.857143    0.863636    0.745455    0.780000  \n",
      "7     0.545455    0.575758    0.602941    0.582090  \n",
      "8     0.981000    0.981000    0.955400    0.965100  \n",
      "9     0.666667    0.690909    0.666667    0.666667  \n",
      "10    0.897012    0.903591    0.888087    0.891830  \n",
      "11    0.805894    0.819461    0.801352    0.803194  \n",
      "12    0.763234    0.778385    0.779178    0.773584  \n",
      "13    0.636243    0.659336    0.608380    0.616964  \n",
      "14    0.911800    0.917200    0.917400    0.915700  \n",
      "15    0.763234    0.778385    0.779178    0.773584  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_knn_7)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet7_cat = np.where(((Y_testSet7>=2) | (Y_testSet7<=-2)), 1, 0) \n",
    "y_pred_knn_7_cat = np.where(((y_pred_knn_7 >= 2) | (y_pred_knn_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_knn_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:25:40,496] Trial 400 finished with value: 0.7085214009769445 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:40,793] Trial 401 finished with value: 0.6815236192173421 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:41,092] Trial 402 finished with value: 0.7085214009769445 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:41,390] Trial 403 finished with value: 0.7085214009769445 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:41,689] Trial 404 finished with value: 0.7085214009769445 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:41,988] Trial 405 finished with value: 0.7085214009769445 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:42,257] Trial 406 finished with value: 0.6576418114253848 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:42,539] Trial 407 finished with value: 0.7085214009769445 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:42,810] Trial 408 finished with value: 0.6880991043541159 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:43,082] Trial 409 finished with value: 0.7085214009769445 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:43,264] Trial 410 finished with value: 0.7013855765673067 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 50}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:43,527] Trial 411 finished with value: 0.7085214009769445 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:43,807] Trial 412 finished with value: 0.7085214009769445 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:44,093] Trial 413 finished with value: 0.7085214009769445 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:44,390] Trial 414 finished with value: 0.7085214009769445 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:44,688] Trial 415 finished with value: 0.7085214009769445 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:44,987] Trial 416 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:45,287] Trial 417 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:45,591] Trial 418 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:45,894] Trial 419 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:46,196] Trial 420 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:46,504] Trial 421 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:46,806] Trial 422 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:47,069] Trial 423 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:47,342] Trial 424 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:47,637] Trial 425 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:47,936] Trial 426 finished with value: 0.6688934951944953 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:48,247] Trial 427 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:48,570] Trial 428 finished with value: 0.6778913281120438 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:48,878] Trial 429 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:49,188] Trial 430 finished with value: 0.7014510888102975 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:49,506] Trial 431 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:49,706] Trial 432 finished with value: 0.6956285943674634 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:49,992] Trial 433 finished with value: 0.7014510888102975 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:50,264] Trial 434 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:50,555] Trial 435 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:50,856] Trial 436 finished with value: 0.7014510888102975 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:51,159] Trial 437 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:51,445] Trial 438 finished with value: 0.6341530502480144 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:51,718] Trial 439 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:52,008] Trial 440 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:52,312] Trial 441 finished with value: 0.7014510888102975 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:52,591] Trial 442 finished with value: 0.6962100057222949 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:52,881] Trial 443 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:53,181] Trial 444 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:53,483] Trial 445 finished with value: 0.5972297826101379 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:53,773] Trial 446 finished with value: 0.7014510888102975 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:54,061] Trial 447 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:54,359] Trial 448 finished with value: 0.6698058828355957 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:54,674] Trial 449 finished with value: 0.7053162952855878 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 400 with value: 0.7085214009769445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7085\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 98\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686112    0.708065    0.676224    0.691604   \n",
      "1                    TP   35.000000   41.000000   38.000000   38.000000   \n",
      "2                    TN  308.000000  303.000000  305.000000  308.000000   \n",
      "3                    FP    8.000000   12.000000   10.000000    6.000000   \n",
      "4                    FN   31.000000   26.000000   29.000000   30.000000   \n",
      "5              Accuracy    0.897906    0.900524    0.897906    0.905759   \n",
      "6             Precision    0.813953    0.773585    0.791667    0.863636   \n",
      "7           Sensitivity    0.530303    0.611940    0.567164    0.558824   \n",
      "8           Specificity    0.974700    0.961900    0.968300    0.980900   \n",
      "9              F1 score    0.642202    0.683333    0.660870    0.678571   \n",
      "10  F1 score (weighted)    0.888927    0.895802    0.890966    0.897396   \n",
      "11     F1 score (macro)    0.791330    0.812164    0.800389    0.811678   \n",
      "12    Balanced Accuracy    0.752493    0.786923    0.767709    0.769858   \n",
      "13                  MCC    0.604034    0.631323    0.614316    0.646695   \n",
      "14                  NPV    0.908600    0.921000    0.913200    0.911200   \n",
      "15              ROC_AUC    0.752493    0.786923    0.767709    0.769858   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.707325    0.746283    0.675420    0.681963    0.664113  \n",
      "1    36.000000   38.000000   41.000000   39.000000   39.000000  \n",
      "2   310.000000  310.000000  300.000000  304.000000  303.000000  \n",
      "3     6.000000    6.000000   14.000000   11.000000   10.000000  \n",
      "4    30.000000   28.000000   27.000000   28.000000   30.000000  \n",
      "5     0.905759    0.910995    0.892670    0.897906    0.895288  \n",
      "6     0.857143    0.863636    0.745455    0.780000    0.795918  \n",
      "7     0.545455    0.575758    0.602941    0.582090    0.565217  \n",
      "8     0.981000    0.981000    0.955400    0.965100    0.968100  \n",
      "9     0.666667    0.690909    0.666667    0.666667    0.661017  \n",
      "10    0.897012    0.903591    0.888087    0.891830    0.888035  \n",
      "11    0.805894    0.819461    0.801352    0.803194    0.799549  \n",
      "12    0.763234    0.778385    0.779178    0.773584    0.766634  \n",
      "13    0.636243    0.659336    0.608380    0.616964    0.613511  \n",
      "14    0.911800    0.917200    0.917400    0.915700    0.909900  \n",
      "15    0.763234    0.778385    0.779178    0.773584    0.766634  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_knn_8)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet8_cat = np.where(((Y_testSet8>=2) | (Y_testSet8<=-2)), 1, 0) \n",
    "y_pred_knn_8_cat = np.where(((y_pred_knn_8 >= 2) | (y_pred_knn_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_knn_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:25:55,044] Trial 450 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:55,334] Trial 451 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:55,619] Trial 452 finished with value: 0.6839882789002683 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:55,893] Trial 453 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:56,168] Trial 454 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:56,442] Trial 455 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:56,626] Trial 456 finished with value: 0.6785177176726827 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:56,901] Trial 457 finished with value: 0.6839882789002683 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:57,175] Trial 458 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:57,471] Trial 459 finished with value: 0.6839882789002683 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:57,781] Trial 460 finished with value: 0.6802198336881156 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:58,086] Trial 461 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:58,399] Trial 462 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:58,704] Trial 463 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:59,005] Trial 464 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:59,309] Trial 465 finished with value: 0.6839882789002683 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:59,611] Trial 466 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:25:59,895] Trial 467 finished with value: 0.6839882789002683 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:00,175] Trial 468 finished with value: 0.621467603885681 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:00,459] Trial 469 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:00,751] Trial 470 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:01,052] Trial 471 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:01,338] Trial 472 finished with value: 0.6839882789002683 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:01,612] Trial 473 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:01,887] Trial 474 finished with value: 0.6839882789002683 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:02,160] Trial 475 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:02,437] Trial 476 finished with value: 0.6002449353099152 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:02,622] Trial 477 finished with value: 0.6785177176726827 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:02,911] Trial 478 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:03,205] Trial 479 finished with value: 0.6802198336881156 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:03,505] Trial 480 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:03,808] Trial 481 finished with value: 0.6839882789002683 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:04,111] Trial 482 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:04,423] Trial 483 finished with value: 0.6839882789002683 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:04,716] Trial 484 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:05,001] Trial 485 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:05,298] Trial 486 finished with value: 0.6839882789002683 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:05,612] Trial 487 finished with value: 0.6626372624565644 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:05,918] Trial 488 finished with value: 0.594750147093676 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:06,208] Trial 489 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:06,484] Trial 490 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:06,759] Trial 491 finished with value: 0.6839882789002683 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:07,015] Trial 492 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:07,291] Trial 493 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:07,584] Trial 494 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:07,871] Trial 495 finished with value: 0.6839882789002683 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:08,146] Trial 496 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:08,422] Trial 497 finished with value: 0.6944711617088134 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:08,608] Trial 498 finished with value: 0.6027657714869123 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 92}. Best is trial 400 with value: 0.7085214009769445.\n",
      "[I 2023-12-12 04:26:08,886] Trial 499 finished with value: 0.6871658511070657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 400 with value: 0.7085214009769445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7085\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 98\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686112    0.708065    0.676224    0.691604   \n",
      "1                    TP   35.000000   41.000000   38.000000   38.000000   \n",
      "2                    TN  308.000000  303.000000  305.000000  308.000000   \n",
      "3                    FP    8.000000   12.000000   10.000000    6.000000   \n",
      "4                    FN   31.000000   26.000000   29.000000   30.000000   \n",
      "5              Accuracy    0.897906    0.900524    0.897906    0.905759   \n",
      "6             Precision    0.813953    0.773585    0.791667    0.863636   \n",
      "7           Sensitivity    0.530303    0.611940    0.567164    0.558824   \n",
      "8           Specificity    0.974700    0.961900    0.968300    0.980900   \n",
      "9              F1 score    0.642202    0.683333    0.660870    0.678571   \n",
      "10  F1 score (weighted)    0.888927    0.895802    0.890966    0.897396   \n",
      "11     F1 score (macro)    0.791330    0.812164    0.800389    0.811678   \n",
      "12    Balanced Accuracy    0.752493    0.786923    0.767709    0.769858   \n",
      "13                  MCC    0.604034    0.631323    0.614316    0.646695   \n",
      "14                  NPV    0.908600    0.921000    0.913200    0.911200   \n",
      "15              ROC_AUC    0.752493    0.786923    0.767709    0.769858   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.707325    0.746283    0.675420    0.681963    0.664113    0.723107  \n",
      "1    36.000000   38.000000   41.000000   39.000000   39.000000   41.000000  \n",
      "2   310.000000  310.000000  300.000000  304.000000  303.000000  304.000000  \n",
      "3     6.000000    6.000000   14.000000   11.000000   10.000000   11.000000  \n",
      "4    30.000000   28.000000   27.000000   28.000000   30.000000   26.000000  \n",
      "5     0.905759    0.910995    0.892670    0.897906    0.895288    0.903141  \n",
      "6     0.857143    0.863636    0.745455    0.780000    0.795918    0.788462  \n",
      "7     0.545455    0.575758    0.602941    0.582090    0.565217    0.611940  \n",
      "8     0.981000    0.981000    0.955400    0.965100    0.968100    0.965100  \n",
      "9     0.666667    0.690909    0.666667    0.666667    0.661017    0.689076  \n",
      "10    0.897012    0.903591    0.888087    0.891830    0.888035    0.898163  \n",
      "11    0.805894    0.819461    0.801352    0.803194    0.799549    0.815856  \n",
      "12    0.763234    0.778385    0.779178    0.773584    0.766634    0.788510  \n",
      "13    0.636243    0.659336    0.608380    0.616964    0.613511    0.639918  \n",
      "14    0.911800    0.917200    0.917400    0.915700    0.909900    0.921200  \n",
      "15    0.763234    0.778385    0.779178    0.773584    0.766634    0.788510  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_knn_9)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet9_cat = np.where(((Y_testSet9>=2) | (Y_testSet9<=-2)), 1, 0) \n",
    "y_pred_knn_9_cat = np.where(((y_pred_knn_9 >= 2) | (y_pred_knn_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_knn_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.714922</td>\n",
       "      <td>0.046246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>2.806738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>152.500000</td>\n",
       "      <td>1.957890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.024846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>3.020302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.908901</td>\n",
       "      <td>0.018337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.807304</td>\n",
       "      <td>0.066426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.632336</td>\n",
       "      <td>0.087787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.967660</td>\n",
       "      <td>0.012780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.706110</td>\n",
       "      <td>0.068509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.904069</td>\n",
       "      <td>0.020763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.826085</td>\n",
       "      <td>0.039367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.799996</td>\n",
       "      <td>0.043986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.662111</td>\n",
       "      <td>0.074487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>0.016970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.799996</td>\n",
       "      <td>0.043986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.714922     0.046246\n",
       "1                    TP        21.100000     2.806738\n",
       "2                    TN       152.500000     1.957890\n",
       "3                    FP         5.100000     2.024846\n",
       "4                    FN        12.300000     3.020302\n",
       "5              Accuracy         0.908901     0.018337\n",
       "6             Precision         0.807304     0.066426\n",
       "7           Sensitivity         0.632336     0.087787\n",
       "8           Specificity         0.967660     0.012780\n",
       "9              F1 score         0.706110     0.068509\n",
       "10  F1 score (weighted)         0.904069     0.020763\n",
       "11     F1 score (macro)         0.826085     0.039367\n",
       "12    Balanced Accuracy         0.799996     0.043986\n",
       "13                  MCC         0.662111     0.074487\n",
       "14                  NPV         0.925620     0.016970\n",
       "15              ROC_AUC         0.799996     0.043986"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.686112</td>\n",
       "      <td>0.708065</td>\n",
       "      <td>0.676224</td>\n",
       "      <td>0.691604</td>\n",
       "      <td>0.707325</td>\n",
       "      <td>0.746283</td>\n",
       "      <td>0.675420</td>\n",
       "      <td>0.681963</td>\n",
       "      <td>0.664113</td>\n",
       "      <td>0.723107</td>\n",
       "      <td>0.696021</td>\n",
       "      <td>0.025127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>38.600000</td>\n",
       "      <td>2.065591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>305.500000</td>\n",
       "      <td>3.341656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>2.796824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>1.779513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.905759</td>\n",
       "      <td>0.905759</td>\n",
       "      <td>0.910995</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.900785</td>\n",
       "      <td>0.005581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.773585</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.807346</td>\n",
       "      <td>0.041247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.611940</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.611940</td>\n",
       "      <td>0.575163</td>\n",
       "      <td>0.027588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.974700</td>\n",
       "      <td>0.961900</td>\n",
       "      <td>0.968300</td>\n",
       "      <td>0.980900</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>0.955400</td>\n",
       "      <td>0.965100</td>\n",
       "      <td>0.968100</td>\n",
       "      <td>0.965100</td>\n",
       "      <td>0.970150</td>\n",
       "      <td>0.008919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.642202</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.660870</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.690909</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.670598</td>\n",
       "      <td>0.014977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.888927</td>\n",
       "      <td>0.895802</td>\n",
       "      <td>0.890966</td>\n",
       "      <td>0.897396</td>\n",
       "      <td>0.897012</td>\n",
       "      <td>0.903591</td>\n",
       "      <td>0.888087</td>\n",
       "      <td>0.891830</td>\n",
       "      <td>0.888035</td>\n",
       "      <td>0.898163</td>\n",
       "      <td>0.893981</td>\n",
       "      <td>0.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.791330</td>\n",
       "      <td>0.812164</td>\n",
       "      <td>0.800389</td>\n",
       "      <td>0.811678</td>\n",
       "      <td>0.805894</td>\n",
       "      <td>0.819461</td>\n",
       "      <td>0.801352</td>\n",
       "      <td>0.803194</td>\n",
       "      <td>0.799549</td>\n",
       "      <td>0.815856</td>\n",
       "      <td>0.806087</td>\n",
       "      <td>0.008604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.752493</td>\n",
       "      <td>0.786923</td>\n",
       "      <td>0.767709</td>\n",
       "      <td>0.769858</td>\n",
       "      <td>0.763234</td>\n",
       "      <td>0.778385</td>\n",
       "      <td>0.779178</td>\n",
       "      <td>0.773584</td>\n",
       "      <td>0.766634</td>\n",
       "      <td>0.788510</td>\n",
       "      <td>0.772651</td>\n",
       "      <td>0.011030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.604034</td>\n",
       "      <td>0.631323</td>\n",
       "      <td>0.614316</td>\n",
       "      <td>0.646695</td>\n",
       "      <td>0.636243</td>\n",
       "      <td>0.659336</td>\n",
       "      <td>0.608380</td>\n",
       "      <td>0.616964</td>\n",
       "      <td>0.613511</td>\n",
       "      <td>0.639918</td>\n",
       "      <td>0.627072</td>\n",
       "      <td>0.018325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.908600</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>0.913200</td>\n",
       "      <td>0.911200</td>\n",
       "      <td>0.911800</td>\n",
       "      <td>0.917200</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>0.909900</td>\n",
       "      <td>0.921200</td>\n",
       "      <td>0.914720</td>\n",
       "      <td>0.004468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.752493</td>\n",
       "      <td>0.786923</td>\n",
       "      <td>0.767709</td>\n",
       "      <td>0.769858</td>\n",
       "      <td>0.763234</td>\n",
       "      <td>0.778385</td>\n",
       "      <td>0.779178</td>\n",
       "      <td>0.773584</td>\n",
       "      <td>0.766634</td>\n",
       "      <td>0.788510</td>\n",
       "      <td>0.772651</td>\n",
       "      <td>0.011030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.686112    0.708065    0.676224    0.691604   \n",
       "1                    TP   35.000000   41.000000   38.000000   38.000000   \n",
       "2                    TN  308.000000  303.000000  305.000000  308.000000   \n",
       "3                    FP    8.000000   12.000000   10.000000    6.000000   \n",
       "4                    FN   31.000000   26.000000   29.000000   30.000000   \n",
       "5              Accuracy    0.897906    0.900524    0.897906    0.905759   \n",
       "6             Precision    0.813953    0.773585    0.791667    0.863636   \n",
       "7           Sensitivity    0.530303    0.611940    0.567164    0.558824   \n",
       "8           Specificity    0.974700    0.961900    0.968300    0.980900   \n",
       "9              F1 score    0.642202    0.683333    0.660870    0.678571   \n",
       "10  F1 score (weighted)    0.888927    0.895802    0.890966    0.897396   \n",
       "11     F1 score (macro)    0.791330    0.812164    0.800389    0.811678   \n",
       "12    Balanced Accuracy    0.752493    0.786923    0.767709    0.769858   \n",
       "13                  MCC    0.604034    0.631323    0.614316    0.646695   \n",
       "14                  NPV    0.908600    0.921000    0.913200    0.911200   \n",
       "15              ROC_AUC    0.752493    0.786923    0.767709    0.769858   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.707325    0.746283    0.675420    0.681963    0.664113    0.723107   \n",
       "1    36.000000   38.000000   41.000000   39.000000   39.000000   41.000000   \n",
       "2   310.000000  310.000000  300.000000  304.000000  303.000000  304.000000   \n",
       "3     6.000000    6.000000   14.000000   11.000000   10.000000   11.000000   \n",
       "4    30.000000   28.000000   27.000000   28.000000   30.000000   26.000000   \n",
       "5     0.905759    0.910995    0.892670    0.897906    0.895288    0.903141   \n",
       "6     0.857143    0.863636    0.745455    0.780000    0.795918    0.788462   \n",
       "7     0.545455    0.575758    0.602941    0.582090    0.565217    0.611940   \n",
       "8     0.981000    0.981000    0.955400    0.965100    0.968100    0.965100   \n",
       "9     0.666667    0.690909    0.666667    0.666667    0.661017    0.689076   \n",
       "10    0.897012    0.903591    0.888087    0.891830    0.888035    0.898163   \n",
       "11    0.805894    0.819461    0.801352    0.803194    0.799549    0.815856   \n",
       "12    0.763234    0.778385    0.779178    0.773584    0.766634    0.788510   \n",
       "13    0.636243    0.659336    0.608380    0.616964    0.613511    0.639918   \n",
       "14    0.911800    0.917200    0.917400    0.915700    0.909900    0.921200   \n",
       "15    0.763234    0.778385    0.779178    0.773584    0.766634    0.788510   \n",
       "\n",
       "           ave       std  \n",
       "0     0.696021  0.025127  \n",
       "1    38.600000  2.065591  \n",
       "2   305.500000  3.341656  \n",
       "3     9.400000  2.796824  \n",
       "4    28.500000  1.779513  \n",
       "5     0.900785  0.005581  \n",
       "6     0.807346  0.041247  \n",
       "7     0.575163  0.027588  \n",
       "8     0.970150  0.008919  \n",
       "9     0.670598  0.014977  \n",
       "10    0.893981  0.005200  \n",
       "11    0.806087  0.008604  \n",
       "12    0.772651  0.011030  \n",
       "13    0.627072  0.018325  \n",
       "14    0.914720  0.004468  \n",
       "15    0.772651  0.011030  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test['std'] = mat_met_knn_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.716725</td>\n",
       "      <td>0.050838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.905131</td>\n",
       "      <td>0.018643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.801272</td>\n",
       "      <td>0.085042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.618526</td>\n",
       "      <td>0.081769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.965858</td>\n",
       "      <td>0.017652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.693725</td>\n",
       "      <td>0.063005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.900081</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.818766</td>\n",
       "      <td>0.036619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.792193</td>\n",
       "      <td>0.040259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.649107</td>\n",
       "      <td>0.071957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.923034</td>\n",
       "      <td>0.015408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.792193</td>\n",
       "      <td>0.040259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.716725     0.050838\n",
       "1              Accuracy         0.905131     0.018643\n",
       "2             Precision         0.801272     0.085042\n",
       "3           Sensitivity         0.618526     0.081769\n",
       "4           Specificity         0.965858     0.017652\n",
       "5              F1 score         0.693725     0.063005\n",
       "6   F1 score (weighted)         0.900081     0.019800\n",
       "7      F1 score (macro)         0.818766     0.036619\n",
       "8     Balanced Accuracy         0.792193     0.040259\n",
       "9                   MCC         0.649107     0.071957\n",
       "10                  NPV         0.923034     0.015408\n",
       "11              ROC_AUC         0.792193     0.040259"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_knn_cat = np.where(((y_pred_optimized_knn >= 2) | (y_pred_optimized_knn <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_knn_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_knn))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "knn_run0 = data_knn[['y_test_idx0', 'y_test0', 'y_pred_knn0']]\n",
    "knn_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "knn_run0.reset_index(inplace=True, drop=True)\n",
    "knn_run1 = data_knn[['y_test_idx1', 'y_test1', 'y_pred_knn1']]\n",
    "knn_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "knn_run1.reset_index(inplace=True, drop=True)\n",
    "knn_run2 = data_knn[['y_test_idx2', 'y_test2', 'y_pred_knn2']]\n",
    "knn_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "knn_run2.reset_index(inplace=True, drop=True)\n",
    "knn_run3 = data_knn[['y_test_idx3', 'y_test3', 'y_pred_knn3']]\n",
    "knn_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "knn_run3.reset_index(inplace=True, drop=True)\n",
    "knn_run4 = data_knn[['y_test_idx4', 'y_test4', 'y_pred_knn4']]\n",
    "knn_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "knn_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "knn_5preds = pd.concat([chembl_id,knn_run0, knn_run1, knn_run2, knn_run3, knn_run4], axis=1)\n",
    "knn_5preds = knn_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_knn0', 'y_pred_knn1', 'y_pred_knn2', 'y_pred_knn3', 'y_pred_knn4']]\n",
    "knn_5preds['y_pred_knn_ave'] = knn_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "knn_5preds['y_pred_knn_std'] = knn_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "\n",
    "mat_met_optimized_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0bc43db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjYAAAGxCAYAAABx6/zIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB4EElEQVR4nO3deXhU5dkG8PvMko0khJBAgAABAwgoop87oKCfWi11RS0tKhYUWdTWhRBRAZElgrYWgUvrLlWhKmq17hWt0q9Yd8QiFMJOyDCZhJCEZGbO98fJTOZsM+fMktnu33XlksycOfPOnJh58rzP+z6CKIoiiIiIiFKAJd4DICIiIooWBjZERESUMhjYEBERUcpgYENEREQpg4ENERERpQwGNkRERJQyGNgQERFRymBgQ0RERCmDgQ0RERGlDFu8BxAvdXV1cLvd8R5G2IqLi1FbWxvvYVA7Xo/EwWuROHgtEkcqXAubzYZu3bqFPq4TxpKQ3G432tra4j2MsAiCAEB6DeyIEX+8HomD1yJx8FokjnS7FpyKIiIiopTBwIaIiIhSBgMbIiIiShkMbIiIiChlMLAhIiKilMHAhoiIiFIGAxsiIiJKGQxsiIiIKGUwsCEiIqKUwcCGiIiIUgYDGyIiIkoZDGyIiIgoZTCwISIiopTBwIaIiIhSBgMbIiIiShkMbIiIiChlMLAhIiKilMHAhoiIiFIGAxsiIiJKGQxsiIiIKGUwsCEiIqKUwcCGiIiIUgYDGyIiIkoZDGyIiIgoZTCwSTPz589Hnz59MHXqVHg8nngPh4iIKKoY2CSx3/72t+jTpw/69OmDfv364bTTTsOcOXPgcrk0j3/00Ufx4osvoqqqCl9++SUqKipUx2zcuBE33ngjTj75ZJSXl+OCCy7Aa6+9FuNXAhw7dgz33nsvTjjhBJSXl2Py5MnYv39/0Me43W5UVVXhzDPPxHHHHYezzjoLv//97+H1ev3HiKKIhx9+GKeccgqOO+44TJgwAVu3bpWdp7q6GlOmTMGJJ56IIUOGYNq0aaitrY3J6yQiothiYJPkxo0bh6+//hr/93//h+XLl+ODDz7APffcozpuzZo1eOKJJ/DSSy9h0qRJePXVV/Hpp59i0aJFsuP+/e9/Y+jQoXjiiSfw4Ycf4pe//CVuv/12vP/++zF9HfPmzcM777yDVatW4fXXX8fRo0dxww03BM0qrVy5Ei+88AIefPBBbNiwAXPnzsXq1avx9NNP+49ZtWoVnnjiCTz44IN4++23UVxcjIkTJ6KxsREA0NTUhF/96lcQBAHr1q3D66+/jra2NkyePFkWIBERUXKwxXsAFJmMjAz06NEDANC7d29ceumlWLduneyYt956Cw8//DDWrl2LE044AQAwcOBArF+/Htdccw26deuGGTNmAABuu+022WOnTJmCDRs24N1338WFF14Yk9fQ0NCAl19+GY8++ijOOeccAMCKFStw2mmn4R//+AfGjh2r+bgvv/wSF110Ef73f/8XANC3b1+88cYb+PbbbwFI2Zonn3wSt912Gy655BIAwB/+8AeMHDkS69evx3XXXYcvvvgCe/bswXvvvYe8vDwAwCOPPILhw4fjs88+84+HiIiSAzM2KWTXrl3YsGED7Ha77Pbx48fj66+/9gc1Pn369MHnn3/uD2r0HDlyBAUFBUGPGTduHAYNGqT7NW7cON3Hfvfdd2hra8O5557rv62kpARDhgzBv//9b93HnX766fjss8/w3//+FwDwww8/YNOmTTj//PMBALt378ahQ4dk583MzMSZZ57pP++xY8cgCAIyMjJkx1gsFnzxxRdBXzMRESUeZmyS3IcffohBgwbB6/WipaUFgDStEy1vvfUWvv32W1RVVQU97oUXXkBbW5vu/cpgK1BtbS0yMjJUwVNxcTEOHTqk+7iZM2fiyJEjOPfcc2G1WuHxeFBRUYHLL78cAPyPLSoqUp137969AID/+Z//QU5ODhYtWoTKykqIoohFixbB6/WipqYm2EsmIqIElNSBzfr16/HSSy/hkksuweTJk+M9nLg4++yzsWTJEjQ3N+Oll17Cjh078Jvf/CYq5964cSN+97vf4aGHHsKQIUOCHltaWhqV5wwkiiIEQdC9/80338Srr76KlStXYvDgwfjhhx8wb9489OzZE9dcc43/OOU5As/bvXt3PP7446isrMTTTz8Ni8WCyy67DCeeeCKsVmvUXxMREcVW0gY227dvx4cffoj+/fvHeyhxlZOTgwEDBgAAFi5ciAkTJuCRRx7B7NmzIzrvP//5T0yePBnz5s3D1VdfHfL4cePG+bMgWkpLS/Hxxx9r3ldcXIzW1la4XC5Z1sbhcODUU0/VPefChQsxa9YsXHbZZQCAoUOHYu/evXjsscdwzTXX+GuPamtr0bNnT9l5A7M45557LjZu3Ain0wmr1YquXbti5MiR6NevX8jXTUREiSUpA5uWlhasWLEC06ZN65SlyMnkjjvuwHXXXYfrr78eJSUlYZ1j48aNuOGGGzB37lxMmjTJ0GMimYoaMWIE7HY7Pv30U1x66aUAgJqaGmzduhX33nuv7uOam5tV2Rir1epfzdSvXz/06NEDn376qb++qLW1Ff/3f/+nuXKssLAQAPDZZ5/B4XDgggsu0H1uIiJKTEkZ2Dz55JM4+eSTMWLEiJCBTVtbm+wDVxAEZGdn+/+djJTjDvx+1KhRGDx4MFasWIHFixebPvfGjRtx/fXXY+rUqfj5z3/u38/FbrejW7duuo/r27ev6efy6dq1KyZOnIgHHngAhYWFKCgowMKFC3H88cfjnHPO8b++a665Bj/72c/8U20XXnghVqxYgdLSUgwZMgSbN2/GE088gV/+8pcQBAGCIGDq1KlYsWIFBg4ciAEDBuCPf/wjsrOzceWVV/rP+/LLL2PQoEHo3r07vvzyS9x///24+eabMWjQIEPj950nWX+eUgmvReLgtUgcaXctxCTz2WefiXfccYd47NgxURRFcd68eeIzzzyje/zatWvFq6++2v81e/bsThpp7N1www3iZZddprr9z3/+s5iRkSHu3r07rHMCUH2de+65kQ84iObmZnHWrFliYWGhmJ2dLY4fP141/v79+4vz5s3zf9/Q0CDefvvtYr9+/cSsrCxx4MCB4ty5c/0/G6Ioil6vV5w3b55YUlIiZmZmiuecc474/fffy85bUVEh9uzZU7Tb7eKgQYPEhx9+WPR6vTF9vUREFBuCKIpiXCMrExwOByorKzF37lyUlZUBkFoElJWV6RYP62Vsamtr4Xa7O2HU0ScIAkpKSnDw4EEk0eVLWbweiYPXInHwWiSOVLkWNpsNxcXFoY/rhLFEzY4dO1BfX485c+b4b/N6vfjxxx/x7rvv4sUXX4TFIt+ax26369Z3JPMFBqTxJ/trSCW8HomD1yJx8FokjnS5FkkV2Jx44olYvny57LbVq1ejd+/euOyyy1RBDREREaWXpApssrOzVUtwMzMzkZeXx6W5RERExJYKRERElDqSKmOjZf78+fEeAhERESUIZmyIiIgoZTCwISIiopTBwIaIiIhSBgMbIiIiShlJXzxMRESUTsSGOnhXLwVcTqCgEJbplRDyC+I9rITBjA0REVES8a5eCmz/EXDUANt/hHf1kngPKaEwY0NERBRHwTIwYkMdvCseBPZWSweXlgENLvkJnLXwVFUwg9OOgQ0REVGYZEFJbr50Y2ODqQDDn4EBAEcNvHOnwbLocQj5BdJ91ds6Dq7eBmRmyU/QdBRwOjoev3oJrBVVkb+4JMWpKCIiojDJpoWqt0lfZqeIXE759y3NHY9V3gcAHjdQPhQo6in9Nyc3+PnSDDM2RERE4QoWRPiyKKEUFErBkNZ5te6DIMvIeKoqAGet/HxpjBkbIiKicAULIo4eMXQKy/RKICtbfqPLCU9VBYRJM9VTT6Vl6scHZHAs0ysNPW+qYsaGiIgoTJbpldK0kcupzqx43IbOIeQXwLLocek81dsBd5v0tf1HiGtWwrL4iY7naK/dUT4+nWtqlBjYEBERhSkwqPBMv0oKSDruNX0eT+VN8gDJ5WTgYhKnooiIiKJBMUWk+t4I38oqve8pJGZsiIiIosBy6326U0ahdgv23793ZxxGnloY2BAREUVBsCkj1V41ir1mZPcHamwAEBD4OB1AU6O0xLuwKO0349PCqSgiIqJYUy4LD/W9T/uqK3/g46wFWpql/7KdgiZmbIiIiMJgqhmlcj8a5TJx5f02O1BW3jGdpRf4pPlmfFqYsSEiIgqDmWaUofaaUd1f9RSsFVUdgZLefjlpvhmfFmZsiIiIwmF0Ogmh95oJdb9/vxyNGhuSY2BDREQUjlDTS1HEvWyMY2BDREQUglY9jWzXYY0dgY3U4Jiq0yFDGNgQEVFaiCSI0FuuHSyLEmqJt9FjyBwWDxMRUVowU+yrYqKextRjwjkvBcWMDRERJTXDmZhIgohw6mmMPCbIMZymCg8zNkRElNQMZ2K09o4xKNRybbOPERvq4KmqkFY5ZWUDhcWqYyLKMKUx0xmbH374AV999RW2bt0Kp9OJ1tZW5OXlobS0FCeccALOOuss5OezaRcREXUSjUxMOMW+wYSzKslwiwUAKC1TH8tpqrAYDmw2bNiAN954A/v370dWVhb69++PgQMHIiMjA42Njdi9ezc2bdqE559/HmeddRauvfZaFBcXx3LsREREmtM54RT7KsmCI1+X7cYGw9NCQaeSjAQtnbicPJUYCmwqKipw6NAhjBkzBjNnzsTAgQNhsahnsRobG7Fp0yZ88skn+N3vfodZs2bhzDPPjPqgiYiIfLQyMd4ld8sPCiPboQyO/Bw18M6dJgU7AQGLMpCB2w1Ub+t4TOCKJwNBSyQZpnRmKLA55ZRT8Itf/AI5OTlBj8vNzcV5552H8847D1u2bEFjY2NUBklERKRHc8onwmyH2FAHVG/XP6ClWfoKCFhUgZDNLn9MQHBlJGjhpnzhMRTYXHvttaZPPGzYMNOPISIiioZIsx3e1UsBd5uxg30BS6iskCy4Ek2Nh4zjcm8iIkopyikhYdIMVZATctm0s1b+vcUK9Bso1dg0NkjZGh9fwKLMEpWWATabZnDFjflix1Bgs2XLFlMnZbaGiIg6mz+gqd7ekW1x1EBcOrsjEDEaRDQdlX+fkQHr3Ifbn8elmQ3SyhLpBlBc8RQzhgKbBQsWmDrp2rVrwxoMERFRuFRLqH1aj8m/1wgiVIW/WTnyrExOrv+fgbUv0uNMZoMArniKIcNTUTk5OTjrrLNw4oknQhCEWI6JiIiSVGCAUNOjBOLUu4C8rp3z5HpZj4xMeZDS2ABP5U2yQERV+JuVLT9HYZHmqc1OKfnfH9/GfDm5QGERVzxFkaHAZsaMGdiwYQM++ugjfPvttxg3bhzGjh2LoiLtC01EROkp8IO+1VEDrFocdu1IsH1gtO5TZUFsdqCsHMKkmRDXrJSCiTqHfEXTPTfDsvgJdVCUkwuUlAJ7q6Xv3W6IDS51NsbklJLexnz+nYjZPiFihloqnHvuuZg3bx7++Mc/YvTo0fjoo48wa9YsLFy4EBs3boTb7Y71OImIKBlEsXYkWEsBrfss0yuBskFSQGOzA6VlUoCQlyc9qMEFiIrVSMdapPMqp4IKi6TCX3eb9FW9Tbulgdk2DTrvD9snRI+pXlE9e/bExIkTsXLlSsyePRvZ2dl47LHHcPPNN+Nvf/tbrMZIRETJIoJ+TCrBgiSN+/wZjsBgZPZv4J17ixQ06C3fdjq0+zo5HarjlEz3kNJ7f1hMHDVhNcG0WCw45ZRTcMstt+DSSy9FU1OT6ZVTRESUegI/6DOGnQTrjHvCP5kyCGivjfFUVXS0OPBxOaXb9+yU3+5xy+trtDQ1QsgvgGX6HOk5XU4pY3L0iOo4oKOBpadiirQDsdNhePpINxCKZkCY5gRRVOblQvvmm2/w8ccf49///jcyMjJw1lln4cILL0RZWVkMhhgbtbW1aGszuPlSghEEAb169cKBAwcQxuWjKOP1SBy8FokjGtdCtqxauXdM2SBpqihwaXe4CothmbtcyuwEPofFCng9Hd8XdId12TNSAKW1+qp8aAT1RPIl5MKkGRDXrIpKzU2q/H9ht9sN9aA0vCrq0KFD+Pvf/45PPvkETqcTw4YNw7Rp03DmmWciIyMjosESEREpBS6r9lTeJA86GuqAwmJ54BFMVra/t5OshxMAFBZJNS7KzI7XK/++pUn6r940UQTTR8r2CbLgiRv4mWJ4H5sff/wRhYWFOPfcczFu3Dj07Nkz1mMjIiKSKFc8NR3VrHlBZhZwrKXje6sNGDBIsaJKvcGeqmkmANisUhDk49vLRjmWwDFGSwxqbsT6OnjC2XMnyRjeeTg7Oxv9+vXDrl278Oyzz+oeKwgCZs+eHa3xERFRFARbOp0MlLv6wulQTBtZgIFDOpZ2K15nyOXUymAlK1ta7q3I7MjG4nRIdTex2IsmBhv4eVYvSYsskKHAxrdfzZ49e0Iey837iIgSj95GcokW8OiNR3OqJrCf08AhHbsBT58D74oHgert8FZMkXo2AR1BiqMG3hULpRodZ62U/cnK0dwwT6t1Qmd03Y60iaemNFl5FVbxcCpg8TBFC69H4uC10OepvEmeASjqCeuSP6kLYSMogPXxBSfWxgZ4cvNNBUtGx+OfTtLImngDMxM+Nru8yFj5fYjni5XOCCx9/1/suf26qF/rzmS0eDis5d5ERJRkOnH/FF92yHNwn/nN5jTG419e3b7U27cDsLWiCsgvkKaknLXSc61YGNlrqN4OscEV/uNN6syN+awz7jG3506SMrwqSs/+/fuxe/du5OfnY+jQoZyKIiJKQLpTG7FoxhhGsCTLXCgeK1uGrawN8bU88NlbDZSVq4t7rdaOfyunpgK52zq39qQTp4c6YwotERgObN599118/vnnsNlsGDNmDM477zysWbMGb731lj/lW15ejvvuuw9ZWVkxGzAREZmn96EWk1qOMIIldWduAYDYsYtwoGAf/qIIHFN08xYE+Uopm0163RVTtKejOrP2hF2+o85QYPPJJ5/gmWeeQXFxMbKysvD444+jtrYWb7/9Ns4//3z0798fO3fuxMcff4y33noLEyZMiPW4iYgoCmLxV7wvWAqssQlJGUxYBMCrUyfV/uEvNtRJmZjA4MTrAfbskB+vrLdyOqQ6lrJy7Y32OjG4iElgmeYMBTbvv/8+zjrrLNx+++0QBAGvv/461q5di0svvRQTJ070H5eTk4N//vOfDGyIiNKYkF8A25yHzBVyKzMXGZnqDfPal3R3rFhaKs/EAOogRouzFp6KKVJ9TtkgqTlmiGXbsSryTZfpoc5kKLDZv38/rrrqKn/9zLhx4/DSSy/hxBNPlB03YsQIfPjhh9EfJRERpSyxoU7aCM9ml27oVQpAUGdevF7g8us6MhyRTBk5a6Wv8qGwVj0V8nC95fKUeAwFNk1NTcjP72g4ltfeAj4nJ0d2XE5ODlpaFNEzERGlPGVGwzrjHqBXL0OP9a5eKi/krT2o37jy4XsB0at9n09GJtB6LPgxPkaDozTZAyYVcLk3ERFFTLls2bNqsfEHK4OEYEGJKqgRgG7dpc31Coul5cy9+8kPycySvrQYradh9+2kYXhV1A8//IDDhw8DgH++9IcffkBtbcfOjwcOHIjy8IiIKCkog5MdW1Fz9xSIU+8C8roGf6yR+hpdItC9h3xX4sqb5IfkdYWlcllEbRAiLfJNtB2eU5nhwObFF19U3bZmzZqoDoaIiGInph+uyuDE60Xrlm+Be272d9XWez5V76WMLGnJdqgpJx9lUKWxhFrIL4Bl+hzp9Vsspl9/pEW+rNHpPIZaKmzZssXUSYcNGxb2gDoLWypQtPB6JI5UvBbRDEZCtSsw8lx6x/hbHOzYKhX5asnKDhrkqMZnlOK8gLrHk5BfEJP2EUbptbToDKny/4XRlgqGMjbJEKgQEaWiqP6lH6IA1shz6R3jy2gEDU5amqWv9iaU1rkPBx+fEYJFft728Wi+R/EsAM7Nlwc2ufn6x1JEIm6pQEREMRTND+NQu9waea4QxwTWoghHj0BsbtIeS/U2aS+Zo0ekpd6CIG97YJRyumrHVniqKlQZIbGhDjhSLz+WwUVKMhTYeL1efPLJJ+jZs6c/eyOKIh566CHZcTk5OZg5cyYsFi62IiKKiihuuR+yANbIc4U4xpe5EQQBPbIzsX/e7UD1du3WBc5a+ffuNmlayd0mBTvh8Hr9zSQDszaam/mZFNG0YGND0O9ZXBw9hiKQr776Ck888QRyc3P9t4miiK+++go7duzA7t27sXv3bvzrX//Cxo0bYzZYIqJ0Y5leGXFHZl93bO+Su6VzVi7zTx8FEibNkAILiwXIyoYwaWZE47EWFMJqdrwtzR0ZnEg4HbKO4HA61MeY7OIdTidu33uvWeAc4blJm6GMzYYNG3DGGWegX79+qvsqKiowcOBAAMDzzz+PjRs3YvTo0dEdJRFRmorGlvvBamf8mQJnLVDn7JjaaWmG+ODv4Ckrl2UPzI7Hs3qJdrYmFFGEvxFmOJoaOzJCjhopYNM6xowwpgVVzT1tdqD9PY303KTNUMbmv//9L0499dSQxw0dOhQ7d+6MeFBERBRFQT40/R+8Toe6XsXdJmUP5k6D2ODyZx98WRAxIOOhe19EH9Ahghq9uhybXdqnJlBOLmCzqW8zI5xN+jQyNVrZMm4AGD2GMjb19fUoKiqS3SYIAi6++GIUFBT4b8vLy0NDg2IekYiIOpUsC9N0VJ0xCfzQNBJ4tDR3TI1oZH7Ehjp4597Ssameowae2b9BzeBhQJ5iNVC0CBage7H0WtxueUuGsnLpv4E1PIVF0ldg9qRQ/rkWSlib9BmskWKX7+gxFNjY7XZVDyhBEDB58mTZbS0tLbApI+IoWr9+PTZt2oR9+/YhIyMDgwcPxqRJk9C7d++YPScRUbJRTX/4BEyDyIpVjQiyQsq7eql6p2B3m7RBn9UmZVaivX+K6JUCBkeN1EJBsEi3CRbg8uth6dVHM1CIJHgIZ1rQaMDCLt/RYygK6dmzJ3766SeMHDky6HE//fQTevbsGY1xadqyZQsuuugiHHfccfB4PHj55Zfx4IMP4pFHHkFWlk4fECKiOAlnpUtUVsfoBSvt0yBAGJvhNTYAJaXa2YdgwZEnxOqmaAQ9+/fAP20leoGH5wLLn1XUEQUEF5XLOm3FUboELIm0qstQjc3IkSPxwQcfoL6+XvcYl8uFDz74AKecckrUBqc0d+5cjB07Fn379kVZWRlmzJgBh8OBHTt2hH4wEVEnC2eli+oxc6dJdSuL7pS+NOpbVPT2ZzE7BRWopRk4sEfK+tjsQNmgjuxDuPUgmVlAt1DTQUZWRykCI9Ere6+54ij2Euk9NpSx+fnPf46///3vuO+++zBp0iSMHDkSGRkZAIDW1lZ8/fXX/r5Rl1xySexGq9DUJG38FLgMXamtrU3WOkEQBGRnZ/v/nYx8407W8acaXo/EkXDXQqNoN+TYlI8J2FXXr72+xTZHvpdYUFnZsEya2bH0WLmvihGB+8Ac3AvvojullUVZOQhrBZPHHWJlkiDVwdQ5gmd1tLI+ge91ONchhXTK/xcJ9B4bCmy6du2K2bNnY9myZXj44YdhsViQny/9RdDQ0ACv1+s/xnd7rImiiOeeew7HH3+85jJ0n/Xr1+OVV17xfz9gwABUVVUZ6jeR6EpKSuI9BArA65E4lNfCU3cYjsWz4XE6YC0sQtHcZbB2wqqTmh4laA0ISDJ6lKBnr16mHqPH2tiAXjrn2t/SBE/gDTY7MvoNROuiO4C2Vv/NQnYOLF27wVtfp79DsB5fwOX7dzjcbunLYtHpLyV2FAAH2bhP6NYdYuBSdQAZhUX+9zqc65CKYvk7KpHeY0NNMH2amprw4Ycf4vvvv4fDIW12VFRUhBEjRuD8889HTk5OzAaq9OSTT+Lrr7/GAw88gO7du+sep5exqa2thTvcnS3jTBAElJSU4ODBg0nd0CxV8HokDr1r4V46W9X80Ei2Q6yvk/Zhaa8bsM64x1TdgNjggmfVYlOPFxtc8Mz+Tei9X4K8BtXrzcrWDj6KesK29MmOcdY5gMMOhL13TLj7zhQUAvV1wbMyNrv+e5KZBfTqK18Z1XeAdLvLKU3NCQCONIR1HZNdZ/yOCudn3SybzRa9Jpg+OTk5uPTSS3HppZeGPbBoePrpp/Hll19iwYIFQYMaQFrRZbfbNe9L9g8hURST/jWkEl6PxKG6FhppciPXyrN6iWx5s2fVYnOFoHldVceHfN68rtJyZeWmbqVl0r8bG/zFmd56p2bBpnIlDpwO7cCmoFB+jm5FQF6BPEBQCRK8lJVLe8W0P2/vBY+ipqam4wPPV/uzt1oepLQ0awQ1JoIkj1s9tXZgb8dzOGqkTt4B3bTT8f/VmP6OCudnPUZMr82eNWsW7rrrLpSVlanu2717Nx566CE89thj0RibiiiKePrpp7Fp0ybMnz8fPXr0iMnzEFH8xGR1Rbj9luK0G6zWEmGt98Cz6M6OICSgY7ZyJY7UUkDRl6m9ZYJyV2JkZkmBFKCRIWmveZGdS5Dez5YmqUVBYREslctg6doNEEXZX/HCpBkQ16wC8guk5pcet/R4rUyMAOC4oR3BmXKvGuXBymusPGeQa5dIK3oocqYDm2BTOG1tbaitrdW8LxqeeuopfPbZZ5g9ezays7PhcrkASJkkXzEzESW3YNv/hyvszc+i2IDSjFBLhP0fxMoP+r3V8vt9QcHl1wGPLZRnbVqaIa5Zqf7AlzWKVGRNbFZ1sW/58dJ/t/8ond9ZC++d18MrCNgvCB21M44aiPNvg+EsTGaW7D0QG1zwrlgI7N2pUWcjSreVDVJng3yCXLtY/MzFAwM0SVR306upqfGvOIqF999/HwAwf/582e0zZszA2LFjY/a8RNSJYpAlCXcvkUTdDVZ3Az6t+x016qDGp3q7NMWlW6ysCEIsVvl5MrOkgGL3fzUeKmpML4UKatoDqcwsCHOWye/JL4Dl1nulHY6VgY0vm1M+VB2MWizAwCHBr12K9GlKlQAtUoabYH7yySf+75988klVANPa2opdu3Zh2LBh0R1hgHXr1sXs3ESUIDo5SxLsr9yE3VxN74PX3QbP9KugCiD0Vi2524DWVqm4uPWY9DBlv6hArcfk33s8IepxTLDZYal60n8txDUrIQZcC1XbBi1OhzqjNHBI6GsYp8xc1KVIgBYpQxv0tba2oqGhwd8H6ujRo/7vfV8ejwdnn302br755pgOmIhSm2V6pfSXd1FPoHxozLMkibCxmFYDyWANJ9UfvAH7hegsida1f7cULHi97UGNib1HormytLQs6LXQbNug1NQoPyYr29DPj2V6pTSNZbMBEICd2+BZdGfwTRATERtpAjCYsbnwwgtx4YUXAgBmzpyJO++8U7N4mIgoUp2eJUmAv3K1phAAyG+bOw2WRY/rr3pSFgcbpsju2Gyhl5rrPdYIraXZWdkQJt8GceHv5Mc6HR3/1rsuNhtQ0F179VduvqEaEyG/oP11twdqHmlqK9mmchJ16rSzGcrYBFq5ciWDGiJKHYnwV67yQ3vHVqn+JVB7h21V36Pplaa7VMtkKvvsxXiJbq++ECbfJk1/WSxSUDNnmbRaStlXqqnRn7nSDWxKB8C65E9SAKJ8HxobjLWgAII2+UwWvj8KfO9HOhYOAxEUD9fX16O2thatra2q+2JZZ0NEFE0J8VeussbD69XeidflhHfFg/Il3n+YJ3XQttqkmhebVfp34Oomm13KSARmM9o7fQuTZkqro6q3m5/GCofLCXHpbNmuxeKaldoZp5zckIXSgWTXsrFB1o4iMPuiVVelugaAbpDL1UeJzXRgU1dXh8ceewybN2/WPWbt2rURDYqIqLNEc+or3A88/wfyjq2KgEax3LqgUJ3J2VMtP6ZskLRfzNLZUrFvRiYw6z7glWfal0q3Bz+lZbBMr4R4xKW/RDoW6uvUBcouJ9B0VH1sYZE6a6JsvxCwMV/gtfRU3iQP5ALOozX1Z5le2b6cvFq6vf390cLVR4nNdGDz1FNPYefOnfj1r3+N/v376+7qS0SUbsL9wPN9IHuqKuTZCcVOvpbplfBWTFE8WjF1tP1HiAtu7wgeWprVy73bl0d7K6ZIWZ5gK6GiTWtD4ewugOuw+lBfNikwk5KRKX8telOHwVY6aTVszC+Ade7Dhl5CpHVZzPjElunA5scff8R1112HcePGxWI8RJQg+Ms3DBF+4PkzN85aKYMRsJOvkF8AsaEOsFrl2ZXMLMWmelAHKspl2j6dlaUJpDXFdmi/5hSYuGalaqrQH+yEmDoMOsUY6fLuCB/PjE9smS4eBhCyPxMRJb9EWAaddCIsRPZPpRQW+3fx9b33/n1cAoOYzCygR++OFgh6Er0tUpu6VhMAUL0d3iV3AwAslctgraiCpU8/WKbPkd5bl7P9vXGpHhqskDbSLQUi3pIgAVbipTLTGZuzzjoLX331FUaMGBGL8RBRoujkX76pkCGKWiGyxnuvuY+LxwPs2RH6fKIXEASga6F2jYuenDyg6YixY320MkihKKeXfNxtUmCtyGqYzXho/WxFkiGJuC4rVTYETFCGApsdOzr+xznrrLPw+OOPw+v14tRTT0Vubq7q+IEDB0ZvhEQUH538yzfW6XkzgVO4QZbWB16ocynvFybNUHeqbs9OREQUpUaVJppmGw5qinrKAjlvxRT5NFewppp9yyBMubN9ZVT7zsE5udI0XODxga9f+V5Ub4fY4NK9Rok29ZMQK/FSmCAa6Ct+7bXXmjppMqyKqq2tRVtbHOaXo0AQBPTq1QsHDhyIW1t46pCq10NscBnqMB0tnsqb5IFUUU9Yl/zJ1DmCXQtVYW75UM0PN82t+9uXRpt5DzoaVW6Xf0Arnlc1LmXk4dvQTrlyKSsbKC4B9uw0NJ6YyciEZckT0mv1BSZtbfI9adqna7yrlwDb/wPZ68vKhnWF+jND9b5kZfs3KFS/Z/L7VeeKws9WMkuV31F2ux3FxcUhjzOUsZk+fXrEAyKi5NLpOwArM0S5+R0bs0UQWPkDjB1b5XfoZEA0p3zcbf5al2B7ofgKfDUDGr3nVY1D8cFzrEW+S6/NLjWuBKTl2/EWbK8ZewbQ/zj/e2OtqIJn2uWAN+A16hQ2C5NmqFZ3+d5//+qwwPc34H6VGGYfU2EKNdUYCmzYOZuIYk3VJsDtNj19INbXoeaRe+E+dLBjebTeh67eh1uwKZ/AvVCUG+WtWAjr3IdDbyh3pF4+baK1MVwwXg9wcG/ovkmxYM9QF/q2NAV9z6wz7gHyunbcoKynycjUfJy4ZpX2fjdob4FQVq5+n3XGoV5ZNSMqQTOQeNNcFMHOw0RE0aTMEHkqb5IfYKDGxLN6CTzKnktaG7wNHCKra5D91a2sbwkUGAz5NnJTfh/Y30jLsRbZh5//Q1cvw6Pk9UY3qCkf6t/XRpfVBvTuCxw6oL6v9RhwWKdPVVsrPKsWwzJ9Tsf7W1winaetFcjIhDBnmfZjta53wPtvmV4J79xphva0Uf1sBU5lRRqMcIVTwjEd2KxatUr3PovFgpycHJSXl+P000+Hzca4iSjdGEnNG0rfhzN9oPUhozyPPUNaRlwxRdpd9tb71FmWrGwgN1/6AqRgx2iRZ1Nj6GOctaqMAQB477wBnbs2W5CmfNqXVOvyeKRgRGu1k9a+NIF8K7oC31+b3R9camVKxIY6dYCp6NQt5BfAsujx8IpwoxmMcIVTwjEdefzwww9oampCU1MTLBYL8vLycOTIEXi9XuTk5AAA3n77bfTu3Rvz5s1DQUFBtMdMRAnMSGreyDFhrRzR+JDR7B/k097BWfXBlpuvW1zqD8qUAYiv7iUnN3RGpe5wR2bHUQPvH+bDev8f9Dtr22xA6YDgWZVwZGRAnH8bQgdTovEl3Da7/DVorejSqFkKpKpz0ikMDrsOLIrBCFc4JR7Tgc2dd96J5cuX46abbsKZZ54Ji8UCr9eLf/7zn/jzn/+MO+64Ax6PB8uXL8dLL73EwmOidKOxFNdTeZM8M2PgL+ZwPrSsM+6B9cnlaA2osfGdR2yog7diqvZ4TXzQqWtoBCnwgLSSDIVF8oaOmjsDKwKJPTukx5aWaQcvbjdwYA/QdwBwYG/kOwYLgjRuvR2JBUv4bRZKy/xtIDJ6lMAz9S54Vi3WriPSy5RoBJrRLMiNZjDS6UX2FJLpwOb555/HL37xC5x99tn+2ywWC0aNGoX6+no899xzWLhwIS677DL89a9/jepgiSgJKIMErU3WDAQS4aw2EfIL0HPZU5rLWr2rl2oHBMqsTqgPOq1VTO42f/ZHtwWAqsGlnHfFQt+rgGYG5VgLsHcX0K27didsM0RR+zkAaRpu1n3A8nvMndNqAwYM8l8nQRDQs32JcUcd0TZ56wTfVJ9SjFbI+TAYSW2mWyr897//RWlpqeZ9ffv2RXV1NQCgrKwMR46Y3LGSiGTEhjp4qirgqbwJnqoKza3jE41su3nlVv/tQYGRLemj3tJBKztQ1vFBbGSbfgDBpy1cTqgDhvbvLSF+3e6tbs/WBJkWEr3mgxqr1dzx2TlS00yzunVXtS7w8QcSpQOCnsL38w6nQwqwCoulnxOA7T3IMNMZm+zsbPzwww848cQTVfdt3rwZ2dnZAIDW1lb/v4koPMm4lDTwr2HVRmrtQYGhv5ijvdpEmQVQbJRn5L0WG+qkjIMvYLNa5dNMBYWqZeDi/FsjG3ekzG7IVqfusm2IkToVZUGw4nvVNF9pmbT3TRgr5Ch9mQ5sRo8ejTfeeAOiKOKss85C165dUV9fj40bN+Kvf/0rLrnkEgBSG4Y+ffpEfcBEaSXJl5KarWUIuuw6wtUmwqQZEJfOlupKMjIhTJopP8DAey1tvBdQAxNQT+LbH8VUIONbfeXbtyfaxcFA6FVL4bLZgLJBERd3y+hdA648IhNMBza/+tWvUFdXh9dffx2vv/667L5Ro0Zh4sSJAIDBgwdj5MiR0RgjUfpK8l/oZmsZdJdd63xw+gMhZy3QdBTokouanr0hTr1Lvikc2jd88620aWmW6l4Cx2bkvVYVRm8DsrIhzFkGIS9PasVgRk6XjlVDevUmGZlAz95xaJ0QoqlU6QDT2cOQga7ONfA/zteywemAp6qCu/ySJkO9orTs3bsXW7ZsQWNjI3JzczFs2DDd2ptExF5RFC2xvB6d3a8p3sz29NHsGQRo9oEKdW6991qWRTpSr73sOStbyt4E23FYi9FO2FnZQPcewL5d5s6vJdSKp/YNDHUzSAEbHAb7WVT+f2Fsf6PgP+9G+32RXKp8ZkS1V5SW0tLSpApkiJJR2q3eMJuhMrpc2MC59d7rkC0SACkTpOxFFchqk5Zqe9zyzIvRvWFamqWxZ2TqL9E2qlt34OgR/eceOKR9ebwL3rsmq4OgfscBALxL7jYcbKsai+rUMIX8eU/yqVnqHNwamCjFJVOTPtP7i+j1WdIIiMLeu8Toh6dWLUtmFoTK5bD06ae/j45Rba3y5pHhamqUL7n2CViuDbT3Y9KbjdIoslb+nFln3ANPVgbcS2drt4sIJyhRXu/GBnnfLSIYnIq69tprsWjRIpSXl+Paa68NfkJBwMsvvxy1AcYKp6IoWhL9eqRy+t4/deGrvcjOgdDSDDG7C1BYFJUgzrPozsiLeq02qdjWaIZGi9Fpq8wsoFdfqfjaTHNNmx3W1a/KbvLceq1qB2Dk5mtO6al+zsoGQajZB7G5Sfv5wvg5FBtc6v5QKfTzHCuJ/jvKqKhORU2YMAGFhdJfQFdddRUEQYhsdETUeVI4fa/V3FDc/iPQ3AQ4a+OzPF7ZUgCQpqA8GlkSowTB+OqmvK6wzn1Yetpbrgzref3Zl6wc4NgxKXNjzwCKe0k7IAfyZceUP1d7qyHq7ZCs6PtklJBfIAVWgYFNCv08U3QYCmyuvvpq/7+vueaamA2GiGIgyVdWmWIyiDM0TRes27dMe2sFqzXylgeqgYrSVJQRgde3r6K/VLCsj6/XFTTqiqx2Kbjas6PjNpsdKCuHZXqldtNKLQGPCTuTlk4/zxQW0zsPB7NlyxYsWLAgmqckoggZ2eU3ZSg/5Nq34tfbudnQ7saGPzjbWysca2nvxaRBiOqvXDmLRXV9LbfeJ7v2QuXyju/LBklfvp+LW+/rOJdW00plY8+CQv9Ow1pNKwMDJd9twr2PAG43vBVT4Jl+FTyL7jS9m3Za/TxTWKJaPNzQ0IAtW7ZE85REFKF0WlmlbIIJtzv4bsLK9gS+jtsBOvocaRTA6g7Eqi7QzciUHh+rEgevF9hbDe8DtwMN9dL0UUYmhDnLYOnTr+M4jUJfVQZFryg7UGDAp9Ud/bb7VQ1J/f2ifNr7a5n5+Uynn2cKD1dFEVHKUDbB9FT8Rn5AQOAiNtQBdU7V/cqN33wfpJ6KKcb7NGmtOop0mbYRLc0dmRNR+l6cPwuewmJZMbWqfcTcabAsetz/mvWDufbpttIyeaZEY3pIsyGp1tSgRjAZTcm0KpCiI4Z5USKiyEXUCLTpqPz7usP+83j/MF9jozpRNiXl3VcNz63XwjPt8uBBTd8B0vSILUH/VnTWSq9r7jTp/VMGGC3NHfcBHU1BS8va+2L5ptbap9tsNllwYHh6SGtar6kxsteG4D8jUW+mSgkvQf8vJKJkoPfXcLh/JYsNdVITyb3V0g2+Oo2AppLBpi7E+jrUPHIv3L6pqOwcee2H6JU+4Bw16Piw1lC9Xdqp+PCh0E0kBQtwpEEqzO1smVmAx2N8iqylWfpg15pqar/P996q+mIFUgRGRqeHLNMr4Z39G/lKrZxcY2MPImgD0xReFUjamLEhSnARZSxiTO+v4XD/SvZ/mLrbpK/qbR1Bjk+QDybP6iVo3fKt/3mht4cKgKDFLu426RxG9vwQvYDrsPR8WlNQsXSsBYAoBTgF3aWi3VBcTimjonWss9b/s4bq7frnCHMlkpBfAAwYJL+xsCisc8kEC16UY+UqqpRnKGNz1113GTpZc3Nz6IOIyJSgf41qiKSmwPRj9T5Qwv0r2chxGh9M/kyPMsOQkwuUlErBkTKrIQjGApdIaO1pE21ut/R17BhQVi7tMxNsI7/2+hfLosfVm901HdWveQnRkNSosHeADibIEvCYPB8lNEOBTW5urqFN+fLy8tCjR4+IB0VEAUwGCWYDoYgeq/eBEu5eI1pTJKVlUu1KkA8m3WkTXzZAK7goKJQyOsplzNEU66BGRlS/B8rgLWBjPH9wE/ChD6dD/n7Y7NLtERbdxrqAN1jwwlVU6cdQYDN//vwYD4NSFVckREGkjSHN1BSYfKzeB0q4fyVbplfCu2KhrMbGcut9oX9mtMZps0vnW3K39mO694AwaQbEBbcH73adzJQJqdx82XuptXOzrEi6rDwqQUEkwbYRDF4oEIuHKaZi/QstHUTcGLJ9kzpDwaXJIErvAyXcDxohv8DfDsAUrUxPWbn0OvX2ZHEcgrjk7iQLavS6UupRHJudI/V/aj2muceN7GctNx9wu6V6m4JCKQhcsyq8P1JYwEudyFDxsMMR3j4DTid/eNMef6FFzBckWJf8yb/TazDKpbcADBfyJtuurr7CajgdUgGtzSb1NCobJMseSUuWFVyHjTWVzMiI8qgjoQhUMrOkpeZar08pM0vKhLU0S5v5tTRDXCrPZvl+1iyVDwEH90pTW+0/N+LS2eEvm2YBL3UiQxmb22+/Hf/7v/+Liy++GCUlJUGPdbvd+OKLL/Daa6/hjDPOwIQJE6IyUEpS7OvS6VTTC5U3yQ8IElwmW0pf1dOofCj6PvoC9v9nCzyBtSOlZeF16BaE2LZBiFSXPCAzC5aqp6QpvGCv0eNRF0u3NMNz67WyzI3YUAfv3FvUtUfKDQZ9S+INZG9YwEudyVBgc++99+K5557Du+++i/LycgwfPhwDBgxA165dYbfb0djYiJqaGvz000/49ttv0dLSgksuuQTjx4+P9fgpwfEXWgJI5eBSJyPoWb1ENgWKskHSqh6zhcIWq/Hmk/HgrPV3Mbfcep96lVMgvaXovszNirUAoO775JORKb/dtyTewBRzsgXMlNwMBTZDhw7F0qVL8fXXX+ODDz7AO++8g9ZW9f/sPXr0wEUXXYQLLrgA3bp1i/pgKfnwF1r8pWpwqdlR2he0KQOevdUQ7v09xDUrgR1bpakYIzweIDPT2JQVIDWiNHruaNr+H3gX3SktbQeABhdw9Ihi3EFqcwKzMVoZvaxsCHOWSe+fyyl9Ba744hQzJRBTxcMnn3wyTj75ZLjdblRXV6Ourg6tra3Iy8tDaWkpCgtT6C9BohThCy59K9S8S+5OmBVqkayaU2UWMrMAjxv7p1ymDnjcbRCffVSqwbFYTQQfojqosVgBr0f78HgFNhD92RtkZbfvbaOx1NxmAyCo7wuMeZQZvqzsjj5S7X+keKoq5FOAqZQFpKQX1qoom82G8vLyaI+FiGKos1eohQpaVLUcZsekzBIcOwbs3AadkEO9SZ/NJhXdZmYB9XUGXxX0gxog9jsPW21At+7SiqW9O7WfL9h0m82ufb8gBKx+mtmRmdEJNlM1C0ipgcu9idKFwRVq0er/pBdI+c+j6hytPyZNqmXcIZZBKwtn87vBMne5FFwlC7sdlsplEPIL1FmTUGx2aSdmrcDG6/HXy4hrVoYMLjnFTIksgcv9iSiqcvODf98uav2fdAIp/3n0dgMOwt83q+I3wJ6dwZ9fyaPIbjQ16hfKJipfE0u0L2M3s2KrrFzdl8lml7JAgfRaKhAlCWZsiJJIrHZylp03Wv2f9FZj6T0uYLt/vdepWt4dCXebVEic0ATAIsjrdhyHpHvyC6RpqcCdggNZbdJ0W1urtBnfpJkQ8vJVU0jeudPkQV9TY+xeDlEnYMaGKImE2zUbgLqgNuB72XmVmZTA/k9at+vQ3exP+TibXbrfV6CqHE/76xQb6oJ3nDbL7Y5Toa8ZonqMgfVAwTpj22xS4bNvM741K7U3e8zJlT9O+T1RkmFgQ5RMItnJOVhgYiCL4g9UCouklTdOBzxVFRAbXJoP1dsx2X+egu7SVIrXA+ythvfAXmmaqfImdQDjckrBTqc2lUxQAf2I/e+l1s7DysJivWusDI6CBUtESSAqgU1rayv27dsHb8L/9UOU5CLYmj5ouwS98wQ0TfQXjBYWS3UpzlrzWaPA87Q0SX2a2jMKWH6PftboSD1rP3wyMv3/9LdAqHpKHdwIgvz79p5hnsqbZAFpR8BabChgJUp0pgObd955B6+88or/+x07dmD69Om44447cPvtt4fdV4qIQoukl5NWBkXWaykru32fkwBaAU+0+n8pt+gP5lgLaz8sFv9GeUpCfoFUHByotMxQz7COgLUoooCVKFGYLh7++9//jvPOO8///Z///Gfk5ubiqquuwt/+9je89tpruPnmm6M6SCKSRHuZraoYt2yQFNwE258kzBYNyoJg2DOM7+gLAG1t0vgaG6QGlqH2jLFnSEWxqZJJtlgBtxvis49CvPU+AKLs/Qy1/0zInmFsWEspwnRg43A40KdPHwBAc3MztmzZgt/+9rc444wzkJubi7Vr10Z9kEQUI8oPr8YGWJf8KehDwt2czbviwY4mjY4aKfAww+MGbDZYl/wJnluvDR3YJHKPp0CCBcjrKrVBCLYXj296rnpbRzYlYJ8gcc1KWKbP8Qc73tVL5MFNqIA0lXuKUVoxHdi0tbXBarUCAH766SeIoogTTzwRAFBcXAyXyxXVARJRDIXxYRZ21mhvtfz7cAKPHVulqbPsnOTaf0ZGkKZ9Couk4Kx6G9AQbOdjAaqARyub4iuw1tldOlRAyt2EKVWYDmyKiorw448/Yvjw4fjiiy9QVlaGnJwcAEBDQ4P/30SU+JLuw8zrjd4+NvFSfnzHDswVU+X32exAfoFUT5SdAzQ3SVNwys0FfQGoMihVBjzV2/2tEizTK9mBm9KC6cBmzJgxeOWVV/DFF19g165duO666/z3/fe//0WvXr2iOkCiRBVYM1LTowTi1LukKYUkEuzDLOqbAZaWdUxFmRqkBYCobomQjAKWz2suXy8r918PdcsEQap/Ki0LOIdis73VS+TBjrvN3yoh1r3BiBKF6cDmyiuvhNVqxdatW3H66afj4osv9t+3Z88enHHGGVEdIFGiCkz7tzpqgFWLU+qDI9pNMy233gfvioXte9ToBCmCRVoC7mOzS6t9fFM2yS4ntyMYUWZXbHZ5xkx5v0UAysplAabyesgycC6nPHBiMTClCdPLvQVBwOWXX46KigpcddVVsFg6TlFRUYGf//znUR0gUcJK9VUkUX59Qn4BrHMfBop6KO/xL2XGnYukpcm+/kXuNim4aj0m3W9J8j1F6w7r79VTVu4PWMSGOvVO0e3TcMGWYQcu6Vct/3Y5uT8NpYWwf0s0NTXhm2++wT/+8Q80Nqb5/hKUniLYLC9W/E0iFZuwhSVWr0/ZfLNvGTBwiHT768+3N3dUbC63f49ULJzsS7dFxfhtds09iYI25zQYYKp2JW4PEr1zpzG4oZQWVhPMV155BW+88QZaW6VVDUuWLEFubi4eeOABjBgxApdffnk0x0iUkALT/hk9SuCZele8hxTV6SOtwuKYNOE8dKBjP5v2MasleX1NVrZ2EXBATY1MsODFYIDpy954Km+S1920dwhPpWlTokCmA5v33nsPr7zyCi688EKcfPLJWLp0qf++U045BZs2bWJgQ2nB98EhCAJ69uqFAwcOQIx3gWsUp4+0CotlBa0BgZOpgEc5xaLcpM/lBHr1BfbsCHvsCSMr29/c0zP9KsWdAoRJM6T3VPm+KZfhZ2VLGa1wVq4pzwWk3rQpUQDTgc27776L8ePHY9KkSareUL3af7kTUZzEepM15Qdie18hVG/vqBlx1MA7d5qsW7dMbr76g1b5HIk4VWK1Ah6P8eP7DoDltws63gPlqrCycohrVmkGilrZsnAzY5bplfDOnSaf2kqAaVOiWDFdY3Po0CGcdNJJmvdlZ2ejqakp4kERUXhi3tBQ+YHY1Ch9MCsLYdunOwyz2eBvW+1uM9dHqrOE2ik5M6ujXubh52G9/1FZMGK59T55n69b71M39mz/Xq8zejiE/AJYFj0edo8xomRjOmOTk5OD+vp6zfsOHTqE/Px8zfui6b333sObb74Jl8uF0tJSTJ48GUOHDo358xLFQjTrVvx1FVUVUjPD9qaG0aqpUGYS4HSYL3JVTkUBgNuDhK+jUQZvgBTM5HWVXlNLszStplPbpLlnkLKxZ4wafXLzPUonpjM2J5xwAt544w20tHTMiwuCAI/Hgw8++EA3mxMtGzduxLPPPosrr7wSVVVVGDp0KBYvXsyu4pS0/AW/iq7LEYnRUnRlJgGFRfoHH66FZ9rl8Nx6Lbz7dnfcrjkNkuBBDaDdm8rjgTDrXvV9zlpjq9NycoN/T0SmmQ5srr32WjgcDtxxxx14/vnnAUh1N/fccw8OHjyICRMmRH2Qgd566y2cd955OP/88/3ZmqKiIrz//vsxfV6imIlFENJJS9Et0yulKa9ANnvHRnteL9DSDHHp3fLH+KZFfEuRk4ZiGbq7DeKSu9XZnKajxoJVZWAYLFAkIkNMT0WVlJRg4cKFeO655/Dee+8BAD799FMMHz4ct956K4qKYvc/ptvtxo4dO1SrrkaMGIGtW7dqPqatrQ1tbR2/dARBQHZ2tv/fycg37mQdf6oxcz3E+jp4AqZyrDPu0Sz4jfTaWmfcA8+qxbLnicXPi9C1G7y5+erCVGetPAnT0gzPrdfCUrkMlj79YZnzkPRezJ0mDwpstgSalhKArCz5aysrV++crFzVZbOpAx2XU/P976zrFA/8PZU40u1ahLWPTWlpKebOnYu2tjYcOXIEubm5yMgIUVgXBQ0NDfB6vejaVd6Pp2vXrrpdxdevX49XXnnF//2AAQNQVVWF4uLiWA61U5SUlMR7CBTAyPWoeeReeAJWwVifXI6eCx6FY9Hd8DgdsBYWoWjuMlgjzbD06gU8+kJk5zCopkeJ1FKiXUaPErQdPQKxWbGQoKUZ3gW3oeSFd2EtKJTei4CgQcjOQY/lz8D56AK07dwOALD3HQD3vt0Qj8Whk7fFgt5PvaG6Nvsnjw/emdxmV9UdZfQoQU+tPnqdeJ3ihb+nEke6XIuwAhsfu92OwsLOXzaoFXXqRaJXXHEFxo8frzqutrYWbq058yQgCAJKSkpw8ODB+O+b0gm0shwRbwoXRWauh/vQQdn3rYcO4lDzMeCOByEA8ALS982x3TZB9p76dgJubAjr/RWn3gUEZB08U++CcKRemqJRFhZ7vdg/73bY5jykei/EYy2ouXOy7DFtXi/g0SjaNUoQAAjqHX+NyMjQvjZ9+suXbWdkyldxKbM1Njs8U+9Ku60w0u33VCJLlWths9kMJSVMBzaB2Q89saqzyc/Ph8ViUWVn6uvrVVkcH7vdDrtdex4/mS8wII0/2V+DEZ7VS2R7fXji1Gwy1OolQ9dDY9opHtdQ+Z76Kd5fQyu28rqqVwDldYV1xVp4br1WHdw4a+FeOltdS9RejyOzZ6d6t14zMjKB7j2A/btDHysjQJizTPPaWG69T70yzFkre6xMWTmQ1zUt/l/Vki6/p5JBulwL04HNX/7yl5DHxCqwsdlsGDhwIL777jucfvrp/tu/++47nHbaaTF5TkoACdJsUtWuYMVCqZ7C5URNjxIpc5GnHWD7aG28FhfB3sOA+4y2aPAHQE6HtGQ5J1cqhJ11P/DwXHnGpOmoev8WPZEENYBU/xIqqNGq6yk/HpY+/TQPVy6d9i+t9ykt8/9cxPUaE6Up04HN2rVrVbc1NjZi06ZN+Nvf/oY5c+ZEZWB6xo8fjxUrVmDgwIEYPHgwPvzwQzgcDlxwwQUxfV6KI2WWw+WEZ9Gd0r/bp0+i0rMoFGUwsLfaP+3Q6qgBDGSSEmY/Ea1t9gPv89HbaViRwZEFQIB//xy8/jwsy58NsfeNgKgVCyunhUISgLJBECbNhLhmZVjBSDR3CU40MekNRhRjghjFvNTrr7+Obdu24e677w59cAR8G/TV1dWhb9++uOGGGzBs2DBT56itrZWtlkomgiD421ekQ1pRbHBJHxyB2/YrlQ+NecAg65MESEWigeMp6gnrkj/FdAzR4n9PA2tsGlyybIv/AzvwNWdly4OS8qGwTJ8Db8VU7Wuj8Z6o3kezBAHQ+rnPzFKvUDLK9zoMfoinywe+6lqZ+P8s3X5PJbJUuRZ2uz02NTbBlJeXY/369dE8paaLLroIF110UcyfhxKDbpfiQJ0wPaXaddftlheRRnmvmFh9eGqdFxDhnXuLFLQE7FZsmV4pTbntrZYerPxjYMdW6XF6AafiPREb6to3swszS1M2CMLk26XsinLay+kIP7Cp3i4Pztr7XSE3X1VcLf0cRK+LekJLkGlgIjOiGthUV1cjKysrmqck6mB0+iRGlNNIgVmPjB4l8Ey9K6rPF6sPT63zAlAX7rqcUiCltS+L/2QaBb+A9JiyQaru1apg0IysbFhuvU8ak8b7oKp1AfSzO0par88X5CmKq/3BbSCdD/ykz+zEuqkqUQyYDmw++eQT1W1tbW3YvXs3Pv74Y4wZMyYqAyNSkmVMsnOAQwek/UQyMiFMmtnp4/EFOoIgoGcs0rw6H54Rf1gqz1u9HdB6vO9DTHm8zQ54PVJQo6ege0fPKtnqqwg2CGtphnfFQlhuvRfeFQ92ZJFKy2C59b6O7NK+XVIwU1rWkd1xOQHXYXnrA6sN6NZdfXsoviDNwAd+smd2EqbYncgE04HNqlWrNG+32+0YM2YMrrvuuogHRaQlMGPiqaromHZoaZY+vBLgAyOqf6HrfHiG+2EpG1sgd5tG80UBcLvh3bdL3bSyrFz6b7A6mSP1UoH37v8qR6F9fGYW0CUPOHok+HTS3mrpNQRmfaq3+d8D272PqGsJKqqk137XjfJz2e2wLvmTtCTdTGDjn44KqFFyu6WpUuU1T/KpnIQpdicywXRg89hjj6lus9vtKCgoiMZ4iIxJ0A8MvaAjnIBH96/lMF+7auVSIHcbUDYoYKWXCFRvg7h0tnyqKSvbPw7/2FxO9VTOsRZzU05trVJwZaRGRuv1Vm/zBxaeBY+q7vauXqrepM/XcDInV79DOQD0HSBldxQr8GRBtl6gyakcok5nOrBJhVYElAIS9QNDJ+gIJ8ui+9dyuK89WADkdkt1McpzK5dO5+b7AzJ/wBaseNgovVodpV59gVqNHXzdbmncjho4Ft0N3PGg/H6t1+5rOFlYpK7NCZSZFfxaBQk0OZVD1PlMd/cmSgSyDtHlQxPnA0Ovq3YUM0xhv3ZVAKSod/HVjgTKyNQ8h9hQB09VhbSSyEhAkhmFRQWZWQDEkM/Xuu1HiA0u+Y3K1xWQebJMrwzeZTzUtQrSSd0XnFqX/Emqx0qmwmGiJGVoH5uZM2ca7goqCAJWrFgR8cBijfvYULQEXg9vfZ3mZm2R7AcSLbK9a7RWKLUHSYHH+Deuc9YCRxvbdwIWAKvV2LSRxQIMHAIcOwbs2RH5i1DuHaQnK1uqfQnIkgTbRC/o3johrpXyfRUmzYC4ZlXyroSKEv6eShypci2M7mNjKLBZuXKlqXbnM2bMMHxsvDCwoWgxcj00N8TrzF2TQ40pxDgMb6onWOS1LO1BgeeWK421RxAEKUukFzRZbebbLBgIIo1cH6N1UokQxCYC/p5KHKlyLaK6Qd/MmZ2/lJYolRguNo2A2QJlUyteQk3H2OxSj6S2VmDfbgAikJnVsQzf4zH2PBCk1VF6gY0tjMBmx1Z4qiqCvh9G3gvDdVIJWthOlC5YY0PU2cL84PPVtXgqb4KnqkJVR+L/4HXUANt/7Nh4Lxq0CpSzsjvqfKqekoKOfbvgX9J9rAXi0rul1UrKZd4Wq3bdjehVLz0PzBbrBTxlg6QvLV6v9H7MnaauvTHD6HULUnNDRLEX9s7DTU1N2L9/P1pbW1X3me3bRJRWwlzVFDJjEMNMgaq1QvumeLIMiNbz+XbvVfJ6gGM6WRzl8UFT5wJQVg7LrfeFDuRamjvaJIQzBWjwunElFFF8mQ5sPB4P/vSnP+GTTz6BV2fnUa0O4EQkURfoztDsmK0SKnDR6oIeYgrGKCG/ANa5D8tu82WQ/MXIufn6LS+izWaXAppgm+Fp9aMKaJNgdgrQaMDCTe2I4st0YPP222/jyy+/xPTp07Fy5UpMmTIFVqsVH330EZqamnDjjTeGPglRGlN+8BmuuQmRMfB/8Pq6oLvb/FMwwbIU3n3VEJdWSHvWZGRCmLMMQl5eyHodZQYJZYMi67BthlbwoHx/ysqBg3v1l4ebzGgxYCFKDqZrbD799FNcccUVGD16NACpo/f555+PxYsXo7i4GD/88EPUB0mU0gxOIYXav8b/waucIvE1ctSpuxGXVkjHtG+SJy6921i9jnKcjQ1AXteQL1dOCG+PG1ULCECYNEOq+7FYpNVZLidQUioFXEU9pfsCsfaFKCWZztjU1NSgrKzMv/w7cMn0BRdcgGeeeQa/+tWvojdColSnzDTk5mtOTRnOGATrgu50qG9T7i7cesxYsKWXQVI+d7Al2oVFUgNOsx2/fe0QELAarHqbvOeT67D0VTYI1iV/0txvxv8+By7xjnC5NxHFl+mMTVZWFtxuNwRBQG5uLmprO7Yiz8jIQGOj+i8pItInyzRkZUtBQASrmyzTK9XZCR+NTIdqd+GMTEMreyzTK6VsiM3evnGeW1reHZhVevh5WB56Wn88hUXqJpuaFPto+dohIGBKTK+RZXvBs3IXYHHNqo73uXqb9KX8d8D7H9NVZ0QUNaYDm969e+PQoUMAgMGDB+Ptt9/G4cOHUV9fjzfeeAO9e/eO+iCJUpm4ZpVsKggH9soPCKMWxJ91UMrKUR8/Z5kssBLmLOuY9ioslu5zOlRLzIX8AmmJt6+ep3obxDUrYZk+RwqEXM6OD/9Z90rTQz753Tqm04JNCVksQPlQCPNXqIIo/1jCXf1l9HG+47g/DVFSMD0VdfbZZ2P//v0AgGuuuQbz5s3z7zRss9lw5513RneERKnObC+iEMSGOv0sSEuT7Dj/1EppmWpqxVpRJU3VOGulgMtZC+/cabAselx/JZLLqV6WvmIhsOu/8h2JjzUDbje8FVOk5dyCoL2sOyMTcDmltg5ARzuF6m0dRdbBpt4AaeNALaEeF3ic1vGs0SFKSKYDm4suusj/7wEDBuCRRx7BF198AUEQMGLECGZsKGXFrMZC+YFZWiZlQgzsg6I1Ju/qpforgbI7MjaqAKRiClBWLu91pAxcWprlq7Y06oNQvV3+mOrtUC27PtYSvK7GYpGCmoDl2apGle1j868GczqApkYIOV0g1jml2auMTAiTb9d8n2TLt0PU2Mieh/vTECU0Q72iUhF7RSW2aAYR0TqXXg+gSK+HkZ5Neq9Ba0xwOfUzEVnZsK6Q9pnyVN6kfVyoJduFRdIUlcspBUqHDkitFEQAFsFE+4QgtF6HsgGmRg8mQRBgeeRetG75Vn4ugP2bOlk6/J5KFqlyLYz2ijJdYzNnzhy89957LBKmmIpmoWbUzhWjGgtlUatW0KX7GpRj2LkNcBzSf7L2Gpug01Wh9qGpc3aMZc9O6XivV5pqikZQk5WtXXtTWhZ0ubuPR7nySyvzxPoYopRleirKYrHg6aefxvPPP4/TTjsN48aNw4gRI0x1/yYKKZofRNE6VwxrLPQyMv7bd2yVP8D3GpRjCtUgsr3GJuh0lZbArt2BtTIR09gdODdfus3t7ph+0mrhoMOaXwDPwX3y89lsrI8hShOmMzaLFy/G73//e1x88cX48ccfsXjxYsyYMQMvv/wyDh48GIsxUjqKZiPBKJ0r1AZ5kdDLyPhvV7YvaX8NsjEpa1AAqVYlkG//F7PBXaz+binq0TFV5FNQGLAvTfuKK5vNUFAj1tehdfcO1e2xvHZElFgiqrHxer345ptvsGHDBnz55Zdwu904/vjjsWDBgmiOMSZYY5PYjNSdxONcWpTXI5yaHlW9S1FPWJf8SX27xQIMHKJ5TlW9DSAt1Q7MzJQNkrIXvrYLkbBYgH7HAQf2yKev2peMiw/+LvRztAcZyuvjXXK35vsRiuZ7YPCxFF3p8HsqWaTKtTBaYxN2d29AmpY65ZRTcMopp+A///kPHn30UfznP/+J5JREAKLbl6eze/yE7MKtRW+aS3l7RqZuoKTVgVuYfLu0VNrXqNLtln/w2+xSXUw400sDh8BaUQXvvl0Ql9zdHtwIQGExxGcfVR+vLP612f2vRfX+KBtq6uzLowwiUaexszKnnYjSSkSBTXNzMz7//HNs2LAB27ZtQ0ZGBkaNGhWtsRElpzBqevSWElumV0pNLH1ZF+Vy6wCBHbh9H/jiHxdIuw37pqACNtgDIH3oe73SXjU+Vpu0p4w3SCFwe1ACtG8w6M/YiMD+3apjUVYuBVWBS7zLyiPOnKkacSp3OBYs/s0F2QKBKD2EFdhs3rwZH3/8MTZt2oTW1laUl5dj6tSpGDVqFHJy1DubEqWVMIqM9bJK/l2EA6eTDARKsg98wL/BnuqDv7FB3SuqZ29g/57gTxAYlBjYYNBaUaU5Jajkz8Ds/q96nFqUz52Ti4yBg9F66KD0GN/rdtbqBoTsAUWUWkwHNjNnzoTD4UDXrl1x4YUXYty4cSgtLY3F2IiSUtQ3clMGSo4aeG65Eug7QH+lkF6wkZMrLZt2OTs++JUO7IFqpRIgBUU5uUBhkfw1hdrB90g9xAaXoSlBVUAW+BxalM/d1Cgt9/ZlogwEhGFNHRJRwjId2JSVleHGG2/EKaecAotyxQURRb2mRzUdBUjLuqu3qVscIMQeNYVF/rF5Kn6jHdhotTgQBFhXrPVnN7xL7pbv4Ft5kzrz43OsBd7VS2CZPkcKItp3CPYXNgcGS8rgI6BYWve98QWR7YGatNR7nzo7pRccKZ9zx1bV1JXYUAfvigdl9UvKoJKZH6LEwJ2Hk1CqVLinAt+HmbWxAZ7c/Jh9mOnuEgyodtHVXBnk06070NwkBRN1Du3+TFoKusO67Bnd3Zc9i+4M3iKhqKcUWOiNq/1cAMLeIVj1HhUWS7skhwg0dN+vgOfWPCbU+57muxvz91TiSJVr0SmroojSnW8aQyqz3eefxoh2SwjdDAzgL471rwxS7rwbqO6w9F8zm/MB0n4zgGZhtNhQ15HJ0NPYIC9Q1uJywlK5LPxpPOW0VEB2Khh/1mfHVvl+QYGvVWsaK1SROHc3JooLBjZEkdD5MItm3UbIXYKPHukIGhw10kqgaArc0E4ZPDQ2SFM0mvvVCNKeOVarsUDK5WyfsgovCPQFKIHZMyN8U4eqjEvg1JVWHZHWxo/c3Zgo7lgkQxSC2FAHT1UFPJU3wVNVATFwybTersaxbAlhU/w9ogwqotnyICtb1r9KmDRDHji1NOtna4p6wLr6VSCvq/o+wQIUdJfqYHyvx90WUS8vIb8AtjkPofdTb8A25yHTwVGw3Ykt0yvbNze0ty9fH6QKnIRJM6TXY7FImxROmhnW6yCiyDBjQxRCsOyLbpYgmn+9h1p1FHHjSY1+TT5du0v1M40NHRv8GQ2c9DYZBIDuxf7dgFW1MXGawglW9B24R5Aecc0q2X5D4pqVQBrX2BDFCwMbSkumamCCZF98WQJlYV40l3wrz4U9O6UAI1r6DgAO7dfu6l2zt+Pfjhp1tgiQlo/bbB2rnRRLwjVXdQWb5knWKRzW2BAlBAY2lJZM1cBEccO9cCjP5amYoh2EBLLZpSXhgSsgMrOkZdzKepc9O7QbaGpxK7JDgiVk120hvwCWRY/rBnpR3/cnXlIlQCNKcoYCm5kzZ0IQjLf3feyxx8IeEFGnMPHXdTw+eINmlAqLQq8wcruhml7qkgfhtnkQl95tflVUx8jk33brbqiWJeQ0TxSnbDx1h+FeOrvT95NJmQCNKMkZCmyGDRsmC2w2b94Ml8uFIUOGoGvXrqivr8fWrVvRrVs3DB8+PGaDJYoaE39dR/rBG87SbyN1ParlyfJnVd9UWARLn37AirXqFUCB00lHj0jZHghAr77601Tt50w0jsWz47KTcGc3WyUibYYzNj6ffvoptm7dij/+8Y8oKur4pVZbW4sHH3wQw4YNi/4oiaJM9td1bj7gdktTPIoakWj8pR/W0u8QdT2ay5MFi35hr8YqH2V2QbbLri8Qy8wEuuTJAxubXQoEEzQr4VHu48NaF6K0YrrG5vXXX8fVV18tC2oAoLi4GBMmTMBrr72GsWPHRmt8lAbisRV94F/XqgChvXGimb/0ZdMfufnSjb6VROF80BrIKKmKip0O7Smq9iXbgYJlF0J2zC4rlz020VoJWAuL2tsqtGOtC1FaMR3Y1NTU6Hbw7tKlCw4dOhTxoCi9xL0JoV6gYeIvfeX0R8cdGoFBbr5sp2CtQMBIvYaqqLiqQh3YZGVDmLMMgH4AorxdFYgFNs7UGEvcr59C0dxl2D/vdta6EKUp04FNcXEx/v73v+OUU05R3ffRRx8Z6uNAJBPvZbJ6+8SY+EtfNf0RSBEYwO0OGQiEU6+hHQyJ8K5eCo+ym3fA86oyNMqdi0O1Jgjj+sUyy2MtKIRtzkNJ3ROHiMJnOrC5/PLLsXr1alRWVmLUqFEoKCiAy+XC559/jh07duCWW26JxTgplcV5maw/INDZh8UI1fRHIEVg4Km8SX5/9XbptiAf8EYCAa1gKGhDTF8AogxERK+UZcrNN5bxCOP6dUaWJ9GmyIioc5gObHz1My+//DJeeOEF/+0FBQWYNm0axo0bF7XBUXqI9zLZaKxmKZq7DPvnzpTaC4iitMKoSx5QWARh0gx5k8rcfHkg4G6Tvg/yAa8KBCqmAGXloT+sg2VPgu0MnNNFul2nf5MsaMjNl9oNtNcUGbp+nZClS7QpMiLqHGFt0Dd27Fice+652L9/P44cOYK8vDz07t3b1F43RD6JvEzW6F/91oJCKZjx9W3yuIG+A9Srlxw1UhBQPlQ6p8sp7/UU8AEve27lB39AX6VghbyqIEojE6O5M3DT0Y5aG42gQDV9VT7U3yLBkM7I0infs+rtEBtczNoQpbiwm2AKgoA+ffrg+OOPR58+fRjUUEryf4A7akI3aNTLQihvb2yAtaJKCgTKylX3efftgqeqAt6KqR3Prdk9G8D2H+FZdKe/MadyvACkIKqwWApqcnJVAZpvZ+DABpDIyZU/z46t8gagEWZcgjWcjBplsORuC7vBJhElj7ACm3379uEPf/gDbr75ZkycOBE7duwAAPzlL3/B5s2bozpAolgJ2rXbx8wHuF6nb73b0f4BH7hqqqUZ4tL2FVbKYMbXWVqpelvHB7ayiLnBJWVaCov8y9i1AjRf1sy65E8dxwfyeuWPC/KajFA+XyyyKJbpler3i3vaEKU804FNdXU1Kisr8eOPP2LYsGHwBux82tLSgg8++CCqA6T4MPShn+QMZWMMfICL9XWouXuKFFRkZUvZkYAsRLDshJBf0LHvjU/rMe0Bl5XDUvWUdnDj+8BuapTfXndYKkyu3q59vA7/mC2KXxHtjwv2mhLlZ0fIL1BnxLinDVHKM11j8+c//xn9+/fHvffeC5vNhn/+85/++8rLy/Gvf/0rqgOk+EiLwksD2Rgjhc2e1UvgUbQnCHyvQtYQKetNMjLVvZyysiFMmtnxYa1c6eT7wM7JlT9W9Ia1lF13d+P2x5nZ4C+ePzvxLkwnos5nOrDZunUrbr31VmRmZsqyNQDQtWtXuFyuaI2N4inee8t0BgMFrIYKm6NQbxL44StMmglxzUopy+Kbjmpplm6rqJKOX7FQWoEFAKVlHR/YwRpkhtEKIazAIIF+dhK5MJ2IYsN0YCOKImw27YcdPXoUdrtGmpyST5z3lukMUftrPuL3Sr6RnJCXD0tFlTSFFHje9gBByC+Ade7DmmeSvabADfkAVSuEoCNSrgarXGa8DiYNfnaIKHGZDmz69++PTZs24eSTT1bd980332DgwIFRGRjFVzqk8KP117x1xj2wPrkcrYcOhvVe6U7dhAgQ9Jai+16T2OAK+xpGMp2UDj87RJS4TAc2l1xyCR599FFkZmbinHPOAQA4HA5s3rwZH3/8Me64446oD5I6H1P4xgn5Bei57CkcOHAgvG38daZuQgUIoYKPiK5hBNNJ/NkhongyHdicffbZOHjwIP7yl7/gnXfeAQA8/PDDsFqtuOaaa3DqqadGfZBEKU0nMxMyQIhlLQunk4goSYW18/CVV16Jc889F99++y1cLhfy8/Nx0kknsQEmJZ1E6CcU9tRNDIMPTicRUbISRJO58y1btmDgwIHIyspS3dfS0oIdO3Zg2LBhURtgrNTW1qKtTWc31wQnCAJ69eoV/tQH+amWM5cPNT2Nonc9Yh00adXQpHu7AP6/kTh4LRJHqlwLu91uKIFieoO+BQsWYO/evZr37d+/HwsWLDB7SqKoMrVBnEY/oWhtLGeqHUMYOmP3XiKiZBPWVJQet9sNi3KnUqJOpiqqXbFQalCpldlQTucY6LRtmE4NTCJMfyUrvndEFIqhKKSpqQkOhwMOh9SHxuVy+b/3fe3fvx+ffPIJCgoKYjleotCUAcXeat3Miaw1QLT7Cum0Y4h1JseoRGl9YEaivHdElLgMZWzefvttvPLKK/7vly1bpnvsFVdcEfmoiCKhzMIoBQQsgSuP9NoHhEu3ADdBduZNpNYHhiXIe0dEictQYHPSSSchKysLoijiz3/+M372s5+hqEje/ddut6Nfv35JUThMqU0ZUMDtBqq3dRygE7BEeyWQ7nLtRFlKnYxBQqK8d0SUsAwFNoMHD8bgwYMBAMeOHcP555+PwkL+QqHEpAwojO7A21kbyyXMUuokDBIS5r0jooRlerl3quBybwqnEFXrMZau3cK+HvEshk3F5eL8fyNx8FokjlS5FkaXe5teFfXcc8+hvr4et912m+q+P/7xj+jWrRuuu+46s6cN6dChQ3j11VexefNmuFwuFBYWYsyYMbjyyit1m3ISBRNOjYnWYyxzHurUMUQLWx8QUSoyvTb73//+N0aMGKF530knnYR///vfEQ9Ky/79+yGKIm6++WY88sgjuOGGG/DBBx/gxRdfjMnzURoIp8Yk2nUpyVjnQkSUwEynOpxOJ3r06KF5X3FxMQ4fPhzxoLSMHDkSI0eO9H/fs2dP7N+/H++//z6uv/76mDwnpbhwakyiXZeShHUuRESJzHRgk5WV5d/PRsnhcMBut2veFwtNTU3Izc0NekxbW5uslkYQBGRnZ/v/nYx8407W8ScK64x74Fm12F9jYp1xT8j3NNhjwrke4YyB9PH/jcTBa5E40u1amC4eXrp0KZxOJxYvXiyrbXG73Zg7dy66deuGOXPmRH2gSgcPHkRFRQWuv/56nH/++brHrVu3TrYHz4ABA1BVxboCIiKiVGQ6sNm2bRvmzZuH4uJinHfeeSgsLMThw4fx8ccfw+FwYMGCBSgvLzd8PmXgoWXJkiU47rjj/N87nU7Mnz8fw4YNwy233BL0sXoZm9raWrjdbsPjTCSCIKCkpAQHDx5M6gr3VMHrkTh4LRIHr0XiSJVrYbPZYrMqatCgQZg9ezaeeuopWeFuz549MXv2bFNBDQD87Gc/w6hRo4IeE/hCnE4nFixYgMGDB+Pmm28OeX673a47PZbMFxiQxp/sryEcidovyOj1SNTxp5J0/X8jEfFaJI50uRZhrZMeOXIkVqxYgQMHDqChoQH5+fno1atXWAPIz89Hfn6+oWN9Qc2AAQMwY8YMNtxMU0aXSCdqAJGUrQyIiJJERJFBr169MGTIkLCDGjN800/du3fH9ddfj4aGBrhcLrhcrpg/NyUYg0ukE7ZhIpd4ExHFjKGMzZYtWzBw4EBkZWVhy5YtIY+PRb+o7777DgcPHsTBgwdVdTXr1q2L+vNRAlMukc7NlxpYKjMzBgKIuGR1uMSbiChmDAU2CxYswKJFi1BeXo4FCxaEPH7t2rURD0xp7NixGDt2bNTPS8lHs8ml1tSOgQAiHtNC7HdERBQ7hgKbefPmobS01P9vonhStgLwVN4kP6A9M2MogIjDtBBbGRARxY6hwCZwaikW00wUXYlaNBszOpkZQwEEp4WIiFIKlxWloHgUzYoNdfBUVcBTeRM8VRUQG1wxf04fy/RKoHwoUNQTKB9qamonkscSEVHiMZSxWbVqleETCoKA6dOnhz0gioI4TK8ka5dqTgsREaUWQ4HNDz/8IPu+qakJTU1NsFgsyMvLw5EjR+D1epGTk4MuXbrEZKBkQjymV7iEmYiIEoChwGblypX+f2/fvh0PP/wwpkyZgrPPPhsWiwVerxcbN27EmjVr8Nvf/jZWYyWD4rLqhrUqRESUAEzvPPzCCy/gF7/4BUaPHu2/zWKxYPTo0XC5XHjuueewcOHCqA6SzInH9AqXMBMRUSIwHdjs2LEDEyZM0LyvX79+MdnDhjpPuCuqWKtCRESJwPSqqOzsbHz//fea933//ffIzs6OeFAUPwnbhoCIiMgA0xmbc845B2+++SY8Hg9Gjx6NgoICuFwu/OMf/8Df/vY3jB8/PhbjpM4ShyJgs1mitNunh4iIDDMd2EycOBH19fV466238NZbb8nuGzNmDCZOnBi1wVEcxKEI2OxScXbHJiIiPaYDG6vVipkzZ+KKK67A5s2b0djYiNzcXAwfPhx9+vSJxRipE8WlCNhslkjneL1MDjM8RETpw3Rg49O7d2/07t07mmOhBBCXImCzWSKd4/UyOczwEBGlj7BaKrS1teGDDz7AH/7wBzz44IM4cOAAAOCLL75ATU1NiEcTyZlta6B7vF7mh5sHEhGlDdMZm4aGBixYsAB79+71Fw43NzcDkAKbb7/9FlOnTo36QCl1mc0S6R6vl/nh5oFERGnDdMZmzZo1aGpqwpIlS1Q9pIYPH44tW7ZEbXBEZuhlctjokogofZjO2Hz11Vf49a9/jYEDB8Lr9cru6969Ow4fPhy1wVHiSeRCXL1MDjcPJCJKH6YzNs3NzSguLta8z+12q4IdSi3cwI+IiBKZ6YxNjx498NNPP+GEE05Q3bd9+3aulEp1YRbiRprpiUemKJGzU0REpM10xmb06NF444038MUXX0AURQCAIAjYvn073nnnHYwZMybqg6QEoiy8NViIG06mx7uvGp5br4Vn2uXw3nWj/PFzp0FscAGQAhBPVQU8lTfBU1Xhvz1SzE4RESUf0xmbyy67DFu3bsXy5cvRpUsXAMCiRYtw5MgRjBw5EpdccknUB0mxYzYrEfYGfmFkesSlFUBLs/adLc3wrl4Cy/Q58M69peO4aO5Tw2XiRERJx3RgY7PZUFlZiY0bN+Krr75CfX098vLy8D//8z84++yzYbGEtTUOxYnZzevCLsQNZ8l167Hg97uc0viVwU/1dogNrsinjbhMnIgo6ZgKbFpbW7Fw4UJcffXVGDVqFEaNGhWrcVFn6aSsRFiZnoxM/YwNIAUaWuN1t6kCNCOZKeUxwqSZENes7Nz2EkREFBFTgU1GRgZ2794Nq9Uaq/FQZ+ukrEQ4mR5hzjKIS++WMjcZmcCs+4HXn5cFGt7VS+Tj91EEPN4VDwLV26RvHDXw3nMzkNdVFuQos1fimpVcJk5ElGRMT0UNHjwY27dvx/Dhw2MxHupkZjMp0V4pFOx8lj79gBVr5Q9QBBr+8VdvB9xtHXcoA7S91fLvj7VIX4HTb6ypISJKeqYDm+uuuw7Lli1DQUEBzjjjDGRlZcViXNRJzGZSot1Q0uz5tAIha0UVxAZX+F3JfQEMa2qIiJKe6cDm3nvvhdvtxqpVq7Bq1SpkZmZCEATZMc8991zUBkgJJtpZDZPn0wuEQgZopWUdU1FK7QFM2Cu+iIgoYZgObM444wxVIENpJNpZDbPnCzOwstx6X0fQkpsv3djYIAtg2HqBiCj5mQ5sZs6cGYtxUJKIdlbD9PnCDKwYtBARpQfDgU1rays2bdoEh8OB/Px8nHrqqcjPz4/l2CgBRTtAMHs+ThcREVEwhgIbp9OJefPm4dChQ/7bXnjhBVRWVmLw4MExGxyRUjpmXtiziojIOEPbBL/88stwOp246qqrMGfOHNxwww2w2Wx48sknYz0+orTHnlVERMYZyth8//33uOKKKzBhwgQAwMknn4ySkhJUVVXB5XKhoKAglmOkBMZsQifg/jpERIYZyti4XC4MGzZMdpvv+/r6+uiPipJGvLIJseronZDC7KhORJSODAU2Xq8XGRkZstt833s8nuiPipJHmNmESAOTdJqesUyvBMqHAkU9gfKhLJgmIgrC8Kqo/fv3yzp3e71e/+1KAwcOjMLQKCmEufxab6M9w1NbaTQ9k44F00RE4TIc2KxcuVLz9hUrVqhuW7t2rcaRlIos0yvhXbGwoxeT2w2xwRW6zkYnMDHcYoHtD4iISIOhwGb69OmxHgfFUSQFwEJ+AWCzdTSgrN5mrH+UXmBiMBPD/WyIiEiLocBm7NixMR4GxVPEjS3DmBbSDUwMZmI4PUNERFpMt1SgFKQTmBjO5IQxLaQXmDATQ0REkWBgQ7qBidFMTjSDEWZiiIgoEgxsSD8wMTjFxGCEiIgSBQMb0g9MuPKIiIiSjKEN+ig9cWM4IiJKNszYkC5OMRERUbJhxoaIiIhSBgMbIiIiShkMbIiIiChlMLAhIiKilMHAhoiIiFIGAxsiIiJKGQxsiIiIKGUwsCEiIqKUwcCGiIiIUgYDGyIiIkoZDGyIiIgoZTCwISIiopTBwIaIiIhSBgMbIiIiShkMbIiIiChlMLAhIiKilMHAhoiIiFIGAxsiIiJKGUkZ2LS1teHuu+/GNddcg+rq6ngPh4iIiBJEUgY2a9asQWFhYbyHQURERAkm6QKbr7/+Gt999x2uu+66eA+FiIiIEowt3gMww+Vy4fHHH8fdd9+NjIwMQ49pa2tDW1ub/3tBEJCdne3/dzLyjTtZx59qeD0SB69F4uC1SBzpdi2SJrARRRGrVq3CBRdcgOOOOw6HDh0y9Lj169fjlVde8X8/YMAAVFVVobi4OFZD7TQlJSXxHgIF4PVIHLwWiYPXInGky7WIe2Czbt06WeChZcmSJdi6dSuam5txxRVXmDr/FVdcgfHjx/u/90WstbW1cLvd5gecAARBQElJCQ4ePAhRFOM9nLTH65E4eC0SB69F4kiVa2Gz2QwlJeIe2PzsZz/DqFGjgh5TXFyMV199FT/99BN+9atfye6bM2cORo8ejVmzZmk+1m63w263a96XzBcYkMaf7K8hlfB6JA5ei8TBa5E40uVaxD2wyc/PR35+fsjjfvOb3+CXv/yl//u6ujosWrQIv/3tbzFo0KBYDpGIiIiSRNwDG6OKiopk32dlZQGQ5gy7d+8ejyERERFRgkm65d5EREREepImY6PUo0cPrFu3Lt7DICIiogTCjA0RERGlDAY2RERElDIY2BAREVHKYGBDREREKYOBDREREaUMBjZERESUMhjYEBERUcpgYENEREQpg4ENERERpQwGNkRERJQyGNgQERFRymBgQ0RERCmDgQ0RERGlDAY2RERElDIY2BAREVHKYGBDREREKYOBDREREaUMBjZERESUMhjYEBERUcpgYENEREQpg4ENERERpQwGNkRERJQyGNgQERFRymBgQ0RERCmDgQ0RERGlDAY2RERElDIY2BAREVHKYGBDREREKYOBDREREaUMW7wHEC82W/K/9FR4DamE1yNx8FokDl6LxJHs18Lo+AVRFMUYj4WIiIioU3AqKgk1NzejoqICzc3N8R4KgdcjkfBaJA5ei8SRbteCgU0SEkURO3fuBJNtiYHXI3HwWiQOXovEkW7XgoENERERpQwGNkRERJQyGNgkIbvdjgkTJsBut8d7KARej0TCa5E4eC0SR7pdC66KIiIiopTBjA0RERGlDAY2RERElDIY2BAREVHKYGBDREREKSO5G0eQTFtbG+655x7s2rULDz30EMrKyuI9pLRy6NAhvPrqq9i8eTNcLhcKCwsxZswYXHnllUnfoyUZvPfee3jzzTfhcrlQWlqKyZMnY+jQofEeVtpZv349Nm3ahH379iEjIwODBw/GpEmT0Lt373gPLa2tX78eL730Ei655BJMnjw53sOJKf62TSFr1qxBYWEhdu3aFe+hpKX9+/dDFEXcfPPNKCkpwZ49e/D444+jpaUF119/fbyHl9I2btyIZ599FlOnTsWQIUPw4YcfYvHixfj973+PoqKieA8vrWzZsgUXXXQRjjvuOHg8Hrz88st48MEH8cgjjyArKyvew0tL27dvx4cffoj+/fvHeyidglNRKeLrr7/Gd999h+uuuy7eQ0lbI0eOxIwZM3DSSSehZ8+eOPXUU/GLX/wCmzZtivfQUt5bb72F8847D+eff74/W1NUVIT3338/3kNLO3PnzsXYsWPRt29flJWVYcaMGXA4HNixY0e8h5aWWlpasGLFCkybNg1dunSJ93A6BQObFOByufD4449j1qxZyMjIiPdwKEBTUxNyc3PjPYyU5na7sWPHDpx00kmy20eMGIGtW7fGaVTk09TUBAD8/yBOnnzySZx88skYMWJEvIfSaRjYJDlRFLFq1SpccMEFOO644+I9HApw8OBBvPPOO7jgggviPZSU1tDQAK/Xi65du8pu79q1K1wuV3wGRQCk30/PPfccjj/+ePTr1y/ew0k7n3/+OXbu3Ilf/epX8R5Kp2KNTYJat24dXnnllaDHLFmyBFu3bkVzczOuuOKKThpZ+jF6LQIDS6fTicWLF+Oss87C+eefH+shEgBBEAzdRp3nqaeewu7du/HAAw/Eeyhpx+Fw4Nlnn8XcuXPTLpPPlgoJqqGhAUeOHAl6THFxMf7whz/gyy+/lP0C93q9sFgsGD16NGbNmhXroaY8o9fC98vD6XRiwYIFGDRoEGbMmAGLhYnRWHK73Zg0aRLuuOMOnH766f7bn3nmGVRXV2PBggVxHF36evrpp/HFF19gwYIF6NGjR7yHk3Y2bdqE5cuXy37/eL1eCIIAQRDw4osvpuzvJgY2Sc7hcPjnsAGgrq4OixYtwh133IFBgwahe/fucRxd+vEFNQMGDMBtt92Wsr84Es0999yDgQMHYurUqf7bfve73+G0005LuzR8vImiiKeffhqbNm3C/Pnz0atXr3gPKS01NzejtrZWdtvq1avRu3dvXHbZZSk9NcipqCSnXMrqW05ZUlLCoKaTOZ1OzJ8/H0VFRbj++uvR0NDgv6+goCB+A0sD48ePx4oVKzBw4EAMHjwYH374IRwOB+ub4uCpp57CZ599htmzZyM7O9tf55STk5N2UyLxlJ2drQpeMjMzkZeXl9JBDcDAhihqvvvuOxw8eBAHDx7ELbfcIrtv3bp1cRpVejj77LNx5MgRvPrqq6irq0Pfvn1RWVmJ4uLieA8t7fiW2M+fP192+4wZMzB27NjOHxClHU5FERERUcpgAQARERGlDAY2RERElDIY2BAREVHKYGBDREREKYOBDREREaUMBjZERESUMhjYEBERUcpgYENEREQpgzsPE5HMNddcY+i4efPmYfjw4TEeTedZuXIltmzZgpUrV8Z7KEQUAQY2RCTz4IMPyr5/9dVX8cMPP+D++++X3V5aWtqZwyIiMoSBDRHJDB48WPZ9fn4+BEFQ3a507NgxZGZmxnJoREQhMbAhItPmz5+PI0eOYMqUKXjxxRdRXV2NU089Fb/97W9xzTXXYMKECaoprZkzZ2LYsGGYOXOm/zaXy4V169bhq6++Qn19PQoLCzF27FhceeWVsFqtus//0EMPobq6Go899hgsFnmp4D333AOPx4OqqioAwLvvvot//vOf2LdvH44dO4YePXrgnHPOwc9//nPYbPq/Ag8dOoRZs2ZpNm/Ueo0HDhzAunXr8P3336OpqQk9e/bERRddhJ/97Gf+Y7xeL9avX49PP/0UDocDdrsdRUVFOO+883DJJZfov+FEZBgDGyIKS11dHVasWIHLLrsMEydOhCAIph7vcrlQWVkJi8WCCRMmoGfPnvjpp5/w2muvoba2FjNmzNB97HnnnYeHHnoImzdvxogRI/y379u3D9u3b8eNN97ov62mpgajRo1Cjx49YLPZsGvXLrz22mvYt29f0OcwY+/evbj33ntRVFSE66+/HgUFBfjmm2/wzDPP4MiRI7j66qsBAG+++Sb+8pe/4Morr8SwYcPgdruxf/9+HD16NCrjICIGNkQUpsbGRtxxxx044YQTwnr8unXrcPToUTzyyCMoKioCAJx44onIyMjACy+8gEsvvVS3jufkk09G165dsWHDBllg8/HHH8Nms2H06NH+22644Qb/v71eL4YOHYq8vDysWrUK119/PXJzc8Maf6DnnnsO2dnZeOCBB5CTkwMAGDFiBNxuN15//XVcfPHFyM3NxX/+8x/069dPlukZOXJkxM9PRB243JuIwtKlS5ewgxoA+OqrrzB8+HB069YNHo/H/3XyyScDALZs2aL7WKvVijFjxuBf//oXmpqaAEhByz/+8Q+ceuqpyMvL8x+7c+dOVFVV4Te/+Q1++ctfYuLEiXjsscfg9Xpx4MCBsMfv09rais2bN+O0005DZmam6rW0tbVh27ZtAIDy8nLs2rULTz75JL755hv/2IkoepixIaKwdOvWLaLH19fX48svv8TEiRM1729oaAj6+PPOOw9vvfUWPv/8c1xwwQX45ptvUFdXh3HjxvmPcTgcuP/++9G7d29MnjwZPXr0gN1ux/bt2/HUU0+htbU1otcASJkrj8eDd999F++++67mMUeOHAEAXHHFFcjKysI//vEPfPDBB7BYLBg6dCh+/etf47jjjot4LETEwIaIwqRXU2O32+F2u1W3+z7cffLy8tC/f3/88pe/1DxPqMCptLQU5eXl2LBhAy644AJs2LAB3bp1w0knneQ/ZtOmTTh27BjuuusuFBcX+2+vrq4Oem4AyMjIAAC0tbUFfR1dunSBxWLBOeecg4suukjzXD169AAgZZrGjx+P8ePH4+jRo/j+++/x0ksvYdGiRVi9ejVXlRFFAQMbIoqq4uJi7Nq1S3bb5s2b0dLSIrvtlFNOwddff42ePXuGXecyduxYPPnkk/jPf/6DL7/8Ej//+c9lq6R8wZfdbvffJooiPvroo5Dn7tq1K+x2u+q1fPHFF7LvMzMzMXz4cOzcuRP9+/cPutIqUJcuXXDmmWfC6XTi2WefRW1tLfcGIooCBjZEFFXnnHMO1q5di7Vr12LYsGHYu3cv3n33XX9Rrc+1116L77//Hvfddx8uvvhi9O7dG62traitrcXXX3+Nm266Cd27dw/6XKNHj8bzzz+PRx99FG1tbapl2SNGjIDNZsOjjz6KSy+9FG1tbXj//fcNrUISBAFjxozBxx9/jJKSEvTv3x/bt2/HZ599pjr2xhtvxH333Yf7778fF154IYqLi9Hc3IyDBw/iyy+/xLx58wAAS5cuRb9+/TBw4EDk5+fD4XDg7bffRnFxMUpKSkKOiYhCY2BDRFF16aWXoqmpCRs2bMBf//pXlJeX43e/+x2WLVsmO65bt25YsmQJXn31Vbz55ps4fPgwsrOz0aNHD4wcORJdunQJ+Vw5OTk4/fTT8dlnn2HIkCHo3bu37P4+ffrgzjvvxMsvv4zly5cjLy8Po0ePxvjx47F48eKQ57/++usBAG+88QZaWlpwwgknYM6cObK9eABpWqyqqgqvvvoqXn75ZdTX16NLly7o1auXvxgaAE444QT861//wkcffYTm5mYUFBRgxIgRuOqqqwxneogoOEEURTHegyAiIiKKBi73JiIiopTBwIaIiIhSBgMbIiIiShkMbIiIiChlMLAhIiKilMHAhoiIiFIGAxsiIiJKGQxsiIiIKGUwsCEiIqKUwcCGiIiIUgYDGyIiIkoZ/w+CqueU00ny6QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model r2_score 0.6896 with a standard deviation of 0.0361\n",
      "KNN optimized model r2_score 0.7098 with a standard deviation of 0.0354\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"KNN baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_knn_withSemiSel.joblib']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_reg, \"OUTPUT/knn_reg_withSemiSel.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"OUTPUT/optimizedCV_knn_withSemiSel.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.706392     0.039772\n",
      "1                    TP        14.300000     2.213594\n",
      "2                    TN       155.300000     1.636392\n",
      "3                    FP         2.300000     1.766981\n",
      "4                    FN        19.100000     2.330951\n",
      "5              Accuracy         0.887958     0.014643\n",
      "6             Precision         0.867702     0.092743\n",
      "7           Sensitivity         0.428329     0.066514\n",
      "8           Specificity         0.985430     0.011130\n",
      "9              F1 score         0.570058     0.066023\n",
      "10  F1 score (weighted)         0.871646     0.018187\n",
      "11     F1 score (macro)         0.752807     0.036852\n",
      "12    Balanced Accuracy         0.706878     0.033618\n",
      "13                  MCC         0.558566     0.068427\n",
      "14                  NPV         0.890620     0.012039\n",
      "15              ROC_AUC         0.706878     0.033618\n",
      "CPU times: user 3.68 s, sys: 36 ms, total: 3.72 s\n",
      "Wall time: 3.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    \n",
    "    svm_reg.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:26:17,301] A new study created in memory with name: SVM_regressor_CV\n",
      "[I 2023-12-12 04:26:19,851] Trial 0 finished with value: 0.7197298549340498 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:26:22,948] Trial 1 finished with value: 0.6502386230692203 and parameters: {'C': 64.0, 'gamma': 0.0009765625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:26:25,378] Trial 2 finished with value: 0.28516672849412694 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:26:27,974] Trial 3 finished with value: -0.011740034769186326 and parameters: {'C': 0.015625, 'gamma': 1.0}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:26:30,411] Trial 4 finished with value: 0.5938235847275689 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:26:32,940] Trial 5 finished with value: -0.011320691915643066 and parameters: {'C': 0.015625, 'gamma': 0.5}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:26:35,364] Trial 6 finished with value: 0.631439103205801 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:26:37,880] Trial 7 finished with value: 0.005508242251037898 and parameters: {'C': 0.125, 'gamma': 6.103515625e-05}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:26:40,537] Trial 8 finished with value: 0.021409493042003614 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:26:43,071] Trial 9 finished with value: 0.33063766403915873 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:26:46,196] Trial 10 finished with value: 0.012289220722265348 and parameters: {'C': 16.0, 'gamma': 8.0}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:26:49,013] Trial 11 finished with value: 0.6554644899822858 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:26:52,019] Trial 12 finished with value: 0.6761008736254135 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:26:55,034] Trial 13 finished with value: 0.6761008736254135 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:26:57,596] Trial 14 finished with value: 0.02299767646656815 and parameters: {'C': 0.5, 'gamma': 3.0517578125e-05}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:00,184] Trial 15 finished with value: 0.07251049629750538 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:02,653] Trial 16 finished with value: 0.10693926544419123 and parameters: {'C': 0.25, 'gamma': 0.000244140625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:05,111] Trial 17 finished with value: 0.38564524066695716 and parameters: {'C': 1.0, 'gamma': 0.00048828125}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:07,763] Trial 18 finished with value: 0.3049600959276611 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:10,285] Trial 19 finished with value: 0.2052201545046827 and parameters: {'C': 0.03125, 'gamma': 0.0078125}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:12,680] Trial 20 finished with value: 0.6589485174675104 and parameters: {'C': 2.0, 'gamma': 0.00390625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:15,697] Trial 21 finished with value: 0.6761008736254135 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:18,701] Trial 22 finished with value: 0.6761008736254135 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:21,122] Trial 23 finished with value: 0.48127146167576995 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:23,873] Trial 24 finished with value: 0.014788997116249458 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:26,870] Trial 25 finished with value: 0.6761008736254135 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:29,603] Trial 26 finished with value: 0.01258681952940247 and parameters: {'C': 32.0, 'gamma': 4.0}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:32,023] Trial 27 finished with value: 0.2736747335706148 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:34,644] Trial 28 finished with value: -0.011674315150741622 and parameters: {'C': 0.0078125, 'gamma': 0.25}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:37,214] Trial 29 finished with value: 0.6632545078428272 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:39,780] Trial 30 finished with value: 0.7192601274188604 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:42,343] Trial 31 finished with value: 0.7192601274188604 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:44,906] Trial 32 finished with value: 0.7192601274188604 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:47,470] Trial 33 finished with value: 0.7192601274188604 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:50,037] Trial 34 finished with value: 0.7192601274188604 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:52,599] Trial 35 finished with value: 0.7192601274188604 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:55,172] Trial 36 finished with value: 0.7192601274188604 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:27:57,671] Trial 37 finished with value: 0.5950701326350787 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:28:00,230] Trial 38 finished with value: 0.7192601274188604 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:28:02,938] Trial 39 finished with value: 0.035287474045397095 and parameters: {'C': 64.0, 'gamma': 0.5}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:28:05,425] Trial 40 finished with value: 0.4506028730479154 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:28:07,993] Trial 41 finished with value: 0.7192601274188604 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:28:10,566] Trial 42 finished with value: 0.7192601274188604 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:28:13,057] Trial 43 finished with value: 0.1364860105325889 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:28:16,169] Trial 44 finished with value: 0.012289185214582199 and parameters: {'C': 64.0, 'gamma': 8.0}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:28:18,651] Trial 45 finished with value: 0.38091858670273215 and parameters: {'C': 0.5, 'gamma': 0.0009765625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:28:21,300] Trial 46 finished with value: 0.021409493041335038 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:28:23,777] Trial 47 finished with value: 0.631439103205801 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:28:26,293] Trial 48 finished with value: 0.10842276961897084 and parameters: {'C': 2.0, 'gamma': 3.0517578125e-05}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:28:28,711] Trial 49 finished with value: 0.5591523461550937 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 0 with value: 0.7197298549340498.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.7197\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_regressor_CV\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.693504\n",
      "1                    TP   26.000000\n",
      "2                    TN  310.000000\n",
      "3                    FP    6.000000\n",
      "4                    FN   40.000000\n",
      "5              Accuracy    0.879581\n",
      "6             Precision    0.812500\n",
      "7           Sensitivity    0.393939\n",
      "8           Specificity    0.981000\n",
      "9              F1 score    0.530612\n",
      "10  F1 score (weighted)    0.861766\n",
      "11     F1 score (macro)    0.730772\n",
      "12    Balanced Accuracy    0.687476\n",
      "13                  MCC    0.511661\n",
      "14                  NPV    0.885700\n",
      "15              ROC_AUC    0.687476\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_svm_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_svm_0_cat = np.where(((y_pred_svm_0 >= 2) | (y_pred_svm_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_svm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:28:31,552] Trial 50 finished with value: 0.2768464137123333 and parameters: {'C': 1.0, 'gamma': 0.000244140625}. Best is trial 0 with value: 0.7197298549340498.\n",
      "[I 2023-12-12 04:28:33,982] Trial 51 finished with value: 0.7204832983815803 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.7204832983815803.\n",
      "[I 2023-12-12 04:28:36,417] Trial 52 finished with value: 0.7204832983815803 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.7204832983815803.\n",
      "[I 2023-12-12 04:28:39,127] Trial 53 finished with value: 0.6298851199091605 and parameters: {'C': 64.0, 'gamma': 0.00048828125}. Best is trial 51 with value: 0.7204832983815803.\n",
      "[I 2023-12-12 04:28:41,609] Trial 54 finished with value: 0.003327774528021188 and parameters: {'C': 0.03125, 'gamma': 0.125}. Best is trial 51 with value: 0.7204832983815803.\n",
      "[I 2023-12-12 04:28:44,318] Trial 55 finished with value: 0.7028130957756173 and parameters: {'C': 64.0, 'gamma': 0.0078125}. Best is trial 51 with value: 0.7204832983815803.\n",
      "[I 2023-12-12 04:28:46,730] Trial 56 finished with value: 0.3814828783447064 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 51 with value: 0.7204832983815803.\n",
      "[I 2023-12-12 04:28:49,697] Trial 57 finished with value: 0.6719347770791062 and parameters: {'C': 64.0, 'gamma': 0.00390625}. Best is trial 51 with value: 0.7204832983815803.\n",
      "[I 2023-12-12 04:28:52,195] Trial 58 finished with value: -0.00330879673162785 and parameters: {'C': 0.125, 'gamma': 2.0}. Best is trial 51 with value: 0.7204832983815803.\n",
      "[I 2023-12-12 04:28:54,770] Trial 59 finished with value: -0.009137511158603829 and parameters: {'C': 0.015625, 'gamma': 4.0}. Best is trial 51 with value: 0.7204832983815803.\n",
      "[I 2023-12-12 04:28:57,153] Trial 60 finished with value: 0.2613970902893815 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 51 with value: 0.7204832983815803.\n",
      "[I 2023-12-12 04:28:59,590] Trial 61 finished with value: 0.7204832983815803 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.7204832983815803.\n",
      "[I 2023-12-12 04:29:02,022] Trial 62 finished with value: 0.7204832983815803 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.7204832983815803.\n",
      "[I 2023-12-12 04:29:04,459] Trial 63 finished with value: 0.7204832983815803 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.7204832983815803.\n",
      "[I 2023-12-12 04:29:06,927] Trial 64 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:09,404] Trial 65 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:11,876] Trial 66 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:14,518] Trial 67 finished with value: 0.07492980726179713 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:17,106] Trial 68 finished with value: 0.03464742824293284 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:19,556] Trial 69 finished with value: 0.5823133536680095 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:22,034] Trial 70 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:24,519] Trial 71 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:26,985] Trial 72 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:29,454] Trial 73 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:31,924] Trial 74 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:34,391] Trial 75 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:37,399] Trial 76 finished with value: 0.014992548099061554 and parameters: {'C': 16.0, 'gamma': 8.0}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:39,876] Trial 77 finished with value: 0.6445562421711544 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:42,235] Trial 78 finished with value: 0.3822866822045163 and parameters: {'C': 16.0, 'gamma': 3.0517578125e-05}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:44,827] Trial 79 finished with value: 0.022686672363195624 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:47,301] Trial 80 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:49,773] Trial 81 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:52,243] Trial 82 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:54,714] Trial 83 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:57,236] Trial 84 finished with value: 0.2846743389396281 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:29:59,634] Trial 85 finished with value: 0.46823481963238295 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:01,996] Trial 86 finished with value: 0.5829268105534521 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:04,467] Trial 87 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:06,933] Trial 88 finished with value: -0.0010326802427124359 and parameters: {'C': 0.0078125, 'gamma': 0.00048828125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:09,448] Trial 89 finished with value: 0.7119889227877207 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:11,918] Trial 90 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:14,385] Trial 91 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:16,855] Trial 92 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:19,325] Trial 93 finished with value: 0.7221050497786112 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:22,393] Trial 94 finished with value: 0.667640210186277 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:24,923] Trial 95 finished with value: 0.009792430118124796 and parameters: {'C': 0.5, 'gamma': 2.0}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:27,348] Trial 96 finished with value: 0.2782379035595224 and parameters: {'C': 2.0, 'gamma': 0.0001220703125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:29,634] Trial 97 finished with value: 0.6892110650455625 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:32,260] Trial 98 finished with value: 0.01524563467770107 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:34,663] Trial 99 finished with value: 0.2161482479443518 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7221\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.693504    0.714602\n",
      "1                    TP   26.000000   42.000000\n",
      "2                    TN  310.000000  307.000000\n",
      "3                    FP    6.000000    8.000000\n",
      "4                    FN   40.000000   25.000000\n",
      "5              Accuracy    0.879581    0.913613\n",
      "6             Precision    0.812500    0.840000\n",
      "7           Sensitivity    0.393939    0.626866\n",
      "8           Specificity    0.981000    0.974600\n",
      "9              F1 score    0.530612    0.717949\n",
      "10  F1 score (weighted)    0.861766    0.908471\n",
      "11     F1 score (macro)    0.730772    0.833472\n",
      "12    Balanced Accuracy    0.687476    0.800734\n",
      "13                  MCC    0.511661    0.678191\n",
      "14                  NPV    0.885700    0.924700\n",
      "15              ROC_AUC    0.687476    0.800734\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_svm_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet1_cat = np.where(((Y_testSet1>=2) | (Y_testSet1<=-2)), 1, 0) \n",
    "y_pred_svm_1_cat = np.where(((y_pred_svm_1 >= 2) | (y_pred_svm_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_svm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:30:37,636] Trial 100 finished with value: 0.513481982489479 and parameters: {'C': 0.25, 'gamma': 0.03125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:40,188] Trial 101 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:42,735] Trial 102 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:45,296] Trial 103 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:48,007] Trial 104 finished with value: 0.07996962503136991 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:50,552] Trial 105 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:53,069] Trial 106 finished with value: 0.7199562579599041 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:55,633] Trial 107 finished with value: 0.11473457438650314 and parameters: {'C': 0.0625, 'gamma': 0.0625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:30:58,191] Trial 108 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:00,670] Trial 109 finished with value: 0.43839396712859313 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:03,394] Trial 110 finished with value: 0.0350281217636475 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:05,939] Trial 111 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:08,479] Trial 112 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:11,062] Trial 113 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:14,116] Trial 114 finished with value: -0.014262844154000864 and parameters: {'C': 0.015625, 'gamma': 8.0}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:16,721] Trial 115 finished with value: 0.6504591910560756 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:19,336] Trial 116 finished with value: 0.01560070989671607 and parameters: {'C': 0.0078125, 'gamma': 0.001953125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:22,092] Trial 117 finished with value: 0.021784734537406515 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:24,624] Trial 118 finished with value: 0.7209923425019203 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:27,214] Trial 119 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:29,689] Trial 120 finished with value: 0.38215518289423106 and parameters: {'C': 16.0, 'gamma': 3.0517578125e-05}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:32,284] Trial 121 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:34,874] Trial 122 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:37,323] Trial 123 finished with value: 0.47536270760750254 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:39,919] Trial 124 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:42,517] Trial 125 finished with value: 0.14442897064586505 and parameters: {'C': 0.5, 'gamma': 0.125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:45,123] Trial 126 finished with value: 0.7118594104309118 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:47,593] Trial 127 finished with value: 0.3799498775979392 and parameters: {'C': 2.0, 'gamma': 0.000244140625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:50,056] Trial 128 finished with value: 0.6370599277859132 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:52,650] Trial 129 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:55,101] Trial 130 finished with value: 0.21167991017308693 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:31:57,455] Trial 131 finished with value: 0.6914666864480475 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:00,049] Trial 132 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:02,851] Trial 133 finished with value: 0.6851665153083217 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:05,322] Trial 134 finished with value: 0.5193793171532867 and parameters: {'C': 0.25, 'gamma': 0.0078125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:07,710] Trial 135 finished with value: 0.5472214111457243 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:10,300] Trial 136 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:12,899] Trial 137 finished with value: 0.7128343198525823 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:15,657] Trial 138 finished with value: 0.01598708177216366 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:18,400] Trial 139 finished with value: 0.014664708838576668 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:20,976] Trial 140 finished with value: 0.6977626755476705 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:23,574] Trial 141 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:26,165] Trial 142 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:28,763] Trial 143 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:31,355] Trial 144 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:33,987] Trial 145 finished with value: -0.005678571108155739 and parameters: {'C': 0.0625, 'gamma': 0.25}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:36,581] Trial 146 finished with value: 0.716817221202538 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:39,109] Trial 147 finished with value: 0.43839396712859313 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:41,687] Trial 148 finished with value: 0.12839678105330626 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:44,168] Trial 149 finished with value: 0.5920127250101526 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 64 with value: 0.7221050497786112.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7221\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.693504    0.714602    0.720508\n",
      "1                    TP   26.000000   42.000000   35.000000\n",
      "2                    TN  310.000000  307.000000  312.000000\n",
      "3                    FP    6.000000    8.000000    3.000000\n",
      "4                    FN   40.000000   25.000000   32.000000\n",
      "5              Accuracy    0.879581    0.913613    0.908377\n",
      "6             Precision    0.812500    0.840000    0.921053\n",
      "7           Sensitivity    0.393939    0.626866    0.522388\n",
      "8           Specificity    0.981000    0.974600    0.990500\n",
      "9              F1 score    0.530612    0.717949    0.666667\n",
      "10  F1 score (weighted)    0.861766    0.908471    0.897740\n",
      "11     F1 score (macro)    0.730772    0.833472    0.806778\n",
      "12    Balanced Accuracy    0.687476    0.800734    0.756432\n",
      "13                  MCC    0.511661    0.678191    0.651665\n",
      "14                  NPV    0.885700    0.924700    0.907000\n",
      "15              ROC_AUC    0.687476    0.800734    0.756432\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_svm_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet2_cat = np.where(((Y_testSet2>=2) | (Y_testSet2<=-2)), 1, 0) \n",
    "y_pred_svm_2_cat = np.where(((y_pred_svm_2 >= 2) | (y_pred_svm_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_svm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:32:47,104] Trial 150 finished with value: 0.08042314280296495 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:49,540] Trial 151 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:51,980] Trial 152 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:54,589] Trial 153 finished with value: 0.04806838273049766 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:57,020] Trial 154 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:32:59,452] Trial 155 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:02,452] Trial 156 finished with value: 0.028930384104147323 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:04,885] Trial 157 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:07,437] Trial 158 finished with value: 0.6643574658613559 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:09,894] Trial 159 finished with value: 0.6458714347575173 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:12,369] Trial 160 finished with value: 0.021639827608473116 and parameters: {'C': 0.5, 'gamma': 1.0}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:14,798] Trial 161 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:17,226] Trial 162 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:19,663] Trial 163 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:22,125] Trial 164 finished with value: 0.7194218437570141 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:24,564] Trial 165 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:26,932] Trial 166 finished with value: 0.7160656653216669 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:29,359] Trial 167 finished with value: 0.06276348912815875 and parameters: {'C': 1.0, 'gamma': 3.0517578125e-05}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:31,700] Trial 168 finished with value: 0.6058969759627422 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:34,175] Trial 169 finished with value: 0.08746296374881567 and parameters: {'C': 0.25, 'gamma': 0.125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:36,607] Trial 170 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:39,034] Trial 171 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:41,265] Trial 172 finished with value: 0.4920618475611344 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:43,589] Trial 173 finished with value: 0.22921587449861497 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:46,019] Trial 174 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:48,390] Trial 175 finished with value: 0.6345384812836272 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:50,827] Trial 176 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:53,258] Trial 177 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:55,704] Trial 178 finished with value: 0.7195579190872566 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:33:58,323] Trial 179 finished with value: 0.7056111064268256 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:00,645] Trial 180 finished with value: 0.721241348066713 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:03,077] Trial 181 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:05,397] Trial 182 finished with value: 0.721241348066713 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:07,713] Trial 183 finished with value: 0.721241348066713 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:09,967] Trial 184 finished with value: 0.40159706739866285 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:12,583] Trial 185 finished with value: 0.029268605632883703 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:14,935] Trial 186 finished with value: 0.3355558359077253 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:17,472] Trial 187 finished with value: 0.031451991263426925 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:19,850] Trial 188 finished with value: 0.3599509394102466 and parameters: {'C': 0.125, 'gamma': 0.00390625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:22,151] Trial 189 finished with value: 0.7052397429202715 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:24,587] Trial 190 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:26,905] Trial 191 finished with value: 0.721241348066713 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:29,230] Trial 192 finished with value: 0.721241348066713 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:31,549] Trial 193 finished with value: 0.721241348066713 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:33,899] Trial 194 finished with value: 0.14367359615411363 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:36,328] Trial 195 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:38,836] Trial 196 finished with value: 0.08042314280296495 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:41,359] Trial 197 finished with value: 0.08803211078557063 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:43,681] Trial 198 finished with value: 0.721241348066713 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:46,109] Trial 199 finished with value: 0.7204694522080753 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 64 with value: 0.7221050497786112.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7221\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.693504    0.714602    0.720508    0.693961\n",
      "1                    TP   26.000000   42.000000   35.000000   29.000000\n",
      "2                    TN  310.000000  307.000000  312.000000  310.000000\n",
      "3                    FP    6.000000    8.000000    3.000000    4.000000\n",
      "4                    FN   40.000000   25.000000   32.000000   39.000000\n",
      "5              Accuracy    0.879581    0.913613    0.908377    0.887435\n",
      "6             Precision    0.812500    0.840000    0.921053    0.878788\n",
      "7           Sensitivity    0.393939    0.626866    0.522388    0.426471\n",
      "8           Specificity    0.981000    0.974600    0.990500    0.987300\n",
      "9              F1 score    0.530612    0.717949    0.666667    0.574257\n",
      "10  F1 score (weighted)    0.861766    0.908471    0.897740    0.870902\n",
      "11     F1 score (macro)    0.730772    0.833472    0.806778    0.754700\n",
      "12    Balanced Accuracy    0.687476    0.800734    0.756432    0.706866\n",
      "13                  MCC    0.511661    0.678191    0.651665    0.563337\n",
      "14                  NPV    0.885700    0.924700    0.907000    0.888300\n",
      "15              ROC_AUC    0.687476    0.800734    0.756432    0.706866\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_svm_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet3_cat = np.where(((Y_testSet3>=2) | (Y_testSet3<=-2)), 1, 0) \n",
    "y_pred_svm_3_cat = np.where(((y_pred_svm_3 >= 2) | (y_pred_svm_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_svm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:34:49,256] Trial 200 finished with value: 0.049666486519135924 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 64 with value: 0.7221050497786112.\n",
      "[I 2023-12-12 04:34:51,801] Trial 201 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:34:54,353] Trial 202 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:34:56,814] Trial 203 finished with value: 0.5912000911955507 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:34:59,365] Trial 204 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:01,913] Trial 205 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:04,462] Trial 206 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:07,009] Trial 207 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:09,560] Trial 208 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:12,115] Trial 209 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:14,661] Trial 210 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:17,214] Trial 211 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:19,762] Trial 212 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:22,312] Trial 213 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:24,864] Trial 214 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:27,414] Trial 215 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:29,970] Trial 216 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:33,144] Trial 217 finished with value: 0.029605607937089263 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:35,696] Trial 218 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:38,242] Trial 219 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:40,794] Trial 220 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:43,346] Trial 221 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:45,890] Trial 222 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:48,437] Trial 223 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:50,988] Trial 224 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:53,536] Trial 225 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:56,115] Trial 226 finished with value: 0.6579576731411197 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:35:58,669] Trial 227 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:01,232] Trial 228 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:03,948] Trial 229 finished with value: 0.03786566744878668 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:06,500] Trial 230 finished with value: 0.637747901928185 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:09,050] Trial 231 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:11,604] Trial 232 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:14,157] Trial 233 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:16,711] Trial 234 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:19,268] Trial 235 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:21,821] Trial 236 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:24,287] Trial 237 finished with value: 0.2855147035791045 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:26,843] Trial 238 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:29,400] Trial 239 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:31,869] Trial 240 finished with value: 0.5519466297565396 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:34,424] Trial 241 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:36,978] Trial 242 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:39,534] Trial 243 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:41,992] Trial 244 finished with value: 0.3920924933279303 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:44,546] Trial 245 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:47,097] Trial 246 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:49,782] Trial 247 finished with value: 0.29665644933061436 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:52,238] Trial 248 finished with value: 0.601335133681349 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:36:54,787] Trial 249 finished with value: 0.7256842948031623 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7257\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.693504    0.714602    0.720508    0.693961   \n",
      "1                    TP   26.000000   42.000000   35.000000   29.000000   \n",
      "2                    TN  310.000000  307.000000  312.000000  310.000000   \n",
      "3                    FP    6.000000    8.000000    3.000000    4.000000   \n",
      "4                    FN   40.000000   25.000000   32.000000   39.000000   \n",
      "5              Accuracy    0.879581    0.913613    0.908377    0.887435   \n",
      "6             Precision    0.812500    0.840000    0.921053    0.878788   \n",
      "7           Sensitivity    0.393939    0.626866    0.522388    0.426471   \n",
      "8           Specificity    0.981000    0.974600    0.990500    0.987300   \n",
      "9              F1 score    0.530612    0.717949    0.666667    0.574257   \n",
      "10  F1 score (weighted)    0.861766    0.908471    0.897740    0.870902   \n",
      "11     F1 score (macro)    0.730772    0.833472    0.806778    0.754700   \n",
      "12    Balanced Accuracy    0.687476    0.800734    0.756432    0.706866   \n",
      "13                  MCC    0.511661    0.678191    0.651665    0.563337   \n",
      "14                  NPV    0.885700    0.924700    0.907000    0.888300   \n",
      "15              ROC_AUC    0.687476    0.800734    0.756432    0.706866   \n",
      "\n",
      "          Set4  \n",
      "0     0.726193  \n",
      "1    33.000000  \n",
      "2   312.000000  \n",
      "3     4.000000  \n",
      "4    33.000000  \n",
      "5     0.903141  \n",
      "6     0.891892  \n",
      "7     0.500000  \n",
      "8     0.987300  \n",
      "9     0.640777  \n",
      "10    0.891631  \n",
      "11    0.792400  \n",
      "12    0.743671  \n",
      "13    0.622929  \n",
      "14    0.904300  \n",
      "15    0.743671  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_svm_4)\n",
    "# now convert the resuls to binary with cutoff 6.4\n",
    "Y_testSet4_cat = np.where(((Y_testSet4>=2) | (Y_testSet4<=-2)), 1, 0) \n",
    "y_pred_svm_4_cat = np.where(((y_pred_svm_4 >= 2) | (y_pred_svm_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_svm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:36:57,626] Trial 250 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:00,032] Trial 251 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:02,573] Trial 252 finished with value: 0.7060360166591877 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:04,975] Trial 253 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:07,520] Trial 254 finished with value: 0.6829177540695747 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:09,938] Trial 255 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:12,345] Trial 256 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:14,981] Trial 257 finished with value: 0.027720007443954076 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:17,382] Trial 258 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:19,650] Trial 259 finished with value: 0.4792952938174448 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:22,321] Trial 260 finished with value: 0.02584725707214238 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:24,723] Trial 261 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:27,126] Trial 262 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:29,479] Trial 263 finished with value: 0.6920368159070645 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:31,892] Trial 264 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:34,458] Trial 265 finished with value: 0.0839668720414708 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:36,861] Trial 266 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:39,267] Trial 267 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:41,674] Trial 268 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:44,081] Trial 269 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:46,487] Trial 270 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:49,073] Trial 271 finished with value: 0.04474130891121436 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:51,478] Trial 272 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:53,883] Trial 273 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:56,289] Trial 274 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:37:58,707] Trial 275 finished with value: 0.5770050420504734 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:01,074] Trial 276 finished with value: 0.6248959681746857 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:03,452] Trial 277 finished with value: 0.6502388288392839 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:05,996] Trial 278 finished with value: 0.03293685682903343 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:08,403] Trial 279 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:10,811] Trial 280 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:13,221] Trial 281 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:15,630] Trial 282 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:18,031] Trial 283 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:20,439] Trial 284 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:23,495] Trial 285 finished with value: 0.025574340093156032 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:25,860] Trial 286 finished with value: 0.3928785705555164 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:28,157] Trial 287 finished with value: 0.2913357362624015 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:30,427] Trial 288 finished with value: 0.539184306852688 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:32,836] Trial 289 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:35,404] Trial 290 finished with value: 0.289227179626156 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:37,735] Trial 291 finished with value: 0.5888285099671 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:40,000] Trial 292 finished with value: 0.679000498995 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:42,336] Trial 293 finished with value: 0.6263955798567122 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:44,685] Trial 294 finished with value: 0.5498161596872365 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:46,980] Trial 295 finished with value: 0.6832698394611546 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:49,384] Trial 296 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:51,797] Trial 297 finished with value: 0.7153954955800031 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:54,345] Trial 298 finished with value: 0.6829177540695747 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 201 with value: 0.7256842948031623.\n",
      "[I 2023-12-12 04:38:56,742] Trial 299 finished with value: 0.004993248426423891 and parameters: {'C': 0.03125, 'gamma': 0.0001220703125}. Best is trial 201 with value: 0.7256842948031623.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7257\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.693504    0.714602    0.720508    0.693961   \n",
      "1                    TP   26.000000   42.000000   35.000000   29.000000   \n",
      "2                    TN  310.000000  307.000000  312.000000  310.000000   \n",
      "3                    FP    6.000000    8.000000    3.000000    4.000000   \n",
      "4                    FN   40.000000   25.000000   32.000000   39.000000   \n",
      "5              Accuracy    0.879581    0.913613    0.908377    0.887435   \n",
      "6             Precision    0.812500    0.840000    0.921053    0.878788   \n",
      "7           Sensitivity    0.393939    0.626866    0.522388    0.426471   \n",
      "8           Specificity    0.981000    0.974600    0.990500    0.987300   \n",
      "9              F1 score    0.530612    0.717949    0.666667    0.574257   \n",
      "10  F1 score (weighted)    0.861766    0.908471    0.897740    0.870902   \n",
      "11     F1 score (macro)    0.730772    0.833472    0.806778    0.754700   \n",
      "12    Balanced Accuracy    0.687476    0.800734    0.756432    0.706866   \n",
      "13                  MCC    0.511661    0.678191    0.651665    0.563337   \n",
      "14                  NPV    0.885700    0.924700    0.907000    0.888300   \n",
      "15              ROC_AUC    0.687476    0.800734    0.756432    0.706866   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.726193    0.743756  \n",
      "1    33.000000   29.000000  \n",
      "2   312.000000  311.000000  \n",
      "3     4.000000    5.000000  \n",
      "4    33.000000   37.000000  \n",
      "5     0.903141    0.890052  \n",
      "6     0.891892    0.852941  \n",
      "7     0.500000    0.439394  \n",
      "8     0.987300    0.984200  \n",
      "9     0.640777    0.580000  \n",
      "10    0.891631    0.875110  \n",
      "11    0.792400    0.758373  \n",
      "12    0.743671    0.711786  \n",
      "13    0.622929    0.562358  \n",
      "14    0.904300    0.893700  \n",
      "15    0.743671    0.711786  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_svm_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "Y_testSet5_cat = np.where(((Y_testSet5>=2) | (Y_testSet5<=-2)), 1, 0) \n",
    "y_pred_svm_5_cat = np.where(((y_pred_svm_5 >= 2) | (y_pred_svm_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_svm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:38:59,779] Trial 300 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:02,448] Trial 301 finished with value: 0.026163919722783712 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:05,063] Trial 302 finished with value: 0.3315254520959642 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:07,585] Trial 303 finished with value: 0.7018672175641584 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:10,180] Trial 304 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:12,772] Trial 305 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:15,471] Trial 306 finished with value: 0.023451194784214213 and parameters: {'C': 32.0, 'gamma': 4.0}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:18,061] Trial 307 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:20,744] Trial 308 finished with value: 0.08446359283133699 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:23,331] Trial 309 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:25,920] Trial 310 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:28,508] Trial 311 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:31,104] Trial 312 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:33,706] Trial 313 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:36,311] Trial 314 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:39,038] Trial 315 finished with value: 0.04561685555117971 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:41,621] Trial 316 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:44,120] Trial 317 finished with value: 0.5846769887789024 and parameters: {'C': 32.0, 'gamma': 0.0625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:46,709] Trial 318 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:49,292] Trial 319 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:51,881] Trial 320 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:54,477] Trial 321 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:57,062] Trial 322 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:39:59,795] Trial 323 finished with value: 0.6538081134832876 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:02,381] Trial 324 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:05,464] Trial 325 finished with value: 0.02304076949662731 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:08,144] Trial 326 finished with value: 0.033049437319246866 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:10,732] Trial 327 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:13,321] Trial 328 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:16,278] Trial 329 finished with value: 0.672975803922813 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:18,869] Trial 330 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:21,236] Trial 331 finished with value: 0.4862246293951098 and parameters: {'C': 32.0, 'gamma': 3.0517578125e-05}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:23,830] Trial 332 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:26,436] Trial 333 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:29,026] Trial 334 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:31,619] Trial 335 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:34,123] Trial 336 finished with value: 0.6318156738783274 and parameters: {'C': 32.0, 'gamma': 0.000244140625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:36,702] Trial 337 finished with value: 0.29162400159705576 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:39,296] Trial 338 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:41,887] Trial 339 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:44,475] Trial 340 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:47,070] Trial 341 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:49,560] Trial 342 finished with value: 0.5508946567266513 and parameters: {'C': 32.0, 'gamma': 6.103515625e-05}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:52,144] Trial 343 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:54,760] Trial 344 finished with value: 0.6412776257409918 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:40:57,492] Trial 345 finished with value: 0.7173650685834938 and parameters: {'C': 32.0, 'gamma': 0.0078125}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:41:00,077] Trial 346 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:41:02,658] Trial 347 finished with value: 0.7257421422924062 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:41:05,578] Trial 348 finished with value: 0.6954193207384536 and parameters: {'C': 32.0, 'gamma': 0.00390625}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:41:07,912] Trial 349 finished with value: 0.5991560497625664 and parameters: {'C': 32.0, 'gamma': 0.0001220703125}. Best is trial 300 with value: 0.7257421422924062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7257\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.693504    0.714602    0.720508    0.693961   \n",
      "1                    TP   26.000000   42.000000   35.000000   29.000000   \n",
      "2                    TN  310.000000  307.000000  312.000000  310.000000   \n",
      "3                    FP    6.000000    8.000000    3.000000    4.000000   \n",
      "4                    FN   40.000000   25.000000   32.000000   39.000000   \n",
      "5              Accuracy    0.879581    0.913613    0.908377    0.887435   \n",
      "6             Precision    0.812500    0.840000    0.921053    0.878788   \n",
      "7           Sensitivity    0.393939    0.626866    0.522388    0.426471   \n",
      "8           Specificity    0.981000    0.974600    0.990500    0.987300   \n",
      "9              F1 score    0.530612    0.717949    0.666667    0.574257   \n",
      "10  F1 score (weighted)    0.861766    0.908471    0.897740    0.870902   \n",
      "11     F1 score (macro)    0.730772    0.833472    0.806778    0.754700   \n",
      "12    Balanced Accuracy    0.687476    0.800734    0.756432    0.706866   \n",
      "13                  MCC    0.511661    0.678191    0.651665    0.563337   \n",
      "14                  NPV    0.885700    0.924700    0.907000    0.888300   \n",
      "15              ROC_AUC    0.687476    0.800734    0.756432    0.706866   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.726193    0.743756    0.705405  \n",
      "1    33.000000   29.000000   28.000000  \n",
      "2   312.000000  311.000000  308.000000  \n",
      "3     4.000000    5.000000    6.000000  \n",
      "4    33.000000   37.000000   40.000000  \n",
      "5     0.903141    0.890052    0.879581  \n",
      "6     0.891892    0.852941    0.823529  \n",
      "7     0.500000    0.439394    0.411765  \n",
      "8     0.987300    0.984200    0.980900  \n",
      "9     0.640777    0.580000    0.549020  \n",
      "10    0.891631    0.875110    0.862604  \n",
      "11    0.792400    0.758373    0.739767  \n",
      "12    0.743671    0.711786    0.696328  \n",
      "13    0.622929    0.562358    0.527476  \n",
      "14    0.904300    0.893700    0.885100  \n",
      "15    0.743671    0.711786    0.696328  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_svm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "Y_testSet6_cat = np.where(((Y_testSet6>=2) | (Y_testSet6<=-2)), 1, 0) \n",
    "y_pred_svm_6_cat = np.where(((y_pred_svm_6 >= 2) | (y_pred_svm_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_svm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:41:10,820] Trial 350 finished with value: 0.015370829585127598 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 300 with value: 0.7257421422924062.\n",
      "[I 2023-12-12 04:41:13,185] Trial 351 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:15,583] Trial 352 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:18,001] Trial 353 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:20,412] Trial 354 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:22,824] Trial 355 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:25,227] Trial 356 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:27,753] Trial 357 finished with value: 0.013576338517635322 and parameters: {'C': 32.0, 'gamma': 4.0}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:30,331] Trial 358 finished with value: 0.07047618364185335 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:32,715] Trial 359 finished with value: 0.7090615707694469 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:35,116] Trial 360 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:37,517] Trial 361 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:39,831] Trial 362 finished with value: 0.5875427406118825 and parameters: {'C': 32.0, 'gamma': 0.0625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:42,457] Trial 363 finished with value: 0.033148235278398865 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:44,865] Trial 364 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:47,267] Trial 365 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:49,669] Trial 366 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:52,073] Trial 367 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:54,479] Trial 368 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:41:57,334] Trial 369 finished with value: 0.013340226217631678 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:00,206] Trial 370 finished with value: 0.6660233442591099 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:02,610] Trial 371 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:05,142] Trial 372 finished with value: 0.6476137023760811 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:07,544] Trial 373 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:10,063] Trial 374 finished with value: 0.021074265313550954 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:12,468] Trial 375 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:14,871] Trial 376 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:17,275] Trial 377 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:19,653] Trial 378 finished with value: 0.4874456694533638 and parameters: {'C': 32.0, 'gamma': 3.0517578125e-05}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:22,062] Trial 379 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:24,465] Trial 380 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:26,856] Trial 381 finished with value: 0.6239357190977629 and parameters: {'C': 32.0, 'gamma': 0.000244140625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:29,262] Trial 382 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:31,509] Trial 383 finished with value: 0.4545066349645099 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:33,915] Trial 384 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:36,286] Trial 385 finished with value: 0.5463361761283894 and parameters: {'C': 32.0, 'gamma': 6.103515625e-05}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:38,735] Trial 386 finished with value: 0.2768480952590847 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:41,084] Trial 387 finished with value: 0.14355276952349966 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:43,483] Trial 388 finished with value: 0.07955097719927848 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:45,892] Trial 389 finished with value: 0.639761138814703 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:48,254] Trial 390 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:51,013] Trial 391 finished with value: 0.6901736499366239 and parameters: {'C': 32.0, 'gamma': 0.00390625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:53,365] Trial 392 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:55,639] Trial 393 finished with value: 0.5946364829596631 and parameters: {'C': 32.0, 'gamma': 0.0001220703125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:42:58,011] Trial 394 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:00,479] Trial 395 finished with value: 0.015370829585127598 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:02,976] Trial 396 finished with value: 0.7168944836295305 and parameters: {'C': 32.0, 'gamma': 0.0078125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:05,341] Trial 397 finished with value: 0.730483142268576 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:07,814] Trial 398 finished with value: 0.013576338517635322 and parameters: {'C': 32.0, 'gamma': 4.0}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:10,177] Trial 399 finished with value: 0.7294364286223367 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7305\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.693504    0.714602    0.720508    0.693961   \n",
      "1                    TP   26.000000   42.000000   35.000000   29.000000   \n",
      "2                    TN  310.000000  307.000000  312.000000  310.000000   \n",
      "3                    FP    6.000000    8.000000    3.000000    4.000000   \n",
      "4                    FN   40.000000   25.000000   32.000000   39.000000   \n",
      "5              Accuracy    0.879581    0.913613    0.908377    0.887435   \n",
      "6             Precision    0.812500    0.840000    0.921053    0.878788   \n",
      "7           Sensitivity    0.393939    0.626866    0.522388    0.426471   \n",
      "8           Specificity    0.981000    0.974600    0.990500    0.987300   \n",
      "9              F1 score    0.530612    0.717949    0.666667    0.574257   \n",
      "10  F1 score (weighted)    0.861766    0.908471    0.897740    0.870902   \n",
      "11     F1 score (macro)    0.730772    0.833472    0.806778    0.754700   \n",
      "12    Balanced Accuracy    0.687476    0.800734    0.756432    0.706866   \n",
      "13                  MCC    0.511661    0.678191    0.651665    0.563337   \n",
      "14                  NPV    0.885700    0.924700    0.907000    0.888300   \n",
      "15              ROC_AUC    0.687476    0.800734    0.756432    0.706866   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.726193    0.743756    0.705405    0.701961  \n",
      "1    33.000000   29.000000   28.000000   27.000000  \n",
      "2   312.000000  311.000000  308.000000  311.000000  \n",
      "3     4.000000    5.000000    6.000000    4.000000  \n",
      "4    33.000000   37.000000   40.000000   40.000000  \n",
      "5     0.903141    0.890052    0.879581    0.884817  \n",
      "6     0.891892    0.852941    0.823529    0.870968  \n",
      "7     0.500000    0.439394    0.411765    0.402985  \n",
      "8     0.987300    0.984200    0.980900    0.987300  \n",
      "9     0.640777    0.580000    0.549020    0.551020  \n",
      "10    0.891631    0.875110    0.862604    0.866774  \n",
      "11    0.792400    0.758373    0.739767    0.742477  \n",
      "12    0.743671    0.711786    0.696328    0.695143  \n",
      "13    0.622929    0.562358    0.527476    0.543553  \n",
      "14    0.904300    0.893700    0.885100    0.886000  \n",
      "15    0.743671    0.711786    0.696328    0.695143  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_svm_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "Y_testSet7_cat = np.where(((Y_testSet7>=2) | (Y_testSet7<=-2)), 1, 0) \n",
    "y_pred_svm_7_cat = np.where(((y_pred_svm_7 >= 2) | (y_pred_svm_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_svm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:43:13,080] Trial 400 finished with value: 0.7009217881330263 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:15,425] Trial 401 finished with value: 0.6994371063211258 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:17,852] Trial 402 finished with value: 0.6495694988935086 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:20,241] Trial 403 finished with value: 0.5643898071052318 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:22,737] Trial 404 finished with value: 0.222673459545786 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:25,430] Trial 405 finished with value: 0.0805796475168697 and parameters: {'C': 2.0, 'gamma': 0.25}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:27,950] Trial 406 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:30,454] Trial 407 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:32,967] Trial 408 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:35,653] Trial 409 finished with value: 0.04748900541572217 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:38,090] Trial 410 finished with value: 0.5807393503750101 and parameters: {'C': 32.0, 'gamma': 0.0625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:40,603] Trial 411 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:43,116] Trial 412 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:45,519] Trial 413 finished with value: 0.3310427516922466 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:48,026] Trial 414 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:50,535] Trial 415 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:53,660] Trial 416 finished with value: 0.027051924658545 and parameters: {'C': 64.0, 'gamma': 8.0}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:56,116] Trial 417 finished with value: 0.45460402527130095 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:43:59,119] Trial 418 finished with value: 0.6566827488705652 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:01,909] Trial 419 finished with value: 0.6493443633249241 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:04,477] Trial 420 finished with value: -0.0019482833126330701 and parameters: {'C': 0.015625, 'gamma': 1.0}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:07,028] Trial 421 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:09,567] Trial 422 finished with value: 0.0778536853113024 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:12,112] Trial 423 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:14,668] Trial 424 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:17,232] Trial 425 finished with value: 0.6405560349409654 and parameters: {'C': 32.0, 'gamma': 0.000244140625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:19,645] Trial 426 finished with value: 0.4953323577394606 and parameters: {'C': 32.0, 'gamma': 3.0517578125e-05}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:22,075] Trial 427 finished with value: 0.560633230491864 and parameters: {'C': 32.0, 'gamma': 6.103515625e-05}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:24,625] Trial 428 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:27,178] Trial 429 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:29,808] Trial 430 finished with value: 0.28239263826147337 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:32,371] Trial 431 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:34,981] Trial 432 finished with value: 0.6519686424753255 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:37,461] Trial 433 finished with value: 0.6495694988935086 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:40,020] Trial 434 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:42,725] Trial 435 finished with value: 0.7067804948998129 and parameters: {'C': 32.0, 'gamma': 0.0078125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:45,284] Trial 436 finished with value: 0.719407895065593 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:47,762] Trial 437 finished with value: 0.667697377663385 and parameters: {'C': 2.0, 'gamma': 0.00390625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:50,233] Trial 438 finished with value: 0.6075534139776335 and parameters: {'C': 32.0, 'gamma': 0.0001220703125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:52,656] Trial 439 finished with value: 0.5643898071052318 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:55,388] Trial 440 finished with value: 0.02981136010805302 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:44:57,766] Trial 441 finished with value: 0.6994371063211258 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:00,254] Trial 442 finished with value: 0.222673459545786 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:02,800] Trial 443 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:05,573] Trial 444 finished with value: 0.02738558497805934 and parameters: {'C': 32.0, 'gamma': 4.0}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:08,036] Trial 445 finished with value: 0.7009217881330263 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:10,593] Trial 446 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:13,151] Trial 447 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:15,612] Trial 448 finished with value: 0.3310427516922466 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:18,169] Trial 449 finished with value: 0.7194312315736999 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7305\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.693504    0.714602    0.720508    0.693961   \n",
      "1                    TP   26.000000   42.000000   35.000000   29.000000   \n",
      "2                    TN  310.000000  307.000000  312.000000  310.000000   \n",
      "3                    FP    6.000000    8.000000    3.000000    4.000000   \n",
      "4                    FN   40.000000   25.000000   32.000000   39.000000   \n",
      "5              Accuracy    0.879581    0.913613    0.908377    0.887435   \n",
      "6             Precision    0.812500    0.840000    0.921053    0.878788   \n",
      "7           Sensitivity    0.393939    0.626866    0.522388    0.426471   \n",
      "8           Specificity    0.981000    0.974600    0.990500    0.987300   \n",
      "9              F1 score    0.530612    0.717949    0.666667    0.574257   \n",
      "10  F1 score (weighted)    0.861766    0.908471    0.897740    0.870902   \n",
      "11     F1 score (macro)    0.730772    0.833472    0.806778    0.754700   \n",
      "12    Balanced Accuracy    0.687476    0.800734    0.756432    0.706866   \n",
      "13                  MCC    0.511661    0.678191    0.651665    0.563337   \n",
      "14                  NPV    0.885700    0.924700    0.907000    0.888300   \n",
      "15              ROC_AUC    0.687476    0.800734    0.756432    0.706866   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.726193    0.743756    0.705405    0.701961    0.694226  \n",
      "1    33.000000   29.000000   28.000000   27.000000   31.000000  \n",
      "2   312.000000  311.000000  308.000000  311.000000  310.000000  \n",
      "3     4.000000    5.000000    6.000000    4.000000    3.000000  \n",
      "4    33.000000   37.000000   40.000000   40.000000   38.000000  \n",
      "5     0.903141    0.890052    0.879581    0.884817    0.892670  \n",
      "6     0.891892    0.852941    0.823529    0.870968    0.911765  \n",
      "7     0.500000    0.439394    0.411765    0.402985    0.449275  \n",
      "8     0.987300    0.984200    0.980900    0.987300    0.990400  \n",
      "9     0.640777    0.580000    0.549020    0.551020    0.601942  \n",
      "10    0.891631    0.875110    0.862604    0.866774    0.877276  \n",
      "11    0.792400    0.758373    0.739767    0.742477    0.769957  \n",
      "12    0.743671    0.711786    0.696328    0.695143    0.719845  \n",
      "13    0.622929    0.562358    0.527476    0.543553    0.594039  \n",
      "14    0.904300    0.893700    0.885100    0.886000    0.890800  \n",
      "15    0.743671    0.711786    0.696328    0.695143    0.719845  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_svm_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "Y_testSet8_cat = np.where(((Y_testSet8>=2) | (Y_testSet8<=-2)), 1, 0) \n",
    "y_pred_svm_8_cat = np.where(((y_pred_svm_8 >= 2) | (y_pred_svm_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_svm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 04:45:21,217] Trial 450 finished with value: 0.04312884582647104 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:23,802] Trial 451 finished with value: 0.08414779288569925 and parameters: {'C': 64.0, 'gamma': 0.25}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:26,163] Trial 452 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:28,453] Trial 453 finished with value: 0.19876859967337118 and parameters: {'C': 0.125, 'gamma': 0.0625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:30,818] Trial 454 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:33,179] Trial 455 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:35,543] Trial 456 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:37,965] Trial 457 finished with value: 0.1367791702968893 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:40,851] Trial 458 finished with value: 0.6716828291121325 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:43,215] Trial 459 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:45,717] Trial 460 finished with value: 0.010700011837711387 and parameters: {'C': 0.0078125, 'gamma': 0.0009765625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:48,720] Trial 461 finished with value: 0.02359671533692581 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:51,320] Trial 462 finished with value: 0.03151234856495859 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:53,709] Trial 463 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:56,133] Trial 464 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:45:58,526] Trial 465 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:00,772] Trial 466 finished with value: 0.464378328304926 and parameters: {'C': 32.0, 'gamma': 3.0517578125e-05}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:03,099] Trial 467 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:05,383] Trial 468 finished with value: 0.18570128518095805 and parameters: {'C': 0.5, 'gamma': 0.000244140625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:07,704] Trial 469 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:10,036] Trial 470 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:12,475] Trial 471 finished with value: 0.2194563265775277 and parameters: {'C': 1.0, 'gamma': 0.125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:14,765] Trial 472 finished with value: 0.1874737186707095 and parameters: {'C': 2.0, 'gamma': 6.103515625e-05}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:17,153] Trial 473 finished with value: 0.7152977531371494 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:19,472] Trial 474 finished with value: 0.5416522568998782 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:21,883] Trial 475 finished with value: 0.6278886621787804 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:24,242] Trial 476 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:26,642] Trial 477 finished with value: 0.20303334881484442 and parameters: {'C': 0.03125, 'gamma': 0.0078125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:29,012] Trial 478 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:31,386] Trial 479 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:34,234] Trial 480 finished with value: 0.6939479064507127 and parameters: {'C': 32.0, 'gamma': 0.00390625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:36,591] Trial 481 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:38,898] Trial 482 finished with value: 0.318541026039699 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:41,262] Trial 483 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:43,662] Trial 484 finished with value: 0.5798594089587379 and parameters: {'C': 32.0, 'gamma': 0.0001220703125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:46,255] Trial 485 finished with value: 0.025929415959232427 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:48,895] Trial 486 finished with value: 0.02388850961468144 and parameters: {'C': 32.0, 'gamma': 4.0}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:51,258] Trial 487 finished with value: 0.7156609531995093 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:53,635] Trial 488 finished with value: 0.43158321470136307 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:55,995] Trial 489 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:46:58,358] Trial 490 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:47:00,724] Trial 491 finished with value: 0.6897662764730992 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:47:03,089] Trial 492 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:47:05,555] Trial 493 finished with value: -0.004652227600069536 and parameters: {'C': 0.015625, 'gamma': 0.5}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:47:08,131] Trial 494 finished with value: 0.08414779288569914 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:47:10,489] Trial 495 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:47:12,851] Trial 496 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:47:15,207] Trial 497 finished with value: 0.575336900258508 and parameters: {'C': 32.0, 'gamma': 0.0625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:47:17,599] Trial 498 finished with value: 0.07670301210562827 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n",
      "[I 2023-12-12 04:47:19,955] Trial 499 finished with value: 0.7166905431554212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 351 with value: 0.730483142268576.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7305\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.693504    0.714602    0.720508    0.693961   \n",
      "1                    TP   26.000000   42.000000   35.000000   29.000000   \n",
      "2                    TN  310.000000  307.000000  312.000000  310.000000   \n",
      "3                    FP    6.000000    8.000000    3.000000    4.000000   \n",
      "4                    FN   40.000000   25.000000   32.000000   39.000000   \n",
      "5              Accuracy    0.879581    0.913613    0.908377    0.887435   \n",
      "6             Precision    0.812500    0.840000    0.921053    0.878788   \n",
      "7           Sensitivity    0.393939    0.626866    0.522388    0.426471   \n",
      "8           Specificity    0.981000    0.974600    0.990500    0.987300   \n",
      "9              F1 score    0.530612    0.717949    0.666667    0.574257   \n",
      "10  F1 score (weighted)    0.861766    0.908471    0.897740    0.870902   \n",
      "11     F1 score (macro)    0.730772    0.833472    0.806778    0.754700   \n",
      "12    Balanced Accuracy    0.687476    0.800734    0.756432    0.706866   \n",
      "13                  MCC    0.511661    0.678191    0.651665    0.563337   \n",
      "14                  NPV    0.885700    0.924700    0.907000    0.888300   \n",
      "15              ROC_AUC    0.687476    0.800734    0.756432    0.706866   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.726193    0.743756    0.705405    0.701961    0.694226    0.700289  \n",
      "1    33.000000   29.000000   28.000000   27.000000   31.000000   32.000000  \n",
      "2   312.000000  311.000000  308.000000  311.000000  310.000000  313.000000  \n",
      "3     4.000000    5.000000    6.000000    4.000000    3.000000    2.000000  \n",
      "4    33.000000   37.000000   40.000000   40.000000   38.000000   35.000000  \n",
      "5     0.903141    0.890052    0.879581    0.884817    0.892670    0.903141  \n",
      "6     0.891892    0.852941    0.823529    0.870968    0.911765    0.941176  \n",
      "7     0.500000    0.439394    0.411765    0.402985    0.449275    0.477612  \n",
      "8     0.987300    0.984200    0.980900    0.987300    0.990400    0.993700  \n",
      "9     0.640777    0.580000    0.549020    0.551020    0.601942    0.633663  \n",
      "10    0.891631    0.875110    0.862604    0.866774    0.877276    0.889728  \n",
      "11    0.792400    0.758373    0.739767    0.742477    0.769957    0.788928  \n",
      "12    0.743671    0.711786    0.696328    0.695143    0.719845    0.735631  \n",
      "13    0.622929    0.562358    0.527476    0.543553    0.594039    0.629400  \n",
      "14    0.904300    0.893700    0.885100    0.886000    0.890800    0.899400  \n",
      "15    0.743671    0.711786    0.696328    0.695143    0.719845    0.735631  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_svm_9)\n",
    "# now convert the resuls to binary with cutoff 6.9\n",
    "Y_testSet9_cat = np.where(((Y_testSet9>=2) | (Y_testSet9<=-2)), 1, 0) \n",
    "y_pred_svm_9_cat = np.where(((y_pred_svm_9 >= 2) | (y_pred_svm_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_svm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7b0e56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7305\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "#plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.734461</td>\n",
       "      <td>0.034117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>17.700000</td>\n",
       "      <td>1.494434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>154.200000</td>\n",
       "      <td>1.988858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>2.270585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>1.766981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.014059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.846001</td>\n",
       "      <td>0.089536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.530317</td>\n",
       "      <td>0.047339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.978470</td>\n",
       "      <td>0.014303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.649498</td>\n",
       "      <td>0.047007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.890572</td>\n",
       "      <td>0.014946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.795580</td>\n",
       "      <td>0.027447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.754389</td>\n",
       "      <td>0.024514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.618317</td>\n",
       "      <td>0.058020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.907680</td>\n",
       "      <td>0.009588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.754389</td>\n",
       "      <td>0.024514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.734461     0.034117\n",
       "1                    TP        17.700000     1.494434\n",
       "2                    TN       154.200000     1.988858\n",
       "3                    FP         3.400000     2.270585\n",
       "4                    FN        15.700000     1.766981\n",
       "5              Accuracy         0.900000     0.014059\n",
       "6             Precision         0.846001     0.089536\n",
       "7           Sensitivity         0.530317     0.047339\n",
       "8           Specificity         0.978470     0.014303\n",
       "9              F1 score         0.649498     0.047007\n",
       "10  F1 score (weighted)         0.890572     0.014946\n",
       "11     F1 score (macro)         0.795580     0.027447\n",
       "12    Balanced Accuracy         0.754389     0.024514\n",
       "13                  MCC         0.618317     0.058020\n",
       "14                  NPV         0.907680     0.009588\n",
       "15              ROC_AUC         0.754389     0.024514"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.693504</td>\n",
       "      <td>0.714602</td>\n",
       "      <td>0.720508</td>\n",
       "      <td>0.693961</td>\n",
       "      <td>0.726193</td>\n",
       "      <td>0.743756</td>\n",
       "      <td>0.705405</td>\n",
       "      <td>0.701961</td>\n",
       "      <td>0.694226</td>\n",
       "      <td>0.700289</td>\n",
       "      <td>0.709441</td>\n",
       "      <td>0.016628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>31.200000</td>\n",
       "      <td>4.709329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>310.400000</td>\n",
       "      <td>1.837873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.779513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.900000</td>\n",
       "      <td>4.817791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.879581</td>\n",
       "      <td>0.913613</td>\n",
       "      <td>0.908377</td>\n",
       "      <td>0.887435</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.879581</td>\n",
       "      <td>0.884817</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.894241</td>\n",
       "      <td>0.012104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.874461</td>\n",
       "      <td>0.042729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.402985</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.465069</td>\n",
       "      <td>0.070707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>0.974600</td>\n",
       "      <td>0.990500</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>0.984200</td>\n",
       "      <td>0.980900</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>0.990400</td>\n",
       "      <td>0.993700</td>\n",
       "      <td>0.985720</td>\n",
       "      <td>0.005651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.574257</td>\n",
       "      <td>0.640777</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.633663</td>\n",
       "      <td>0.604591</td>\n",
       "      <td>0.059461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.861766</td>\n",
       "      <td>0.908471</td>\n",
       "      <td>0.897740</td>\n",
       "      <td>0.870902</td>\n",
       "      <td>0.891631</td>\n",
       "      <td>0.875110</td>\n",
       "      <td>0.862604</td>\n",
       "      <td>0.866774</td>\n",
       "      <td>0.877276</td>\n",
       "      <td>0.889728</td>\n",
       "      <td>0.880200</td>\n",
       "      <td>0.015913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.730772</td>\n",
       "      <td>0.833472</td>\n",
       "      <td>0.806778</td>\n",
       "      <td>0.754700</td>\n",
       "      <td>0.792400</td>\n",
       "      <td>0.758373</td>\n",
       "      <td>0.739767</td>\n",
       "      <td>0.742477</td>\n",
       "      <td>0.769957</td>\n",
       "      <td>0.788928</td>\n",
       "      <td>0.771763</td>\n",
       "      <td>0.032981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.687476</td>\n",
       "      <td>0.800734</td>\n",
       "      <td>0.756432</td>\n",
       "      <td>0.706866</td>\n",
       "      <td>0.743671</td>\n",
       "      <td>0.711786</td>\n",
       "      <td>0.696328</td>\n",
       "      <td>0.695143</td>\n",
       "      <td>0.719845</td>\n",
       "      <td>0.735631</td>\n",
       "      <td>0.725391</td>\n",
       "      <td>0.034698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.511661</td>\n",
       "      <td>0.678191</td>\n",
       "      <td>0.651665</td>\n",
       "      <td>0.563337</td>\n",
       "      <td>0.622929</td>\n",
       "      <td>0.562358</td>\n",
       "      <td>0.527476</td>\n",
       "      <td>0.543553</td>\n",
       "      <td>0.594039</td>\n",
       "      <td>0.629400</td>\n",
       "      <td>0.588461</td>\n",
       "      <td>0.055666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.885700</td>\n",
       "      <td>0.924700</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>0.888300</td>\n",
       "      <td>0.904300</td>\n",
       "      <td>0.893700</td>\n",
       "      <td>0.885100</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>0.890800</td>\n",
       "      <td>0.899400</td>\n",
       "      <td>0.896500</td>\n",
       "      <td>0.012645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.687476</td>\n",
       "      <td>0.800734</td>\n",
       "      <td>0.756432</td>\n",
       "      <td>0.706866</td>\n",
       "      <td>0.743671</td>\n",
       "      <td>0.711786</td>\n",
       "      <td>0.696328</td>\n",
       "      <td>0.695143</td>\n",
       "      <td>0.719845</td>\n",
       "      <td>0.735631</td>\n",
       "      <td>0.725391</td>\n",
       "      <td>0.034698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.693504    0.714602    0.720508    0.693961   \n",
       "1                    TP   26.000000   42.000000   35.000000   29.000000   \n",
       "2                    TN  310.000000  307.000000  312.000000  310.000000   \n",
       "3                    FP    6.000000    8.000000    3.000000    4.000000   \n",
       "4                    FN   40.000000   25.000000   32.000000   39.000000   \n",
       "5              Accuracy    0.879581    0.913613    0.908377    0.887435   \n",
       "6             Precision    0.812500    0.840000    0.921053    0.878788   \n",
       "7           Sensitivity    0.393939    0.626866    0.522388    0.426471   \n",
       "8           Specificity    0.981000    0.974600    0.990500    0.987300   \n",
       "9              F1 score    0.530612    0.717949    0.666667    0.574257   \n",
       "10  F1 score (weighted)    0.861766    0.908471    0.897740    0.870902   \n",
       "11     F1 score (macro)    0.730772    0.833472    0.806778    0.754700   \n",
       "12    Balanced Accuracy    0.687476    0.800734    0.756432    0.706866   \n",
       "13                  MCC    0.511661    0.678191    0.651665    0.563337   \n",
       "14                  NPV    0.885700    0.924700    0.907000    0.888300   \n",
       "15              ROC_AUC    0.687476    0.800734    0.756432    0.706866   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.726193    0.743756    0.705405    0.701961    0.694226    0.700289   \n",
       "1    33.000000   29.000000   28.000000   27.000000   31.000000   32.000000   \n",
       "2   312.000000  311.000000  308.000000  311.000000  310.000000  313.000000   \n",
       "3     4.000000    5.000000    6.000000    4.000000    3.000000    2.000000   \n",
       "4    33.000000   37.000000   40.000000   40.000000   38.000000   35.000000   \n",
       "5     0.903141    0.890052    0.879581    0.884817    0.892670    0.903141   \n",
       "6     0.891892    0.852941    0.823529    0.870968    0.911765    0.941176   \n",
       "7     0.500000    0.439394    0.411765    0.402985    0.449275    0.477612   \n",
       "8     0.987300    0.984200    0.980900    0.987300    0.990400    0.993700   \n",
       "9     0.640777    0.580000    0.549020    0.551020    0.601942    0.633663   \n",
       "10    0.891631    0.875110    0.862604    0.866774    0.877276    0.889728   \n",
       "11    0.792400    0.758373    0.739767    0.742477    0.769957    0.788928   \n",
       "12    0.743671    0.711786    0.696328    0.695143    0.719845    0.735631   \n",
       "13    0.622929    0.562358    0.527476    0.543553    0.594039    0.629400   \n",
       "14    0.904300    0.893700    0.885100    0.886000    0.890800    0.899400   \n",
       "15    0.743671    0.711786    0.696328    0.695143    0.719845    0.735631   \n",
       "\n",
       "           ave       std  \n",
       "0     0.709441  0.016628  \n",
       "1    31.200000  4.709329  \n",
       "2   310.400000  1.837873  \n",
       "3     4.500000  1.779513  \n",
       "4    35.900000  4.817791  \n",
       "5     0.894241  0.012104  \n",
       "6     0.874461  0.042729  \n",
       "7     0.465069  0.070707  \n",
       "8     0.985720  0.005651  \n",
       "9     0.604591  0.059461  \n",
       "10    0.880200  0.015913  \n",
       "11    0.771763  0.032981  \n",
       "12    0.725391  0.034698  \n",
       "13    0.588461  0.055666  \n",
       "14    0.896500  0.012645  \n",
       "15    0.725391  0.034698  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test['std'] = mat_met_svm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.732133</td>\n",
       "      <td>0.040554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.898115</td>\n",
       "      <td>0.019738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.844665</td>\n",
       "      <td>0.078076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.514670</td>\n",
       "      <td>0.101328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.979312</td>\n",
       "      <td>0.012275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.633773</td>\n",
       "      <td>0.087549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.887120</td>\n",
       "      <td>0.023983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.787264</td>\n",
       "      <td>0.048967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.746991</td>\n",
       "      <td>0.050647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.606295</td>\n",
       "      <td>0.085942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.905352</td>\n",
       "      <td>0.017845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.746991</td>\n",
       "      <td>0.050647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.732133     0.040554\n",
       "1              Accuracy         0.898115     0.019738\n",
       "2             Precision         0.844665     0.078076\n",
       "3           Sensitivity         0.514670     0.101328\n",
       "4           Specificity         0.979312     0.012275\n",
       "5              F1 score         0.633773     0.087549\n",
       "6   F1 score (weighted)         0.887120     0.023983\n",
       "7      F1 score (macro)         0.787264     0.048967\n",
       "8     Balanced Accuracy         0.746991     0.050647\n",
       "9                   MCC         0.606295     0.085942\n",
       "10                  NPV         0.905352     0.017845\n",
       "11              ROC_AUC         0.746991     0.050647"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_svm_cat = np.where(((y_pred_optimized_svm >= 2) | (y_pred_optimized_svm <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_svm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_svm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "svm_run0 = data_svm[['y_test_idx0', 'y_test0', 'y_pred_svm0']]\n",
    "svm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "svm_run0.reset_index(inplace=True, drop=True)\n",
    "svm_run1 = data_svm[['y_test_idx1', 'y_test1', 'y_pred_svm1']]\n",
    "svm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "svm_run1.reset_index(inplace=True, drop=True)\n",
    "svm_run2 = data_svm[['y_test_idx2', 'y_test2', 'y_pred_svm2']]\n",
    "svm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "svm_run2.reset_index(inplace=True, drop=True)\n",
    "svm_run3 = data_svm[['y_test_idx3', 'y_test3', 'y_pred_svm3']]\n",
    "svm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "svm_run3.reset_index(inplace=True, drop=True)\n",
    "svm_run4 = data_svm[['y_test_idx4', 'y_test4', 'y_pred_svm4']]\n",
    "svm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "svm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "svm_5preds = pd.concat([chembl_id,svm_run0, svm_run1, svm_run2, svm_run3, svm_run4], axis=1)\n",
    "svm_5preds = svm_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_svm0', 'y_pred_svm1', 'y_pred_svm2', 'y_pred_svm3', 'y_pred_svm4']]\n",
    "svm_5preds['y_pred_svm_ave'] = svm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "svm_5preds['y_pred_svm_std'] = svm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "\n",
    "mat_met_optimized_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2869d8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model r2_score 0.6985 with a standard deviation of 0.0390\n",
      "SVM optimized model r2_score 0.7276 with a standard deviation of 0.0375\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVR \n",
    "svm_baseline_CVscore = cross_val_score(svm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"SVM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_svm_withSemiSel.joblib']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_reg, \"OUTPUT/svm_reg_withSemiSel.joblib\")\n",
    "joblib.dump(optimizedCV_svm, \"OUTPUT/optimizedCV_svm_withSemiSel.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2f79bd46-edc3-436d-8645-d799fe5d49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of test set \n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/TestSet_EvaluationResults_withSemiSel.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_rf_test.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_lgbm_test.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_xgb_test.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_knn_test.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_svm_test.to_excel(writer, sheet_name=\"SVM\", )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "84941152-089d-4746-a005-cb43e7de4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/EvaluationResults_withSemiSel.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_optimized_rf.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_optimized_lgbm.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_optimized_xgb.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_optimized_knn.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_optimized_svm.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1c758d92-e3b4-4361-96f2-aaabf7965382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/PredResults_withSemiSel.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    rf_5preds.to_excel(writer, sheet_name=\"RF\", )\n",
    "    lgbm_5preds.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    xgb_5preds.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    knn_5preds.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    svm_5preds.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
