{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/miniforge3/envs/teachopencadd/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "HDAC1and6 = Path(HERE).resolve().parents[1]/'input'\n",
    "output = HERE/'OUTPUT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL3957055</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[3356504, 10511, 2043088, 188815, 1375616, 229...</td>\n",
       "      <td>-0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL494139</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[7030628, 192820, 2475943, 12610622, 16526612,...</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4562156</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[12658752, 17323238, 1578989, 1744108, 3074600...</td>\n",
       "      <td>-0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3656010</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[569161, 2478511, 10872982, 28863463, 11932100...</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4098975</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[2770663, 83582, 137380, 5467685, 28861, 20399...</td>\n",
       "      <td>2.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL3957055  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1       CHEMBL494139  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL4562156  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL3656010  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL4098975  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  SelectivityWindow  \n",
       "0  [3356504, 10511, 2043088, 188815, 1375616, 229...              -0.51  \n",
       "1  [7030628, 192820, 2475943, 12610622, 16526612,...               0.87  \n",
       "2  [12658752, 17323238, 1578989, 1744108, 3074600...              -0.58  \n",
       "3  [569161, 2478511, 10872982, 28863463, 11932100...               0.51  \n",
       "4  [2770663, 83582, 137380, 5467685, 28861, 20399...               2.89  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC1and6/\"HDAC1and6_SemiSel_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>type_HDAC1</th>\n",
       "      <th>Standard_Value_HDAC1</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "      <th>type_HDAC6</th>\n",
       "      <th>Standard_Value_HDAC6</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "      <th>SelectivityRatio</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4098975</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>109.647820</td>\n",
       "      <td>6.96</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.141254</td>\n",
       "      <td>9.85</td>\n",
       "      <td>776.247117</td>\n",
       "      <td>2.89</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3912061</td>\n",
       "      <td>CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12</td>\n",
       "      <td>IC50</td>\n",
       "      <td>616.595002</td>\n",
       "      <td>6.21</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.169824</td>\n",
       "      <td>9.77</td>\n",
       "      <td>3630.780548</td>\n",
       "      <td>3.56</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4243347</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1.995262</td>\n",
       "      <td>8.70</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.199526</td>\n",
       "      <td>9.70</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4247128</td>\n",
       "      <td>C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>83.176377</td>\n",
       "      <td>7.08</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.251189</td>\n",
       "      <td>9.60</td>\n",
       "      <td>331.131122</td>\n",
       "      <td>2.52</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4126811</td>\n",
       "      <td>CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>436.515832</td>\n",
       "      <td>6.36</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.331131</td>\n",
       "      <td>9.48</td>\n",
       "      <td>1318.256739</td>\n",
       "      <td>3.12</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>CHEMBL4167599</td>\n",
       "      <td>NCCCCNCCCCNCCCN1C(=O)c2ccc3c4c(ccc(c24)C1=O)C(...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>4073.802778</td>\n",
       "      <td>5.39</td>\n",
       "      <td>IC50</td>\n",
       "      <td>50.118723</td>\n",
       "      <td>7.30</td>\n",
       "      <td>81.283052</td>\n",
       "      <td>1.91</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>CHEMBL4282471</td>\n",
       "      <td>CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(NC(=O)CCCCC(=O)NO)...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>3388.441561</td>\n",
       "      <td>5.47</td>\n",
       "      <td>IC50</td>\n",
       "      <td>117.489756</td>\n",
       "      <td>6.93</td>\n",
       "      <td>28.840315</td>\n",
       "      <td>1.46</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>CHEMBL470843</td>\n",
       "      <td>O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO</td>\n",
       "      <td>IC50</td>\n",
       "      <td>6309.573445</td>\n",
       "      <td>5.20</td>\n",
       "      <td>IC50</td>\n",
       "      <td>173.780083</td>\n",
       "      <td>6.76</td>\n",
       "      <td>36.307805</td>\n",
       "      <td>1.56</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>CHEMBL3215861</td>\n",
       "      <td>CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC</td>\n",
       "      <td>Ki</td>\n",
       "      <td>28.183829</td>\n",
       "      <td>7.55</td>\n",
       "      <td>Ki</td>\n",
       "      <td>245.470892</td>\n",
       "      <td>6.61</td>\n",
       "      <td>0.114815</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>CHEMBL3233708</td>\n",
       "      <td>O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>354.813389</td>\n",
       "      <td>6.45</td>\n",
       "      <td>IC50</td>\n",
       "      <td>295.120923</td>\n",
       "      <td>6.53</td>\n",
       "      <td>1.202264</td>\n",
       "      <td>0.08</td>\n",
       "      <td>Non-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1910 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                             smiles  \\\n",
       "0         CHEMBL4098975  O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...   \n",
       "1         CHEMBL3912061     CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12   \n",
       "2         CHEMBL4243347  O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...   \n",
       "3         CHEMBL4247128  C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...   \n",
       "4         CHEMBL4126811  CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...   \n",
       "...                 ...                                                ...   \n",
       "1905      CHEMBL4167599  NCCCCNCCCCNCCCN1C(=O)c2ccc3c4c(ccc(c24)C1=O)C(...   \n",
       "1906      CHEMBL4282471  CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(NC(=O)CCCCC(=O)NO)...   \n",
       "1907       CHEMBL470843          O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO   \n",
       "1908      CHEMBL3215861           CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC   \n",
       "1909      CHEMBL3233708  O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...   \n",
       "\n",
       "     type_HDAC1  Standard_Value_HDAC1  pChEMBL_HDAC1 type_HDAC6  \\\n",
       "0          IC50            109.647820           6.96       IC50   \n",
       "1          IC50            616.595002           6.21       IC50   \n",
       "2          IC50              1.995262           8.70       IC50   \n",
       "3          IC50             83.176377           7.08       IC50   \n",
       "4          IC50            436.515832           6.36       IC50   \n",
       "...         ...                   ...            ...        ...   \n",
       "1905       IC50           4073.802778           5.39       IC50   \n",
       "1906       IC50           3388.441561           5.47       IC50   \n",
       "1907       IC50           6309.573445           5.20       IC50   \n",
       "1908         Ki             28.183829           7.55         Ki   \n",
       "1909       IC50            354.813389           6.45       IC50   \n",
       "\n",
       "      Standard_Value_HDAC6  pChEMBL_HDAC6  SelectivityRatio  \\\n",
       "0                 0.141254           9.85        776.247117   \n",
       "1                 0.169824           9.77       3630.780548   \n",
       "2                 0.199526           9.70         10.000000   \n",
       "3                 0.251189           9.60        331.131122   \n",
       "4                 0.331131           9.48       1318.256739   \n",
       "...                    ...            ...               ...   \n",
       "1905             50.118723           7.30         81.283052   \n",
       "1906            117.489756           6.93         28.840315   \n",
       "1907            173.780083           6.76         36.307805   \n",
       "1908            245.470892           6.61          0.114815   \n",
       "1909            295.120923           6.53          1.202264   \n",
       "\n",
       "      SelectivityWindow            label  \n",
       "0                  2.89  HDAC6-selective  \n",
       "1                  3.56  HDAC6-selective  \n",
       "2                  1.00      Dual-binder  \n",
       "3                  2.52  HDAC6-selective  \n",
       "4                  3.12  HDAC6-selective  \n",
       "...                 ...              ...  \n",
       "1905               1.91   Semi-selective  \n",
       "1906               1.46   Semi-selective  \n",
       "1907               1.56   Semi-selective  \n",
       "1908              -0.94      Dual-binder  \n",
       "1909               0.08       Non-binder  \n",
       "\n",
       "[1910 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC1and6/\"HDAC1and6_SemiSel_dataset.csv\", )\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL3957055</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[3356504, 10511, 2043088, 188815, 1375616, 229...</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL494139</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[7030628, 192820, 2475943, 12610622, 16526612,...</td>\n",
       "      <td>0.87</td>\n",
       "      <td>Non-binder</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4562156</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[12658752, 17323238, 1578989, 1744108, 3074600...</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3656010</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[569161, 2478511, 10872982, 28863463, 11932100...</td>\n",
       "      <td>0.51</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL3957055  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1       CHEMBL494139  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL4562156  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL3656010  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  SelectivityWindow  \\\n",
       "0  [3356504, 10511, 2043088, 188815, 1375616, 229...              -0.51   \n",
       "1  [7030628, 192820, 2475943, 12610622, 16526612,...               0.87   \n",
       "2  [12658752, 17323238, 1578989, 1744108, 3074600...              -0.58   \n",
       "3  [569161, 2478511, 10872982, 28863463, 11932100...               0.51   \n",
       "\n",
       "         label  Class  \n",
       "0  Dual-binder    3.0  \n",
       "1   Non-binder    4.0  \n",
       "2  Dual-binder    3.0  \n",
       "3  Dual-binder    3.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'hDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'hDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'hDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "df.loc[df[df.label == 'Semi-selective'].index, \"Class\"] = 5.0\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for selectivity\n",
    "df[\"selectivity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as selective if SelectivityWindow is >=2 or >=-2, 0 otherwise\n",
    "df.loc[df[df.SelectivityWindow >= 2.0].index, \"selectivity\"] = 1.0\n",
    "df.loc[df[df.SelectivityWindow <= -2.0].index, \"selectivity\"] = 1.0\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "indices =  np.array(df.index)\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y = df[\"SelectivityWindow\"].values\n",
    "Y_cat =  df[\"selectivity\"].values\n",
    "Y_class = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['SelectivityWindow'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_cat\"] = np.array(list(df.iloc[index_tr_all[i]]['selectivity'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['SelectivityWindow'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_cat\"] = np.array(list(df.iloc[index_te_all[i]]['selectivity'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.707617     0.052025\n",
      "1                    TP        16.700000     3.093003\n",
      "2                    TN       154.700000     1.636392\n",
      "3                    FP         2.900000     1.449138\n",
      "4                    FN        16.700000     3.743142\n",
      "5              Accuracy         0.897382     0.022647\n",
      "6             Precision         0.850761     0.073574\n",
      "7           Sensitivity         0.501793     0.102448\n",
      "8           Specificity         0.981610     0.009163\n",
      "9              F1 score         0.627260     0.093761\n",
      "10  F1 score (weighted)         0.885486     0.027754\n",
      "11     F1 score (macro)         0.783870     0.053228\n",
      "12    Balanced Accuracy         0.741699     0.052802\n",
      "13                  MCC         0.601968     0.094369\n",
      "14                  NPV         0.902870     0.020338\n",
      "15              ROC_AUC         0.741699     0.052802\n",
      "CPU times: user 1min 49s, sys: 83.7 ms, total: 1min 49s\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_reg =  RandomForestRegressor(random_state=1121218, max_features = None, n_jobs=4,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_reg.fit(x_train, y_train)\n",
    "        y_pred = rf_reg.predict(x_test)  \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=4, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=4, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 15:47:18,980] A new study created in memory with name: RFRegressor\n",
      "[I 2023-12-19 15:49:39,431] Trial 0 finished with value: 0.6978296967251142 and parameters: {'n_estimators': 633}. Best is trial 0 with value: 0.6978296967251142.\n",
      "[I 2023-12-19 15:52:47,074] Trial 1 finished with value: 0.6983451174948774 and parameters: {'n_estimators': 715}. Best is trial 1 with value: 0.6983451174948774.\n",
      "[I 2023-12-19 15:54:15,011] Trial 2 finished with value: 0.6985189358102145 and parameters: {'n_estimators': 323}. Best is trial 2 with value: 0.6985189358102145.\n",
      "[I 2023-12-19 15:56:47,814] Trial 3 finished with value: 0.6976914159085738 and parameters: {'n_estimators': 558}. Best is trial 2 with value: 0.6985189358102145.\n",
      "[I 2023-12-19 15:57:42,478] Trial 4 finished with value: 0.6964507024044121 and parameters: {'n_estimators': 199}. Best is trial 2 with value: 0.6985189358102145.\n",
      "[I 2023-12-19 15:59:12,954] Trial 5 finished with value: 0.6985216355823264 and parameters: {'n_estimators': 335}. Best is trial 5 with value: 0.6985216355823264.\n",
      "[I 2023-12-19 16:01:03,803] Trial 6 finished with value: 0.6984410262261294 and parameters: {'n_estimators': 408}. Best is trial 5 with value: 0.6985216355823264.\n",
      "[I 2023-12-19 16:05:11,289] Trial 7 finished with value: 0.6987826261388295 and parameters: {'n_estimators': 941}. Best is trial 7 with value: 0.6987826261388295.\n",
      "[I 2023-12-19 16:06:17,478] Trial 8 finished with value: 0.6977943039296017 and parameters: {'n_estimators': 248}. Best is trial 7 with value: 0.6987826261388295.\n",
      "[I 2023-12-19 16:08:37,298] Trial 9 finished with value: 0.6978562333909915 and parameters: {'n_estimators': 526}. Best is trial 7 with value: 0.6987826261388295.\n",
      "[I 2023-12-19 16:12:48,954] Trial 10 finished with value: 0.6985292954849013 and parameters: {'n_estimators': 967}. Best is trial 7 with value: 0.6987826261388295.\n",
      "[I 2023-12-19 16:17:06,404] Trial 11 finished with value: 0.6985778577156427 and parameters: {'n_estimators': 990}. Best is trial 7 with value: 0.6987826261388295.\n",
      "[I 2023-12-19 16:21:15,270] Trial 12 finished with value: 0.6987138967376468 and parameters: {'n_estimators': 955}. Best is trial 7 with value: 0.6987826261388295.\n",
      "[I 2023-12-19 16:24:52,780] Trial 13 finished with value: 0.6986919660762607 and parameters: {'n_estimators': 847}. Best is trial 7 with value: 0.6987826261388295.\n",
      "[I 2023-12-19 16:28:24,885] Trial 14 finished with value: 0.6983601201402858 and parameters: {'n_estimators': 809}. Best is trial 7 with value: 0.6987826261388295.\n",
      "[I 2023-12-19 16:32:05,110] Trial 15 finished with value: 0.6986540308686914 and parameters: {'n_estimators': 862}. Best is trial 7 with value: 0.6987826261388295.\n",
      "[I 2023-12-19 16:35:12,793] Trial 16 finished with value: 0.6986062683392262 and parameters: {'n_estimators': 744}. Best is trial 7 with value: 0.6987826261388295.\n",
      "[I 2023-12-19 16:39:10,600] Trial 17 finished with value: 0.6987995856199372 and parameters: {'n_estimators': 929}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 16:42:10,431] Trial 18 finished with value: 0.6981764866344948 and parameters: {'n_estimators': 690}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 16:46:06,270] Trial 19 finished with value: 0.6987271194427698 and parameters: {'n_estimators': 915}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 16:49:28,394] Trial 20 finished with value: 0.6987146635016707 and parameters: {'n_estimators': 785}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 16:53:18,388] Trial 21 finished with value: 0.6986977386447236 and parameters: {'n_estimators': 894}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 16:57:13,264] Trial 22 finished with value: 0.6986896096413695 and parameters: {'n_estimators': 902}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:01:06,913] Trial 23 finished with value: 0.6986969014370402 and parameters: {'n_estimators': 912}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:05:19,836] Trial 24 finished with value: 0.6985778577156428 and parameters: {'n_estimators': 990}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:08:02,814] Trial 25 finished with value: 0.6978637847874687 and parameters: {'n_estimators': 638}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:11:29,694] Trial 26 finished with value: 0.6984729244953229 and parameters: {'n_estimators': 816}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:14:40,246] Trial 27 finished with value: 0.6986279021572305 and parameters: {'n_estimators': 757}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:18:36,432] Trial 28 finished with value: 0.6987397645034965 and parameters: {'n_estimators': 924}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:21:05,865] Trial 29 finished with value: 0.6977092485467362 and parameters: {'n_estimators': 583}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:23:10,320] Trial 30 finished with value: 0.698031134381217 and parameters: {'n_estimators': 487}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:27:05,336] Trial 31 finished with value: 0.6987518559774664 and parameters: {'n_estimators': 928}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:30:43,032] Trial 32 finished with value: 0.6987062951739371 and parameters: {'n_estimators': 857}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:33:29,580] Trial 33 finished with value: 0.6979685827410893 and parameters: {'n_estimators': 661}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:37:27,922] Trial 34 finished with value: 0.6987891855444073 and parameters: {'n_estimators': 946}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:41:30,328] Trial 35 finished with value: 0.6987927483746288 and parameters: {'n_estimators': 949}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:45:44,779] Trial 36 finished with value: 0.6986820411359591 and parameters: {'n_estimators': 1000}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:49:18,644] Trial 37 finished with value: 0.6986441014813779 and parameters: {'n_estimators': 843}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:52:23,302] Trial 38 finished with value: 0.6982476401338111 and parameters: {'n_estimators': 722}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:55:41,005] Trial 39 finished with value: 0.698640148165419 and parameters: {'n_estimators': 776}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 17:56:07,552] Trial 40 finished with value: 0.6984774417319354 and parameters: {'n_estimators': 104}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:00:06,932] Trial 41 finished with value: 0.6987558371562865 and parameters: {'n_estimators': 945}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:04:04,321] Trial 42 finished with value: 0.6987826261388295 and parameters: {'n_estimators': 941}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:07:47,323] Trial 43 finished with value: 0.6986451283499081 and parameters: {'n_estimators': 881}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:11:50,126] Trial 44 finished with value: 0.6987746856038168 and parameters: {'n_estimators': 948}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:15:54,851] Trial 45 finished with value: 0.6985433956928134 and parameters: {'n_estimators': 968}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:19:19,870] Trial 46 finished with value: 0.6983894124709352 and parameters: {'n_estimators': 811}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:20:44,982] Trial 47 finished with value: 0.6986411627741994 and parameters: {'n_estimators': 336}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:24:19,932] Trial 48 finished with value: 0.6986638980285458 and parameters: {'n_estimators': 863}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:28:31,328] Trial 49 finished with value: 0.6987005185511759 and parameters: {'n_estimators': 996}. Best is trial 17 with value: 0.6987995856199372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6988\n",
      "\tBest params:\n",
      "\t\tn_estimators: 929\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFRegressor\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.681830\n",
      "1                    TP   30.000000\n",
      "2                    TN  311.000000\n",
      "3                    FP    5.000000\n",
      "4                    FN   36.000000\n",
      "5              Accuracy    0.892670\n",
      "6             Precision    0.857143\n",
      "7           Sensitivity    0.454545\n",
      "8           Specificity    0.984200\n",
      "9              F1 score    0.594059\n",
      "10  F1 score (weighted)    0.878708\n",
      "11     F1 score (macro)    0.766110\n",
      "12    Balanced Accuracy    0.719361\n",
      "13                  MCC    0.574919\n",
      "14                  NPV    0.896300\n",
      "15              ROC_AUC    0.719361\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_rf_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet0 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_0_cat = np.where(((y_pred_rf_0 >= 2) | (y_pred_rf_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_rf_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 18:32:28,521] Trial 50 finished with value: 0.6896775731198448 and parameters: {'n_estimators': 824}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:36:28,387] Trial 51 finished with value: 0.6898233603323618 and parameters: {'n_estimators': 950}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:40:24,292] Trial 52 finished with value: 0.6898478509386852 and parameters: {'n_estimators': 956}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:44:04,370] Trial 53 finished with value: 0.6897949718610709 and parameters: {'n_estimators': 875}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:47:58,733] Trial 54 finished with value: 0.6897229703788579 and parameters: {'n_estimators': 927}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:51:43,934] Trial 55 finished with value: 0.6899195383113705 and parameters: {'n_estimators': 891}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:55:49,230] Trial 56 finished with value: 0.6897990036959535 and parameters: {'n_estimators': 967}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 18:59:36,084] Trial 57 finished with value: 0.6898304365997519 and parameters: {'n_estimators': 901}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:02:58,252] Trial 58 finished with value: 0.6897662767378633 and parameters: {'n_estimators': 789}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:07:04,863] Trial 59 finished with value: 0.6898046543614192 and parameters: {'n_estimators': 973}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:11:07,187] Trial 60 finished with value: 0.689739662443419 and parameters: {'n_estimators': 934}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:15:07,991] Trial 61 finished with value: 0.6898597157945627 and parameters: {'n_estimators': 945}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:18:41,581] Trial 62 finished with value: 0.6896694678429373 and parameters: {'n_estimators': 849}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:22:27,940] Trial 63 finished with value: 0.689805726261546 and parameters: {'n_estimators': 887}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:26:24,846] Trial 64 finished with value: 0.6897139966958286 and parameters: {'n_estimators': 939}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:30:39,430] Trial 65 finished with value: 0.689970476383589 and parameters: {'n_estimators': 998}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:34:09,332] Trial 66 finished with value: 0.6896804981378267 and parameters: {'n_estimators': 833}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:37:59,849] Trial 67 finished with value: 0.6897871714223742 and parameters: {'n_estimators': 916}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:42:08,256] Trial 68 finished with value: 0.6898511451812162 and parameters: {'n_estimators': 968}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:44:33,461] Trial 69 finished with value: 0.6903678425180655 and parameters: {'n_estimators': 576}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:46:13,762] Trial 70 finished with value: 0.6900825744438115 and parameters: {'n_estimators': 388}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:50:09,141] Trial 71 finished with value: 0.6896881608274511 and parameters: {'n_estimators': 926}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:53:57,920] Trial 72 finished with value: 0.6898384307184496 and parameters: {'n_estimators': 899}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 19:58:05,341] Trial 73 finished with value: 0.6897536177920249 and parameters: {'n_estimators': 975}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:01:43,974] Trial 74 finished with value: 0.6898437547197034 and parameters: {'n_estimators': 871}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:05:35,330] Trial 75 finished with value: 0.6897426767807334 and parameters: {'n_estimators': 914}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:09:32,770] Trial 76 finished with value: 0.6897380503345939 and parameters: {'n_estimators': 938}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:13:40,776] Trial 77 finished with value: 0.6897215049973024 and parameters: {'n_estimators': 985}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:17:37,036] Trial 78 finished with value: 0.689828450294385 and parameters: {'n_estimators': 948}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:18:29,889] Trial 79 finished with value: 0.6875213023338544 and parameters: {'n_estimators': 200}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:20:30,627] Trial 80 finished with value: 0.6899113593972285 and parameters: {'n_estimators': 473}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:24:19,743] Trial 81 finished with value: 0.6897426767807334 and parameters: {'n_estimators': 914}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:28:00,146] Trial 82 finished with value: 0.6897679333619509 and parameters: {'n_estimators': 869}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:32:00,144] Trial 83 finished with value: 0.6898418999137645 and parameters: {'n_estimators': 957}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:35:34,335] Trial 84 finished with value: 0.6895978822385864 and parameters: {'n_estimators': 841}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:39:19,176] Trial 85 finished with value: 0.6899195383113705 and parameters: {'n_estimators': 891}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:43:32,657] Trial 86 finished with value: 0.6899644544498315 and parameters: {'n_estimators': 999}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:47:24,005] Trial 87 finished with value: 0.6896961808057448 and parameters: {'n_estimators': 925}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:50:43,927] Trial 88 finished with value: 0.6897862231090939 and parameters: {'n_estimators': 793}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:54:48,065] Trial 89 finished with value: 0.6898125944563516 and parameters: {'n_estimators': 974}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 20:57:59,639] Trial 90 finished with value: 0.6898666913703833 and parameters: {'n_estimators': 752}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 21:01:48,688] Trial 91 finished with value: 0.6896897047917415 and parameters: {'n_estimators': 909}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 21:05:47,009] Trial 92 finished with value: 0.6898271244817991 and parameters: {'n_estimators': 949}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 21:09:23,046] Trial 93 finished with value: 0.6897619188804136 and parameters: {'n_estimators': 860}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 21:13:04,310] Trial 94 finished with value: 0.6898048075434982 and parameters: {'n_estimators': 884}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 21:16:57,072] Trial 95 finished with value: 0.6897701135212819 and parameters: {'n_estimators': 933}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 21:21:04,651] Trial 96 finished with value: 0.689754224575642 and parameters: {'n_estimators': 983}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 21:24:53,309] Trial 97 finished with value: 0.6897859079362162 and parameters: {'n_estimators': 906}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 21:28:52,150] Trial 98 finished with value: 0.6898499073466464 and parameters: {'n_estimators': 958}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 21:32:21,424] Trial 99 finished with value: 0.6897247087175871 and parameters: {'n_estimators': 822}. Best is trial 17 with value: 0.6987995856199372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6988\n",
      "\tBest params:\n",
      "\t\tn_estimators: 929\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.681830    0.697684\n",
      "1                    TP   30.000000   30.000000\n",
      "2                    TN  311.000000  308.000000\n",
      "3                    FP    5.000000    7.000000\n",
      "4                    FN   36.000000   37.000000\n",
      "5              Accuracy    0.892670    0.884817\n",
      "6             Precision    0.857143    0.810811\n",
      "7           Sensitivity    0.454545    0.447761\n",
      "8           Specificity    0.984200    0.977800\n",
      "9              F1 score    0.594059    0.576923\n",
      "10  F1 score (weighted)    0.878708    0.870822\n",
      "11     F1 score (macro)    0.766110    0.755128\n",
      "12    Balanced Accuracy    0.719361    0.712769\n",
      "13                  MCC    0.574919    0.547169\n",
      "14                  NPV    0.896300    0.892800\n",
      "15              ROC_AUC    0.719361    0.712769\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_rf_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_1_cat = np.where(((y_pred_rf_1 >= 2) | (y_pred_rf_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_rf_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 21:36:44,007] Trial 100 finished with value: 0.6949497643187319 and parameters: {'n_estimators': 928}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 21:39:20,115] Trial 101 finished with value: 0.6954113486614883 and parameters: {'n_estimators': 618}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 21:42:18,977] Trial 102 finished with value: 0.695478078655125 and parameters: {'n_estimators': 703}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 21:46:02,231] Trial 103 finished with value: 0.6951276091562681 and parameters: {'n_estimators': 881}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 21:50:03,990] Trial 104 finished with value: 0.6949804705768583 and parameters: {'n_estimators': 959}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 21:53:48,562] Trial 105 finished with value: 0.6950333867940711 and parameters: {'n_estimators': 899}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 21:57:25,929] Trial 106 finished with value: 0.6953252034214648 and parameters: {'n_estimators': 850}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:01:31,318] Trial 107 finished with value: 0.6950088491117576 and parameters: {'n_estimators': 981}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:05:28,187] Trial 108 finished with value: 0.6949524800138347 and parameters: {'n_estimators': 941}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:09:18,984] Trial 109 finished with value: 0.6949714616969803 and parameters: {'n_estimators': 921}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:12:10,725] Trial 110 finished with value: 0.6952946176791397 and parameters: {'n_estimators': 678}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:16:09,391] Trial 111 finished with value: 0.694958187171858 and parameters: {'n_estimators': 957}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:20:22,855] Trial 112 finished with value: 0.6950265167793372 and parameters: {'n_estimators': 999}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:24:18,192] Trial 113 finished with value: 0.6949887510998813 and parameters: {'n_estimators': 939}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:28:21,486] Trial 114 finished with value: 0.6949251076856072 and parameters: {'n_estimators': 976}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:32:11,015] Trial 115 finished with value: 0.6949261870294328 and parameters: {'n_estimators': 908}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:35:52,277] Trial 116 finished with value: 0.6950858390572245 and parameters: {'n_estimators': 889}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:39:28,756] Trial 117 finished with value: 0.6953476085904643 and parameters: {'n_estimators': 862}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:42:53,152] Trial 118 finished with value: 0.6954219002380999 and parameters: {'n_estimators': 805}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:46:08,954] Trial 119 finished with value: 0.6952425415954998 and parameters: {'n_estimators': 772}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:50:01,682] Trial 120 finished with value: 0.6949816735699268 and parameters: {'n_estimators': 924}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:54:03,833] Trial 121 finished with value: 0.6949510729216007 and parameters: {'n_estimators': 960}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 22:58:06,769] Trial 122 finished with value: 0.6949632489356452 and parameters: {'n_estimators': 938}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:01:52,348] Trial 123 finished with value: 0.6950011845197677 and parameters: {'n_estimators': 900}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:05:25,682] Trial 124 finished with value: 0.6953089459266943 and parameters: {'n_estimators': 839}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:09:07,806] Trial 125 finished with value: 0.6951035308513427 and parameters: {'n_estimators': 873}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:13:15,403] Trial 126 finished with value: 0.6949534042543846 and parameters: {'n_estimators': 973}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:17:15,226] Trial 127 finished with value: 0.6949022656126064 and parameters: {'n_estimators': 920}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:21:15,846] Trial 128 finished with value: 0.694953011303227 and parameters: {'n_estimators': 950}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:23:31,637] Trial 129 finished with value: 0.6945813675511617 and parameters: {'n_estimators': 527}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:27:43,427] Trial 130 finished with value: 0.6949834661521183 and parameters: {'n_estimators': 984}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:32:02,008] Trial 131 finished with value: 0.6950265167793372 and parameters: {'n_estimators': 999}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:36:12,866] Trial 132 finished with value: 0.6949518231553032 and parameters: {'n_estimators': 964}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:40:09,115] Trial 133 finished with value: 0.6949857779727827 and parameters: {'n_estimators': 940}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:44:05,535] Trial 134 finished with value: 0.6949012033485568 and parameters: {'n_estimators': 911}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:48:26,242] Trial 135 finished with value: 0.6950350359544044 and parameters: {'n_estimators': 982}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:52:24,634] Trial 136 finished with value: 0.6950809296174759 and parameters: {'n_estimators': 891}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:55:05,452] Trial 137 finished with value: 0.6953110122788349 and parameters: {'n_estimators': 724}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-19 23:58:20,346] Trial 138 finished with value: 0.6949557902505126 and parameters: {'n_estimators': 929}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:01:38,649] Trial 139 finished with value: 0.6949305875925209 and parameters: {'n_estimators': 946}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:04:58,581] Trial 140 finished with value: 0.6949911971530743 and parameters: {'n_estimators': 965}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:08:01,907] Trial 141 finished with value: 0.6953476085904643 and parameters: {'n_estimators': 862}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:11:10,072] Trial 142 finished with value: 0.6949608510980771 and parameters: {'n_estimators': 913}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:14:14,869] Trial 143 finished with value: 0.6951389433550659 and parameters: {'n_estimators': 880}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:17:25,892] Trial 144 finished with value: 0.6950333867940711 and parameters: {'n_estimators': 899}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:20:41,081] Trial 145 finished with value: 0.6949461259823191 and parameters: {'n_estimators': 927}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:24:01,635] Trial 146 finished with value: 0.6949278829181105 and parameters: {'n_estimators': 947}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:27:29,190] Trial 147 finished with value: 0.6949738374297951 and parameters: {'n_estimators': 986}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:30:57,400] Trial 148 finished with value: 0.6949518231553032 and parameters: {'n_estimators': 964}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:34:07,703] Trial 149 finished with value: 0.6949608510980769 and parameters: {'n_estimators': 913}. Best is trial 17 with value: 0.6987995856199372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6988\n",
      "\tBest params:\n",
      "\t\tn_estimators: 929\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.681830    0.697684    0.699576\n",
      "1                    TP   30.000000   30.000000   32.000000\n",
      "2                    TN  311.000000  308.000000  309.000000\n",
      "3                    FP    5.000000    7.000000    5.000000\n",
      "4                    FN   36.000000   37.000000   36.000000\n",
      "5              Accuracy    0.892670    0.884817    0.892670\n",
      "6             Precision    0.857143    0.810811    0.864865\n",
      "7           Sensitivity    0.454545    0.447761    0.470588\n",
      "8           Specificity    0.984200    0.977800    0.984100\n",
      "9              F1 score    0.594059    0.576923    0.609524\n",
      "10  F1 score (weighted)    0.878708    0.870822    0.879351\n",
      "11     F1 score (macro)    0.766110    0.755128    0.773654\n",
      "12    Balanced Accuracy    0.719361    0.712769    0.727332\n",
      "13                  MCC    0.574919    0.547169    0.588031\n",
      "14                  NPV    0.896300    0.892800    0.895700\n",
      "15              ROC_AUC    0.719361    0.712769    0.727332\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_rf_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_2_cat = np.where(((y_pred_rf_2 >= 2) | (y_pred_rf_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_rf_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 00:37:38,187] Trial 150 finished with value: 0.6928719974590882 and parameters: {'n_estimators': 884}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:40:51,135] Trial 151 finished with value: 0.6929869958940617 and parameters: {'n_estimators': 932}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:44:15,391] Trial 152 finished with value: 0.6929932429310106 and parameters: {'n_estimators': 951}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:47:26,538] Trial 153 finished with value: 0.6928008300847452 and parameters: {'n_estimators': 902}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:50:27,479] Trial 154 finished with value: 0.6929305210278385 and parameters: {'n_estimators': 849}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:53:54,431] Trial 155 finished with value: 0.6927512511566312 and parameters: {'n_estimators': 1000}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:54:20,796] Trial 156 finished with value: 0.6910894136780281 and parameters: {'n_estimators': 117}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:55:20,287] Trial 157 finished with value: 0.693140442242384 and parameters: {'n_estimators': 284}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 00:58:30,916] Trial 158 finished with value: 0.6929222474616485 and parameters: {'n_estimators': 923}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:01:49,612] Trial 159 finished with value: 0.6929613684864109 and parameters: {'n_estimators': 965}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:03:19,845] Trial 160 finished with value: 0.6921704622573023 and parameters: {'n_estimators': 445}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:06:12,316] Trial 161 finished with value: 0.6929206411127978 and parameters: {'n_estimators': 863}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:08:56,119] Trial 162 finished with value: 0.6926794427921831 and parameters: {'n_estimators': 824}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:11:56,459] Trial 163 finished with value: 0.6928373155282387 and parameters: {'n_estimators': 895}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:14:55,954] Trial 164 finished with value: 0.692778563631682 and parameters: {'n_estimators': 876}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:18:09,426] Trial 165 finished with value: 0.6929697109711002 and parameters: {'n_estimators': 941}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:21:17,320] Trial 166 finished with value: 0.6928945124187553 and parameters: {'n_estimators': 913}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:24:08,493] Trial 167 finished with value: 0.6928263318253751 and parameters: {'n_estimators': 835}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:26:53,436] Trial 168 finished with value: 0.692792801045014 and parameters: {'n_estimators': 804}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:30:16,130] Trial 169 finished with value: 0.6928717269765865 and parameters: {'n_estimators': 980}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:33:26,234] Trial 170 finished with value: 0.6929927475863985 and parameters: {'n_estimators': 933}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:36:41,620] Trial 171 finished with value: 0.692977394109542 and parameters: {'n_estimators': 953}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:39:44,932] Trial 172 finished with value: 0.6927945248252548 and parameters: {'n_estimators': 898}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:42:55,709] Trial 173 finished with value: 0.6929161497883622 and parameters: {'n_estimators': 924}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:45:57,168] Trial 174 finished with value: 0.6927994044683412 and parameters: {'n_estimators': 871}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:49:05,467] Trial 175 finished with value: 0.69285477936728 and parameters: {'n_estimators': 910}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:52:24,645] Trial 176 finished with value: 0.6929923163076164 and parameters: {'n_estimators': 967}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:55:38,693] Trial 177 finished with value: 0.6929319172396139 and parameters: {'n_estimators': 945}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 01:58:40,434] Trial 178 finished with value: 0.6929288885658 and parameters: {'n_estimators': 883}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:01:34,070] Trial 179 finished with value: 0.6929302485865245 and parameters: {'n_estimators': 847}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:04:45,044] Trial 180 finished with value: 0.6929869958940617 and parameters: {'n_estimators': 932}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:08:07,109] Trial 181 finished with value: 0.6928695186366763 and parameters: {'n_estimators': 981}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:11:31,283] Trial 182 finished with value: 0.6927778600047556 and parameters: {'n_estimators': 998}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:14:48,601] Trial 183 finished with value: 0.6929558804625866 and parameters: {'n_estimators': 956}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:18:05,398] Trial 184 finished with value: 0.6929769430495228 and parameters: {'n_estimators': 972}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:21:11,200] Trial 185 finished with value: 0.6928936205987578 and parameters: {'n_estimators': 914}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:24:25,383] Trial 186 finished with value: 0.6929319172396139 and parameters: {'n_estimators': 945}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:27:28,814] Trial 187 finished with value: 0.6928215951573244 and parameters: {'n_estimators': 893}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:30:49,384] Trial 188 finished with value: 0.6927839101542075 and parameters: {'n_estimators': 986}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:34:09,272] Trial 189 finished with value: 0.6929924578122484 and parameters: {'n_estimators': 962}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:37:18,541] Trial 190 finished with value: 0.6929123814207545 and parameters: {'n_estimators': 927}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:40:28,452] Trial 191 finished with value: 0.6928764255965141 and parameters: {'n_estimators': 909}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:43:24,106] Trial 192 finished with value: 0.6928305117213119 and parameters: {'n_estimators': 870}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:46:18,004] Trial 193 finished with value: 0.6928904841451271 and parameters: {'n_estimators': 855}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:48:28,140] Trial 194 finished with value: 0.6932664221656406 and parameters: {'n_estimators': 627}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:51:37,942] Trial 195 finished with value: 0.6929927475863985 and parameters: {'n_estimators': 933}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:54:41,127] Trial 196 finished with value: 0.6928297469999938 and parameters: {'n_estimators': 892}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 02:57:20,198] Trial 197 finished with value: 0.6927667827678895 and parameters: {'n_estimators': 768}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:00:37,624] Trial 198 finished with value: 0.6930166330203353 and parameters: {'n_estimators': 950}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:03:28,947] Trial 199 finished with value: 0.6927347778570687 and parameters: {'n_estimators': 829}. Best is trial 17 with value: 0.6987995856199372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6988\n",
      "\tBest params:\n",
      "\t\tn_estimators: 929\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.681830    0.697684    0.699576    0.707352\n",
      "1                    TP   30.000000   30.000000   32.000000   34.000000\n",
      "2                    TN  311.000000  308.000000  309.000000  314.000000\n",
      "3                    FP    5.000000    7.000000    5.000000    2.000000\n",
      "4                    FN   36.000000   37.000000   36.000000   32.000000\n",
      "5              Accuracy    0.892670    0.884817    0.892670    0.910995\n",
      "6             Precision    0.857143    0.810811    0.864865    0.944444\n",
      "7           Sensitivity    0.454545    0.447761    0.470588    0.515152\n",
      "8           Specificity    0.984200    0.977800    0.984100    0.993700\n",
      "9              F1 score    0.594059    0.576923    0.609524    0.666667\n",
      "10  F1 score (weighted)    0.878708    0.870822    0.879351    0.899922\n",
      "11     F1 score (macro)    0.766110    0.755128    0.773654    0.807654\n",
      "12    Balanced Accuracy    0.719361    0.712769    0.727332    0.754411\n",
      "13                  MCC    0.574919    0.547169    0.588031    0.658404\n",
      "14                  NPV    0.896300    0.892800    0.895700    0.907500\n",
      "15              ROC_AUC    0.719361    0.712769    0.727332    0.754411\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_rf_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_3_cat = np.where(((y_pred_rf_3 >= 2) | (y_pred_rf_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_rf_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 03:07:09,981] Trial 200 finished with value: 0.6816899511963179 and parameters: {'n_estimators': 969}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:10:03,335] Trial 201 finished with value: 0.681650911135939 and parameters: {'n_estimators': 858}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:13:02,281] Trial 202 finished with value: 0.6817357481707506 and parameters: {'n_estimators': 881}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:15:42,510] Trial 203 finished with value: 0.681523637439169 and parameters: {'n_estimators': 793}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:18:48,765] Trial 204 finished with value: 0.6817689563076668 and parameters: {'n_estimators': 906}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:21:57,272] Trial 205 finished with value: 0.6817305030624154 and parameters: {'n_estimators': 919}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:25:06,133] Trial 206 finished with value: 0.6818979937528449 and parameters: {'n_estimators': 933}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:27:58,215] Trial 207 finished with value: 0.6816167635163386 and parameters: {'n_estimators': 846}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:30:59,995] Trial 208 finished with value: 0.6817180496456642 and parameters: {'n_estimators': 895}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:34:12,615] Trial 209 finished with value: 0.6817875633092765 and parameters: {'n_estimators': 943}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:36:55,155] Trial 210 finished with value: 0.6816746409851936 and parameters: {'n_estimators': 819}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:39:54,320] Trial 211 finished with value: 0.6817596898342155 and parameters: {'n_estimators': 878}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:42:50,783] Trial 212 finished with value: 0.6816500770905028 and parameters: {'n_estimators': 860}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:45:56,308] Trial 213 finished with value: 0.6817884489702886 and parameters: {'n_estimators': 911}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:49:08,068] Trial 214 finished with value: 0.6817647306943975 and parameters: {'n_estimators': 959}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:52:17,379] Trial 215 finished with value: 0.6818592485622519 and parameters: {'n_estimators': 923}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:55:16,078] Trial 216 finished with value: 0.6817297742973423 and parameters: {'n_estimators': 891}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 03:58:37,639] Trial 217 finished with value: 0.681979368572897 and parameters: {'n_estimators': 998}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:01:35,343] Trial 218 finished with value: 0.6817354327971179 and parameters: {'n_estimators': 872}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:04:55,573] Trial 219 finished with value: 0.681887577641519 and parameters: {'n_estimators': 976}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:08:08,379] Trial 220 finished with value: 0.6818244235218649 and parameters: {'n_estimators': 942}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:10:59,474] Trial 221 finished with value: 0.681663408715362 and parameters: {'n_estimators': 840}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:13:52,765] Trial 222 finished with value: 0.6816788209826768 and parameters: {'n_estimators': 856}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:16:54,287] Trial 223 finished with value: 0.6817664958044166 and parameters: {'n_estimators': 900}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:19:41,978] Trial 224 finished with value: 0.6816383815901336 and parameters: {'n_estimators': 836}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:20:57,253] Trial 225 finished with value: 0.6824610970885261 and parameters: {'n_estimators': 363}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:23:44,700] Trial 226 finished with value: 0.681636309142865 and parameters: {'n_estimators': 818}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:26:41,960] Trial 227 finished with value: 0.6817336525783612 and parameters: {'n_estimators': 871}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:29:50,106] Trial 228 finished with value: 0.6818592485622519 and parameters: {'n_estimators': 923}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:33:02,587] Trial 229 finished with value: 0.6818420752982647 and parameters: {'n_estimators': 954}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:36:10,382] Trial 230 finished with value: 0.6818112759051437 and parameters: {'n_estimators': 908}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:38:13,567] Trial 231 finished with value: 0.6813505183082451 and parameters: {'n_estimators': 600}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:39:08,338] Trial 232 finished with value: 0.6829306987844902 and parameters: {'n_estimators': 260}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:40:58,641] Trial 233 finished with value: 0.681657744760655 and parameters: {'n_estimators': 545}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:43:14,607] Trial 234 finished with value: 0.6813253619018984 and parameters: {'n_estimators': 653}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:46:24,755] Trial 235 finished with value: 0.6819006426479672 and parameters: {'n_estimators': 936}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:49:26,805] Trial 236 finished with value: 0.6817397229844043 and parameters: {'n_estimators': 882}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:52:45,987] Trial 237 finished with value: 0.6818763196764659 and parameters: {'n_estimators': 981}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:55:54,163] Trial 238 finished with value: 0.6818455412408786 and parameters: {'n_estimators': 922}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:56:21,830] Trial 239 finished with value: 0.6814793549314477 and parameters: {'n_estimators': 128}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:57:28,266] Trial 240 finished with value: 0.6823708694398248 and parameters: {'n_estimators': 320}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 04:58:58,676] Trial 241 finished with value: 0.6816153668536031 and parameters: {'n_estimators': 440}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:01:33,251] Trial 242 finished with value: 0.6819995842655534 and parameters: {'n_estimators': 753}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:04:13,443] Trial 243 finished with value: 0.6817131442282554 and parameters: {'n_estimators': 779}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:06:38,392] Trial 244 finished with value: 0.6818477012331605 and parameters: {'n_estimators': 716}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:09:55,467] Trial 245 finished with value: 0.681790795999827 and parameters: {'n_estimators': 952}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:12:57,886] Trial 246 finished with value: 0.6817281971013672 and parameters: {'n_estimators': 897}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:15:52,345] Trial 247 finished with value: 0.6816591826640094 and parameters: {'n_estimators': 857}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:19:03,376] Trial 248 finished with value: 0.6818751422312409 and parameters: {'n_estimators': 938}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:19:44,735] Trial 249 finished with value: 0.6826224130332716 and parameters: {'n_estimators': 202}. Best is trial 17 with value: 0.6987995856199372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6988\n",
      "\tBest params:\n",
      "\t\tn_estimators: 929\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77894dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681830    0.697684    0.699576    0.707352   \n",
      "1                    TP   30.000000   30.000000   32.000000   34.000000   \n",
      "2                    TN  311.000000  308.000000  309.000000  314.000000   \n",
      "3                    FP    5.000000    7.000000    5.000000    2.000000   \n",
      "4                    FN   36.000000   37.000000   36.000000   32.000000   \n",
      "5              Accuracy    0.892670    0.884817    0.892670    0.910995   \n",
      "6             Precision    0.857143    0.810811    0.864865    0.944444   \n",
      "7           Sensitivity    0.454545    0.447761    0.470588    0.515152   \n",
      "8           Specificity    0.984200    0.977800    0.984100    0.993700   \n",
      "9              F1 score    0.594059    0.576923    0.609524    0.666667   \n",
      "10  F1 score (weighted)    0.878708    0.870822    0.879351    0.899922   \n",
      "11     F1 score (macro)    0.766110    0.755128    0.773654    0.807654   \n",
      "12    Balanced Accuracy    0.719361    0.712769    0.727332    0.754411   \n",
      "13                  MCC    0.574919    0.547169    0.588031    0.658404   \n",
      "14                  NPV    0.896300    0.892800    0.895700    0.907500   \n",
      "15              ROC_AUC    0.719361    0.712769    0.727332    0.754411   \n",
      "\n",
      "          Set4  \n",
      "0     0.751351  \n",
      "1    35.000000  \n",
      "2   313.000000  \n",
      "3     3.000000  \n",
      "4    31.000000  \n",
      "5     0.910995  \n",
      "6     0.921053  \n",
      "7     0.530303  \n",
      "8     0.990500  \n",
      "9     0.673077  \n",
      "10    0.900901  \n",
      "11    0.810781  \n",
      "12    0.760405  \n",
      "13    0.657845  \n",
      "14    0.909900  \n",
      "15    0.760405  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_rf_4)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_4_cat = np.where(((y_pred_rf_4 >= 2) | (y_pred_rf_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_rf_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 05:23:16,352] Trial 250 finished with value: 0.6890926068206391 and parameters: {'n_estimators': 914}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:25:59,273] Trial 251 finished with value: 0.6892939357253219 and parameters: {'n_estimators': 800}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:29:12,102] Trial 252 finished with value: 0.6893857971975899 and parameters: {'n_estimators': 959}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:32:06,366] Trial 253 finished with value: 0.6892467114121257 and parameters: {'n_estimators': 833}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:35:07,304] Trial 254 finished with value: 0.6891081877637217 and parameters: {'n_estimators': 882}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:37:41,050] Trial 255 finished with value: 0.6895835804350723 and parameters: {'n_estimators': 743}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:40:49,067] Trial 256 finished with value: 0.6891720556004143 and parameters: {'n_estimators': 931}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:44:06,516] Trial 257 finished with value: 0.6894731970242426 and parameters: {'n_estimators': 971}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:47:59,009] Trial 258 finished with value: 0.6895738212013593 and parameters: {'n_estimators': 1000}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:51:51,018] Trial 259 finished with value: 0.6892365988116513 and parameters: {'n_estimators': 904}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:55:35,665] Trial 260 finished with value: 0.6890879652294726 and parameters: {'n_estimators': 866}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 05:57:59,195] Trial 261 finished with value: 0.6894676747574392 and parameters: {'n_estimators': 681}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:00:45,788] Trial 262 finished with value: 0.6892492844948679 and parameters: {'n_estimators': 946}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:03:09,423] Trial 263 finished with value: 0.6893971635747904 and parameters: {'n_estimators': 815}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:05:38,103] Trial 264 finished with value: 0.6891111710301249 and parameters: {'n_estimators': 842}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:08:14,178] Trial 265 finished with value: 0.6891584474557463 and parameters: {'n_estimators': 890}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:10:56,945] Trial 266 finished with value: 0.6891086520229054 and parameters: {'n_estimators': 923}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:13:47,689] Trial 267 finished with value: 0.6894478819975437 and parameters: {'n_estimators': 967}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:16:28,484] Trial 268 finished with value: 0.6890955162436153 and parameters: {'n_estimators': 911}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:19:20,421] Trial 269 finished with value: 0.6896368474936538 and parameters: {'n_estimators': 984}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:20:46,963] Trial 270 finished with value: 0.6887089969299733 and parameters: {'n_estimators': 492}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:23:30,329] Trial 271 finished with value: 0.6891956440692866 and parameters: {'n_estimators': 940}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:26:06,159] Trial 272 finished with value: 0.6891072225398032 and parameters: {'n_estimators': 874}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:28:35,705] Trial 273 finished with value: 0.6890573730506774 and parameters: {'n_estimators': 850}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:31:12,521] Trial 274 finished with value: 0.6891205270964269 and parameters: {'n_estimators': 896}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:33:59,092] Trial 275 finished with value: 0.6893710012052934 and parameters: {'n_estimators': 955}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:36:42,501] Trial 276 finished with value: 0.6891362315088675 and parameters: {'n_estimators': 929}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:39:23,408] Trial 277 finished with value: 0.689080887502902 and parameters: {'n_estimators': 915}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:42:14,655] Trial 278 finished with value: 0.6894981994854638 and parameters: {'n_estimators': 973}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:44:48,894] Trial 279 finished with value: 0.6890856201571885 and parameters: {'n_estimators': 884}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:47:35,902] Trial 280 finished with value: 0.6892201955951357 and parameters: {'n_estimators': 945}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:50:07,954] Trial 281 finished with value: 0.6891145052464602 and parameters: {'n_estimators': 859}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:52:46,962] Trial 282 finished with value: 0.6892193487475526 and parameters: {'n_estimators': 901}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:55:28,030] Trial 283 finished with value: 0.6891362315088675 and parameters: {'n_estimators': 929}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 06:57:52,543] Trial 284 finished with value: 0.6893383289828283 and parameters: {'n_estimators': 823}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:00:40,556] Trial 285 finished with value: 0.6894372979522557 and parameters: {'n_estimators': 961}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:03:33,562] Trial 286 finished with value: 0.6896780152279085 and parameters: {'n_estimators': 985}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:06:15,223] Trial 287 finished with value: 0.6891123964886433 and parameters: {'n_estimators': 918}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:08:36,643] Trial 288 finished with value: 0.6894591012424595 and parameters: {'n_estimators': 737}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:11:11,803] Trial 289 finished with value: 0.6893957789790268 and parameters: {'n_estimators': 788}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:13:54,887] Trial 290 finished with value: 0.6890783172816872 and parameters: {'n_estimators': 873}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:16:41,622] Trial 291 finished with value: 0.6892905982631387 and parameters: {'n_estimators': 948}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:19:09,905] Trial 292 finished with value: 0.6891111710301249 and parameters: {'n_estimators': 842}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:21:46,337] Trial 293 finished with value: 0.6891005881441266 and parameters: {'n_estimators': 897}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:24:36,221] Trial 294 finished with value: 0.6894847090304342 and parameters: {'n_estimators': 968}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:27:20,499] Trial 295 finished with value: 0.6891829755078257 and parameters: {'n_estimators': 933}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:28:30,171] Trial 296 finished with value: 0.6887541548619628 and parameters: {'n_estimators': 392}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:31:05,598] Trial 297 finished with value: 0.6891081877637217 and parameters: {'n_estimators': 882}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:33:44,429] Trial 298 finished with value: 0.6890955162436153 and parameters: {'n_estimators': 911}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:36:37,357] Trial 299 finished with value: 0.6896444963546056 and parameters: {'n_estimators': 988}. Best is trial 17 with value: 0.6987995856199372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6988\n",
      "\tBest params:\n",
      "\t\tn_estimators: 929\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681830    0.697684    0.699576    0.707352   \n",
      "1                    TP   30.000000   30.000000   32.000000   34.000000   \n",
      "2                    TN  311.000000  308.000000  309.000000  314.000000   \n",
      "3                    FP    5.000000    7.000000    5.000000    2.000000   \n",
      "4                    FN   36.000000   37.000000   36.000000   32.000000   \n",
      "5              Accuracy    0.892670    0.884817    0.892670    0.910995   \n",
      "6             Precision    0.857143    0.810811    0.864865    0.944444   \n",
      "7           Sensitivity    0.454545    0.447761    0.470588    0.515152   \n",
      "8           Specificity    0.984200    0.977800    0.984100    0.993700   \n",
      "9              F1 score    0.594059    0.576923    0.609524    0.666667   \n",
      "10  F1 score (weighted)    0.878708    0.870822    0.879351    0.899922   \n",
      "11     F1 score (macro)    0.766110    0.755128    0.773654    0.807654   \n",
      "12    Balanced Accuracy    0.719361    0.712769    0.727332    0.754411   \n",
      "13                  MCC    0.574919    0.547169    0.588031    0.658404   \n",
      "14                  NPV    0.896300    0.892800    0.895700    0.907500   \n",
      "15              ROC_AUC    0.719361    0.712769    0.727332    0.754411   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.751351    0.708267  \n",
      "1    35.000000   31.000000  \n",
      "2   313.000000  308.000000  \n",
      "3     3.000000    7.000000  \n",
      "4    31.000000   36.000000  \n",
      "5     0.910995    0.887435  \n",
      "6     0.921053    0.815789  \n",
      "7     0.530303    0.462687  \n",
      "8     0.990500    0.977800  \n",
      "9     0.673077    0.590476  \n",
      "10    0.900901    0.874367  \n",
      "11    0.810781    0.762613  \n",
      "12    0.760405    0.720232  \n",
      "13    0.657845    0.559671  \n",
      "14    0.909900    0.895300  \n",
      "15    0.760405    0.720232  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_rf_5)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_5_cat = np.where(((y_pred_rf_5 >= 2) | (y_pred_rf_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_rf_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:39:40,982] Trial 300 finished with value: 0.6871644791470614 and parameters: {'n_estimators': 948}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:42:12,156] Trial 301 finished with value: 0.687051332969808 and parameters: {'n_estimators': 862}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:44:53,902] Trial 302 finished with value: 0.6869498553565601 and parameters: {'n_estimators': 926}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:47:41,692] Trial 303 finished with value: 0.6870905168416002 and parameters: {'n_estimators': 962}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:50:19,542] Trial 304 finished with value: 0.6868856812501468 and parameters: {'n_estimators': 906}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:52:40,045] Trial 305 finished with value: 0.6874657705527213 and parameters: {'n_estimators': 806}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:55:15,647] Trial 306 finished with value: 0.6868482259817119 and parameters: {'n_estimators': 890}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 07:57:41,002] Trial 307 finished with value: 0.6873786984516708 and parameters: {'n_estimators': 830}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:00:24,839] Trial 308 finished with value: 0.6870513065521429 and parameters: {'n_estimators': 939}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:03:19,209] Trial 309 finished with value: 0.6871823526047619 and parameters: {'n_estimators': 1000}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:04:13,251] Trial 310 finished with value: 0.6855258705667984 and parameters: {'n_estimators': 305}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:06:53,707] Trial 311 finished with value: 0.6869617279128565 and parameters: {'n_estimators': 921}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:08:56,703] Trial 312 finished with value: 0.6868289613518171 and parameters: {'n_estimators': 700}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:09:24,373] Trial 313 finished with value: 0.6811663852193726 and parameters: {'n_estimators': 151}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:11:55,570] Trial 314 finished with value: 0.6869858849776451 and parameters: {'n_estimators': 866}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:14:24,009] Trial 315 finished with value: 0.6871631598778277 and parameters: {'n_estimators': 847}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:17:11,258] Trial 316 finished with value: 0.6871906837540609 and parameters: {'n_estimators': 968}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:19:56,660] Trial 317 finished with value: 0.6871644791470614 and parameters: {'n_estimators': 948}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:22:36,396] Trial 318 finished with value: 0.6869990832921091 and parameters: {'n_estimators': 902}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:23:16,411] Trial 319 finished with value: 0.6837394968369875 and parameters: {'n_estimators': 221}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:25:49,968] Trial 320 finished with value: 0.6868681371524877 and parameters: {'n_estimators': 881}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:28:42,588] Trial 321 finished with value: 0.6872139584497446 and parameters: {'n_estimators': 979}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:31:29,540] Trial 322 finished with value: 0.6869952158034175 and parameters: {'n_estimators': 930}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:34:01,321] Trial 323 finished with value: 0.6870418872575083 and parameters: {'n_estimators': 768}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:36:57,312] Trial 324 finished with value: 0.6869075743143783 and parameters: {'n_estimators': 912}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:39:42,650] Trial 325 finished with value: 0.6870772783630417 and parameters: {'n_estimators': 955}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:42:09,646] Trial 326 finished with value: 0.68702600896282 and parameters: {'n_estimators': 857}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:44:42,796] Trial 327 finished with value: 0.686868704232439 and parameters: {'n_estimators': 891}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:47:25,338] Trial 328 finished with value: 0.6869905821360346 and parameters: {'n_estimators': 938}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:50:04,465] Trial 329 finished with value: 0.6869617279128565 and parameters: {'n_estimators': 921}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:52:52,792] Trial 330 finished with value: 0.6872200043287003 and parameters: {'n_estimators': 975}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:55:21,072] Trial 331 finished with value: 0.6869835877410476 and parameters: {'n_estimators': 872}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 08:57:42,466] Trial 332 finished with value: 0.6874286770892976 and parameters: {'n_estimators': 828}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:00:26,047] Trial 333 finished with value: 0.6870911249492193 and parameters: {'n_estimators': 960}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:01:49,793] Trial 334 finished with value: 0.6863148176119312 and parameters: {'n_estimators': 489}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:03:30,096] Trial 335 finished with value: 0.6864572536848953 and parameters: {'n_estimators': 584}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:04:43,550] Trial 336 finished with value: 0.6855852716566146 and parameters: {'n_estimators': 428}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:07:17,753] Trial 337 finished with value: 0.686952444523597 and parameters: {'n_estimators': 904}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:09:57,736] Trial 338 finished with value: 0.6870513065521429 and parameters: {'n_estimators': 939}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:12:45,389] Trial 339 finished with value: 0.6872181808247246 and parameters: {'n_estimators': 989}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:15:15,726] Trial 340 finished with value: 0.6868173605918313 and parameters: {'n_estimators': 884}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:17:50,514] Trial 341 finished with value: 0.6869273178790559 and parameters: {'n_estimators': 916}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:20:14,145] Trial 342 finished with value: 0.6871429500534133 and parameters: {'n_estimators': 846}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:22:31,501] Trial 343 finished with value: 0.6874657705527213 and parameters: {'n_estimators': 806}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:25:10,932] Trial 344 finished with value: 0.6869970691165525 and parameters: {'n_estimators': 953}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:27:44,351] Trial 345 finished with value: 0.6869498553565601 and parameters: {'n_estimators': 926}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:30:12,273] Trial 346 finished with value: 0.6868482259817119 and parameters: {'n_estimators': 890}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:31:11,643] Trial 347 finished with value: 0.6863001254424242 and parameters: {'n_estimators': 359}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:33:33,195] Trial 348 finished with value: 0.6869858849776451 and parameters: {'n_estimators': 866}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:36:08,705] Trial 349 finished with value: 0.6870513065521429 and parameters: {'n_estimators': 939}. Best is trial 17 with value: 0.6987995856199372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6988\n",
      "\tBest params:\n",
      "\t\tn_estimators: 929\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681830    0.697684    0.699576    0.707352   \n",
      "1                    TP   30.000000   30.000000   32.000000   34.000000   \n",
      "2                    TN  311.000000  308.000000  309.000000  314.000000   \n",
      "3                    FP    5.000000    7.000000    5.000000    2.000000   \n",
      "4                    FN   36.000000   37.000000   36.000000   32.000000   \n",
      "5              Accuracy    0.892670    0.884817    0.892670    0.910995   \n",
      "6             Precision    0.857143    0.810811    0.864865    0.944444   \n",
      "7           Sensitivity    0.454545    0.447761    0.470588    0.515152   \n",
      "8           Specificity    0.984200    0.977800    0.984100    0.993700   \n",
      "9              F1 score    0.594059    0.576923    0.609524    0.666667   \n",
      "10  F1 score (weighted)    0.878708    0.870822    0.879351    0.899922   \n",
      "11     F1 score (macro)    0.766110    0.755128    0.773654    0.807654   \n",
      "12    Balanced Accuracy    0.719361    0.712769    0.727332    0.754411   \n",
      "13                  MCC    0.574919    0.547169    0.588031    0.658404   \n",
      "14                  NPV    0.896300    0.892800    0.895700    0.907500   \n",
      "15              ROC_AUC    0.719361    0.712769    0.727332    0.754411   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.751351    0.708267    0.734025  \n",
      "1    35.000000   31.000000   31.000000  \n",
      "2   313.000000  308.000000  309.000000  \n",
      "3     3.000000    7.000000    7.000000  \n",
      "4    31.000000   36.000000   35.000000  \n",
      "5     0.910995    0.887435    0.890052  \n",
      "6     0.921053    0.815789    0.815789  \n",
      "7     0.530303    0.462687    0.469697  \n",
      "8     0.990500    0.977800    0.977800  \n",
      "9     0.673077    0.590476    0.596154  \n",
      "10    0.900901    0.874367    0.877584  \n",
      "11    0.810781    0.762613    0.766259  \n",
      "12    0.760405    0.720232    0.723773  \n",
      "13    0.657845    0.559671    0.565303  \n",
      "14    0.909900    0.895300    0.898300  \n",
      "15    0.760405    0.720232    0.723773  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_rf_6)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_6_cat = np.where(((y_pred_rf_6 >= 2) | (y_pred_rf_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_rf_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 09:39:13,127] Trial 350 finished with value: 0.6716361795473209 and parameters: {'n_estimators': 969}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:41:50,362] Trial 351 finished with value: 0.6714647927590383 and parameters: {'n_estimators': 917}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:43:19,447] Trial 352 finished with value: 0.6717191137307325 and parameters: {'n_estimators': 519}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:45:54,728] Trial 353 finished with value: 0.6714534812443913 and parameters: {'n_estimators': 902}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:48:46,627] Trial 354 finished with value: 0.6716209576312855 and parameters: {'n_estimators': 999}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:51:29,874] Trial 355 finished with value: 0.6717556417364683 and parameters: {'n_estimators': 947}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:54:18,142] Trial 356 finished with value: 0.6716943401620278 and parameters: {'n_estimators': 977}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:56:45,717] Trial 357 finished with value: 0.6713227930321177 and parameters: {'n_estimators': 853}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 09:59:25,828] Trial 358 finished with value: 0.6716069673932468 and parameters: {'n_estimators': 929}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:01:55,037] Trial 359 finished with value: 0.6712540742084392 and parameters: {'n_estimators': 869}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:04:15,723] Trial 360 finished with value: 0.6713372469702712 and parameters: {'n_estimators': 818}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:07:00,119] Trial 361 finished with value: 0.6716386320730908 and parameters: {'n_estimators': 958}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:09:35,599] Trial 362 finished with value: 0.671479497344429 and parameters: {'n_estimators': 903}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:12:06,006] Trial 363 finished with value: 0.6712720030737125 and parameters: {'n_estimators': 886}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:14:28,573] Trial 364 finished with value: 0.6713717109046605 and parameters: {'n_estimators': 836}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:17:07,954] Trial 365 finished with value: 0.6716599123640876 and parameters: {'n_estimators': 936}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:19:43,521] Trial 366 finished with value: 0.671477213260671 and parameters: {'n_estimators': 915}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:22:30,776] Trial 367 finished with value: 0.6716978895976004 and parameters: {'n_estimators': 980}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:24:21,758] Trial 368 finished with value: 0.6717067007230504 and parameters: {'n_estimators': 648}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:26:26,404] Trial 369 finished with value: 0.6717157662526655 and parameters: {'n_estimators': 727}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:28:42,007] Trial 370 finished with value: 0.6713312701771738 and parameters: {'n_estimators': 791}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:31:25,354] Trial 371 finished with value: 0.6716397691885134 and parameters: {'n_estimators': 954}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:33:54,964] Trial 372 finished with value: 0.6712905483383306 and parameters: {'n_estimators': 879}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:36:28,528] Trial 373 finished with value: 0.6714505677367444 and parameters: {'n_estimators': 901}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:39:07,270] Trial 374 finished with value: 0.6717037728175933 and parameters: {'n_estimators': 932}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:41:32,190] Trial 375 finished with value: 0.6713092071391211 and parameters: {'n_estimators': 850}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:44:13,778] Trial 376 finished with value: 0.6716618349220662 and parameters: {'n_estimators': 949}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:46:58,154] Trial 377 finished with value: 0.6716361795473209 and parameters: {'n_estimators': 969}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:49:33,902] Trial 378 finished with value: 0.6714438992076652 and parameters: {'n_estimators': 916}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:52:02,061] Trial 379 finished with value: 0.6712392531311824 and parameters: {'n_estimators': 870}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:54:33,817] Trial 380 finished with value: 0.6714536745098558 and parameters: {'n_estimators': 894}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:56:45,215] Trial 381 finished with value: 0.6714218897647014 and parameters: {'n_estimators': 772}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 10:59:34,455] Trial 382 finished with value: 0.671601757640559 and parameters: {'n_estimators': 998}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:02:12,591] Trial 383 finished with value: 0.6716579390791159 and parameters: {'n_estimators': 930}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:04:56,249] Trial 384 finished with value: 0.6715918818714608 and parameters: {'n_estimators': 961}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:07:18,491] Trial 385 finished with value: 0.6714003172772308 and parameters: {'n_estimators': 831}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:09:54,727] Trial 386 finished with value: 0.6715023721017133 and parameters: {'n_estimators': 913}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:12:35,761] Trial 387 finished with value: 0.6717731856784989 and parameters: {'n_estimators': 944}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:14:30,176] Trial 388 finished with value: 0.6716427950665589 and parameters: {'n_estimators': 670}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:17:17,779] Trial 389 finished with value: 0.6717132878508324 and parameters: {'n_estimators': 982}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:19:49,106] Trial 390 finished with value: 0.671316576345337 and parameters: {'n_estimators': 887}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:22:12,712] Trial 391 finished with value: 0.6713036137468859 and parameters: {'n_estimators': 851}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:24:49,541] Trial 392 finished with value: 0.6714995527303825 and parameters: {'n_estimators': 921}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:25:32,730] Trial 393 finished with value: 0.671249311233425 and parameters: {'n_estimators': 252}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:28:01,786] Trial 394 finished with value: 0.6712392531311824 and parameters: {'n_estimators': 870}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:30:42,935] Trial 395 finished with value: 0.6717510695811529 and parameters: {'n_estimators': 943}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:33:15,142] Trial 396 finished with value: 0.6714462113701457 and parameters: {'n_estimators': 899}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:35:58,204] Trial 397 finished with value: 0.6715124923262402 and parameters: {'n_estimators': 966}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:38:35,143] Trial 398 finished with value: 0.6716579390791159 and parameters: {'n_estimators': 930}. Best is trial 17 with value: 0.6987995856199372.\n",
      "[I 2023-12-20 11:41:16,222] Trial 399 finished with value: 0.6715513613501927 and parameters: {'n_estimators': 953}. Best is trial 17 with value: 0.6987995856199372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6988\n",
      "\tBest params:\n",
      "\t\tn_estimators: 929\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681830    0.697684    0.699576    0.707352   \n",
      "1                    TP   30.000000   30.000000   32.000000   34.000000   \n",
      "2                    TN  311.000000  308.000000  309.000000  314.000000   \n",
      "3                    FP    5.000000    7.000000    5.000000    2.000000   \n",
      "4                    FN   36.000000   37.000000   36.000000   32.000000   \n",
      "5              Accuracy    0.892670    0.884817    0.892670    0.910995   \n",
      "6             Precision    0.857143    0.810811    0.864865    0.944444   \n",
      "7           Sensitivity    0.454545    0.447761    0.470588    0.515152   \n",
      "8           Specificity    0.984200    0.977800    0.984100    0.993700   \n",
      "9              F1 score    0.594059    0.576923    0.609524    0.666667   \n",
      "10  F1 score (weighted)    0.878708    0.870822    0.879351    0.899922   \n",
      "11     F1 score (macro)    0.766110    0.755128    0.773654    0.807654   \n",
      "12    Balanced Accuracy    0.719361    0.712769    0.727332    0.754411   \n",
      "13                  MCC    0.574919    0.547169    0.588031    0.658404   \n",
      "14                  NPV    0.896300    0.892800    0.895700    0.907500   \n",
      "15              ROC_AUC    0.719361    0.712769    0.727332    0.754411   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.751351    0.708267    0.734025    0.763680  \n",
      "1    35.000000   31.000000   31.000000   34.000000  \n",
      "2   313.000000  308.000000  309.000000  311.000000  \n",
      "3     3.000000    7.000000    7.000000    4.000000  \n",
      "4    31.000000   36.000000   35.000000   33.000000  \n",
      "5     0.910995    0.887435    0.890052    0.903141  \n",
      "6     0.921053    0.815789    0.815789    0.894737  \n",
      "7     0.530303    0.462687    0.469697    0.507463  \n",
      "8     0.990500    0.977800    0.977800    0.987300  \n",
      "9     0.673077    0.590476    0.596154    0.647619  \n",
      "10    0.900901    0.874367    0.877584    0.891897  \n",
      "11    0.810781    0.762613    0.766259    0.795737  \n",
      "12    0.760405    0.720232    0.723773    0.747382  \n",
      "13    0.657845    0.559671    0.565303    0.628666  \n",
      "14    0.909900    0.895300    0.898300    0.904100  \n",
      "15    0.760405    0.720232    0.723773    0.747382  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_rf_7)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_7_cat = np.where(((y_pred_rf_7 >= 2) | (y_pred_rf_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_rf_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 11:43:43,171] Trial 400 finished with value: 0.7089979316235635 and parameters: {'n_estimators': 809}. Best is trial 400 with value: 0.7089979316235635.\n",
      "[I 2023-12-20 11:45:55,028] Trial 401 finished with value: 0.7091159119380763 and parameters: {'n_estimators': 819}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 11:48:05,784] Trial 402 finished with value: 0.7090349401726448 and parameters: {'n_estimators': 812}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 11:50:15,363] Trial 403 finished with value: 0.7089814967221745 and parameters: {'n_estimators': 804}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 11:52:22,028] Trial 404 finished with value: 0.7090142465654039 and parameters: {'n_estimators': 794}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 11:54:31,840] Trial 405 finished with value: 0.7090596714423174 and parameters: {'n_estimators': 813}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 11:56:41,337] Trial 406 finished with value: 0.7090465212941327 and parameters: {'n_estimators': 815}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 11:58:46,064] Trial 407 finished with value: 0.7090948942165272 and parameters: {'n_estimators': 780}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:00:51,890] Trial 408 finished with value: 0.7090932131361647 and parameters: {'n_estimators': 788}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:03:02,575] Trial 409 finished with value: 0.7090932131361647 and parameters: {'n_estimators': 788}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:05:12,568] Trial 410 finished with value: 0.7090758668907807 and parameters: {'n_estimators': 782}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:07:17,754] Trial 411 finished with value: 0.7090886566267174 and parameters: {'n_estimators': 787}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:09:23,252] Trial 412 finished with value: 0.709078152969423 and parameters: {'n_estimators': 785}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:11:28,135] Trial 413 finished with value: 0.709078152969423 and parameters: {'n_estimators': 785}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:13:33,261] Trial 414 finished with value: 0.7090673365083717 and parameters: {'n_estimators': 781}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:15:34,757] Trial 415 finished with value: 0.7089265541172016 and parameters: {'n_estimators': 758}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:17:35,514] Trial 416 finished with value: 0.7089265541172018 and parameters: {'n_estimators': 758}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:19:37,772] Trial 417 finished with value: 0.7089168205643891 and parameters: {'n_estimators': 762}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:21:40,040] Trial 418 finished with value: 0.7089393913133868 and parameters: {'n_estimators': 757}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:23:44,056] Trial 419 finished with value: 0.7089265541172016 and parameters: {'n_estimators': 758}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:25:48,169] Trial 420 finished with value: 0.7089758817550303 and parameters: {'n_estimators': 755}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:27:48,813] Trial 421 finished with value: 0.7090429721529432 and parameters: {'n_estimators': 748}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:29:50,841] Trial 422 finished with value: 0.7089588467394625 and parameters: {'n_estimators': 756}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:31:52,203] Trial 423 finished with value: 0.7089393913133868 and parameters: {'n_estimators': 757}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:33:53,619] Trial 424 finished with value: 0.7089393913133868 and parameters: {'n_estimators': 757}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:35:54,678] Trial 425 finished with value: 0.7089393913133868 and parameters: {'n_estimators': 757}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:37:54,281] Trial 426 finished with value: 0.708969265659281 and parameters: {'n_estimators': 745}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:39:54,806] Trial 427 finished with value: 0.7089803340112757 and parameters: {'n_estimators': 746}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:42:00,636] Trial 428 finished with value: 0.7090758668907807 and parameters: {'n_estimators': 782}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:44:07,148] Trial 429 finished with value: 0.7090550529721094 and parameters: {'n_estimators': 783}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:46:13,390] Trial 430 finished with value: 0.7090948942165272 and parameters: {'n_estimators': 780}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:48:21,952] Trial 431 finished with value: 0.709031492277866 and parameters: {'n_estimators': 791}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:50:28,624] Trial 432 finished with value: 0.7090550529721095 and parameters: {'n_estimators': 783}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:52:36,129] Trial 433 finished with value: 0.7090886566267174 and parameters: {'n_estimators': 787}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:54:43,870] Trial 434 finished with value: 0.7090825595130389 and parameters: {'n_estimators': 786}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:56:51,224] Trial 435 finished with value: 0.7090547748346284 and parameters: {'n_estimators': 784}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 12:58:58,977] Trial 436 finished with value: 0.709078152969423 and parameters: {'n_estimators': 785}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:01:06,305] Trial 437 finished with value: 0.7090825595130389 and parameters: {'n_estimators': 786}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:03:12,877] Trial 438 finished with value: 0.7090758668907807 and parameters: {'n_estimators': 782}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:05:18,961] Trial 439 finished with value: 0.7090758668907807 and parameters: {'n_estimators': 782}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:07:26,458] Trial 440 finished with value: 0.7090932131361647 and parameters: {'n_estimators': 788}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:09:32,785] Trial 441 finished with value: 0.7090673365083717 and parameters: {'n_estimators': 781}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:11:39,043] Trial 442 finished with value: 0.7091023114466306 and parameters: {'n_estimators': 779}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:13:44,616] Trial 443 finished with value: 0.7090948942165272 and parameters: {'n_estimators': 780}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:15:50,729] Trial 444 finished with value: 0.7090948942165272 and parameters: {'n_estimators': 780}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:17:56,317] Trial 445 finished with value: 0.7091023114466306 and parameters: {'n_estimators': 779}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:20:01,470] Trial 446 finished with value: 0.7091033242697098 and parameters: {'n_estimators': 778}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:22:07,153] Trial 447 finished with value: 0.7090793544820622 and parameters: {'n_estimators': 777}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:24:12,926] Trial 448 finished with value: 0.7090793544820622 and parameters: {'n_estimators': 777}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:26:17,304] Trial 449 finished with value: 0.7089814188388325 and parameters: {'n_estimators': 771}. Best is trial 401 with value: 0.7091159119380763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7091\n",
      "\tBest params:\n",
      "\t\tn_estimators: 819\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681830    0.697684    0.699576    0.707352   \n",
      "1                    TP   30.000000   30.000000   32.000000   34.000000   \n",
      "2                    TN  311.000000  308.000000  309.000000  314.000000   \n",
      "3                    FP    5.000000    7.000000    5.000000    2.000000   \n",
      "4                    FN   36.000000   37.000000   36.000000   32.000000   \n",
      "5              Accuracy    0.892670    0.884817    0.892670    0.910995   \n",
      "6             Precision    0.857143    0.810811    0.864865    0.944444   \n",
      "7           Sensitivity    0.454545    0.447761    0.470588    0.515152   \n",
      "8           Specificity    0.984200    0.977800    0.984100    0.993700   \n",
      "9              F1 score    0.594059    0.576923    0.609524    0.666667   \n",
      "10  F1 score (weighted)    0.878708    0.870822    0.879351    0.899922   \n",
      "11     F1 score (macro)    0.766110    0.755128    0.773654    0.807654   \n",
      "12    Balanced Accuracy    0.719361    0.712769    0.727332    0.754411   \n",
      "13                  MCC    0.574919    0.547169    0.588031    0.658404   \n",
      "14                  NPV    0.896300    0.892800    0.895700    0.907500   \n",
      "15              ROC_AUC    0.719361    0.712769    0.727332    0.754411   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.751351    0.708267    0.734025    0.763680    0.608475  \n",
      "1    35.000000   31.000000   31.000000   34.000000   37.000000  \n",
      "2   313.000000  308.000000  309.000000  311.000000  306.000000  \n",
      "3     3.000000    7.000000    7.000000    4.000000    8.000000  \n",
      "4    31.000000   36.000000   35.000000   33.000000   31.000000  \n",
      "5     0.910995    0.887435    0.890052    0.903141    0.897906  \n",
      "6     0.921053    0.815789    0.815789    0.894737    0.822222  \n",
      "7     0.530303    0.462687    0.469697    0.507463    0.544118  \n",
      "8     0.990500    0.977800    0.977800    0.987300    0.974500  \n",
      "9     0.673077    0.590476    0.596154    0.647619    0.654867  \n",
      "10    0.900901    0.874367    0.877584    0.891897    0.889319  \n",
      "11    0.810781    0.762613    0.766259    0.795737    0.797480  \n",
      "12    0.760405    0.720232    0.723773    0.747382    0.759320  \n",
      "13    0.657845    0.559671    0.565303    0.628666    0.615409  \n",
      "14    0.909900    0.895300    0.898300    0.904100    0.908000  \n",
      "15    0.760405    0.720232    0.723773    0.747382    0.759320  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_rf_8)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_8_cat = np.where(((y_pred_rf_8 >= 2) | (y_pred_rf_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_rf_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 13:28:41,018] Trial 450 finished with value: 0.6926816431870766 and parameters: {'n_estimators': 776}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:30:52,838] Trial 451 finished with value: 0.6926264519953728 and parameters: {'n_estimators': 795}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:32:52,721] Trial 452 finished with value: 0.692525910162647 and parameters: {'n_estimators': 732}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:34:59,158] Trial 453 finished with value: 0.6926769420603065 and parameters: {'n_estimators': 780}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:37:09,853] Trial 454 finished with value: 0.6926190066148976 and parameters: {'n_estimators': 794}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:39:15,707] Trial 455 finished with value: 0.6926816431870766 and parameters: {'n_estimators': 776}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:41:25,090] Trial 456 finished with value: 0.6926190066148977 and parameters: {'n_estimators': 794}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:43:31,960] Trial 457 finished with value: 0.6928081047772494 and parameters: {'n_estimators': 771}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:45:40,828] Trial 458 finished with value: 0.6926314892850965 and parameters: {'n_estimators': 798}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:47:50,141] Trial 459 finished with value: 0.6927393346153141 and parameters: {'n_estimators': 775}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:49:50,872] Trial 460 finished with value: 0.6924624424513612 and parameters: {'n_estimators': 734}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:52:00,507] Trial 461 finished with value: 0.6926314892850965 and parameters: {'n_estimators': 798}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:54:06,826] Trial 462 finished with value: 0.6926732713433476 and parameters: {'n_estimators': 778}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:56:17,333] Trial 463 finished with value: 0.6926436987898512 and parameters: {'n_estimators': 799}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 13:58:13,642] Trial 464 finished with value: 0.692699120172059 and parameters: {'n_estimators': 711}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:00:27,211] Trial 465 finished with value: 0.6927393346153141 and parameters: {'n_estimators': 775}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:02:43,271] Trial 466 finished with value: 0.6926493465603464 and parameters: {'n_estimators': 788}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:04:59,864] Trial 467 finished with value: 0.6926040463865211 and parameters: {'n_estimators': 811}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:07:05,993] Trial 468 finished with value: 0.6927986183439562 and parameters: {'n_estimators': 772}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:09:07,087] Trial 469 finished with value: 0.692505320792445 and parameters: {'n_estimators': 741}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:11:17,670] Trial 470 finished with value: 0.6925864177699022 and parameters: {'n_estimators': 800}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:13:22,869] Trial 471 finished with value: 0.6927603971699904 and parameters: {'n_estimators': 770}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:15:30,744] Trial 472 finished with value: 0.6926404404807849 and parameters: {'n_estimators': 789}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:17:29,199] Trial 473 finished with value: 0.6925366376916581 and parameters: {'n_estimators': 731}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:19:40,330] Trial 474 finished with value: 0.6925743961229112 and parameters: {'n_estimators': 812}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:21:45,109] Trial 475 finished with value: 0.6927603971699904 and parameters: {'n_estimators': 770}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:23:52,554] Trial 476 finished with value: 0.6926404404807849 and parameters: {'n_estimators': 789}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:26:04,067] Trial 477 finished with value: 0.6925963910264015 and parameters: {'n_estimators': 814}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:28:08,747] Trial 478 finished with value: 0.6927210915732868 and parameters: {'n_estimators': 768}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:30:17,619] Trial 479 finished with value: 0.6926314892850965 and parameters: {'n_estimators': 798}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:32:18,657] Trial 480 finished with value: 0.6924332283027771 and parameters: {'n_estimators': 746}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:34:25,060] Trial 481 finished with value: 0.6927380047036014 and parameters: {'n_estimators': 781}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:36:37,204] Trial 482 finished with value: 0.6925096470130074 and parameters: {'n_estimators': 817}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:38:44,440] Trial 483 finished with value: 0.6926493465603464 and parameters: {'n_estimators': 788}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:40:41,751] Trial 484 finished with value: 0.6926042814210513 and parameters: {'n_estimators': 725}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:42:45,491] Trial 485 finished with value: 0.6927158033976437 and parameters: {'n_estimators': 764}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:44:55,964] Trial 486 finished with value: 0.6925982957202061 and parameters: {'n_estimators': 807}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:47:02,361] Trial 487 finished with value: 0.6927338213894266 and parameters: {'n_estimators': 782}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:49:02,245] Trial 488 finished with value: 0.6924921238950462 and parameters: {'n_estimators': 739}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:51:06,586] Trial 489 finished with value: 0.6926934927150308 and parameters: {'n_estimators': 767}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:53:15,242] Trial 490 finished with value: 0.6926730651349446 and parameters: {'n_estimators': 796}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:55:27,901] Trial 491 finished with value: 0.6925216267264485 and parameters: {'n_estimators': 823}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:57:36,569] Trial 492 finished with value: 0.6926186416342859 and parameters: {'n_estimators': 803}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 14:59:38,224] Trial 493 finished with value: 0.6925167299619506 and parameters: {'n_estimators': 756}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 15:01:42,371] Trial 494 finished with value: 0.6926769420603065 and parameters: {'n_estimators': 780}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 15:03:44,595] Trial 495 finished with value: 0.69277530901238 and parameters: {'n_estimators': 769}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 15:05:54,594] Trial 496 finished with value: 0.6925216267264485 and parameters: {'n_estimators': 823}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 15:07:52,540] Trial 497 finished with value: 0.6924332283027771 and parameters: {'n_estimators': 746}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 15:09:58,149] Trial 498 finished with value: 0.6926434486316916 and parameters: {'n_estimators': 792}. Best is trial 401 with value: 0.7091159119380763.\n",
      "[I 2023-12-20 15:12:05,941] Trial 499 finished with value: 0.6926104848701308 and parameters: {'n_estimators': 805}. Best is trial 401 with value: 0.7091159119380763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7091\n",
      "\tBest params:\n",
      "\t\tn_estimators: 819\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681830    0.697684    0.699576    0.707352   \n",
      "1                    TP   30.000000   30.000000   32.000000   34.000000   \n",
      "2                    TN  311.000000  308.000000  309.000000  314.000000   \n",
      "3                    FP    5.000000    7.000000    5.000000    2.000000   \n",
      "4                    FN   36.000000   37.000000   36.000000   32.000000   \n",
      "5              Accuracy    0.892670    0.884817    0.892670    0.910995   \n",
      "6             Precision    0.857143    0.810811    0.864865    0.944444   \n",
      "7           Sensitivity    0.454545    0.447761    0.470588    0.515152   \n",
      "8           Specificity    0.984200    0.977800    0.984100    0.993700   \n",
      "9              F1 score    0.594059    0.576923    0.609524    0.666667   \n",
      "10  F1 score (weighted)    0.878708    0.870822    0.879351    0.899922   \n",
      "11     F1 score (macro)    0.766110    0.755128    0.773654    0.807654   \n",
      "12    Balanced Accuracy    0.719361    0.712769    0.727332    0.754411   \n",
      "13                  MCC    0.574919    0.547169    0.588031    0.658404   \n",
      "14                  NPV    0.896300    0.892800    0.895700    0.907500   \n",
      "15              ROC_AUC    0.719361    0.712769    0.727332    0.754411   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.751351    0.708267    0.734025    0.763680    0.608475    0.715142  \n",
      "1    35.000000   31.000000   31.000000   34.000000   37.000000   31.000000  \n",
      "2   313.000000  308.000000  309.000000  311.000000  306.000000  310.000000  \n",
      "3     3.000000    7.000000    7.000000    4.000000    8.000000    3.000000  \n",
      "4    31.000000   36.000000   35.000000   33.000000   31.000000   38.000000  \n",
      "5     0.910995    0.887435    0.890052    0.903141    0.897906    0.892670  \n",
      "6     0.921053    0.815789    0.815789    0.894737    0.822222    0.911765  \n",
      "7     0.530303    0.462687    0.469697    0.507463    0.544118    0.449275  \n",
      "8     0.990500    0.977800    0.977800    0.987300    0.974500    0.990400  \n",
      "9     0.673077    0.590476    0.596154    0.647619    0.654867    0.601942  \n",
      "10    0.900901    0.874367    0.877584    0.891897    0.889319    0.877276  \n",
      "11    0.810781    0.762613    0.766259    0.795737    0.797480    0.769957  \n",
      "12    0.760405    0.720232    0.723773    0.747382    0.759320    0.719845  \n",
      "13    0.657845    0.559671    0.565303    0.628666    0.615409    0.594039  \n",
      "14    0.909900    0.895300    0.898300    0.904100    0.908000    0.890800  \n",
      "15    0.760405    0.720232    0.723773    0.747382    0.759320    0.719845  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_rf_9)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_9_cat = np.where(((y_pred_rf_9 >= 2) | (y_pred_rf_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_rf_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7091\n",
      "\tBest params:\n",
      "\t\tn_estimators: 819\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.713690</td>\n",
       "      <td>0.052563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.788867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>154.200000</td>\n",
       "      <td>1.751190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>1.712698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>3.405877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.896335</td>\n",
       "      <td>0.021909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.833371</td>\n",
       "      <td>0.084387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.510532</td>\n",
       "      <td>0.093030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.978450</td>\n",
       "      <td>0.010809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.629994</td>\n",
       "      <td>0.087130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.885373</td>\n",
       "      <td>0.026140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.784851</td>\n",
       "      <td>0.049752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.744486</td>\n",
       "      <td>0.048626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.599264</td>\n",
       "      <td>0.091933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.904100</td>\n",
       "      <td>0.018688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.744486</td>\n",
       "      <td>0.048626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.713690     0.052563\n",
       "1                    TP        17.000000     2.788867\n",
       "2                    TN       154.200000     1.751190\n",
       "3                    FP         3.400000     1.712698\n",
       "4                    FN        16.400000     3.405877\n",
       "5              Accuracy         0.896335     0.021909\n",
       "6             Precision         0.833371     0.084387\n",
       "7           Sensitivity         0.510532     0.093030\n",
       "8           Specificity         0.978450     0.010809\n",
       "9              F1 score         0.629994     0.087130\n",
       "10  F1 score (weighted)         0.885373     0.026140\n",
       "11     F1 score (macro)         0.784851     0.049752\n",
       "12    Balanced Accuracy         0.744486     0.048626\n",
       "13                  MCC         0.599264     0.091933\n",
       "14                  NPV         0.904100     0.018688\n",
       "15              ROC_AUC         0.744486     0.048626"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.681830</td>\n",
       "      <td>0.697684</td>\n",
       "      <td>0.699576</td>\n",
       "      <td>0.707352</td>\n",
       "      <td>0.751351</td>\n",
       "      <td>0.708267</td>\n",
       "      <td>0.734025</td>\n",
       "      <td>0.763680</td>\n",
       "      <td>0.608475</td>\n",
       "      <td>0.715142</td>\n",
       "      <td>0.706738</td>\n",
       "      <td>0.042765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>2.368778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>309.900000</td>\n",
       "      <td>2.424413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.078995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>2.549510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.884817</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.910995</td>\n",
       "      <td>0.910995</td>\n",
       "      <td>0.887435</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.896335</td>\n",
       "      <td>0.009251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.865862</td>\n",
       "      <td>0.049653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.469697</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.485159</td>\n",
       "      <td>0.035732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.984200</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.984100</td>\n",
       "      <td>0.993700</td>\n",
       "      <td>0.990500</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>0.974500</td>\n",
       "      <td>0.990400</td>\n",
       "      <td>0.983810</td>\n",
       "      <td>0.006612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.594059</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.673077</td>\n",
       "      <td>0.590476</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>0.654867</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.621131</td>\n",
       "      <td>0.035536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.878708</td>\n",
       "      <td>0.870822</td>\n",
       "      <td>0.879351</td>\n",
       "      <td>0.899922</td>\n",
       "      <td>0.900901</td>\n",
       "      <td>0.874367</td>\n",
       "      <td>0.877584</td>\n",
       "      <td>0.891897</td>\n",
       "      <td>0.889319</td>\n",
       "      <td>0.877276</td>\n",
       "      <td>0.884015</td>\n",
       "      <td>0.010710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.766110</td>\n",
       "      <td>0.755128</td>\n",
       "      <td>0.773654</td>\n",
       "      <td>0.807654</td>\n",
       "      <td>0.810781</td>\n",
       "      <td>0.762613</td>\n",
       "      <td>0.766259</td>\n",
       "      <td>0.795737</td>\n",
       "      <td>0.797480</td>\n",
       "      <td>0.769957</td>\n",
       "      <td>0.780537</td>\n",
       "      <td>0.020293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.719361</td>\n",
       "      <td>0.712769</td>\n",
       "      <td>0.727332</td>\n",
       "      <td>0.754411</td>\n",
       "      <td>0.760405</td>\n",
       "      <td>0.720232</td>\n",
       "      <td>0.723773</td>\n",
       "      <td>0.747382</td>\n",
       "      <td>0.759320</td>\n",
       "      <td>0.719845</td>\n",
       "      <td>0.734483</td>\n",
       "      <td>0.018666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.574919</td>\n",
       "      <td>0.547169</td>\n",
       "      <td>0.588031</td>\n",
       "      <td>0.658404</td>\n",
       "      <td>0.657845</td>\n",
       "      <td>0.559671</td>\n",
       "      <td>0.565303</td>\n",
       "      <td>0.628666</td>\n",
       "      <td>0.615409</td>\n",
       "      <td>0.594039</td>\n",
       "      <td>0.598946</td>\n",
       "      <td>0.039769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.896300</td>\n",
       "      <td>0.892800</td>\n",
       "      <td>0.895700</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>0.909900</td>\n",
       "      <td>0.895300</td>\n",
       "      <td>0.898300</td>\n",
       "      <td>0.904100</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.890800</td>\n",
       "      <td>0.899870</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.719361</td>\n",
       "      <td>0.712769</td>\n",
       "      <td>0.727332</td>\n",
       "      <td>0.754411</td>\n",
       "      <td>0.760405</td>\n",
       "      <td>0.720232</td>\n",
       "      <td>0.723773</td>\n",
       "      <td>0.747382</td>\n",
       "      <td>0.759320</td>\n",
       "      <td>0.719845</td>\n",
       "      <td>0.734483</td>\n",
       "      <td>0.018666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.681830    0.697684    0.699576    0.707352   \n",
       "1                    TP   30.000000   30.000000   32.000000   34.000000   \n",
       "2                    TN  311.000000  308.000000  309.000000  314.000000   \n",
       "3                    FP    5.000000    7.000000    5.000000    2.000000   \n",
       "4                    FN   36.000000   37.000000   36.000000   32.000000   \n",
       "5              Accuracy    0.892670    0.884817    0.892670    0.910995   \n",
       "6             Precision    0.857143    0.810811    0.864865    0.944444   \n",
       "7           Sensitivity    0.454545    0.447761    0.470588    0.515152   \n",
       "8           Specificity    0.984200    0.977800    0.984100    0.993700   \n",
       "9              F1 score    0.594059    0.576923    0.609524    0.666667   \n",
       "10  F1 score (weighted)    0.878708    0.870822    0.879351    0.899922   \n",
       "11     F1 score (macro)    0.766110    0.755128    0.773654    0.807654   \n",
       "12    Balanced Accuracy    0.719361    0.712769    0.727332    0.754411   \n",
       "13                  MCC    0.574919    0.547169    0.588031    0.658404   \n",
       "14                  NPV    0.896300    0.892800    0.895700    0.907500   \n",
       "15              ROC_AUC    0.719361    0.712769    0.727332    0.754411   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.751351    0.708267    0.734025    0.763680    0.608475    0.715142   \n",
       "1    35.000000   31.000000   31.000000   34.000000   37.000000   31.000000   \n",
       "2   313.000000  308.000000  309.000000  311.000000  306.000000  310.000000   \n",
       "3     3.000000    7.000000    7.000000    4.000000    8.000000    3.000000   \n",
       "4    31.000000   36.000000   35.000000   33.000000   31.000000   38.000000   \n",
       "5     0.910995    0.887435    0.890052    0.903141    0.897906    0.892670   \n",
       "6     0.921053    0.815789    0.815789    0.894737    0.822222    0.911765   \n",
       "7     0.530303    0.462687    0.469697    0.507463    0.544118    0.449275   \n",
       "8     0.990500    0.977800    0.977800    0.987300    0.974500    0.990400   \n",
       "9     0.673077    0.590476    0.596154    0.647619    0.654867    0.601942   \n",
       "10    0.900901    0.874367    0.877584    0.891897    0.889319    0.877276   \n",
       "11    0.810781    0.762613    0.766259    0.795737    0.797480    0.769957   \n",
       "12    0.760405    0.720232    0.723773    0.747382    0.759320    0.719845   \n",
       "13    0.657845    0.559671    0.565303    0.628666    0.615409    0.594039   \n",
       "14    0.909900    0.895300    0.898300    0.904100    0.908000    0.890800   \n",
       "15    0.760405    0.720232    0.723773    0.747382    0.759320    0.719845   \n",
       "\n",
       "           ave       std  \n",
       "0     0.706738  0.042765  \n",
       "1    32.500000  2.368778  \n",
       "2   309.900000  2.424413  \n",
       "3     5.100000  2.078995  \n",
       "4    34.500000  2.549510  \n",
       "5     0.896335  0.009251  \n",
       "6     0.865862  0.049653  \n",
       "7     0.485159  0.035732  \n",
       "8     0.983810  0.006612  \n",
       "9     0.621131  0.035536  \n",
       "10    0.884015  0.010710  \n",
       "11    0.780537  0.020293  \n",
       "12    0.734483  0.018666  \n",
       "13    0.598946  0.039769  \n",
       "14    0.899870  0.006900  \n",
       "15    0.734483  0.018666  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test['std'] = mat_met_rf_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.713742</td>\n",
       "      <td>0.050191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.898743</td>\n",
       "      <td>0.016063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.862050</td>\n",
       "      <td>0.073262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.505121</td>\n",
       "      <td>0.085207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.982100</td>\n",
       "      <td>0.011558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.632220</td>\n",
       "      <td>0.072289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.887234</td>\n",
       "      <td>0.019614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.786725</td>\n",
       "      <td>0.040307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.743611</td>\n",
       "      <td>0.041953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.608930</td>\n",
       "      <td>0.070889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.903814</td>\n",
       "      <td>0.014735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.743611</td>\n",
       "      <td>0.041953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.713742     0.050191\n",
       "1              Accuracy         0.898743     0.016063\n",
       "2             Precision         0.862050     0.073262\n",
       "3           Sensitivity         0.505121     0.085207\n",
       "4           Specificity         0.982100     0.011558\n",
       "5              F1 score         0.632220     0.072289\n",
       "6   F1 score (weighted)         0.887234     0.019614\n",
       "7      F1 score (macro)         0.786725     0.040307\n",
       "8     Balanced Accuracy         0.743611     0.041953\n",
       "9                   MCC         0.608930     0.070889\n",
       "10                  NPV         0.903814     0.014735\n",
       "11              ROC_AUC         0.743611     0.041953"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_rf_cat = np.where(((y_pred_optimized_rf >= 2) | (y_pred_optimized_rf <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_rf_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_rf))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "rf_run0 = data_rf[['y_test_idx0', 'y_test0', 'y_pred_rf0']]\n",
    "rf_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "rf_run0.reset_index(inplace=True, drop=True)\n",
    "rf_run1 = data_rf[['y_test_idx1', 'y_test1', 'y_pred_rf1']]\n",
    "rf_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "rf_run1.reset_index(inplace=True, drop=True)\n",
    "rf_run2 = data_rf[['y_test_idx2', 'y_test2', 'y_pred_rf2']]\n",
    "rf_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "rf_run2.reset_index(inplace=True, drop=True)\n",
    "rf_run3 = data_rf[['y_test_idx3', 'y_test3', 'y_pred_rf3']]\n",
    "rf_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "rf_run3.reset_index(inplace=True, drop=True)\n",
    "rf_run4 = data_rf[['y_test_idx4', 'y_test4', 'y_pred_rf4']]\n",
    "rf_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "rf_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "rf_5preds = pd.concat([chembl_id, rf_run0, rf_run1, rf_run2, rf_run3, rf_run4], axis=1)\n",
    "rf_5preds = rf_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_rf0', 'y_pred_rf1', 'y_pred_rf2', 'y_pred_rf3', 'y_pred_rf4']]\n",
    "rf_5preds['y_pred_rf_ave'] = rf_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "rf_5preds['y_pred_rf_std'] = rf_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "\n",
    "mat_met_optimized_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfc78124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (RF)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model r2_score 0.7093 with a standard deviation of 0.0544\n",
      "RF optimized model r2_score 0.7114 with a standard deviation of 0.0584\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"RF baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_rf_withSemiSel.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf_reg, \"OUTPUT/rf_reg_withSemiSel.joblib\")\n",
    "joblib.dump(optimizedCV_rf, \"OUTPUT/optimizedCV_rf_withSemiSel.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.713420     0.046195\n",
      "1                    TP        17.200000     3.084009\n",
      "2                    TN       153.200000     2.740641\n",
      "3                    FP         4.400000     2.547330\n",
      "4                    FN        16.200000     3.614784\n",
      "5              Accuracy         0.892147     0.026946\n",
      "6             Precision         0.800056     0.112857\n",
      "7           Sensitivity         0.516414     0.101359\n",
      "8           Specificity         0.972080     0.016210\n",
      "9              F1 score         0.623993     0.096083\n",
      "10  F1 score (weighted)         0.882115     0.030164\n",
      "11     F1 score (macro)         0.780505     0.055718\n",
      "12    Balanced Accuracy         0.744247     0.054514\n",
      "13                  MCC         0.585270     0.108620\n",
      "14                  NPV         0.904580     0.020428\n",
      "15              ROC_AUC         0.744247     0.054514\n",
      "CPU times: user 7.72 s, sys: 64 ms, total: 7.79 s\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_reg = lgbm.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=4,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_reg.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"rmse\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_reg.predict(X_test) \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=4,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=4,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "         # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 15:32:00,379] A new study created in memory with name: lgbmRegressor\n",
      "[I 2023-12-20 15:32:01,469] Trial 0 finished with value: 0.6732496870992574 and parameters: {'n_estimators': 106, 'learning_rate': 0.10712314672637993, 'max_depth': 8, 'max_bin': 156, 'num_leaves': 736}. Best is trial 0 with value: 0.6732496870992574.\n",
      "[I 2023-12-20 15:32:02,074] Trial 1 finished with value: 0.6527622485348258 and parameters: {'n_estimators': 85, 'learning_rate': 0.19151573106555828, 'max_depth': 4, 'max_bin': 243, 'num_leaves': 651}. Best is trial 0 with value: 0.6732496870992574.\n",
      "[I 2023-12-20 15:32:03,970] Trial 2 finished with value: 0.6532167099751477 and parameters: {'n_estimators': 869, 'learning_rate': 0.06083386933991966, 'max_depth': 3, 'max_bin': 182, 'num_leaves': 484}. Best is trial 0 with value: 0.6732496870992574.\n",
      "[I 2023-12-20 15:32:05,755] Trial 3 finished with value: 0.6844494358416335 and parameters: {'n_estimators': 336, 'learning_rate': 0.18681192217273185, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 424}. Best is trial 3 with value: 0.6844494358416335.\n",
      "[I 2023-12-20 15:32:09,106] Trial 4 finished with value: 0.6952602165556159 and parameters: {'n_estimators': 658, 'learning_rate': 0.050732506817833374, 'max_depth': 11, 'max_bin': 222, 'num_leaves': 65}. Best is trial 4 with value: 0.6952602165556159.\n",
      "[I 2023-12-20 15:32:10,785] Trial 5 finished with value: 0.6803242664914714 and parameters: {'n_estimators': 139, 'learning_rate': 0.05178020505523304, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 76}. Best is trial 4 with value: 0.6952602165556159.\n",
      "[I 2023-12-20 15:32:14,657] Trial 6 finished with value: 0.6698103228265726 and parameters: {'n_estimators': 486, 'learning_rate': 0.01645934277281913, 'max_depth': 8, 'max_bin': 292, 'num_leaves': 421}. Best is trial 4 with value: 0.6952602165556159.\n",
      "[I 2023-12-20 15:32:16,130] Trial 7 finished with value: 0.683602056773171 and parameters: {'n_estimators': 863, 'learning_rate': 0.19573493447418253, 'max_depth': 7, 'max_bin': 245, 'num_leaves': 627}. Best is trial 4 with value: 0.6952602165556159.\n",
      "[I 2023-12-20 15:32:18,223] Trial 8 finished with value: 0.6913239785372929 and parameters: {'n_estimators': 849, 'learning_rate': 0.1547057812199907, 'max_depth': 12, 'max_bin': 190, 'num_leaves': 617}. Best is trial 4 with value: 0.6952602165556159.\n",
      "[I 2023-12-20 15:32:21,902] Trial 9 finished with value: 0.7021717702064129 and parameters: {'n_estimators': 707, 'learning_rate': 0.0490155009180803, 'max_depth': 9, 'max_bin': 293, 'num_leaves': 51}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:32:26,879] Trial 10 finished with value: 0.19264817942022602 and parameters: {'n_estimators': 631, 'learning_rate': 0.00044579152689479956, 'max_depth': 6, 'max_bin': 298, 'num_leaves': 184}. Best is trial 9 with value: 0.7021717702064129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 15:32:29,876] Trial 11 finished with value: 0.694704784468181 and parameters: {'n_estimators': 657, 'learning_rate': 0.061710022160850336, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 31}. Best is trial 9 with value: 0.7021717702064129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 15:32:32,499] Trial 12 finished with value: 0.6962293837015472 and parameters: {'n_estimators': 708, 'learning_rate': 0.09032557491141055, 'max_depth': 10, 'max_bin': 221, 'num_leaves': 230}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:32:34,466] Trial 13 finished with value: 0.6899219850642416 and parameters: {'n_estimators': 530, 'learning_rate': 0.09287568031961188, 'max_depth': 10, 'max_bin': 275, 'num_leaves': 233}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:32:36,586] Trial 14 finished with value: 0.690647995142095 and parameters: {'n_estimators': 727, 'learning_rate': 0.10302694909617019, 'max_depth': 9, 'max_bin': 236, 'num_leaves': 259}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:32:38,319] Trial 15 finished with value: 0.6897724790887887 and parameters: {'n_estimators': 379, 'learning_rate': 0.12703045641350766, 'max_depth': 6, 'max_bin': 206, 'num_leaves': 154}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:32:41,380] Trial 16 finished with value: 0.6950181061964615 and parameters: {'n_estimators': 759, 'learning_rate': 0.079632638323917, 'max_depth': 10, 'max_bin': 152, 'num_leaves': 299}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:32:45,683] Trial 17 finished with value: 0.6831814072010408 and parameters: {'n_estimators': 557, 'learning_rate': 0.0208443036226641, 'max_depth': 9, 'max_bin': 222, 'num_leaves': 338}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:32:47,654] Trial 18 finished with value: 0.6928671483343466 and parameters: {'n_estimators': 762, 'learning_rate': 0.12878341317783273, 'max_depth': 7, 'max_bin': 275, 'num_leaves': 142}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:32:49,839] Trial 19 finished with value: 0.6883561994521734 and parameters: {'n_estimators': 269, 'learning_rate': 0.08012771823916648, 'max_depth': 9, 'max_bin': 177, 'num_leaves': 121}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:32:53,501] Trial 20 finished with value: 0.691904698561622 and parameters: {'n_estimators': 386, 'learning_rate': 0.03252737347356881, 'max_depth': 11, 'max_bin': 207, 'num_leaves': 213}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:32:56,594] Trial 21 finished with value: 0.6920731538524012 and parameters: {'n_estimators': 636, 'learning_rate': 0.05113809112111865, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 37}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:32:59,998] Trial 22 finished with value: 0.6976021514697748 and parameters: {'n_estimators': 703, 'learning_rate': 0.04498194324522869, 'max_depth': 11, 'max_bin': 233, 'num_leaves': 91}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:03,773] Trial 23 finished with value: 0.6943370660419899 and parameters: {'n_estimators': 787, 'learning_rate': 0.034896784780793735, 'max_depth': 10, 'max_bin': 256, 'num_leaves': 144}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:06,639] Trial 24 finished with value: 0.6957670502192439 and parameters: {'n_estimators': 573, 'learning_rate': 0.07675831371832688, 'max_depth': 9, 'max_bin': 284, 'num_leaves': 105}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:09,610] Trial 25 finished with value: 0.6923975516148748 and parameters: {'n_estimators': 733, 'learning_rate': 0.06770523094523646, 'max_depth': 11, 'max_bin': 195, 'num_leaves': 299}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:12,879] Trial 26 finished with value: 0.6903217019015601 and parameters: {'n_estimators': 704, 'learning_rate': 0.03691920677710504, 'max_depth': 8, 'max_bin': 235, 'num_leaves': 211}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:15,720] Trial 27 finished with value: 0.6929604708704422 and parameters: {'n_estimators': 824, 'learning_rate': 0.0890165922646074, 'max_depth': 10, 'max_bin': 252, 'num_leaves': 99}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:18,889] Trial 28 finished with value: 0.691912890734464 and parameters: {'n_estimators': 454, 'learning_rate': 0.048653748042168675, 'max_depth': 11, 'max_bin': 272, 'num_leaves': 365}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:20,920] Trial 29 finished with value: 0.6909285658735371 and parameters: {'n_estimators': 608, 'learning_rate': 0.11041449203263158, 'max_depth': 8, 'max_bin': 162, 'num_leaves': 274}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:23,618] Trial 30 finished with value: 0.6980526613452794 and parameters: {'n_estimators': 900, 'learning_rate': 0.07123340922580866, 'max_depth': 9, 'max_bin': 213, 'num_leaves': 179}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:26,217] Trial 31 finished with value: 0.6945707993274756 and parameters: {'n_estimators': 804, 'learning_rate': 0.06596712043153896, 'max_depth': 9, 'max_bin': 213, 'num_leaves': 181}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:28,625] Trial 32 finished with value: 0.6905518742102636 and parameters: {'n_estimators': 805, 'learning_rate': 0.07713727775886761, 'max_depth': 8, 'max_bin': 234, 'num_leaves': 69}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:31,177] Trial 33 finished with value: 0.6919879692407125 and parameters: {'n_estimators': 688, 'learning_rate': 0.06352065209988074, 'max_depth': 10, 'max_bin': 191, 'num_leaves': 192}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:34,465] Trial 34 finished with value: 0.6896964821884239 and parameters: {'n_estimators': 481, 'learning_rate': 0.04462986406124292, 'max_depth': 9, 'max_bin': 171, 'num_leaves': 539}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:36,285] Trial 35 finished with value: 0.678773535962691 and parameters: {'n_estimators': 856, 'learning_rate': 0.08972034201160525, 'max_depth': 5, 'max_bin': 198, 'num_leaves': 103}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:38,247] Trial 36 finished with value: 0.6568639451028132 and parameters: {'n_estimators': 881, 'learning_rate': 0.061120836426020314, 'max_depth': 3, 'max_bin': 217, 'num_leaves': 739}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:41,864] Trial 37 finished with value: 0.699663695234601 and parameters: {'n_estimators': 597, 'learning_rate': 0.07058096739033233, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 30}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:45,029] Trial 38 finished with value: 0.6919151492153254 and parameters: {'n_estimators': 602, 'learning_rate': 0.05114586412575959, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 42}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:49,763] Trial 39 finished with value: 0.7006411456851429 and parameters: {'n_estimators': 528, 'learning_rate': 0.025403638496121966, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 66}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:54,736] Trial 40 finished with value: 0.6924418981920668 and parameters: {'n_estimators': 511, 'learning_rate': 0.021150778767551455, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 64}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:33:58,440] Trial 41 finished with value: 0.6954845961293519 and parameters: {'n_estimators': 899, 'learning_rate': 0.039333711826182574, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 85}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:34:00,990] Trial 42 finished with value: 0.6934290441476307 and parameters: {'n_estimators': 258, 'learning_rate': 0.05572886021593642, 'max_depth': 11, 'max_bin': 231, 'num_leaves': 143}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:34:06,145] Trial 43 finished with value: 0.6946658353138675 and parameters: {'n_estimators': 672, 'learning_rate': 0.023607397179240514, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 65}. Best is trial 9 with value: 0.7021717702064129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 15:34:10,035] Trial 44 finished with value: 0.6973043711359879 and parameters: {'n_estimators': 439, 'learning_rate': 0.02947161788436204, 'max_depth': 11, 'max_bin': 284, 'num_leaves': 31}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:34:16,005] Trial 45 finished with value: 0.6872421862217529 and parameters: {'n_estimators': 561, 'learning_rate': 0.014054193416701532, 'max_depth': 12, 'max_bin': 298, 'num_leaves': 120}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:34:19,719] Trial 46 finished with value: 0.6999181303849689 and parameters: {'n_estimators': 590, 'learning_rate': 0.04565188305960332, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 179}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:34:24,318] Trial 47 finished with value: 0.6498048353062066 and parameters: {'n_estimators': 597, 'learning_rate': 0.009097858484573652, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 168}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:34:29,204] Trial 48 finished with value: 0.6989978441092154 and parameters: {'n_estimators': 529, 'learning_rate': 0.028660034105319438, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 499}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:34:33,848] Trial 49 finished with value: 0.6991049611350755 and parameters: {'n_estimators': 525, 'learning_rate': 0.02948213015026386, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 469}. Best is trial 9 with value: 0.7021717702064129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.7022\n",
      "\tBest params:\n",
      "\t\tn_estimators: 707\n",
      "\t\tlearning_rate: 0.0490155009180803\n",
      "\t\tmax_depth: 9\n",
      "\t\tmax_bin: 293\n",
      "\t\tnum_leaves: 51\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.683653\n",
      "1                    TP   28.000000\n",
      "2                    TN  308.000000\n",
      "3                    FP    8.000000\n",
      "4                    FN   38.000000\n",
      "5              Accuracy    0.879581\n",
      "6             Precision    0.777778\n",
      "7           Sensitivity    0.424242\n",
      "8           Specificity    0.974700\n",
      "9              F1 score    0.549020\n",
      "10  F1 score (weighted)    0.864601\n",
      "11     F1 score (macro)    0.739767\n",
      "12    Balanced Accuracy    0.699463\n",
      "13                  MCC    0.516201\n",
      "14                  NPV    0.890200\n",
      "15              ROC_AUC    0.699463\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_lgbm_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_0_cat = np.where(((y_pred_lgbm_0 >= 2) | (y_pred_lgbm_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 15:34:37,696] Trial 50 finished with value: 0.685604749592542 and parameters: {'n_estimators': 646, 'learning_rate': 0.042679720171204284, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 421}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:34:42,093] Trial 51 finished with value: 0.6864215500646342 and parameters: {'n_estimators': 533, 'learning_rate': 0.027553488964324274, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 484}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:34:46,457] Trial 52 finished with value: 0.6865342621741682 and parameters: {'n_estimators': 515, 'learning_rate': 0.030264630881907072, 'max_depth': 11, 'max_bin': 259, 'num_leaves': 473}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:34:49,784] Trial 53 finished with value: 0.6899727624548124 and parameters: {'n_estimators': 413, 'learning_rate': 0.05616526628055647, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 509}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:34:55,653] Trial 54 finished with value: 0.6213411749685113 and parameters: {'n_estimators': 475, 'learning_rate': 0.0051292999649836316, 'max_depth': 11, 'max_bin': 239, 'num_leaves': 557}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:01,411] Trial 55 finished with value: 0.6868653849910243 and parameters: {'n_estimators': 552, 'learning_rate': 0.0170637423719759, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 448}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:04,913] Trial 56 finished with value: 0.6862886744755434 and parameters: {'n_estimators': 340, 'learning_rate': 0.03451938276343901, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 587}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:08,414] Trial 57 finished with value: 0.6853770824858921 and parameters: {'n_estimators': 579, 'learning_rate': 0.039137680917430286, 'max_depth': 11, 'max_bin': 262, 'num_leaves': 382}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:12,499] Trial 58 finished with value: 0.6806852595717399 and parameters: {'n_estimators': 627, 'learning_rate': 0.025545846176119134, 'max_depth': 10, 'max_bin': 290, 'num_leaves': 451}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:19,116] Trial 59 finished with value: 0.16296117402425736 and parameters: {'n_estimators': 501, 'learning_rate': 0.0003824604769187452, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 532}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:23,677] Trial 60 finished with value: 0.6705042378270932 and parameters: {'n_estimators': 438, 'learning_rate': 0.014230948560508834, 'max_depth': 10, 'max_bin': 280, 'num_leaves': 584}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:27,343] Trial 61 finished with value: 0.686572675859646 and parameters: {'n_estimators': 539, 'learning_rate': 0.046063738635822415, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 711}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:30,671] Trial 62 finished with value: 0.6843224883479089 and parameters: {'n_estimators': 584, 'learning_rate': 0.056359517910989684, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 331}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:32,859] Trial 63 finished with value: 0.6737767008470394 and parameters: {'n_estimators': 739, 'learning_rate': 0.06849389386508625, 'max_depth': 7, 'max_bin': 228, 'num_leaves': 63}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:36,559] Trial 64 finished with value: 0.6873910427811543 and parameters: {'n_estimators': 621, 'learning_rate': 0.038611403207934034, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 244}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:39,067] Trial 65 finished with value: 0.6725913949830453 and parameters: {'n_estimators': 656, 'learning_rate': 0.04747241495726792, 'max_depth': 6, 'max_bin': 203, 'num_leaves': 400}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:42,755] Trial 66 finished with value: 0.6868798540304095 and parameters: {'n_estimators': 467, 'learning_rate': 0.032358851395097005, 'max_depth': 10, 'max_bin': 279, 'num_leaves': 118}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:45,145] Trial 67 finished with value: 0.677972238818106 and parameters: {'n_estimators': 395, 'learning_rate': 0.07291855640085416, 'max_depth': 9, 'max_bin': 254, 'num_leaves': 50}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:48,274] Trial 68 finished with value: 0.6876264251466936 and parameters: {'n_estimators': 499, 'learning_rate': 0.05930081465877701, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 510}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:49,456] Trial 69 finished with value: 0.6552033857694342 and parameters: {'n_estimators': 83, 'learning_rate': 0.05166403659524969, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 164}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:50,829] Trial 70 finished with value: 0.6224445548982842 and parameters: {'n_estimators': 128, 'learning_rate': 0.024648701789695616, 'max_depth': 8, 'max_bin': 300, 'num_leaves': 85}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:53,869] Trial 71 finished with value: 0.6825389832170166 and parameters: {'n_estimators': 771, 'learning_rate': 0.044346861479039996, 'max_depth': 11, 'max_bin': 227, 'num_leaves': 90}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:35:57,855] Trial 72 finished with value: 0.6864697756539583 and parameters: {'n_estimators': 689, 'learning_rate': 0.04095098041038408, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 199}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:01,341] Trial 73 finished with value: 0.6826967768772634 and parameters: {'n_estimators': 726, 'learning_rate': 0.03574459052105397, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 118}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:03,290] Trial 74 finished with value: 0.6585341627439347 and parameters: {'n_estimators': 818, 'learning_rate': 0.06300149927795838, 'max_depth': 4, 'max_bin': 219, 'num_leaves': 132}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:06,396] Trial 75 finished with value: 0.6850952298762817 and parameters: {'n_estimators': 529, 'learning_rate': 0.05463265607825987, 'max_depth': 11, 'max_bin': 260, 'num_leaves': 41}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:10,808] Trial 76 finished with value: 0.6787860999990216 and parameters: {'n_estimators': 563, 'learning_rate': 0.019786715455720822, 'max_depth': 9, 'max_bin': 234, 'num_leaves': 59}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:13,837] Trial 77 finished with value: 0.6853388767802262 and parameters: {'n_estimators': 676, 'learning_rate': 0.04544708568431938, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 30}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:18,027] Trial 78 finished with value: 0.6846044619570009 and parameters: {'n_estimators': 609, 'learning_rate': 0.027902153876560535, 'max_depth': 11, 'max_bin': 245, 'num_leaves': 100}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:21,089] Trial 79 finished with value: 0.688972771049725 and parameters: {'n_estimators': 837, 'learning_rate': 0.05077483340700636, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 270}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:24,882] Trial 80 finished with value: 0.6880999244350157 and parameters: {'n_estimators': 706, 'learning_rate': 0.03290046145037823, 'max_depth': 10, 'max_bin': 253, 'num_leaves': 84}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:28,879] Trial 81 finished with value: 0.6846184653987214 and parameters: {'n_estimators': 448, 'learning_rate': 0.02907992144866635, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 30}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:32,883] Trial 82 finished with value: 0.6788010075955324 and parameters: {'n_estimators': 436, 'learning_rate': 0.021266952170338572, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 73}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:36,470] Trial 83 finished with value: 0.6883064512650283 and parameters: {'n_estimators': 416, 'learning_rate': 0.041897057524395084, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 47}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:40,179] Trial 84 finished with value: 0.6878973659282479 and parameters: {'n_estimators': 490, 'learning_rate': 0.03660369054228326, 'max_depth': 11, 'max_bin': 286, 'num_leaves': 156}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:44,503] Trial 85 finished with value: 0.6875398202367383 and parameters: {'n_estimators': 587, 'learning_rate': 0.02814863055654435, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 136}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:49,724] Trial 86 finished with value: 0.6696070230681594 and parameters: {'n_estimators': 546, 'learning_rate': 0.011235259887680333, 'max_depth': 10, 'max_bin': 271, 'num_leaves': 107}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:52,692] Trial 87 finished with value: 0.6841964314926304 and parameters: {'n_estimators': 526, 'learning_rate': 0.05962996270439107, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 439}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:36:56,425] Trial 88 finished with value: 0.6712003271101438 and parameters: {'n_estimators': 344, 'learning_rate': 0.017865551037306533, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 219}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:00,020] Trial 89 finished with value: 0.6859388548820011 and parameters: {'n_estimators': 649, 'learning_rate': 0.04837472177585022, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 487}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:01,787] Trial 90 finished with value: 0.678282329401201 and parameters: {'n_estimators': 367, 'learning_rate': 0.08371764543364364, 'max_depth': 9, 'max_bin': 289, 'num_leaves': 179}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:04,432] Trial 91 finished with value: 0.68297233519295 and parameters: {'n_estimators': 780, 'learning_rate': 0.07205495499104034, 'max_depth': 9, 'max_bin': 223, 'num_leaves': 236}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:06,731] Trial 92 finished with value: 0.6801051076973585 and parameters: {'n_estimators': 277, 'learning_rate': 0.0664306573870331, 'max_depth': 10, 'max_bin': 211, 'num_leaves': 57}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:09,186] Trial 93 finished with value: 0.6783079732101842 and parameters: {'n_estimators': 705, 'learning_rate': 0.08387535766420207, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 84}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:13,184] Trial 94 finished with value: 0.6883595023231464 and parameters: {'n_estimators': 752, 'learning_rate': 0.031884451898551436, 'max_depth': 11, 'max_bin': 230, 'num_leaves': 202}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:15,278] Trial 95 finished with value: 0.6849045111173727 and parameters: {'n_estimators': 461, 'learning_rate': 0.0950208704497656, 'max_depth': 11, 'max_bin': 250, 'num_leaves': 284}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:18,821] Trial 96 finished with value: 0.6870702515403642 and parameters: {'n_estimators': 615, 'learning_rate': 0.03899206190411849, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 335}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:22,860] Trial 97 finished with value: 0.67562223785867 and parameters: {'n_estimators': 635, 'learning_rate': 0.02330775067435854, 'max_depth': 8, 'max_bin': 275, 'num_leaves': 149}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:26,448] Trial 98 finished with value: 0.6917172889844435 and parameters: {'n_estimators': 518, 'learning_rate': 0.05419477540727249, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 359}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:29,666] Trial 99 finished with value: 0.6824644631370772 and parameters: {'n_estimators': 870, 'learning_rate': 0.04296028375388371, 'max_depth': 9, 'max_bin': 247, 'num_leaves': 174}. Best is trial 9 with value: 0.7021717702064129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7022\n",
      "\tBest params:\n",
      "\t\tn_estimators: 707\n",
      "\t\tlearning_rate: 0.0490155009180803\n",
      "\t\tmax_depth: 9\n",
      "\t\tmax_bin: 293\n",
      "\t\tnum_leaves: 51\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.683653    0.738698\n",
      "1                    TP   28.000000   35.000000\n",
      "2                    TN  308.000000  308.000000\n",
      "3                    FP    8.000000    7.000000\n",
      "4                    FN   38.000000   32.000000\n",
      "5              Accuracy    0.879581    0.897906\n",
      "6             Precision    0.777778    0.833333\n",
      "7           Sensitivity    0.424242    0.522388\n",
      "8           Specificity    0.974700    0.977800\n",
      "9              F1 score    0.549020    0.642202\n",
      "10  F1 score (weighted)    0.864601    0.888146\n",
      "11     F1 score (macro)    0.739767    0.791330\n",
      "12    Balanced Accuracy    0.699463    0.750083\n",
      "13                  MCC    0.516201    0.608055\n",
      "14                  NPV    0.890200    0.905900\n",
      "15              ROC_AUC    0.699463    0.750083\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_lgbm_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_1_cat = np.where(((y_pred_lgbm_1 >= 2) | (y_pred_lgbm_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 15:37:33,162] Trial 100 finished with value: 0.6973718171724748 and parameters: {'n_estimators': 723, 'learning_rate': 0.07096899535828852, 'max_depth': 10, 'max_bin': 236, 'num_leaves': 74}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:35,561] Trial 101 finished with value: 0.691896801442584 and parameters: {'n_estimators': 800, 'learning_rate': 0.07150028378570687, 'max_depth': 10, 'max_bin': 243, 'num_leaves': 71}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:38,288] Trial 102 finished with value: 0.695204783502729 and parameters: {'n_estimators': 747, 'learning_rate': 0.06252860021169972, 'max_depth': 11, 'max_bin': 225, 'num_leaves': 53}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:41,074] Trial 103 finished with value: 0.6971660913889925 and parameters: {'n_estimators': 681, 'learning_rate': 0.07773268276219578, 'max_depth': 10, 'max_bin': 238, 'num_leaves': 103}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:43,914] Trial 104 finished with value: 0.6935102989627027 and parameters: {'n_estimators': 596, 'learning_rate': 0.06828490996743966, 'max_depth': 10, 'max_bin': 241, 'num_leaves': 101}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:46,815] Trial 105 finished with value: 0.6984594900649034 and parameters: {'n_estimators': 667, 'learning_rate': 0.07977608748729033, 'max_depth': 11, 'max_bin': 236, 'num_leaves': 122}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:49,408] Trial 106 finished with value: 0.6935082169071187 and parameters: {'n_estimators': 568, 'learning_rate': 0.057353871326713095, 'max_depth': 11, 'max_bin': 235, 'num_leaves': 122}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:52,853] Trial 107 finished with value: 0.6979950706828448 and parameters: {'n_estimators': 722, 'learning_rate': 0.04741284993083242, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 75}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:55,896] Trial 108 finished with value: 0.6963066709438728 and parameters: {'n_estimators': 725, 'learning_rate': 0.048714748645547847, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 129}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:37:59,116] Trial 109 finished with value: 0.6993479923855065 and parameters: {'n_estimators': 671, 'learning_rate': 0.05951121249756326, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 77}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:02,243] Trial 110 finished with value: 0.6981092861583748 and parameters: {'n_estimators': 667, 'learning_rate': 0.05309036025222025, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 511}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:05,672] Trial 111 finished with value: 0.7004198695493022 and parameters: {'n_estimators': 665, 'learning_rate': 0.05295005691639402, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 502}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:08,831] Trial 112 finished with value: 0.6985649897900569 and parameters: {'n_estimators': 672, 'learning_rate': 0.05957887343686962, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 521}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:12,119] Trial 113 finished with value: 0.6960251497323188 and parameters: {'n_estimators': 659, 'learning_rate': 0.06298678555831876, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 531}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:15,409] Trial 114 finished with value: 0.6988518822239941 and parameters: {'n_estimators': 642, 'learning_rate': 0.05418625315429412, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 504}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:18,480] Trial 115 finished with value: 0.6985937626507999 and parameters: {'n_estimators': 667, 'learning_rate': 0.05895039038039027, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 511}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:21,735] Trial 116 finished with value: 0.6987157368268984 and parameters: {'n_estimators': 639, 'learning_rate': 0.058295963533297177, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 557}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:25,073] Trial 117 finished with value: 0.6980824292034182 and parameters: {'n_estimators': 635, 'learning_rate': 0.05877800282117354, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 558}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:28,811] Trial 118 finished with value: 0.7005073215446039 and parameters: {'n_estimators': 621, 'learning_rate': 0.05271387941058747, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 467}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:32,267] Trial 119 finished with value: 0.6963204845203145 and parameters: {'n_estimators': 693, 'learning_rate': 0.051539828478738774, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 470}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:35,720] Trial 120 finished with value: 0.699588398726292 and parameters: {'n_estimators': 621, 'learning_rate': 0.054713985156202516, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 557}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:38,827] Trial 121 finished with value: 0.6960424240450791 and parameters: {'n_estimators': 620, 'learning_rate': 0.05531918060258924, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 555}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:41,753] Trial 122 finished with value: 0.6964236487701131 and parameters: {'n_estimators': 647, 'learning_rate': 0.06543628510180714, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 496}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:45,333] Trial 123 finished with value: 0.7007141202944009 and parameters: {'n_estimators': 594, 'learning_rate': 0.05254693546639091, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 548}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:48,923] Trial 124 finished with value: 0.7010372270099192 and parameters: {'n_estimators': 578, 'learning_rate': 0.05110733730983233, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 610}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:52,457] Trial 125 finished with value: 0.6949444284717091 and parameters: {'n_estimators': 573, 'learning_rate': 0.04343897912979126, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 598}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:56,090] Trial 126 finished with value: 0.7000574411265558 and parameters: {'n_estimators': 551, 'learning_rate': 0.05095966561120227, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 660}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:38:59,998] Trial 127 finished with value: 0.6971809908062576 and parameters: {'n_estimators': 549, 'learning_rate': 0.038355531757248235, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 646}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:39:03,493] Trial 128 finished with value: 0.7002836655473781 and parameters: {'n_estimators': 586, 'learning_rate': 0.04857639380682026, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 694}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:39:07,038] Trial 129 finished with value: 0.6973105791715948 and parameters: {'n_estimators': 594, 'learning_rate': 0.04710699494713562, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 713}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:39:10,433] Trial 130 finished with value: 0.6975635324146283 and parameters: {'n_estimators': 575, 'learning_rate': 0.05065650003273951, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 658}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:39:14,354] Trial 131 finished with value: 0.6997687413607824 and parameters: {'n_estimators': 556, 'learning_rate': 0.04136329342086668, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 677}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:39:18,152] Trial 132 finished with value: 0.697031774984106 and parameters: {'n_estimators': 607, 'learning_rate': 0.042606923396342304, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 696}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:39:21,911] Trial 133 finished with value: 0.699046085475884 and parameters: {'n_estimators': 546, 'learning_rate': 0.046485118147582195, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 616}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:39:25,738] Trial 134 finished with value: 0.694035977846686 and parameters: {'n_estimators': 506, 'learning_rate': 0.03517453246379421, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 686}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:39:29,359] Trial 135 finished with value: 0.6977433062585364 and parameters: {'n_estimators': 592, 'learning_rate': 0.049650654064077426, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 644}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:39:33,132] Trial 136 finished with value: 0.695405246628628 and parameters: {'n_estimators': 620, 'learning_rate': 0.040408796903387514, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 622}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:39:36,404] Trial 137 finished with value: 0.6972377660447313 and parameters: {'n_estimators': 566, 'learning_rate': 0.05239102169149187, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 571}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:39:39,950] Trial 138 finished with value: 0.7015567283855877 and parameters: {'n_estimators': 560, 'learning_rate': 0.06266816032365777, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 661}. Best is trial 9 with value: 0.7021717702064129.\n",
      "[I 2023-12-20 15:39:43,391] Trial 139 finished with value: 0.7030588683541302 and parameters: {'n_estimators': 583, 'learning_rate': 0.06575494649862133, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 677}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:39:46,266] Trial 140 finished with value: 0.6981737021003409 and parameters: {'n_estimators': 557, 'learning_rate': 0.06279021301969587, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 721}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:39:49,604] Trial 141 finished with value: 0.699930100029331 and parameters: {'n_estimators': 584, 'learning_rate': 0.06489157429884942, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 677}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:39:52,975] Trial 142 finished with value: 0.697504120614946 and parameters: {'n_estimators': 585, 'learning_rate': 0.055563567284802384, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 667}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:39:56,329] Trial 143 finished with value: 0.6970567507039083 and parameters: {'n_estimators': 604, 'learning_rate': 0.06611535293871691, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 695}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:00,279] Trial 144 finished with value: 0.7024660214140942 and parameters: {'n_estimators': 538, 'learning_rate': 0.045861922274113606, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 681}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:03,808] Trial 145 finished with value: 0.6950617243948117 and parameters: {'n_estimators': 532, 'learning_rate': 0.0447658128604222, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 684}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:07,454] Trial 146 finished with value: 0.701162382357155 and parameters: {'n_estimators': 559, 'learning_rate': 0.05018262713338074, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 669}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:11,201] Trial 147 finished with value: 0.6927391381508583 and parameters: {'n_estimators': 555, 'learning_rate': 0.03714539135674785, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 746}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:14,802] Trial 148 finished with value: 0.6980939144986208 and parameters: {'n_estimators': 577, 'learning_rate': 0.047584561119952046, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 669}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:17,389] Trial 149 finished with value: 0.6790977737946081 and parameters: {'n_estimators': 538, 'learning_rate': 0.05032299530777741, 'max_depth': 5, 'max_bin': 280, 'num_leaves': 635}. Best is trial 139 with value: 0.7030588683541302.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7031\n",
      "\tBest params:\n",
      "\t\tn_estimators: 583\n",
      "\t\tlearning_rate: 0.06575494649862133\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 276\n",
      "\t\tnum_leaves: 677\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.683653    0.738698    0.711055\n",
      "1                    TP   28.000000   35.000000   38.000000\n",
      "2                    TN  308.000000  308.000000  307.000000\n",
      "3                    FP    8.000000    7.000000    7.000000\n",
      "4                    FN   38.000000   32.000000   30.000000\n",
      "5              Accuracy    0.879581    0.897906    0.903141\n",
      "6             Precision    0.777778    0.833333    0.844444\n",
      "7           Sensitivity    0.424242    0.522388    0.558824\n",
      "8           Specificity    0.974700    0.977800    0.977700\n",
      "9              F1 score    0.549020    0.642202    0.672566\n",
      "10  F1 score (weighted)    0.864601    0.888146    0.894995\n",
      "11     F1 score (macro)    0.739767    0.791330    0.807865\n",
      "12    Balanced Accuracy    0.699463    0.750083    0.768265\n",
      "13                  MCC    0.516201    0.608055    0.636638\n",
      "14                  NPV    0.890200    0.905900    0.911000\n",
      "15              ROC_AUC    0.699463    0.750083    0.768265\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_lgbm_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_2_cat = np.where(((y_pred_lgbm_2 >= 2) | (y_pred_lgbm_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 15:40:20,499] Trial 150 finished with value: 0.6710877982238073 and parameters: {'n_estimators': 488, 'learning_rate': 0.03986562213425826, 'max_depth': 7, 'max_bin': 282, 'num_leaves': 724}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:23,416] Trial 151 finished with value: 0.6893197537143648 and parameters: {'n_estimators': 563, 'learning_rate': 0.0626219325922823, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 674}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:26,524] Trial 152 finished with value: 0.6896058244788856 and parameters: {'n_estimators': 517, 'learning_rate': 0.06788013542405076, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 704}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:29,766] Trial 153 finished with value: 0.682913486371748 and parameters: {'n_estimators': 586, 'learning_rate': 0.0435014466441565, 'max_depth': 12, 'max_bin': 297, 'num_leaves': 609}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:33,214] Trial 154 finished with value: 0.6862371137762076 and parameters: {'n_estimators': 598, 'learning_rate': 0.05185229720755777, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 659}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:36,272] Trial 155 finished with value: 0.6855645699344828 and parameters: {'n_estimators': 546, 'learning_rate': 0.0553921208389251, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 679}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:38,708] Trial 156 finished with value: 0.6879161868481558 and parameters: {'n_estimators': 572, 'learning_rate': 0.07346602880175185, 'max_depth': 11, 'max_bin': 270, 'num_leaves': 628}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:41,969] Trial 157 finished with value: 0.6906299420154436 and parameters: {'n_estimators': 615, 'learning_rate': 0.06077565437119687, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 639}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:45,406] Trial 158 finished with value: 0.68287600954689 and parameters: {'n_estimators': 511, 'learning_rate': 0.04670241978189047, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 692}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:50,240] Trial 159 finished with value: 0.6931840669492527 and parameters: {'n_estimators': 602, 'learning_rate': 0.03459070983204402, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 662}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:53,559] Trial 160 finished with value: 0.6959543508715124 and parameters: {'n_estimators': 537, 'learning_rate': 0.06530384833173375, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 650}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:56,621] Trial 161 finished with value: 0.6826846219344804 and parameters: {'n_estimators': 624, 'learning_rate': 0.05542355342973941, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 606}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:40:59,911] Trial 162 finished with value: 0.6865121459443605 and parameters: {'n_estimators': 584, 'learning_rate': 0.04966196863689102, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 733}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:03,037] Trial 163 finished with value: 0.6864930139327768 and parameters: {'n_estimators': 555, 'learning_rate': 0.053944999309587266, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 707}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:06,747] Trial 164 finished with value: 0.6875165077748847 and parameters: {'n_estimators': 631, 'learning_rate': 0.04276691096774429, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 594}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:09,719] Trial 165 finished with value: 0.6845733831762394 and parameters: {'n_estimators': 613, 'learning_rate': 0.05887445748196969, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 541}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:13,297] Trial 166 finished with value: 0.6874479454488884 and parameters: {'n_estimators': 572, 'learning_rate': 0.05281493651817223, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 577}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:16,815] Trial 167 finished with value: 0.6925844482547621 and parameters: {'n_estimators': 597, 'learning_rate': 0.05695729269322844, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 655}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:20,089] Trial 168 finished with value: 0.6826653579561006 and parameters: {'n_estimators': 525, 'learning_rate': 0.04663321020153124, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 679}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:22,404] Trial 169 finished with value: 0.6863557902964792 and parameters: {'n_estimators': 185, 'learning_rate': 0.06116522657334052, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 674}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:25,929] Trial 170 finished with value: 0.6860923600635895 and parameters: {'n_estimators': 560, 'learning_rate': 0.04011424113439123, 'max_depth': 11, 'max_bin': 263, 'num_leaves': 694}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:28,751] Trial 171 finished with value: 0.6918191824998623 and parameters: {'n_estimators': 654, 'learning_rate': 0.05788272599561746, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 42}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:31,769] Trial 172 finished with value: 0.6901218686881014 and parameters: {'n_estimators': 582, 'learning_rate': 0.06354613768144747, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 709}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:34,919] Trial 173 finished with value: 0.6822457275155007 and parameters: {'n_estimators': 629, 'learning_rate': 0.050200323959584556, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 631}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:37,733] Trial 174 finished with value: 0.6902796573239081 and parameters: {'n_estimators': 607, 'learning_rate': 0.07459960069006635, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 663}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:41,185] Trial 175 finished with value: 0.6860839458565793 and parameters: {'n_estimators': 586, 'learning_rate': 0.05362595061435907, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 647}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:43,996] Trial 176 finished with value: 0.6910824539589762 and parameters: {'n_estimators': 496, 'learning_rate': 0.06843523976631495, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 453}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:47,771] Trial 177 finished with value: 0.6868271456105688 and parameters: {'n_estimators': 689, 'learning_rate': 0.04563316408573576, 'max_depth': 12, 'max_bin': 183, 'num_leaves': 680}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:51,092] Trial 178 finished with value: 0.6923462447518395 and parameters: {'n_estimators': 542, 'learning_rate': 0.06088975306025234, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 732}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:54,376] Trial 179 finished with value: 0.6914194056411074 and parameters: {'n_estimators': 649, 'learning_rate': 0.06900197361086913, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 527}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:41:57,739] Trial 180 finished with value: 0.685470515272674 and parameters: {'n_estimators': 561, 'learning_rate': 0.04923137996411117, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 410}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:02,584] Trial 181 finished with value: 0.6910807849406606 and parameters: {'n_estimators': 529, 'learning_rate': 0.03654399671581437, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 463}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:06,719] Trial 182 finished with value: 0.6874308591357987 and parameters: {'n_estimators': 479, 'learning_rate': 0.03210811862255822, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 485}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:10,178] Trial 183 finished with value: 0.6842029906591464 and parameters: {'n_estimators': 546, 'learning_rate': 0.042570891671326494, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 51}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:15,574] Trial 184 finished with value: 0.6893578460230372 and parameters: {'n_estimators': 573, 'learning_rate': 0.024114899635108822, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 422}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:18,562] Trial 185 finished with value: 0.6909182233999401 and parameters: {'n_estimators': 506, 'learning_rate': 0.05720145123551562, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 35}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:21,971] Trial 186 finished with value: 0.6857739556798766 and parameters: {'n_estimators': 600, 'learning_rate': 0.05228620063234135, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 699}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:25,271] Trial 187 finished with value: 0.6935164109036768 and parameters: {'n_estimators': 625, 'learning_rate': 0.06468810501461907, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 544}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:28,552] Trial 188 finished with value: 0.6854642403914968 and parameters: {'n_estimators': 560, 'learning_rate': 0.04875215406043016, 'max_depth': 11, 'max_bin': 260, 'num_leaves': 471}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:33,239] Trial 189 finished with value: 0.6868885316431077 and parameters: {'n_estimators': 522, 'learning_rate': 0.02870183785214317, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 61}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:36,548] Trial 190 finished with value: 0.6892447954386277 and parameters: {'n_estimators': 579, 'learning_rate': 0.05636185113018174, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 690}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:39,858] Trial 191 finished with value: 0.6873564114774959 and parameters: {'n_estimators': 541, 'learning_rate': 0.045095518843261524, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 438}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:43,296] Trial 192 finished with value: 0.6850936465569124 and parameters: {'n_estimators': 554, 'learning_rate': 0.04719234006885618, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 616}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:47,291] Trial 193 finished with value: 0.6859855205912375 and parameters: {'n_estimators': 598, 'learning_rate': 0.03981297234627012, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 627}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:50,432] Trial 194 finished with value: 0.6833646181593075 and parameters: {'n_estimators': 539, 'learning_rate': 0.052652239394810334, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 567}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:53,884] Trial 195 finished with value: 0.6917224817908961 and parameters: {'n_estimators': 569, 'learning_rate': 0.05970529769461957, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 583}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:42:57,395] Trial 196 finished with value: 0.6871156405690513 and parameters: {'n_estimators': 590, 'learning_rate': 0.04349841889464914, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 653}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:43:00,885] Trial 197 finished with value: 0.6879557103891147 and parameters: {'n_estimators': 705, 'learning_rate': 0.04792061922820346, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 640}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:43:04,050] Trial 198 finished with value: 0.6839170789183925 and parameters: {'n_estimators': 614, 'learning_rate': 0.05264312951272238, 'max_depth': 11, 'max_bin': 242, 'num_leaves': 673}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:43:08,478] Trial 199 finished with value: 0.6878960267005253 and parameters: {'n_estimators': 521, 'learning_rate': 0.0314340198043412, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 608}. Best is trial 139 with value: 0.7030588683541302.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7031\n",
      "\tBest params:\n",
      "\t\tn_estimators: 583\n",
      "\t\tlearning_rate: 0.06575494649862133\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 276\n",
      "\t\tnum_leaves: 677\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.683653    0.738698    0.711055    0.746468\n",
      "1                    TP   28.000000   35.000000   38.000000   33.000000\n",
      "2                    TN  308.000000  308.000000  307.000000  311.000000\n",
      "3                    FP    8.000000    7.000000    7.000000    5.000000\n",
      "4                    FN   38.000000   32.000000   30.000000   33.000000\n",
      "5              Accuracy    0.879581    0.897906    0.903141    0.900524\n",
      "6             Precision    0.777778    0.833333    0.844444    0.868421\n",
      "7           Sensitivity    0.424242    0.522388    0.558824    0.500000\n",
      "8           Specificity    0.974700    0.977800    0.977700    0.984200\n",
      "9              F1 score    0.549020    0.642202    0.672566    0.634615\n",
      "10  F1 score (weighted)    0.864601    0.888146    0.894995    0.889243\n",
      "11     F1 score (macro)    0.739767    0.791330    0.807865    0.788520\n",
      "12    Balanced Accuracy    0.699463    0.750083    0.768265    0.742089\n",
      "13                  MCC    0.516201    0.608055    0.636638    0.611574\n",
      "14                  NPV    0.890200    0.905900    0.911000    0.904100\n",
      "15              ROC_AUC    0.699463    0.750083    0.768265    0.742089\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_lgbm_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_3_cat = np.where(((y_pred_lgbm_3 >= 2) | (y_pred_lgbm_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 15:43:13,075] Trial 200 finished with value: 0.6809139303481369 and parameters: {'n_estimators': 551, 'learning_rate': 0.03568282680605167, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 669}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:43:17,888] Trial 201 finished with value: 0.6811531120829432 and parameters: {'n_estimators': 512, 'learning_rate': 0.025306317078308292, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 517}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:43:23,474] Trial 202 finished with value: 0.6792238451197925 and parameters: {'n_estimators': 571, 'learning_rate': 0.01851870310199394, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 684}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:43:27,320] Trial 203 finished with value: 0.6807301660010665 and parameters: {'n_estimators': 540, 'learning_rate': 0.04076949708882506, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 491}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:43:30,616] Trial 204 finished with value: 0.6785531431713926 and parameters: {'n_estimators': 588, 'learning_rate': 0.055010196873126645, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 481}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:43:35,779] Trial 205 finished with value: 0.6781828310514282 and parameters: {'n_estimators': 557, 'learning_rate': 0.020454056610318364, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 380}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:43:40,450] Trial 206 finished with value: 0.6778653386238981 and parameters: {'n_estimators': 609, 'learning_rate': 0.028963648358236366, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 308}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:43:42,068] Trial 207 finished with value: 0.6484373535233747 and parameters: {'n_estimators': 491, 'learning_rate': 0.10741759460793175, 'max_depth': 3, 'max_bin': 246, 'num_leaves': 495}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:43:45,907] Trial 208 finished with value: 0.6807868441172736 and parameters: {'n_estimators': 528, 'learning_rate': 0.04515791268252843, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 714}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:43:48,859] Trial 209 finished with value: 0.681781975884323 and parameters: {'n_estimators': 572, 'learning_rate': 0.06366997534296906, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 661}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:43:51,913] Trial 210 finished with value: 0.6787539109471 and parameters: {'n_estimators': 634, 'learning_rate': 0.05003459583444565, 'max_depth': 12, 'max_bin': 296, 'num_leaves': 552}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:43:55,226] Trial 211 finished with value: 0.680487223099189 and parameters: {'n_estimators': 648, 'learning_rate': 0.056838296923704054, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 502}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:43:58,132] Trial 212 finished with value: 0.6778678005215524 and parameters: {'n_estimators': 679, 'learning_rate': 0.05194934506298582, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 457}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:01,330] Trial 213 finished with value: 0.6765988209667826 and parameters: {'n_estimators': 662, 'learning_rate': 0.05876807224154196, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 438}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:04,319] Trial 214 finished with value: 0.6813874205385513 and parameters: {'n_estimators': 645, 'learning_rate': 0.07012319281865642, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 519}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:07,326] Trial 215 finished with value: 0.676686255255182 and parameters: {'n_estimators': 591, 'learning_rate': 0.05477444587054722, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 475}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:10,920] Trial 216 finished with value: 0.6791644894188755 and parameters: {'n_estimators': 623, 'learning_rate': 0.046765068130104386, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 501}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:13,951] Trial 217 finished with value: 0.678929513267218 and parameters: {'n_estimators': 550, 'learning_rate': 0.07605145351665275, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 535}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:18,022] Trial 218 finished with value: 0.6793737707886376 and parameters: {'n_estimators': 606, 'learning_rate': 0.03722761251685788, 'max_depth': 12, 'max_bin': 163, 'num_leaves': 686}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:21,084] Trial 219 finished with value: 0.6781655228911243 and parameters: {'n_estimators': 693, 'learning_rate': 0.06181852957233135, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 593}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:23,753] Trial 220 finished with value: 0.6617039104478313 and parameters: {'n_estimators': 581, 'learning_rate': 0.049589064511228145, 'max_depth': 6, 'max_bin': 259, 'num_leaves': 30}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:27,005] Trial 221 finished with value: 0.678684371934392 and parameters: {'n_estimators': 639, 'learning_rate': 0.05937938469798771, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 548}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:29,992] Trial 222 finished with value: 0.6794456460559573 and parameters: {'n_estimators': 634, 'learning_rate': 0.0651660193548176, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 558}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:33,239] Trial 223 finished with value: 0.6792113649966337 and parameters: {'n_estimators': 668, 'learning_rate': 0.05447507013288891, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 567}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:36,493] Trial 224 finished with value: 0.6787939381804009 and parameters: {'n_estimators': 617, 'learning_rate': 0.04346562665227738, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 53}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:39,580] Trial 225 finished with value: 0.6809377147013176 and parameters: {'n_estimators': 567, 'learning_rate': 0.05737421333275383, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 522}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:43,030] Trial 226 finished with value: 0.6785762309258581 and parameters: {'n_estimators': 532, 'learning_rate': 0.05159712361719163, 'max_depth': 12, 'max_bin': 249, 'num_leaves': 650}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:45,767] Trial 227 finished with value: 0.6734573704449244 and parameters: {'n_estimators': 599, 'learning_rate': 0.06712493275193815, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 702}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:47,954] Trial 228 finished with value: 0.6779474283360849 and parameters: {'n_estimators': 652, 'learning_rate': 0.10037429193974519, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 502}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:52,737] Trial 229 finished with value: 0.6758236044771408 and parameters: {'n_estimators': 505, 'learning_rate': 0.025917913087669477, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 664}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:56,131] Trial 230 finished with value: 0.678631502435882 and parameters: {'n_estimators': 552, 'learning_rate': 0.04655328749858813, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 576}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:44:59,267] Trial 231 finished with value: 0.6792861065663727 and parameters: {'n_estimators': 675, 'learning_rate': 0.06075129124209217, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 512}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:02,697] Trial 232 finished with value: 0.6823527365182587 and parameters: {'n_estimators': 656, 'learning_rate': 0.05791766314400199, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 539}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:05,838] Trial 233 finished with value: 0.6779688095533067 and parameters: {'n_estimators': 688, 'learning_rate': 0.05375283812052028, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 529}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:08,978] Trial 234 finished with value: 0.677747717339397 and parameters: {'n_estimators': 636, 'learning_rate': 0.04936607748812186, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 492}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:10,873] Trial 235 finished with value: 0.6782974678330523 and parameters: {'n_estimators': 712, 'learning_rate': 0.12149232658903428, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 68}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:13,030] Trial 236 finished with value: 0.675387298879474 and parameters: {'n_estimators': 577, 'learning_rate': 0.16848668110111376, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 511}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:16,729] Trial 237 finished with value: 0.6816326431857263 and parameters: {'n_estimators': 610, 'learning_rate': 0.062048795236827506, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 675}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:21,423] Trial 238 finished with value: 0.680031807690058 and parameters: {'n_estimators': 589, 'learning_rate': 0.03187713640110607, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 483}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:24,095] Trial 239 finished with value: 0.6817945811497138 and parameters: {'n_estimators': 536, 'learning_rate': 0.08380889957412573, 'max_depth': 11, 'max_bin': 244, 'num_leaves': 635}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:27,197] Trial 240 finished with value: 0.6794659559007188 and parameters: {'n_estimators': 620, 'learning_rate': 0.04110519662737235, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 41}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:29,950] Trial 241 finished with value: 0.6752291441435105 and parameters: {'n_estimators': 666, 'learning_rate': 0.059572952170623035, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 524}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:33,236] Trial 242 finished with value: 0.6792910470500989 and parameters: {'n_estimators': 677, 'learning_rate': 0.05541161381661081, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 512}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:35,827] Trial 243 finished with value: 0.6725169614546391 and parameters: {'n_estimators': 663, 'learning_rate': 0.06713895787610555, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 560}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:41,274] Trial 244 finished with value: 0.6741160073127163 and parameters: {'n_estimators': 562, 'learning_rate': 0.014968166315186073, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 534}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:44,694] Trial 245 finished with value: 0.6792718864113463 and parameters: {'n_estimators': 645, 'learning_rate': 0.050650869453067425, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 498}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:47,901] Trial 246 finished with value: 0.6810585986452111 and parameters: {'n_estimators': 704, 'learning_rate': 0.05922760203247421, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 692}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:48,867] Trial 247 finished with value: 0.6305735392649703 and parameters: {'n_estimators': 51, 'learning_rate': 0.06410247703332493, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 545}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:51,704] Trial 248 finished with value: 0.6820493519791251 and parameters: {'n_estimators': 629, 'learning_rate': 0.07228932948213744, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 670}. Best is trial 139 with value: 0.7030588683541302.\n",
      "[I 2023-12-20 15:45:55,029] Trial 249 finished with value: 0.6791281389888923 and parameters: {'n_estimators': 523, 'learning_rate': 0.053856461362829455, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 520}. Best is trial 139 with value: 0.7030588683541302.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7031\n",
      "\tBest params:\n",
      "\t\tn_estimators: 583\n",
      "\t\tlearning_rate: 0.06575494649862133\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 276\n",
      "\t\tnum_leaves: 677\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.683653    0.738698    0.711055    0.746468   \n",
      "1                    TP   28.000000   35.000000   38.000000   33.000000   \n",
      "2                    TN  308.000000  308.000000  307.000000  311.000000   \n",
      "3                    FP    8.000000    7.000000    7.000000    5.000000   \n",
      "4                    FN   38.000000   32.000000   30.000000   33.000000   \n",
      "5              Accuracy    0.879581    0.897906    0.903141    0.900524   \n",
      "6             Precision    0.777778    0.833333    0.844444    0.868421   \n",
      "7           Sensitivity    0.424242    0.522388    0.558824    0.500000   \n",
      "8           Specificity    0.974700    0.977800    0.977700    0.984200   \n",
      "9              F1 score    0.549020    0.642202    0.672566    0.634615   \n",
      "10  F1 score (weighted)    0.864601    0.888146    0.894995    0.889243   \n",
      "11     F1 score (macro)    0.739767    0.791330    0.807865    0.788520   \n",
      "12    Balanced Accuracy    0.699463    0.750083    0.768265    0.742089   \n",
      "13                  MCC    0.516201    0.608055    0.636638    0.611574   \n",
      "14                  NPV    0.890200    0.905900    0.911000    0.904100   \n",
      "15              ROC_AUC    0.699463    0.750083    0.768265    0.742089   \n",
      "\n",
      "          Set4  \n",
      "0     0.786615  \n",
      "1    36.000000  \n",
      "2   308.000000  \n",
      "3     8.000000  \n",
      "4    30.000000  \n",
      "5     0.900524  \n",
      "6     0.818182  \n",
      "7     0.545455  \n",
      "8     0.974700  \n",
      "9     0.654545  \n",
      "10    0.892249  \n",
      "11    0.798221  \n",
      "12    0.760069  \n",
      "13    0.615956  \n",
      "14    0.911200  \n",
      "15    0.760069  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_lgbm_4)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_4_cat = np.where(((y_pred_lgbm_4 >= 2) | (y_pred_lgbm_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 15:45:59,502] Trial 250 finished with value: 0.711277234134928 and parameters: {'n_estimators': 583, 'learning_rate': 0.04607510723700235, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 90}. Best is trial 250 with value: 0.711277234134928.\n",
      "[I 2023-12-20 15:46:03,731] Trial 251 finished with value: 0.715391192209114 and parameters: {'n_estimators': 583, 'learning_rate': 0.043847252608175395, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 83}. Best is trial 251 with value: 0.715391192209114.\n",
      "[I 2023-12-20 15:46:06,807] Trial 252 finished with value: 0.6978733196732303 and parameters: {'n_estimators': 580, 'learning_rate': 0.04511876593973528, 'max_depth': 8, 'max_bin': 272, 'num_leaves': 87}. Best is trial 251 with value: 0.715391192209114.\n",
      "[I 2023-12-20 15:46:10,901] Trial 253 finished with value: 0.7093509878945821 and parameters: {'n_estimators': 596, 'learning_rate': 0.04290236703664538, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 80}. Best is trial 251 with value: 0.715391192209114.\n",
      "[I 2023-12-20 15:46:15,668] Trial 254 finished with value: 0.7140846029091962 and parameters: {'n_estimators': 597, 'learning_rate': 0.03907054399939726, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 76}. Best is trial 251 with value: 0.715391192209114.\n",
      "[I 2023-12-20 15:46:19,752] Trial 255 finished with value: 0.7110960658142276 and parameters: {'n_estimators': 593, 'learning_rate': 0.039343105841821845, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 77}. Best is trial 251 with value: 0.715391192209114.\n",
      "[I 2023-12-20 15:46:24,443] Trial 256 finished with value: 0.7128860163231574 and parameters: {'n_estimators': 596, 'learning_rate': 0.0379490003373467, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 82}. Best is trial 251 with value: 0.715391192209114.\n",
      "[I 2023-12-20 15:46:29,174] Trial 257 finished with value: 0.7112697445825322 and parameters: {'n_estimators': 599, 'learning_rate': 0.03612098254436919, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 80}. Best is trial 251 with value: 0.715391192209114.\n",
      "[I 2023-12-20 15:46:33,575] Trial 258 finished with value: 0.7132060011783434 and parameters: {'n_estimators': 598, 'learning_rate': 0.03889957696878479, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 81}. Best is trial 251 with value: 0.715391192209114.\n",
      "[I 2023-12-20 15:46:38,176] Trial 259 finished with value: 0.7098318866560112 and parameters: {'n_estimators': 599, 'learning_rate': 0.03469631129378998, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 95}. Best is trial 251 with value: 0.715391192209114.\n",
      "[I 2023-12-20 15:46:43,151] Trial 260 finished with value: 0.711971607004469 and parameters: {'n_estimators': 595, 'learning_rate': 0.03697847797657252, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 108}. Best is trial 251 with value: 0.715391192209114.\n",
      "[I 2023-12-20 15:46:47,552] Trial 261 finished with value: 0.7156859762636725 and parameters: {'n_estimators': 585, 'learning_rate': 0.03751719256538861, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 97}. Best is trial 261 with value: 0.7156859762636725.\n",
      "[I 2023-12-20 15:46:50,690] Trial 262 finished with value: 0.6920039460989804 and parameters: {'n_estimators': 597, 'learning_rate': 0.03502795360800748, 'max_depth': 7, 'max_bin': 278, 'num_leaves': 98}. Best is trial 261 with value: 0.7156859762636725.\n",
      "[I 2023-12-20 15:46:55,023] Trial 263 finished with value: 0.710133493492525 and parameters: {'n_estimators': 594, 'learning_rate': 0.03782625468033442, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 83}. Best is trial 261 with value: 0.7156859762636725.\n",
      "[I 2023-12-20 15:46:59,682] Trial 264 finished with value: 0.711559071728178 and parameters: {'n_estimators': 600, 'learning_rate': 0.03718895140306247, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 85}. Best is trial 261 with value: 0.7156859762636725.\n",
      "[I 2023-12-20 15:47:02,345] Trial 265 finished with value: 0.6716854255295575 and parameters: {'n_estimators': 602, 'learning_rate': 0.0377224651493704, 'max_depth': 4, 'max_bin': 282, 'num_leaves': 84}. Best is trial 261 with value: 0.7156859762636725.\n",
      "[I 2023-12-20 15:47:05,320] Trial 266 finished with value: 0.6804892866093766 and parameters: {'n_estimators': 591, 'learning_rate': 0.03349811614650695, 'max_depth': 5, 'max_bin': 279, 'num_leaves': 102}. Best is trial 261 with value: 0.7156859762636725.\n",
      "[I 2023-12-20 15:47:09,756] Trial 267 finished with value: 0.7160461960640584 and parameters: {'n_estimators': 608, 'learning_rate': 0.038359808960956705, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 74}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:47:13,832] Trial 268 finished with value: 0.7118455125196973 and parameters: {'n_estimators': 608, 'learning_rate': 0.03842245121303028, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 74}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:47:18,100] Trial 269 finished with value: 0.7055504753946878 and parameters: {'n_estimators': 607, 'learning_rate': 0.03263779393008031, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 110}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:47:22,854] Trial 270 finished with value: 0.7126290107071727 and parameters: {'n_estimators': 608, 'learning_rate': 0.036642931538849566, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 110}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:47:27,488] Trial 271 finished with value: 0.7118253656909678 and parameters: {'n_estimators': 606, 'learning_rate': 0.03694036759723891, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 107}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:47:32,258] Trial 272 finished with value: 0.7108917916178592 and parameters: {'n_estimators': 609, 'learning_rate': 0.034641449953968666, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 112}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:47:37,076] Trial 273 finished with value: 0.7153934785859086 and parameters: {'n_estimators': 609, 'learning_rate': 0.03733985228289248, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 92}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:47:41,845] Trial 274 finished with value: 0.7122854474491771 and parameters: {'n_estimators': 609, 'learning_rate': 0.036854572185978034, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 112}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:47:46,118] Trial 275 finished with value: 0.7113101877278185 and parameters: {'n_estimators': 612, 'learning_rate': 0.03575195915242035, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 110}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:47:50,565] Trial 276 finished with value: 0.708686468941744 and parameters: {'n_estimators': 608, 'learning_rate': 0.03453681166939072, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 115}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:47:55,364] Trial 277 finished with value: 0.7105306533891249 and parameters: {'n_estimators': 611, 'learning_rate': 0.034359862461253075, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 118}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:48:00,009] Trial 278 finished with value: 0.7098559616468989 and parameters: {'n_estimators': 613, 'learning_rate': 0.03372081763448432, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 115}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:48:04,382] Trial 279 finished with value: 0.7054876073570077 and parameters: {'n_estimators': 612, 'learning_rate': 0.03362939909528569, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 118}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:48:08,781] Trial 280 finished with value: 0.7122428404835467 and parameters: {'n_estimators': 612, 'learning_rate': 0.03605471671726592, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 111}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:48:13,284] Trial 281 finished with value: 0.7114876882928527 and parameters: {'n_estimators': 619, 'learning_rate': 0.03715820956543167, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 130}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:48:17,765] Trial 282 finished with value: 0.714470108135125 and parameters: {'n_estimators': 622, 'learning_rate': 0.03743154807396684, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 136}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:48:22,312] Trial 283 finished with value: 0.7096576703722534 and parameters: {'n_estimators': 621, 'learning_rate': 0.03669194816180553, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 136}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:48:26,014] Trial 284 finished with value: 0.7067065318206758 and parameters: {'n_estimators': 620, 'learning_rate': 0.03839252867669116, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 94}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:48:31,052] Trial 285 finished with value: 0.7064445340139236 and parameters: {'n_estimators': 627, 'learning_rate': 0.03027011582030517, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 95}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:48:35,527] Trial 286 finished with value: 0.7128368530031748 and parameters: {'n_estimators': 612, 'learning_rate': 0.037823092754308764, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 130}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:48:40,171] Trial 287 finished with value: 0.7160180947570011 and parameters: {'n_estimators': 613, 'learning_rate': 0.037681892888899865, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 151}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:48:44,628] Trial 288 finished with value: 0.7147937365615938 and parameters: {'n_estimators': 623, 'learning_rate': 0.039128051626500994, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 147}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:48:48,686] Trial 289 finished with value: 0.7094576109676909 and parameters: {'n_estimators': 633, 'learning_rate': 0.038639719080883185, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 147}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:48:53,666] Trial 290 finished with value: 0.7046029050757122 and parameters: {'n_estimators': 614, 'learning_rate': 0.02861219227218895, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 133}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:48:58,489] Trial 291 finished with value: 0.7124387114858927 and parameters: {'n_estimators': 635, 'learning_rate': 0.03922020832537585, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 157}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:49:02,767] Trial 292 finished with value: 0.712917135063406 and parameters: {'n_estimators': 634, 'learning_rate': 0.04002706693544598, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 149}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:49:07,313] Trial 293 finished with value: 0.7149579868755048 and parameters: {'n_estimators': 638, 'learning_rate': 0.03960717458162518, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 148}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:49:11,184] Trial 294 finished with value: 0.7070662438925872 and parameters: {'n_estimators': 638, 'learning_rate': 0.03993555320153107, 'max_depth': 11, 'max_bin': 284, 'num_leaves': 141}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:49:15,499] Trial 295 finished with value: 0.7122856511972797 and parameters: {'n_estimators': 629, 'learning_rate': 0.039122948593307734, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 161}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:49:19,970] Trial 296 finished with value: 0.714585868025583 and parameters: {'n_estimators': 636, 'learning_rate': 0.04024848040985216, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 166}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:49:24,975] Trial 297 finished with value: 0.7043540717972471 and parameters: {'n_estimators': 639, 'learning_rate': 0.03037012692608207, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 162}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:49:29,285] Trial 298 finished with value: 0.7116386708277095 and parameters: {'n_estimators': 630, 'learning_rate': 0.03960678600429888, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 155}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:49:33,919] Trial 299 finished with value: 0.7149800930296015 and parameters: {'n_estimators': 629, 'learning_rate': 0.04021777521319152, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 155}. Best is trial 267 with value: 0.7160461960640584.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7160\n",
      "\tBest params:\n",
      "\t\tn_estimators: 608\n",
      "\t\tlearning_rate: 0.038359808960956705\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 276\n",
      "\t\tnum_leaves: 74\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.683653    0.738698    0.711055    0.746468   \n",
      "1                    TP   28.000000   35.000000   38.000000   33.000000   \n",
      "2                    TN  308.000000  308.000000  307.000000  311.000000   \n",
      "3                    FP    8.000000    7.000000    7.000000    5.000000   \n",
      "4                    FN   38.000000   32.000000   30.000000   33.000000   \n",
      "5              Accuracy    0.879581    0.897906    0.903141    0.900524   \n",
      "6             Precision    0.777778    0.833333    0.844444    0.868421   \n",
      "7           Sensitivity    0.424242    0.522388    0.558824    0.500000   \n",
      "8           Specificity    0.974700    0.977800    0.977700    0.984200   \n",
      "9              F1 score    0.549020    0.642202    0.672566    0.634615   \n",
      "10  F1 score (weighted)    0.864601    0.888146    0.894995    0.889243   \n",
      "11     F1 score (macro)    0.739767    0.791330    0.807865    0.788520   \n",
      "12    Balanced Accuracy    0.699463    0.750083    0.768265    0.742089   \n",
      "13                  MCC    0.516201    0.608055    0.636638    0.611574   \n",
      "14                  NPV    0.890200    0.905900    0.911000    0.904100   \n",
      "15              ROC_AUC    0.699463    0.750083    0.768265    0.742089   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.786615    0.710276  \n",
      "1    36.000000   34.000000  \n",
      "2   308.000000  304.000000  \n",
      "3     8.000000   11.000000  \n",
      "4    30.000000   33.000000  \n",
      "5     0.900524    0.884817  \n",
      "6     0.818182    0.755556  \n",
      "7     0.545455    0.507463  \n",
      "8     0.974700    0.965100  \n",
      "9     0.654545    0.607143  \n",
      "10    0.892249    0.875447  \n",
      "11    0.798221    0.769829  \n",
      "12    0.760069    0.736271  \n",
      "13    0.615956    0.557458  \n",
      "14    0.911200    0.902100  \n",
      "15    0.760069    0.736271  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_lgbm_5)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_5_cat = np.where(((y_pred_lgbm_5 >= 2) | (y_pred_lgbm_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 15:49:38,939] Trial 300 finished with value: 0.7028444528054899 and parameters: {'n_estimators': 643, 'learning_rate': 0.040907451834723524, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 185}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:49:43,396] Trial 301 finished with value: 0.7031896444694234 and parameters: {'n_estimators': 635, 'learning_rate': 0.04041289727750784, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 164}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:49:48,055] Trial 302 finished with value: 0.6996923702639672 and parameters: {'n_estimators': 646, 'learning_rate': 0.0303056492888822, 'max_depth': 11, 'max_bin': 286, 'num_leaves': 153}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:49:52,092] Trial 303 finished with value: 0.6976180325739086 and parameters: {'n_estimators': 628, 'learning_rate': 0.04107609571309203, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 144}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:49:56,705] Trial 304 finished with value: 0.7022130082273736 and parameters: {'n_estimators': 628, 'learning_rate': 0.03862359269102524, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 175}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:50:01,958] Trial 305 finished with value: 0.702623869266822 and parameters: {'n_estimators': 650, 'learning_rate': 0.027316266023969783, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 160}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:50:05,916] Trial 306 finished with value: 0.6974658475328582 and parameters: {'n_estimators': 627, 'learning_rate': 0.04123552381462806, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 130}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:50:11,249] Trial 307 finished with value: 0.702172974163324 and parameters: {'n_estimators': 649, 'learning_rate': 0.030859033182575137, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 151}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:50:15,598] Trial 308 finished with value: 0.6989019019204935 and parameters: {'n_estimators': 623, 'learning_rate': 0.03768682198071877, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 129}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:50:20,186] Trial 309 finished with value: 0.7025354763061257 and parameters: {'n_estimators': 636, 'learning_rate': 0.04141145055690671, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 190}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:50:25,185] Trial 310 finished with value: 0.70228980640596 and parameters: {'n_estimators': 619, 'learning_rate': 0.03316770209456562, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 171}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:50:29,827] Trial 311 finished with value: 0.7003572559892598 and parameters: {'n_estimators': 655, 'learning_rate': 0.03723132619625556, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 144}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:50:34,820] Trial 312 finished with value: 0.7003491850472713 and parameters: {'n_estimators': 610, 'learning_rate': 0.0323898797997478, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 104}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:50:39,292] Trial 313 finished with value: 0.6999774466803976 and parameters: {'n_estimators': 626, 'learning_rate': 0.042506792998362415, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 65}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:50:44,454] Trial 314 finished with value: 0.6990994322497798 and parameters: {'n_estimators': 604, 'learning_rate': 0.025075139132101014, 'max_depth': 11, 'max_bin': 280, 'num_leaves': 156}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:50:48,833] Trial 315 finished with value: 0.700257468038777 and parameters: {'n_estimators': 640, 'learning_rate': 0.03852675515502406, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 129}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:50:53,115] Trial 316 finished with value: 0.698336364194111 and parameters: {'n_estimators': 654, 'learning_rate': 0.04367016548219256, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 103}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:50:58,060] Trial 317 finished with value: 0.697207388509942 and parameters: {'n_estimators': 603, 'learning_rate': 0.02884294287374728, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 202}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:51:03,236] Trial 318 finished with value: 0.7063761722390973 and parameters: {'n_estimators': 632, 'learning_rate': 0.03595183758257771, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 143}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:51:08,091] Trial 319 finished with value: 0.7059082852670197 and parameters: {'n_estimators': 617, 'learning_rate': 0.040807213157710236, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 122}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:51:12,912] Trial 320 finished with value: 0.6991559067587192 and parameters: {'n_estimators': 597, 'learning_rate': 0.031669785346111985, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 171}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:51:17,820] Trial 321 finished with value: 0.6986264891947264 and parameters: {'n_estimators': 654, 'learning_rate': 0.03628936583508496, 'max_depth': 12, 'max_bin': 295, 'num_leaves': 93}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:51:22,629] Trial 322 finished with value: 0.7006397053582422 and parameters: {'n_estimators': 627, 'learning_rate': 0.04213128117897738, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 71}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:51:27,632] Trial 323 finished with value: 0.7036509392332365 and parameters: {'n_estimators': 608, 'learning_rate': 0.033121508515467, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 156}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:51:34,994] Trial 324 finished with value: 0.40234816174823634 and parameters: {'n_estimators': 591, 'learning_rate': 0.0012343835266638986, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 111}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:51:40,622] Trial 325 finished with value: 0.7020200493455757 and parameters: {'n_estimators': 637, 'learning_rate': 0.024002305366891637, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 137}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:51:45,360] Trial 326 finished with value: 0.7037805725761505 and parameters: {'n_estimators': 615, 'learning_rate': 0.038624503363862514, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 69}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:51:49,805] Trial 327 finished with value: 0.7019534876077385 and parameters: {'n_estimators': 580, 'learning_rate': 0.04373028261046696, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 125}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:51:56,552] Trial 328 finished with value: 0.6717926222337248 and parameters: {'n_estimators': 603, 'learning_rate': 0.0084258163614256, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 93}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:52:01,585] Trial 329 finished with value: 0.6996228827450466 and parameters: {'n_estimators': 658, 'learning_rate': 0.036355288914785816, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 59}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:52:06,847] Trial 330 finished with value: 0.6992463831669401 and parameters: {'n_estimators': 628, 'learning_rate': 0.03089931684675095, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 106}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:52:11,942] Trial 331 finished with value: 0.7010319096685158 and parameters: {'n_estimators': 584, 'learning_rate': 0.0271474852180182, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 175}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:52:16,729] Trial 332 finished with value: 0.7019738315227803 and parameters: {'n_estimators': 644, 'learning_rate': 0.039715123453374766, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 151}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:52:20,786] Trial 333 finished with value: 0.6990217200285579 and parameters: {'n_estimators': 615, 'learning_rate': 0.043137461154762184, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 80}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:52:25,671] Trial 334 finished with value: 0.704116017324789 and parameters: {'n_estimators': 598, 'learning_rate': 0.03399048215347058, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 123}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:52:30,475] Trial 335 finished with value: 0.6985466663361181 and parameters: {'n_estimators': 633, 'learning_rate': 0.037998158252400875, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 161}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:52:34,521] Trial 336 finished with value: 0.6996040940393948 and parameters: {'n_estimators': 618, 'learning_rate': 0.04282294324165431, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 139}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:52:39,181] Trial 337 finished with value: 0.7016794696766983 and parameters: {'n_estimators': 575, 'learning_rate': 0.034922688386653966, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 100}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:52:41,562] Trial 338 finished with value: 0.6926683913408529 and parameters: {'n_estimators': 602, 'learning_rate': 0.09182317209954433, 'max_depth': 6, 'max_bin': 291, 'num_leaves': 185}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:52:44,308] Trial 339 finished with value: 0.6960067151439002 and parameters: {'n_estimators': 664, 'learning_rate': 0.08809161565480361, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 89}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:52:49,221] Trial 340 finished with value: 0.6972464230055313 and parameters: {'n_estimators': 645, 'learning_rate': 0.028753882271190384, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 117}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:52:53,258] Trial 341 finished with value: 0.7014041786568221 and parameters: {'n_estimators': 586, 'learning_rate': 0.039357550968474986, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 62}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:52:57,538] Trial 342 finished with value: 0.6994593294004912 and parameters: {'n_estimators': 624, 'learning_rate': 0.04450396675415402, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 138}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:53:02,084] Trial 343 finished with value: 0.7004711313969192 and parameters: {'n_estimators': 607, 'learning_rate': 0.03224237665372201, 'max_depth': 11, 'max_bin': 280, 'num_leaves': 82}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:53:06,969] Trial 344 finished with value: 0.7039308229362039 and parameters: {'n_estimators': 638, 'learning_rate': 0.03667604105763783, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 105}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:53:09,713] Trial 345 finished with value: 0.6665032225271312 and parameters: {'n_estimators': 229, 'learning_rate': 0.02128670140158548, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 152}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:53:14,338] Trial 346 finished with value: 0.7014991997581767 and parameters: {'n_estimators': 588, 'learning_rate': 0.04086381791520208, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 124}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:53:19,378] Trial 347 finished with value: 0.7049950828051659 and parameters: {'n_estimators': 615, 'learning_rate': 0.032785259926449976, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 170}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:53:23,098] Trial 348 finished with value: 0.6955929983241306 and parameters: {'n_estimators': 656, 'learning_rate': 0.045170629805467974, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 52}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:53:28,206] Trial 349 finished with value: 0.7021879806187525 and parameters: {'n_estimators': 573, 'learning_rate': 0.026878092197910673, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 74}. Best is trial 267 with value: 0.7160461960640584.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.716046\n",
      "\tBest params:\n",
      "\t\tn_estimators: 608\n",
      "\t\tlearning_rate: 0.038359808960956705\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 276\n",
      "\t\tnum_leaves: 74\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.683653    0.738698    0.711055    0.746468   \n",
      "1                    TP   28.000000   35.000000   38.000000   33.000000   \n",
      "2                    TN  308.000000  308.000000  307.000000  311.000000   \n",
      "3                    FP    8.000000    7.000000    7.000000    5.000000   \n",
      "4                    FN   38.000000   32.000000   30.000000   33.000000   \n",
      "5              Accuracy    0.879581    0.897906    0.903141    0.900524   \n",
      "6             Precision    0.777778    0.833333    0.844444    0.868421   \n",
      "7           Sensitivity    0.424242    0.522388    0.558824    0.500000   \n",
      "8           Specificity    0.974700    0.977800    0.977700    0.984200   \n",
      "9              F1 score    0.549020    0.642202    0.672566    0.634615   \n",
      "10  F1 score (weighted)    0.864601    0.888146    0.894995    0.889243   \n",
      "11     F1 score (macro)    0.739767    0.791330    0.807865    0.788520   \n",
      "12    Balanced Accuracy    0.699463    0.750083    0.768265    0.742089   \n",
      "13                  MCC    0.516201    0.608055    0.636638    0.611574   \n",
      "14                  NPV    0.890200    0.905900    0.911000    0.904100   \n",
      "15              ROC_AUC    0.699463    0.750083    0.768265    0.742089   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.786615    0.710276    0.772399  \n",
      "1    36.000000   34.000000   33.000000  \n",
      "2   308.000000  304.000000  307.000000  \n",
      "3     8.000000   11.000000    9.000000  \n",
      "4    30.000000   33.000000   33.000000  \n",
      "5     0.900524    0.884817    0.890052  \n",
      "6     0.818182    0.755556    0.785714  \n",
      "7     0.545455    0.507463    0.500000  \n",
      "8     0.974700    0.965100    0.971500  \n",
      "9     0.654545    0.607143    0.611111  \n",
      "10    0.892249    0.875447    0.879847  \n",
      "11    0.798221    0.769829    0.773543  \n",
      "12    0.760069    0.736271    0.735759  \n",
      "13    0.615956    0.557458    0.569837  \n",
      "14    0.911200    0.902100    0.902900  \n",
      "15    0.760069    0.736271    0.735759  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_lgbm_6)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_6_cat = np.where(((y_pred_lgbm_6 >= 2) | (y_pred_lgbm_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 15:53:32,909] Trial 350 finished with value: 0.6761522906151789 and parameters: {'n_estimators': 626, 'learning_rate': 0.03838201855962258, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 108}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:53:35,178] Trial 351 finished with value: 0.6751593060454446 and parameters: {'n_estimators': 597, 'learning_rate': 0.09867238043596463, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 221}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:53:39,413] Trial 352 finished with value: 0.6792758468578826 and parameters: {'n_estimators': 607, 'learning_rate': 0.03651393741529004, 'max_depth': 12, 'max_bin': 151, 'num_leaves': 136}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:53:42,879] Trial 353 finished with value: 0.6735631440017059 and parameters: {'n_estimators': 634, 'learning_rate': 0.04226655648222106, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 99}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:53:47,069] Trial 354 finished with value: 0.670657800111191 and parameters: {'n_estimators': 675, 'learning_rate': 0.02969550970745827, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 86}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:53:53,228] Trial 355 finished with value: 0.6693446289839581 and parameters: {'n_estimators': 591, 'learning_rate': 0.012691706235620905, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 152}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:53:57,797] Trial 356 finished with value: 0.6796277470909872 and parameters: {'n_estimators': 616, 'learning_rate': 0.034761509931158036, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 122}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:03,555] Trial 357 finished with value: 0.6716144422539159 and parameters: {'n_estimators': 649, 'learning_rate': 0.018106282950151785, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 73}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:06,896] Trial 358 finished with value: 0.6704663989332533 and parameters: {'n_estimators': 624, 'learning_rate': 0.045026211884295494, 'max_depth': 11, 'max_bin': 281, 'num_leaves': 197}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:09,819] Trial 359 finished with value: 0.6741226665660275 and parameters: {'n_estimators': 576, 'learning_rate': 0.07943392785973069, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 139}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:13,486] Trial 360 finished with value: 0.6728017034462727 and parameters: {'n_estimators': 600, 'learning_rate': 0.03943235884101814, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 113}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:18,052] Trial 361 finished with value: 0.6777767513876103 and parameters: {'n_estimators': 639, 'learning_rate': 0.03284009325634528, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 164}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:20,214] Trial 362 finished with value: 0.6711563653147442 and parameters: {'n_estimators': 611, 'learning_rate': 0.13371279335093558, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 93}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:25,421] Trial 363 finished with value: 0.6696247012120538 and parameters: {'n_estimators': 660, 'learning_rate': 0.02312524714825514, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 128}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:29,055] Trial 364 finished with value: 0.6752539906825493 and parameters: {'n_estimators': 587, 'learning_rate': 0.04135916480358066, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 61}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:33,261] Trial 365 finished with value: 0.6763252957302328 and parameters: {'n_estimators': 620, 'learning_rate': 0.03640947607624642, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 182}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:37,720] Trial 366 finished with value: 0.6726683586252966 and parameters: {'n_estimators': 638, 'learning_rate': 0.030338666158280167, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 151}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:40,831] Trial 367 finished with value: 0.6730233539225411 and parameters: {'n_estimators': 571, 'learning_rate': 0.04564549202797018, 'max_depth': 12, 'max_bin': 191, 'num_leaves': 107}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:44,323] Trial 368 finished with value: 0.6756331582483799 and parameters: {'n_estimators': 601, 'learning_rate': 0.040218761110916296, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 85}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:48,533] Trial 369 finished with value: 0.6762812923534945 and parameters: {'n_estimators': 628, 'learning_rate': 0.034943083598788637, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 165}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:52,152] Trial 370 finished with value: 0.6725734247745139 and parameters: {'n_estimators': 603, 'learning_rate': 0.043408388802321174, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 122}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:53,706] Trial 371 finished with value: 0.6566703073197411 and parameters: {'n_estimators': 649, 'learning_rate': 0.18729883381104426, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 72}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:54:58,390] Trial 372 finished with value: 0.673257177815436 and parameters: {'n_estimators': 588, 'learning_rate': 0.027734369315041606, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 139}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:55:01,949] Trial 373 finished with value: 0.6752882365709378 and parameters: {'n_estimators': 619, 'learning_rate': 0.037688002271210275, 'max_depth': 12, 'max_bin': 297, 'num_leaves': 101}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:55:06,252] Trial 374 finished with value: 0.6738249884035811 and parameters: {'n_estimators': 576, 'learning_rate': 0.031241590522923354, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 50}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:55:10,449] Trial 375 finished with value: 0.6787541141776285 and parameters: {'n_estimators': 672, 'learning_rate': 0.03964487167226977, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 94}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:55:17,888] Trial 376 finished with value: 0.6415372930451089 and parameters: {'n_estimators': 638, 'learning_rate': 0.0057520937739794775, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 112}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:55:21,092] Trial 377 finished with value: 0.6717490387800622 and parameters: {'n_estimators': 613, 'learning_rate': 0.046552164854079287, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 151}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:55:24,976] Trial 378 finished with value: 0.6734684678227969 and parameters: {'n_estimators': 598, 'learning_rate': 0.03462362570255586, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 130}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:55:28,493] Trial 379 finished with value: 0.6755649316999234 and parameters: {'n_estimators': 627, 'learning_rate': 0.04269870950395155, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 80}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:55:33,589] Trial 380 finished with value: 0.6733778424311868 and parameters: {'n_estimators': 651, 'learning_rate': 0.025408424956839884, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 170}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:55:36,678] Trial 381 finished with value: 0.6694336670834995 and parameters: {'n_estimators': 293, 'learning_rate': 0.03656168760404212, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 63}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:55:41,197] Trial 382 finished with value: 0.6740214745722952 and parameters: {'n_estimators': 570, 'learning_rate': 0.03223577829051595, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 144}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:55:43,033] Trial 383 finished with value: 0.6664241811547683 and parameters: {'n_estimators': 608, 'learning_rate': 0.1116760979548534, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 95}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:55:46,938] Trial 384 finished with value: 0.6735479555650422 and parameters: {'n_estimators': 589, 'learning_rate': 0.03965861839772087, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 116}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:55:50,214] Trial 385 finished with value: 0.6677196682353451 and parameters: {'n_estimators': 628, 'learning_rate': 0.04643614068281705, 'max_depth': 11, 'max_bin': 183, 'num_leaves': 129}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:55:53,612] Trial 386 finished with value: 0.6736214832594769 and parameters: {'n_estimators': 661, 'learning_rate': 0.04233281678687324, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 160}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:55:57,978] Trial 387 finished with value: 0.6786665446112046 and parameters: {'n_estimators': 613, 'learning_rate': 0.03491307382806914, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 85}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:56:03,656] Trial 388 finished with value: 0.6738397043084877 and parameters: {'n_estimators': 639, 'learning_rate': 0.021121407892190847, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 106}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:56:08,025] Trial 389 finished with value: 0.6721420841558701 and parameters: {'n_estimators': 589, 'learning_rate': 0.029887247722485095, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 185}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:56:10,793] Trial 390 finished with value: 0.6765439230775159 and parameters: {'n_estimators': 604, 'learning_rate': 0.08804105747788726, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 72}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:56:14,646] Trial 391 finished with value: 0.676340868133615 and parameters: {'n_estimators': 682, 'learning_rate': 0.03842368870676785, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 141}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:56:18,143] Trial 392 finished with value: 0.6783716385700893 and parameters: {'n_estimators': 626, 'learning_rate': 0.04522111214292984, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 117}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:56:20,247] Trial 393 finished with value: 0.6075910567438011 and parameters: {'n_estimators': 573, 'learning_rate': 0.03335494814267509, 'max_depth': 3, 'max_bin': 272, 'num_leaves': 94}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:56:23,778] Trial 394 finished with value: 0.6662543078780926 and parameters: {'n_estimators': 644, 'learning_rate': 0.04023135548817263, 'max_depth': 9, 'max_bin': 286, 'num_leaves': 157}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:56:28,927] Trial 395 finished with value: 0.6715244471552521 and parameters: {'n_estimators': 615, 'learning_rate': 0.026510307334850026, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 48}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:56:32,819] Trial 396 finished with value: 0.6765252051469772 and parameters: {'n_estimators': 596, 'learning_rate': 0.03774279748913602, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 131}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:56:36,611] Trial 397 finished with value: 0.6698678011565672 and parameters: {'n_estimators': 658, 'learning_rate': 0.043436180529053764, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 174}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:56:38,132] Trial 398 finished with value: 0.6672448479510227 and parameters: {'n_estimators': 625, 'learning_rate': 0.19378886007978485, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 75}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:56:40,875] Trial 399 finished with value: 0.6756302208445158 and parameters: {'n_estimators': 586, 'learning_rate': 0.08146372883064965, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 105}. Best is trial 267 with value: 0.7160461960640584.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7160462\n",
      "\tBest params:\n",
      "\t\tn_estimators: 608\n",
      "\t\tlearning_rate: 0.038359808960956705\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 276\n",
      "\t\tnum_leaves: 74\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.683653    0.738698    0.711055    0.746468   \n",
      "1                    TP   28.000000   35.000000   38.000000   33.000000   \n",
      "2                    TN  308.000000  308.000000  307.000000  311.000000   \n",
      "3                    FP    8.000000    7.000000    7.000000    5.000000   \n",
      "4                    FN   38.000000   32.000000   30.000000   33.000000   \n",
      "5              Accuracy    0.879581    0.897906    0.903141    0.900524   \n",
      "6             Precision    0.777778    0.833333    0.844444    0.868421   \n",
      "7           Sensitivity    0.424242    0.522388    0.558824    0.500000   \n",
      "8           Specificity    0.974700    0.977800    0.977700    0.984200   \n",
      "9              F1 score    0.549020    0.642202    0.672566    0.634615   \n",
      "10  F1 score (weighted)    0.864601    0.888146    0.894995    0.889243   \n",
      "11     F1 score (macro)    0.739767    0.791330    0.807865    0.788520   \n",
      "12    Balanced Accuracy    0.699463    0.750083    0.768265    0.742089   \n",
      "13                  MCC    0.516201    0.608055    0.636638    0.611574   \n",
      "14                  NPV    0.890200    0.905900    0.911000    0.904100   \n",
      "15              ROC_AUC    0.699463    0.750083    0.768265    0.742089   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.786615    0.710276    0.772399    0.781568  \n",
      "1    36.000000   34.000000   33.000000   37.000000  \n",
      "2   308.000000  304.000000  307.000000  307.000000  \n",
      "3     8.000000   11.000000    9.000000    8.000000  \n",
      "4    30.000000   33.000000   33.000000   30.000000  \n",
      "5     0.900524    0.884817    0.890052    0.900524  \n",
      "6     0.818182    0.755556    0.785714    0.822222  \n",
      "7     0.545455    0.507463    0.500000    0.552239  \n",
      "8     0.974700    0.965100    0.971500    0.974600  \n",
      "9     0.654545    0.607143    0.611111    0.660714  \n",
      "10    0.892249    0.875447    0.879847    0.892432  \n",
      "11    0.798221    0.769829    0.773543    0.801216  \n",
      "12    0.760069    0.736271    0.735759    0.763421  \n",
      "13    0.615956    0.557458    0.569837    0.621515  \n",
      "14    0.911200    0.902100    0.902900    0.911000  \n",
      "15    0.760069    0.736271    0.735759    0.763421  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_lgbm_7)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_7_cat = np.where(((y_pred_lgbm_7 >= 2) | (y_pred_lgbm_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 15:56:48,075] Trial 400 finished with value: 0.691127025484482 and parameters: {'n_estimators': 605, 'learning_rate': 0.011648841500299836, 'max_depth': 12, 'max_bin': 198, 'num_leaves': 142}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:56:53,048] Trial 401 finished with value: 0.7009855686066209 and parameters: {'n_estimators': 639, 'learning_rate': 0.03160361746274281, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 88}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:56:56,978] Trial 402 finished with value: 0.7071730202955304 and parameters: {'n_estimators': 565, 'learning_rate': 0.047069917274126905, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 60}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:03,451] Trial 403 finished with value: 0.7004682347900502 and parameters: {'n_estimators': 616, 'learning_rate': 0.017475701022045993, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 117}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:08,144] Trial 404 finished with value: 0.7040266717200266 and parameters: {'n_estimators': 597, 'learning_rate': 0.035775465931707356, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 204}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:10,111] Trial 405 finished with value: 0.7046179143261541 and parameters: {'n_estimators': 632, 'learning_rate': 0.1773619705918799, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 155}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:14,566] Trial 406 finished with value: 0.7019027569990408 and parameters: {'n_estimators': 665, 'learning_rate': 0.039531516799890754, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 127}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:18,514] Trial 407 finished with value: 0.704350099920233 and parameters: {'n_estimators': 424, 'learning_rate': 0.04255187548093611, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 103}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:23,660] Trial 408 finished with value: 0.702429742360312 and parameters: {'n_estimators': 582, 'learning_rate': 0.030091625274106918, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 83}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:28,393] Trial 409 finished with value: 0.7041764606003402 and parameters: {'n_estimators': 646, 'learning_rate': 0.03662728635673556, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 166}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:30,865] Trial 410 finished with value: 0.7130706513727914 and parameters: {'n_estimators': 615, 'learning_rate': 0.0961636808127051, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 141}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:34,614] Trial 411 finished with value: 0.7071268600431226 and parameters: {'n_estimators': 621, 'learning_rate': 0.04728286359716449, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 144}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:37,138] Trial 412 finished with value: 0.7068763659582238 and parameters: {'n_estimators': 613, 'learning_rate': 0.15051233734602318, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 182}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:39,704] Trial 413 finished with value: 0.7124258367736402 and parameters: {'n_estimators': 636, 'learning_rate': 0.09341557088444347, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 133}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:41,843] Trial 414 finished with value: 0.7057685588486228 and parameters: {'n_estimators': 684, 'learning_rate': 0.1403946384545053, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 129}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:43,960] Trial 415 finished with value: 0.7035559059918517 and parameters: {'n_estimators': 651, 'learning_rate': 0.09461573051672416, 'max_depth': 8, 'max_bin': 296, 'num_leaves': 118}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:46,548] Trial 416 finished with value: 0.7119273195202209 and parameters: {'n_estimators': 633, 'learning_rate': 0.08809822846986794, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 132}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:49,284] Trial 417 finished with value: 0.715025223968736 and parameters: {'n_estimators': 671, 'learning_rate': 0.09726077421664195, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 312}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:51,946] Trial 418 finished with value: 0.7136731209066669 and parameters: {'n_estimators': 655, 'learning_rate': 0.09996042391659715, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 232}. Best is trial 267 with value: 0.7160461960640584.\n",
      "[I 2023-12-20 15:57:55,368] Trial 419 finished with value: 0.7189837580953653 and parameters: {'n_estimators': 689, 'learning_rate': 0.10141054013517255, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 354}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:57:58,301] Trial 420 finished with value: 0.7156542250902576 and parameters: {'n_estimators': 728, 'learning_rate': 0.09947126386293899, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 359}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:01,375] Trial 421 finished with value: 0.712975757001731 and parameters: {'n_estimators': 682, 'learning_rate': 0.09849398544926262, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 336}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:03,794] Trial 422 finished with value: 0.7074599793087891 and parameters: {'n_estimators': 754, 'learning_rate': 0.0993947944848358, 'max_depth': 10, 'max_bin': 294, 'num_leaves': 367}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:06,229] Trial 423 finished with value: 0.7120671462011198 and parameters: {'n_estimators': 721, 'learning_rate': 0.1031766666950037, 'max_depth': 11, 'max_bin': 291, 'num_leaves': 322}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:08,830] Trial 424 finished with value: 0.7129887709393381 and parameters: {'n_estimators': 688, 'learning_rate': 0.09598921176543065, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 357}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:11,589] Trial 425 finished with value: 0.7120988401013083 and parameters: {'n_estimators': 723, 'learning_rate': 0.09320578045684691, 'max_depth': 11, 'max_bin': 290, 'num_leaves': 346}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:14,235] Trial 426 finished with value: 0.7115073774129719 and parameters: {'n_estimators': 738, 'learning_rate': 0.0971150805551082, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 345}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:16,954] Trial 427 finished with value: 0.7107744438583143 and parameters: {'n_estimators': 708, 'learning_rate': 0.09505579688441872, 'max_depth': 11, 'max_bin': 294, 'num_leaves': 261}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:19,749] Trial 428 finished with value: 0.717728535332733 and parameters: {'n_estimators': 696, 'learning_rate': 0.10259166721966397, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 368}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:22,499] Trial 429 finished with value: 0.7132499621653382 and parameters: {'n_estimators': 699, 'learning_rate': 0.10286163406504772, 'max_depth': 11, 'max_bin': 299, 'num_leaves': 381}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:25,519] Trial 430 finished with value: 0.7159671251180881 and parameters: {'n_estimators': 711, 'learning_rate': 0.09951170735360973, 'max_depth': 11, 'max_bin': 294, 'num_leaves': 381}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:28,572] Trial 431 finished with value: 0.7126101811182303 and parameters: {'n_estimators': 690, 'learning_rate': 0.10207546289303832, 'max_depth': 11, 'max_bin': 298, 'num_leaves': 386}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:31,031] Trial 432 finished with value: 0.7141809340188572 and parameters: {'n_estimators': 691, 'learning_rate': 0.10343412484521512, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 399}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:33,246] Trial 433 finished with value: 0.7121899902826202 and parameters: {'n_estimators': 700, 'learning_rate': 0.10524813165051217, 'max_depth': 11, 'max_bin': 297, 'num_leaves': 364}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:35,873] Trial 434 finished with value: 0.7122247483946527 and parameters: {'n_estimators': 711, 'learning_rate': 0.1077305523064919, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 390}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:38,388] Trial 435 finished with value: 0.7136956295098382 and parameters: {'n_estimators': 697, 'learning_rate': 0.10282817345926651, 'max_depth': 11, 'max_bin': 299, 'num_leaves': 404}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:41,098] Trial 436 finished with value: 0.7115873181513624 and parameters: {'n_estimators': 739, 'learning_rate': 0.10068209900672481, 'max_depth': 11, 'max_bin': 299, 'num_leaves': 376}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:43,841] Trial 437 finished with value: 0.7115834084517351 and parameters: {'n_estimators': 708, 'learning_rate': 0.10124071605556546, 'max_depth': 11, 'max_bin': 299, 'num_leaves': 405}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:46,387] Trial 438 finished with value: 0.7154661296177257 and parameters: {'n_estimators': 770, 'learning_rate': 0.10515455147013579, 'max_depth': 11, 'max_bin': 296, 'num_leaves': 354}. Best is trial 419 with value: 0.7189837580953653.\n",
      "[I 2023-12-20 15:58:48,938] Trial 439 finished with value: 0.7190573514891514 and parameters: {'n_estimators': 765, 'learning_rate': 0.1065969142394389, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 354}. Best is trial 439 with value: 0.7190573514891514.\n",
      "[I 2023-12-20 15:58:51,484] Trial 440 finished with value: 0.7136295318558095 and parameters: {'n_estimators': 747, 'learning_rate': 0.10669321410005829, 'max_depth': 10, 'max_bin': 297, 'num_leaves': 402}. Best is trial 439 with value: 0.7190573514891514.\n",
      "[I 2023-12-20 15:58:53,866] Trial 441 finished with value: 0.7094076238032603 and parameters: {'n_estimators': 743, 'learning_rate': 0.10883431285420173, 'max_depth': 10, 'max_bin': 296, 'num_leaves': 398}. Best is trial 439 with value: 0.7190573514891514.\n",
      "[I 2023-12-20 15:58:56,392] Trial 442 finished with value: 0.7155518419133721 and parameters: {'n_estimators': 767, 'learning_rate': 0.10574680022235146, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 414}. Best is trial 439 with value: 0.7190573514891514.\n",
      "[I 2023-12-20 15:58:58,765] Trial 443 finished with value: 0.7070495811093307 and parameters: {'n_estimators': 782, 'learning_rate': 0.1048812397092967, 'max_depth': 10, 'max_bin': 300, 'num_leaves': 418}. Best is trial 439 with value: 0.7190573514891514.\n",
      "[I 2023-12-20 15:59:01,385] Trial 444 finished with value: 0.7097179886301449 and parameters: {'n_estimators': 773, 'learning_rate': 0.11161924979780335, 'max_depth': 11, 'max_bin': 296, 'num_leaves': 378}. Best is trial 439 with value: 0.7190573514891514.\n",
      "[I 2023-12-20 15:59:04,040] Trial 445 finished with value: 0.7120712654228107 and parameters: {'n_estimators': 799, 'learning_rate': 0.11088203800129058, 'max_depth': 11, 'max_bin': 299, 'num_leaves': 430}. Best is trial 439 with value: 0.7190573514891514.\n",
      "[I 2023-12-20 15:59:06,319] Trial 446 finished with value: 0.7099686006623862 and parameters: {'n_estimators': 753, 'learning_rate': 0.10467295415973388, 'max_depth': 10, 'max_bin': 300, 'num_leaves': 353}. Best is trial 439 with value: 0.7190573514891514.\n",
      "[I 2023-12-20 15:59:09,338] Trial 447 finished with value: 0.7156375504970345 and parameters: {'n_estimators': 774, 'learning_rate': 0.10611151209727963, 'max_depth': 11, 'max_bin': 296, 'num_leaves': 401}. Best is trial 439 with value: 0.7190573514891514.\n",
      "[I 2023-12-20 15:59:11,869] Trial 448 finished with value: 0.7192005666366479 and parameters: {'n_estimators': 763, 'learning_rate': 0.10590324427906232, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 404}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:14,380] Trial 449 finished with value: 0.7123503791654564 and parameters: {'n_estimators': 762, 'learning_rate': 0.10770735694214552, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 401}. Best is trial 448 with value: 0.7192005666366479.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.71920057\n",
      "\tBest params:\n",
      "\t\tn_estimators: 763\n",
      "\t\tlearning_rate: 0.10590324427906232\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 295\n",
      "\t\tnum_leaves: 404\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.683653    0.738698    0.711055    0.746468   \n",
      "1                    TP   28.000000   35.000000   38.000000   33.000000   \n",
      "2                    TN  308.000000  308.000000  307.000000  311.000000   \n",
      "3                    FP    8.000000    7.000000    7.000000    5.000000   \n",
      "4                    FN   38.000000   32.000000   30.000000   33.000000   \n",
      "5              Accuracy    0.879581    0.897906    0.903141    0.900524   \n",
      "6             Precision    0.777778    0.833333    0.844444    0.868421   \n",
      "7           Sensitivity    0.424242    0.522388    0.558824    0.500000   \n",
      "8           Specificity    0.974700    0.977800    0.977700    0.984200   \n",
      "9              F1 score    0.549020    0.642202    0.672566    0.634615   \n",
      "10  F1 score (weighted)    0.864601    0.888146    0.894995    0.889243   \n",
      "11     F1 score (macro)    0.739767    0.791330    0.807865    0.788520   \n",
      "12    Balanced Accuracy    0.699463    0.750083    0.768265    0.742089   \n",
      "13                  MCC    0.516201    0.608055    0.636638    0.611574   \n",
      "14                  NPV    0.890200    0.905900    0.911000    0.904100   \n",
      "15              ROC_AUC    0.699463    0.750083    0.768265    0.742089   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.786615    0.710276    0.772399    0.781568    0.623498  \n",
      "1    36.000000   34.000000   33.000000   37.000000   35.000000  \n",
      "2   308.000000  304.000000  307.000000  307.000000  306.000000  \n",
      "3     8.000000   11.000000    9.000000    8.000000    8.000000  \n",
      "4    30.000000   33.000000   33.000000   30.000000   33.000000  \n",
      "5     0.900524    0.884817    0.890052    0.900524    0.892670  \n",
      "6     0.818182    0.755556    0.785714    0.822222    0.813953  \n",
      "7     0.545455    0.507463    0.500000    0.552239    0.514706  \n",
      "8     0.974700    0.965100    0.971500    0.974600    0.974500  \n",
      "9     0.654545    0.607143    0.611111    0.660714    0.630631  \n",
      "10    0.892249    0.875447    0.879847    0.892432    0.882638  \n",
      "11    0.798221    0.769829    0.773543    0.801216    0.783922  \n",
      "12    0.760069    0.736271    0.735759    0.763421    0.744614  \n",
      "13    0.615956    0.557458    0.569837    0.621515    0.592102  \n",
      "14    0.911200    0.902100    0.902900    0.911000    0.902700  \n",
      "15    0.760069    0.736271    0.735759    0.763421    0.744614  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_lgbm_8)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_8_cat = np.where(((y_pred_lgbm_8 >= 2) | (y_pred_lgbm_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 15:59:16,964] Trial 450 finished with value: 0.7004793524816819 and parameters: {'n_estimators': 828, 'learning_rate': 0.1122113303076513, 'max_depth': 11, 'max_bin': 296, 'num_leaves': 411}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:19,412] Trial 451 finished with value: 0.7020914256242299 and parameters: {'n_estimators': 776, 'learning_rate': 0.11406507532598462, 'max_depth': 11, 'max_bin': 294, 'num_leaves': 323}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:21,973] Trial 452 finished with value: 0.7060389733583661 and parameters: {'n_estimators': 759, 'learning_rate': 0.10585163276436171, 'max_depth': 11, 'max_bin': 297, 'num_leaves': 300}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:24,769] Trial 453 finished with value: 0.707498701261156 and parameters: {'n_estimators': 790, 'learning_rate': 0.10246944084482155, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 394}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:27,355] Trial 454 finished with value: 0.7033655304575991 and parameters: {'n_estimators': 812, 'learning_rate': 0.11542044926593253, 'max_depth': 11, 'max_bin': 294, 'num_leaves': 360}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:29,727] Trial 455 finished with value: 0.6976623479144173 and parameters: {'n_estimators': 768, 'learning_rate': 0.10791580058364106, 'max_depth': 11, 'max_bin': 297, 'num_leaves': 423}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:32,386] Trial 456 finished with value: 0.7033952218033875 and parameters: {'n_estimators': 733, 'learning_rate': 0.09976230426381907, 'max_depth': 10, 'max_bin': 293, 'num_leaves': 370}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:35,038] Trial 457 finished with value: 0.702098117982188 and parameters: {'n_estimators': 792, 'learning_rate': 0.10462091688865094, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 284}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:37,874] Trial 458 finished with value: 0.7093665272974683 and parameters: {'n_estimators': 726, 'learning_rate': 0.1158143936006184, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 413}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:40,947] Trial 459 finished with value: 0.7033528074375649 and parameters: {'n_estimators': 752, 'learning_rate': 0.10899503811043942, 'max_depth': 11, 'max_bin': 294, 'num_leaves': 386}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:43,498] Trial 460 finished with value: 0.7034383120473156 and parameters: {'n_estimators': 776, 'learning_rate': 0.0995830534228608, 'max_depth': 11, 'max_bin': 296, 'num_leaves': 394}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:46,342] Trial 461 finished with value: 0.7073472077479306 and parameters: {'n_estimators': 763, 'learning_rate': 0.10414377620744322, 'max_depth': 11, 'max_bin': 297, 'num_leaves': 433}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:49,450] Trial 462 finished with value: 0.7097162196265335 and parameters: {'n_estimators': 735, 'learning_rate': 0.10669454978940848, 'max_depth': 11, 'max_bin': 292, 'num_leaves': 333}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:51,937] Trial 463 finished with value: 0.6994349451451322 and parameters: {'n_estimators': 723, 'learning_rate': 0.10063478027751356, 'max_depth': 10, 'max_bin': 300, 'num_leaves': 242}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:54,455] Trial 464 finished with value: 0.7099101942492645 and parameters: {'n_estimators': 742, 'learning_rate': 0.11582418494073682, 'max_depth': 11, 'max_bin': 293, 'num_leaves': 398}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 15:59:57,487] Trial 465 finished with value: 0.7047858755378426 and parameters: {'n_estimators': 803, 'learning_rate': 0.09039110711488017, 'max_depth': 11, 'max_bin': 297, 'num_leaves': 349}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:00,068] Trial 466 finished with value: 0.7016642726277492 and parameters: {'n_estimators': 720, 'learning_rate': 0.1098605757595608, 'max_depth': 11, 'max_bin': 297, 'num_leaves': 374}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:02,709] Trial 467 finished with value: 0.7046037323028539 and parameters: {'n_estimators': 767, 'learning_rate': 0.09821598217355475, 'max_depth': 11, 'max_bin': 292, 'num_leaves': 317}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:05,240] Trial 468 finished with value: 0.7000705760932616 and parameters: {'n_estimators': 698, 'learning_rate': 0.10484574074560903, 'max_depth': 11, 'max_bin': 293, 'num_leaves': 362}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:07,740] Trial 469 finished with value: 0.7004213259369931 and parameters: {'n_estimators': 793, 'learning_rate': 0.12071307853195774, 'max_depth': 11, 'max_bin': 297, 'num_leaves': 404}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:10,412] Trial 470 finished with value: 0.7047527311925231 and parameters: {'n_estimators': 715, 'learning_rate': 0.10238460835453315, 'max_depth': 11, 'max_bin': 292, 'num_leaves': 413}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:13,022] Trial 471 finished with value: 0.7007484799059489 and parameters: {'n_estimators': 745, 'learning_rate': 0.09440149562158151, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 383}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:15,626] Trial 472 finished with value: 0.7053433798971473 and parameters: {'n_estimators': 754, 'learning_rate': 0.10826007756130115, 'max_depth': 10, 'max_bin': 294, 'num_leaves': 448}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:18,390] Trial 473 finished with value: 0.7038367498504153 and parameters: {'n_estimators': 779, 'learning_rate': 0.09863476939685122, 'max_depth': 11, 'max_bin': 297, 'num_leaves': 374}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:20,763] Trial 474 finished with value: 0.7025457657397676 and parameters: {'n_estimators': 850, 'learning_rate': 0.11162439549456919, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 345}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:23,730] Trial 475 finished with value: 0.7041490564626154 and parameters: {'n_estimators': 694, 'learning_rate': 0.10413864361123541, 'max_depth': 10, 'max_bin': 291, 'num_leaves': 419}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:26,310] Trial 476 finished with value: 0.7039699896324381 and parameters: {'n_estimators': 729, 'learning_rate': 0.09705564419115667, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 361}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:28,949] Trial 477 finished with value: 0.7028648592439913 and parameters: {'n_estimators': 678, 'learning_rate': 0.10683680686573142, 'max_depth': 11, 'max_bin': 291, 'num_leaves': 392}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:31,238] Trial 478 finished with value: 0.6993987569993168 and parameters: {'n_estimators': 706, 'learning_rate': 0.10336424532711096, 'max_depth': 9, 'max_bin': 297, 'num_leaves': 386}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:33,828] Trial 479 finished with value: 0.7040553777201032 and parameters: {'n_estimators': 738, 'learning_rate': 0.11889989319011277, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 411}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:36,716] Trial 480 finished with value: 0.7036615528636577 and parameters: {'n_estimators': 770, 'learning_rate': 0.09153389513491492, 'max_depth': 11, 'max_bin': 294, 'num_leaves': 335}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:39,252] Trial 481 finished with value: 0.700221104944562 and parameters: {'n_estimators': 752, 'learning_rate': 0.11131210771649941, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 435}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:41,622] Trial 482 finished with value: 0.7004803685739425 and parameters: {'n_estimators': 689, 'learning_rate': 0.124070127952468, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 275}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:44,283] Trial 483 finished with value: 0.7056721016481593 and parameters: {'n_estimators': 816, 'learning_rate': 0.1015540156734326, 'max_depth': 11, 'max_bin': 292, 'num_leaves': 219}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:46,954] Trial 484 finished with value: 0.7001648638805157 and parameters: {'n_estimators': 720, 'learning_rate': 0.08640028558681763, 'max_depth': 10, 'max_bin': 297, 'num_leaves': 401}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:50,004] Trial 485 finished with value: 0.7069087617726575 and parameters: {'n_estimators': 701, 'learning_rate': 0.11436885012350548, 'max_depth': 11, 'max_bin': 290, 'num_leaves': 363}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:52,779] Trial 486 finished with value: 0.7011456942669791 and parameters: {'n_estimators': 787, 'learning_rate': 0.0962877671309191, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 351}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:55,649] Trial 487 finished with value: 0.7034445063663335 and parameters: {'n_estimators': 675, 'learning_rate': 0.10835604266480355, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 373}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:00:58,387] Trial 488 finished with value: 0.6996825789651704 and parameters: {'n_estimators': 758, 'learning_rate': 0.10071574129040982, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 307}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:01:01,335] Trial 489 finished with value: 0.7019934607623963 and parameters: {'n_estimators': 740, 'learning_rate': 0.09221996621472409, 'max_depth': 11, 'max_bin': 292, 'num_leaves': 424}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:01:04,455] Trial 490 finished with value: 0.7063244365142685 and parameters: {'n_estimators': 718, 'learning_rate': 0.10714847931543071, 'max_depth': 11, 'max_bin': 298, 'num_leaves': 388}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:01:07,170] Trial 491 finished with value: 0.7048644729769509 and parameters: {'n_estimators': 772, 'learning_rate': 0.09659109684041331, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 406}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:01:09,562] Trial 492 finished with value: 0.6998652873672842 and parameters: {'n_estimators': 784, 'learning_rate': 0.12772161593161543, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 373}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:01:12,885] Trial 493 finished with value: 0.7020229041625516 and parameters: {'n_estimators': 677, 'learning_rate': 0.0758830275864028, 'max_depth': 10, 'max_bin': 300, 'num_leaves': 340}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:01:15,513] Trial 494 finished with value: 0.7020844382408262 and parameters: {'n_estimators': 700, 'learning_rate': 0.11095401251989767, 'max_depth': 11, 'max_bin': 292, 'num_leaves': 399}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:01:17,998] Trial 495 finished with value: 0.6937798923701097 and parameters: {'n_estimators': 741, 'learning_rate': 0.07800767108581114, 'max_depth': 7, 'max_bin': 297, 'num_leaves': 356}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:01:21,247] Trial 496 finished with value: 0.7024907686682156 and parameters: {'n_estimators': 680, 'learning_rate': 0.08360308765860851, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 323}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:01:23,945] Trial 497 finished with value: 0.7034829024087632 and parameters: {'n_estimators': 709, 'learning_rate': 0.10294323844461367, 'max_depth': 11, 'max_bin': 294, 'num_leaves': 428}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:01:26,438] Trial 498 finished with value: 0.7050633630419263 and parameters: {'n_estimators': 754, 'learning_rate': 0.11561453267124625, 'max_depth': 11, 'max_bin': 288, 'num_leaves': 384}. Best is trial 448 with value: 0.7192005666366479.\n",
      "[I 2023-12-20 16:01:28,963] Trial 499 finished with value: 0.7015688340767343 and parameters: {'n_estimators': 805, 'learning_rate': 0.09141777390401287, 'max_depth': 11, 'max_bin': 291, 'num_leaves': 408}. Best is trial 448 with value: 0.7192005666366479.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.719200567\n",
      "\tBest params:\n",
      "\t\tn_estimators: 763\n",
      "\t\tlearning_rate: 0.10590324427906232\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 295\n",
      "\t\tnum_leaves: 404\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.683653    0.738698    0.711055    0.746468   \n",
      "1                    TP   28.000000   35.000000   38.000000   33.000000   \n",
      "2                    TN  308.000000  308.000000  307.000000  311.000000   \n",
      "3                    FP    8.000000    7.000000    7.000000    5.000000   \n",
      "4                    FN   38.000000   32.000000   30.000000   33.000000   \n",
      "5              Accuracy    0.879581    0.897906    0.903141    0.900524   \n",
      "6             Precision    0.777778    0.833333    0.844444    0.868421   \n",
      "7           Sensitivity    0.424242    0.522388    0.558824    0.500000   \n",
      "8           Specificity    0.974700    0.977800    0.977700    0.984200   \n",
      "9              F1 score    0.549020    0.642202    0.672566    0.634615   \n",
      "10  F1 score (weighted)    0.864601    0.888146    0.894995    0.889243   \n",
      "11     F1 score (macro)    0.739767    0.791330    0.807865    0.788520   \n",
      "12    Balanced Accuracy    0.699463    0.750083    0.768265    0.742089   \n",
      "13                  MCC    0.516201    0.608055    0.636638    0.611574   \n",
      "14                  NPV    0.890200    0.905900    0.911000    0.904100   \n",
      "15              ROC_AUC    0.699463    0.750083    0.768265    0.742089   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.786615    0.710276    0.772399    0.781568    0.623498    0.724391  \n",
      "1    36.000000   34.000000   33.000000   37.000000   35.000000   37.000000  \n",
      "2   308.000000  304.000000  307.000000  307.000000  306.000000  307.000000  \n",
      "3     8.000000   11.000000    9.000000    8.000000    8.000000    6.000000  \n",
      "4    30.000000   33.000000   33.000000   30.000000   33.000000   32.000000  \n",
      "5     0.900524    0.884817    0.890052    0.900524    0.892670    0.900524  \n",
      "6     0.818182    0.755556    0.785714    0.822222    0.813953    0.860465  \n",
      "7     0.545455    0.507463    0.500000    0.552239    0.514706    0.536232  \n",
      "8     0.974700    0.965100    0.971500    0.974600    0.974500    0.980800  \n",
      "9     0.654545    0.607143    0.611111    0.660714    0.630631    0.660714  \n",
      "10    0.892249    0.875447    0.879847    0.892432    0.882638    0.890961  \n",
      "11    0.798221    0.769829    0.773543    0.801216    0.783922    0.801216  \n",
      "12    0.760069    0.736271    0.735759    0.763421    0.744614    0.758531  \n",
      "13    0.615956    0.557458    0.569837    0.621515    0.592102    0.629370  \n",
      "14    0.911200    0.902100    0.902900    0.911000    0.902700    0.905600  \n",
      "15    0.760069    0.736271    0.735759    0.763421    0.744614    0.758531  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_lgbm_9)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_9_cat = np.where(((y_pred_lgbm_9 >= 2) | (y_pred_lgbm_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.726045     0.042989\n",
      "1                    TP        18.200000     3.011091\n",
      "2                    TN       153.500000     2.321398\n",
      "3                    FP         4.100000     1.969207\n",
      "4                    FN        15.200000     3.190263\n",
      "5              Accuracy         0.898953     0.021238\n",
      "6             Precision         0.817940     0.081185\n",
      "7           Sensitivity         0.545369     0.092737\n",
      "8           Specificity         0.973980     0.012554\n",
      "9              F1 score         0.651014     0.083058\n",
      "10  F1 score (weighted)         0.890139     0.024625\n",
      "11     F1 score (macro)         0.795945     0.047342\n",
      "12    Balanced Accuracy         0.759671     0.047892\n",
      "13                  MCC         0.613623     0.088125\n",
      "14                  NPV         0.910120     0.017535\n",
      "15              ROC_AUC         0.759671     0.047892\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.683653</td>\n",
       "      <td>0.738698</td>\n",
       "      <td>0.711055</td>\n",
       "      <td>0.746468</td>\n",
       "      <td>0.786615</td>\n",
       "      <td>0.710276</td>\n",
       "      <td>0.772399</td>\n",
       "      <td>0.781568</td>\n",
       "      <td>0.623498</td>\n",
       "      <td>0.724391</td>\n",
       "      <td>0.727862</td>\n",
       "      <td>0.049703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>34.600000</td>\n",
       "      <td>2.875181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>307.300000</td>\n",
       "      <td>1.766981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>1.636392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.400000</td>\n",
       "      <td>2.366432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.879581</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.884817</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.895026</td>\n",
       "      <td>0.007945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.818007</td>\n",
       "      <td>0.036322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.516155</td>\n",
       "      <td>0.038688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.974700</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.977700</td>\n",
       "      <td>0.984200</td>\n",
       "      <td>0.974700</td>\n",
       "      <td>0.965100</td>\n",
       "      <td>0.971500</td>\n",
       "      <td>0.974600</td>\n",
       "      <td>0.974500</td>\n",
       "      <td>0.980800</td>\n",
       "      <td>0.975560</td>\n",
       "      <td>0.005176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.642202</td>\n",
       "      <td>0.672566</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.630631</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.632326</td>\n",
       "      <td>0.036293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.864601</td>\n",
       "      <td>0.888146</td>\n",
       "      <td>0.894995</td>\n",
       "      <td>0.889243</td>\n",
       "      <td>0.892249</td>\n",
       "      <td>0.875447</td>\n",
       "      <td>0.879847</td>\n",
       "      <td>0.892432</td>\n",
       "      <td>0.882638</td>\n",
       "      <td>0.890961</td>\n",
       "      <td>0.885056</td>\n",
       "      <td>0.009490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.739767</td>\n",
       "      <td>0.791330</td>\n",
       "      <td>0.807865</td>\n",
       "      <td>0.788520</td>\n",
       "      <td>0.798221</td>\n",
       "      <td>0.769829</td>\n",
       "      <td>0.773543</td>\n",
       "      <td>0.801216</td>\n",
       "      <td>0.783922</td>\n",
       "      <td>0.801216</td>\n",
       "      <td>0.785543</td>\n",
       "      <td>0.020226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.699463</td>\n",
       "      <td>0.750083</td>\n",
       "      <td>0.768265</td>\n",
       "      <td>0.742089</td>\n",
       "      <td>0.760069</td>\n",
       "      <td>0.736271</td>\n",
       "      <td>0.735759</td>\n",
       "      <td>0.763421</td>\n",
       "      <td>0.744614</td>\n",
       "      <td>0.758531</td>\n",
       "      <td>0.745857</td>\n",
       "      <td>0.019869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.516201</td>\n",
       "      <td>0.608055</td>\n",
       "      <td>0.636638</td>\n",
       "      <td>0.611574</td>\n",
       "      <td>0.615956</td>\n",
       "      <td>0.557458</td>\n",
       "      <td>0.569837</td>\n",
       "      <td>0.621515</td>\n",
       "      <td>0.592102</td>\n",
       "      <td>0.629370</td>\n",
       "      <td>0.595870</td>\n",
       "      <td>0.037646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.890200</td>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.911000</td>\n",
       "      <td>0.904100</td>\n",
       "      <td>0.911200</td>\n",
       "      <td>0.902100</td>\n",
       "      <td>0.902900</td>\n",
       "      <td>0.911000</td>\n",
       "      <td>0.902700</td>\n",
       "      <td>0.905600</td>\n",
       "      <td>0.904670</td>\n",
       "      <td>0.006223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.699463</td>\n",
       "      <td>0.750083</td>\n",
       "      <td>0.768265</td>\n",
       "      <td>0.742089</td>\n",
       "      <td>0.760069</td>\n",
       "      <td>0.736271</td>\n",
       "      <td>0.735759</td>\n",
       "      <td>0.763421</td>\n",
       "      <td>0.744614</td>\n",
       "      <td>0.758531</td>\n",
       "      <td>0.745857</td>\n",
       "      <td>0.019869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.683653    0.738698    0.711055    0.746468   \n",
       "1                    TP   28.000000   35.000000   38.000000   33.000000   \n",
       "2                    TN  308.000000  308.000000  307.000000  311.000000   \n",
       "3                    FP    8.000000    7.000000    7.000000    5.000000   \n",
       "4                    FN   38.000000   32.000000   30.000000   33.000000   \n",
       "5              Accuracy    0.879581    0.897906    0.903141    0.900524   \n",
       "6             Precision    0.777778    0.833333    0.844444    0.868421   \n",
       "7           Sensitivity    0.424242    0.522388    0.558824    0.500000   \n",
       "8           Specificity    0.974700    0.977800    0.977700    0.984200   \n",
       "9              F1 score    0.549020    0.642202    0.672566    0.634615   \n",
       "10  F1 score (weighted)    0.864601    0.888146    0.894995    0.889243   \n",
       "11     F1 score (macro)    0.739767    0.791330    0.807865    0.788520   \n",
       "12    Balanced Accuracy    0.699463    0.750083    0.768265    0.742089   \n",
       "13                  MCC    0.516201    0.608055    0.636638    0.611574   \n",
       "14                  NPV    0.890200    0.905900    0.911000    0.904100   \n",
       "15              ROC_AUC    0.699463    0.750083    0.768265    0.742089   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.786615    0.710276    0.772399    0.781568    0.623498    0.724391   \n",
       "1    36.000000   34.000000   33.000000   37.000000   35.000000   37.000000   \n",
       "2   308.000000  304.000000  307.000000  307.000000  306.000000  307.000000   \n",
       "3     8.000000   11.000000    9.000000    8.000000    8.000000    6.000000   \n",
       "4    30.000000   33.000000   33.000000   30.000000   33.000000   32.000000   \n",
       "5     0.900524    0.884817    0.890052    0.900524    0.892670    0.900524   \n",
       "6     0.818182    0.755556    0.785714    0.822222    0.813953    0.860465   \n",
       "7     0.545455    0.507463    0.500000    0.552239    0.514706    0.536232   \n",
       "8     0.974700    0.965100    0.971500    0.974600    0.974500    0.980800   \n",
       "9     0.654545    0.607143    0.611111    0.660714    0.630631    0.660714   \n",
       "10    0.892249    0.875447    0.879847    0.892432    0.882638    0.890961   \n",
       "11    0.798221    0.769829    0.773543    0.801216    0.783922    0.801216   \n",
       "12    0.760069    0.736271    0.735759    0.763421    0.744614    0.758531   \n",
       "13    0.615956    0.557458    0.569837    0.621515    0.592102    0.629370   \n",
       "14    0.911200    0.902100    0.902900    0.911000    0.902700    0.905600   \n",
       "15    0.760069    0.736271    0.735759    0.763421    0.744614    0.758531   \n",
       "\n",
       "           ave       std  \n",
       "0     0.727862  0.049703  \n",
       "1    34.600000  2.875181  \n",
       "2   307.300000  1.766981  \n",
       "3     7.700000  1.636392  \n",
       "4    32.400000  2.366432  \n",
       "5     0.895026  0.007945  \n",
       "6     0.818007  0.036322  \n",
       "7     0.516155  0.038688  \n",
       "8     0.975560  0.005176  \n",
       "9     0.632326  0.036293  \n",
       "10    0.885056  0.009490  \n",
       "11    0.785543  0.020226  \n",
       "12    0.745857  0.019869  \n",
       "13    0.595870  0.037646  \n",
       "14    0.904670  0.006223  \n",
       "15    0.745857  0.019869  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test['std'] = mat_met_lgbm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.723771</td>\n",
       "      <td>0.044310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.018052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.839621</td>\n",
       "      <td>0.084283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.535663</td>\n",
       "      <td>0.091647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.977162</td>\n",
       "      <td>0.014108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.648532</td>\n",
       "      <td>0.075382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.890421</td>\n",
       "      <td>0.021129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.795084</td>\n",
       "      <td>0.042415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.756408</td>\n",
       "      <td>0.045183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.617399</td>\n",
       "      <td>0.077112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.908854</td>\n",
       "      <td>0.016139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.756408</td>\n",
       "      <td>0.045183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.723771     0.044310\n",
       "1              Accuracy         0.900000     0.018052\n",
       "2             Precision         0.839621     0.084283\n",
       "3           Sensitivity         0.535663     0.091647\n",
       "4           Specificity         0.977162     0.014108\n",
       "5              F1 score         0.648532     0.075382\n",
       "6   F1 score (weighted)         0.890421     0.021129\n",
       "7      F1 score (macro)         0.795084     0.042415\n",
       "8     Balanced Accuracy         0.756408     0.045183\n",
       "9                   MCC         0.617399     0.077112\n",
       "10                  NPV         0.908854     0.016139\n",
       "11              ROC_AUC         0.756408     0.045183"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_lgbm_cat = np.where(((y_pred_optimized_lgbm >= 2) | (y_pred_optimized_lgbm <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_lgbm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_lgbm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "lgbm_run0 = data_lgbm[['y_test_idx0', 'y_test0', 'y_pred_lgbm0']]\n",
    "lgbm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "lgbm_run0.reset_index(inplace=True, drop=True)\n",
    "lgbm_run1 = data_lgbm[['y_test_idx1', 'y_test1', 'y_pred_lgbm1']]\n",
    "lgbm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "lgbm_run1.reset_index(inplace=True, drop=True)\n",
    "lgbm_run2 = data_lgbm[['y_test_idx2', 'y_test2', 'y_pred_lgbm2']]\n",
    "lgbm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "lgbm_run2.reset_index(inplace=True, drop=True)\n",
    "lgbm_run3 = data_lgbm[['y_test_idx3', 'y_test3', 'y_pred_lgbm3']]\n",
    "lgbm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "lgbm_run3.reset_index(inplace=True, drop=True)\n",
    "lgbm_run4 = data_lgbm[['y_test_idx4', 'y_test4', 'y_pred_lgbm4']]\n",
    "lgbm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "lgbm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "lgbm_5preds = pd.concat([chembl_id, lgbm_run0, lgbm_run1, lgbm_run2, lgbm_run3, lgbm_run4], axis=1)\n",
    "lgbm_5preds = lgbm_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_lgbm0', 'y_pred_lgbm1', 'y_pred_lgbm2', 'y_pred_lgbm3', 'y_pred_lgbm4']]\n",
    "lgbm_5preds['y_pred_lgbm_ave'] = lgbm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "lgbm_5preds['y_pred_lgbm_std'] = lgbm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "\n",
    "mat_met_optimized_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db4ac315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model r2_score 0.7184 with a standard deviation of 0.0522\n",
      "LightGBM optimized model r2_score 0.7271 with a standard deviation of 0.0540\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#r2_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"r2\")\n",
    "r2_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"r2\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)r2_score %0.4f with a standard deviation of %0.4f\" % (r2_cv_lgbm_opt_testSet.mean(), r2_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(r2_cv_lgbm_opt), np.std(r2_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_lgbm_withSemiSel.joblib']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm_reg, \"OUTPUT/lgbm_reg_withSemiSel.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"OUTPUT/optimizedCV_lgbm_withSemiSel.joblib\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.712599     0.056262\n",
      "1                    TP        17.500000     4.006938\n",
      "2                    TN       153.800000     2.201010\n",
      "3                    FP         3.800000     1.988858\n",
      "4                    FN        15.900000     4.357624\n",
      "5              Accuracy         0.896859     0.029208\n",
      "6             Precision         0.817065     0.097054\n",
      "7           Sensitivity         0.525122     0.125555\n",
      "8           Specificity         0.975890     0.012606\n",
      "9              F1 score         0.635180     0.117576\n",
      "10  F1 score (weighted)         0.886478     0.034547\n",
      "11     F1 score (macro)         0.787539     0.066974\n",
      "12    Balanced Accuracy         0.750505     0.066205\n",
      "13                  MCC         0.600572     0.123774\n",
      "14                  NPV         0.906680     0.024038\n",
      "15              ROC_AUC         0.750505     0.066205\n",
      "CPU times: user 52.8 s, sys: 76.1 ms, total: 52.9 s\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_reg.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores),np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=4, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=4, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 16:02:10,495] A new study created in memory with name: XGBRegressor\n",
      "[I 2023-12-20 16:02:19,074] Trial 0 finished with value: 0.7095943113467135 and parameters: {'n_estimators': 754, 'eta': 0.04716605607040766, 'max_depth': 5, 'alpha': 0.031900000000000005, 'lambda': 23.652460849810737, 'max_bin': 252}. Best is trial 0 with value: 0.7095943113467135.\n",
      "[I 2023-12-20 16:02:35,342] Trial 1 finished with value: 0.7158830095833487 and parameters: {'n_estimators': 884, 'eta': 0.02093699152435154, 'max_depth': 7, 'alpha': 0.0257, 'lambda': 24.19034538160919, 'max_bin': 276}. Best is trial 1 with value: 0.7158830095833487.\n",
      "[I 2023-12-20 16:02:45,609] Trial 2 finished with value: 0.7185577498660882 and parameters: {'n_estimators': 595, 'eta': 0.06652920859457241, 'max_depth': 7, 'alpha': 0.298, 'lambda': 34.39521004648528, 'max_bin': 322}. Best is trial 2 with value: 0.7185577498660882.\n",
      "[I 2023-12-20 16:02:51,890] Trial 3 finished with value: 0.7105203189390983 and parameters: {'n_estimators': 330, 'eta': 0.0746487321404888, 'max_depth': 7, 'alpha': 0.7729, 'lambda': 35.219186598525084, 'max_bin': 469}. Best is trial 2 with value: 0.7185577498660882.\n",
      "[I 2023-12-20 16:02:59,618] Trial 4 finished with value: 0.7193737477332346 and parameters: {'n_estimators': 418, 'eta': 0.04390880712831702, 'max_depth': 7, 'alpha': 0.19590000000000002, 'lambda': 15.666505308980398, 'max_bin': 417}. Best is trial 4 with value: 0.7193737477332346.\n",
      "[I 2023-12-20 16:03:12,966] Trial 5 finished with value: 0.725612886467719 and parameters: {'n_estimators': 840, 'eta': 0.05565743045077653, 'max_depth': 10, 'alpha': 0.6764, 'lambda': 27.21716233081556, 'max_bin': 359}. Best is trial 5 with value: 0.725612886467719.\n",
      "[I 2023-12-20 16:03:15,764] Trial 6 finished with value: 0.691915721960074 and parameters: {'n_estimators': 104, 'eta': 0.0662034530539478, 'max_depth': 9, 'alpha': 0.4299, 'lambda': 29.409076494289128, 'max_bin': 287}. Best is trial 5 with value: 0.725612886467719.\n",
      "[I 2023-12-20 16:03:22,119] Trial 7 finished with value: 0.7225163553479106 and parameters: {'n_estimators': 577, 'eta': 0.09092479339473612, 'max_depth': 11, 'alpha': 0.7255, 'lambda': 7.299545140050371, 'max_bin': 295}. Best is trial 5 with value: 0.725612886467719.\n",
      "[I 2023-12-20 16:03:33,272] Trial 8 finished with value: 0.6396998438890298 and parameters: {'n_estimators': 315, 'eta': 0.010512889506748776, 'max_depth': 12, 'alpha': 0.8735, 'lambda': 34.64831077352996, 'max_bin': 379}. Best is trial 5 with value: 0.725612886467719.\n",
      "[I 2023-12-20 16:03:36,143] Trial 9 finished with value: 0.6752183046690552 and parameters: {'n_estimators': 168, 'eta': 0.058219959827151765, 'max_depth': 6, 'alpha': 0.6211, 'lambda': 36.14258220296421, 'max_bin': 253}. Best is trial 5 with value: 0.725612886467719.\n",
      "[I 2023-12-20 16:03:54,236] Trial 10 finished with value: 0.7262083373464195 and parameters: {'n_estimators': 898, 'eta': 0.03042850149368616, 'max_depth': 10, 'alpha': 0.9424, 'lambda': 16.70905514055529, 'max_bin': 342}. Best is trial 10 with value: 0.7262083373464195.\n",
      "[I 2023-12-20 16:04:09,791] Trial 11 finished with value: 0.7249788629211233 and parameters: {'n_estimators': 877, 'eta': 0.03351251426131665, 'max_depth': 10, 'alpha': 0.9858, 'lambda': 15.48400698116889, 'max_bin': 351}. Best is trial 10 with value: 0.7262083373464195.\n",
      "[I 2023-12-20 16:04:30,797] Trial 12 finished with value: 0.5405596196286419 and parameters: {'n_estimators': 730, 'eta': 0.002182669588281415, 'max_depth': 9, 'alpha': 0.6125, 'lambda': 18.610911537577778, 'max_bin': 393}. Best is trial 10 with value: 0.7262083373464195.\n",
      "[I 2023-12-20 16:04:43,709] Trial 13 finished with value: 0.7276872717300505 and parameters: {'n_estimators': 735, 'eta': 0.033990629568717436, 'max_depth': 11, 'alpha': 0.9931000000000001, 'lambda': 9.850880805378964, 'max_bin': 341}. Best is trial 13 with value: 0.7276872717300505.\n",
      "[I 2023-12-20 16:04:59,182] Trial 14 finished with value: 0.7253760943122755 and parameters: {'n_estimators': 688, 'eta': 0.028293767417473033, 'max_depth': 12, 'alpha': 0.9883000000000001, 'lambda': 7.886573403259266, 'max_bin': 331}. Best is trial 13 with value: 0.7276872717300505.\n",
      "[I 2023-12-20 16:05:10,247] Trial 15 finished with value: 0.725883002397938 and parameters: {'n_estimators': 561, 'eta': 0.036608052564832606, 'max_depth': 11, 'alpha': 0.8330000000000001, 'lambda': 2.432402764408729, 'max_bin': 436}. Best is trial 13 with value: 0.7276872717300505.\n",
      "[I 2023-12-20 16:05:29,095] Trial 16 finished with value: 0.7259299621171217 and parameters: {'n_estimators': 771, 'eta': 0.021523231477768752, 'max_depth': 10, 'alpha': 0.8956000000000001, 'lambda': 13.586044384054858, 'max_bin': 320}. Best is trial 13 with value: 0.7276872717300505.\n",
      "[I 2023-12-20 16:05:43,402] Trial 17 finished with value: 0.7249219432623698 and parameters: {'n_estimators': 654, 'eta': 0.04070344035144734, 'max_depth': 11, 'alpha': 0.5014000000000001, 'lambda': 19.264111634967126, 'max_bin': 405}. Best is trial 13 with value: 0.7276872717300505.\n",
      "[I 2023-12-20 16:05:59,281] Trial 18 finished with value: 0.7232811201010282 and parameters: {'n_estimators': 814, 'eta': 0.0262057374856484, 'max_depth': 8, 'alpha': 0.9470000000000001, 'lambda': 10.932616114148528, 'max_bin': 353}. Best is trial 13 with value: 0.7276872717300505.\n",
      "[I 2023-12-20 16:06:11,306] Trial 19 finished with value: 0.7077108062243387 and parameters: {'n_estimators': 493, 'eta': 0.010904810701781523, 'max_depth': 9, 'alpha': 0.7891, 'lambda': 2.2665851847356633, 'max_bin': 493}. Best is trial 13 with value: 0.7276872717300505.\n",
      "[I 2023-12-20 16:06:25,493] Trial 20 finished with value: 0.7287478071278296 and parameters: {'n_estimators': 897, 'eta': 0.03857187600010008, 'max_depth': 12, 'alpha': 0.4061, 'lambda': 11.24949412226725, 'max_bin': 306}. Best is trial 20 with value: 0.7287478071278296.\n",
      "[I 2023-12-20 16:06:39,733] Trial 21 finished with value: 0.7296504000963024 and parameters: {'n_estimators': 886, 'eta': 0.039303415262369676, 'max_depth': 12, 'alpha': 0.3921, 'lambda': 11.481057191444393, 'max_bin': 312}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:06:53,071] Trial 22 finished with value: 0.7277870629610697 and parameters: {'n_estimators': 807, 'eta': 0.04150492731637957, 'max_depth': 12, 'alpha': 0.3889, 'lambda': 10.218968578147917, 'max_bin': 312}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:07:07,146] Trial 23 finished with value: 0.7282918420854726 and parameters: {'n_estimators': 811, 'eta': 0.04656304463901132, 'max_depth': 12, 'alpha': 0.3619, 'lambda': 12.96348974346785, 'max_bin': 306}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:07:19,349] Trial 24 finished with value: 0.727049672692538 and parameters: {'n_estimators': 666, 'eta': 0.051124801446365664, 'max_depth': 12, 'alpha': 0.23220000000000002, 'lambda': 13.559480335516948, 'max_bin': 305}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:07:29,266] Trial 25 finished with value: 0.7255210108489034 and parameters: {'n_estimators': 825, 'eta': 0.04842920606310797, 'max_depth': 12, 'alpha': 0.3544, 'lambda': 7.27424503133237, 'max_bin': 276}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:07:45,109] Trial 26 finished with value: 0.7253761388755431 and parameters: {'n_estimators': 789, 'eta': 0.03832079030475373, 'max_depth': 11, 'alpha': 0.4938, 'lambda': 20.219218983715752, 'max_bin': 274}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:07:55,641] Trial 27 finished with value: 0.7252304030124462 and parameters: {'n_estimators': 892, 'eta': 0.053315452967326565, 'max_depth': 12, 'alpha': 0.1801, 'lambda': 11.463145856306049, 'max_bin': 375}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:08:05,813] Trial 28 finished with value: 0.7295134389366557 and parameters: {'n_estimators': 696, 'eta': 0.04564031197188414, 'max_depth': 11, 'alpha': 0.5144000000000001, 'lambda': 5.310463758263895, 'max_bin': 301}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:08:13,427] Trial 29 finished with value: 0.7108522625478396 and parameters: {'n_estimators': 717, 'eta': 0.044911899570352416, 'max_depth': 5, 'alpha': 0.5273, 'lambda': 5.079126498919172, 'max_bin': 263}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:08:24,090] Trial 30 finished with value: 0.7244118193615448 and parameters: {'n_estimators': 492, 'eta': 0.0380122768959723, 'max_depth': 11, 'alpha': 0.083, 'lambda': 5.081958430790802, 'max_bin': 292}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:08:36,449] Trial 31 finished with value: 0.7274385029954689 and parameters: {'n_estimators': 767, 'eta': 0.04708514573685331, 'max_depth': 12, 'alpha': 0.30670000000000003, 'lambda': 13.962158187814, 'max_bin': 302}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:08:48,348] Trial 32 finished with value: 0.726578036886587 and parameters: {'n_estimators': 850, 'eta': 0.04634736531010429, 'max_depth': 11, 'alpha': 0.4345, 'lambda': 11.608576768955135, 'max_bin': 327}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:09:04,392] Trial 33 finished with value: 0.7250343108979145 and parameters: {'n_estimators': 648, 'eta': 0.03236941425581531, 'max_depth': 12, 'alpha': 0.5552, 'lambda': 8.880324742937873, 'max_bin': 276}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:09:12,302] Trial 34 finished with value: 0.7220730016837364 and parameters: {'n_estimators': 851, 'eta': 0.05646439206619733, 'max_depth': 12, 'alpha': 0.2731, 'lambda': 4.882466571489443, 'max_bin': 307}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:09:26,766] Trial 35 finished with value: 0.7271399842839423 and parameters: {'n_estimators': 898, 'eta': 0.04166261331066322, 'max_depth': 11, 'alpha': 0.35600000000000004, 'lambda': 21.764524774201853, 'max_bin': 255}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:09:39,256] Trial 36 finished with value: 0.728950106291435 and parameters: {'n_estimators': 618, 'eta': 0.0505364456338879, 'max_depth': 10, 'alpha': 0.4314, 'lambda': 16.981222293311276, 'max_bin': 318}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:09:49,475] Trial 37 finished with value: 0.7284270440707012 and parameters: {'n_estimators': 598, 'eta': 0.0626109527651533, 'max_depth': 10, 'alpha': 0.4541, 'lambda': 16.2460739410375, 'max_bin': 332}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:09:58,184] Trial 38 finished with value: 0.7232142643771328 and parameters: {'n_estimators': 422, 'eta': 0.05175020299598158, 'max_depth': 8, 'alpha': 0.6168, 'lambda': 17.565911867096034, 'max_bin': 362}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:10:08,386] Trial 39 finished with value: 0.7267769193949821 and parameters: {'n_estimators': 612, 'eta': 0.07131854376531704, 'max_depth': 10, 'alpha': 0.5756, 'lambda': 22.62534959543806, 'max_bin': 288}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:10:19,926] Trial 40 finished with value: 0.7264120875164891 and parameters: {'n_estimators': 541, 'eta': 0.05868864032359278, 'max_depth': 11, 'alpha': 0.467, 'lambda': 24.915615243974667, 'max_bin': 322}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:10:28,993] Trial 41 finished with value: 0.7255504214879037 and parameters: {'n_estimators': 439, 'eta': 0.061795603842130654, 'max_depth': 10, 'alpha': 0.4471, 'lambda': 16.540666776415826, 'max_bin': 335}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:10:37,610] Trial 42 finished with value: 0.7291266502705149 and parameters: {'n_estimators': 335, 'eta': 0.061666636003721936, 'max_depth': 10, 'alpha': 0.40240000000000004, 'lambda': 15.147470536033754, 'max_bin': 317}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:10:45,565] Trial 43 finished with value: 0.7243945369323959 and parameters: {'n_estimators': 342, 'eta': 0.052745346570370236, 'max_depth': 9, 'alpha': 0.4022, 'lambda': 12.222820275241277, 'max_bin': 316}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:10:54,873] Trial 44 finished with value: 0.7256798390769275 and parameters: {'n_estimators': 354, 'eta': 0.04328582232564121, 'max_depth': 10, 'alpha': 0.3115, 'lambda': 9.299056468980629, 'max_bin': 292}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:10:57,850] Trial 45 finished with value: 0.6949888993105147 and parameters: {'n_estimators': 112, 'eta': 0.04971647204116223, 'max_depth': 9, 'alpha': 0.6898000000000001, 'lambda': 14.796898890472754, 'max_bin': 370}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:11:05,483] Trial 46 finished with value: 0.7222869989704004 and parameters: {'n_estimators': 254, 'eta': 0.06927407722034046, 'max_depth': 11, 'alpha': 0.1272, 'lambda': 17.911978050614962, 'max_bin': 346}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:11:12,770] Trial 47 finished with value: 0.7266332957925792 and parameters: {'n_estimators': 268, 'eta': 0.0559161444554978, 'max_depth': 10, 'alpha': 0.25570000000000004, 'lambda': 15.073341250935439, 'max_bin': 269}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:11:19,664] Trial 48 finished with value: 0.7223070023245606 and parameters: {'n_estimators': 374, 'eta': 0.07472911445741987, 'max_depth': 8, 'alpha': 0.40280000000000005, 'lambda': 12.571905741092852, 'max_bin': 285}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:11:34,214] Trial 49 finished with value: 0.7236687244191408 and parameters: {'n_estimators': 536, 'eta': 0.03485580398440402, 'max_depth': 11, 'alpha': 0.5557, 'lambda': 18.616179896086184, 'max_bin': 301}. Best is trial 21 with value: 0.7296504000963024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.7297\n",
      "\tBest params:\n",
      "\t\tn_estimators: 886\n",
      "\t\teta: 0.039303415262369676\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.3921\n",
      "\t\tlambda: 11.481057191444393\n",
      "\t\tmax_bin: 312\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBRegressor\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.703752\n",
      "1                    TP   34.000000\n",
      "2                    TN  304.000000\n",
      "3                    FP   12.000000\n",
      "4                    FN   32.000000\n",
      "5              Accuracy    0.884817\n",
      "6             Precision    0.739130\n",
      "7           Sensitivity    0.515152\n",
      "8           Specificity    0.962000\n",
      "9              F1 score    0.607143\n",
      "10  F1 score (weighted)    0.876299\n",
      "11     F1 score (macro)    0.769829\n",
      "12    Balanced Accuracy    0.738588\n",
      "13                  MCC    0.554302\n",
      "14                  NPV    0.904800\n",
      "15              ROC_AUC    0.738588\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_xgb_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_xgb_0_cat = np.where(((y_pred_xgb_0 >= 2) | (y_pred_xgb_0 <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 16:11:48,858] Trial 50 finished with value: 0.7182347595523727 and parameters: {'n_estimators': 695, 'eta': 0.03843199838546809, 'max_depth': 11, 'alpha': 0.6559, 'lambda': 15.008664215793877, 'max_bin': 428}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:11:58,114] Trial 51 finished with value: 0.7140854767415538 and parameters: {'n_estimators': 615, 'eta': 0.06262288417498246, 'max_depth': 10, 'alpha': 0.4974, 'lambda': 16.473160235975993, 'max_bin': 339}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:12:05,961] Trial 52 finished with value: 0.719252244033832 and parameters: {'n_estimators': 588, 'eta': 0.06477569225770645, 'max_depth': 9, 'alpha': 0.46480000000000005, 'lambda': 15.343920759478047, 'max_bin': 331}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:12:15,274] Trial 53 finished with value: 0.7159567558851819 and parameters: {'n_estimators': 467, 'eta': 0.05962763991982807, 'max_depth': 10, 'alpha': 0.3274, 'lambda': 20.103850577279847, 'max_bin': 312}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:12:25,014] Trial 54 finished with value: 0.7126592506106006 and parameters: {'n_estimators': 751, 'eta': 0.0555083266705808, 'max_depth': 10, 'alpha': 0.4082, 'lambda': 17.19462490699078, 'max_bin': 321}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:12:37,668] Trial 55 finished with value: 0.7144639261710048 and parameters: {'n_estimators': 521, 'eta': 0.029982224732704642, 'max_depth': 11, 'alpha': 0.5273, 'lambda': 10.465822620551121, 'max_bin': 357}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:12:45,090] Trial 56 finished with value: 0.7132454885915067 and parameters: {'n_estimators': 224, 'eta': 0.04354303140046245, 'max_depth': 12, 'alpha': 0.3734, 'lambda': 13.254212207285121, 'max_bin': 297}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:12:54,211] Trial 57 finished with value: 0.7143527759937792 and parameters: {'n_estimators': 637, 'eta': 0.04852004825023082, 'max_depth': 9, 'alpha': 0.43310000000000004, 'lambda': 11.856673345931355, 'max_bin': 387}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:13:05,593] Trial 58 finished with value: 0.7148846898194812 and parameters: {'n_estimators': 859, 'eta': 0.039926437510010175, 'max_depth': 12, 'alpha': 0.5903, 'lambda': 8.395254418175504, 'max_bin': 328}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:13:17,773] Trial 59 finished with value: 0.7152848102792377 and parameters: {'n_estimators': 685, 'eta': 0.03503366900230608, 'max_depth': 10, 'alpha': 0.4761, 'lambda': 10.404059875558715, 'max_bin': 346}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:13:26,205] Trial 60 finished with value: 0.7104927996554394 and parameters: {'n_estimators': 574, 'eta': 0.052764714324839326, 'max_depth': 7, 'alpha': 0.3372, 'lambda': 13.893543995029091, 'max_bin': 284}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:13:37,689] Trial 61 finished with value: 0.7172369913994199 and parameters: {'n_estimators': 794, 'eta': 0.04678180768708472, 'max_depth': 12, 'alpha': 0.37570000000000003, 'lambda': 12.789645182848945, 'max_bin': 312}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:13:51,755] Trial 62 finished with value: 0.7150059215421023 and parameters: {'n_estimators': 875, 'eta': 0.04400320029810675, 'max_depth': 12, 'alpha': 0.5255000000000001, 'lambda': 16.311029773481643, 'max_bin': 305}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:14:00,030] Trial 63 finished with value: 0.715263030975244 and parameters: {'n_estimators': 712, 'eta': 0.05022900140228655, 'max_depth': 11, 'alpha': 0.28340000000000004, 'lambda': 7.191998973244978, 'max_bin': 317}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:14:14,523] Trial 64 finished with value: 0.7174248365555725 and parameters: {'n_estimators': 827, 'eta': 0.03667538216584172, 'max_depth': 12, 'alpha': 0.2373, 'lambda': 14.218957088955882, 'max_bin': 298}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:14:26,990] Trial 65 finished with value: 0.7163250626906377 and parameters: {'n_estimators': 774, 'eta': 0.04774105501711258, 'max_depth': 11, 'alpha': 0.34950000000000003, 'lambda': 18.665686982648197, 'max_bin': 284}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:14:38,157] Trial 66 finished with value: 0.7176008469146702 and parameters: {'n_estimators': 743, 'eta': 0.04190763045881775, 'max_depth': 12, 'alpha': 0.1945, 'lambda': 10.780147918501227, 'max_bin': 308}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:14:55,200] Trial 67 finished with value: 0.7160860098174444 and parameters: {'n_estimators': 816, 'eta': 0.02550663196523339, 'max_depth': 11, 'alpha': 0.4218, 'lambda': 15.912284613115755, 'max_bin': 463}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:15:05,667] Trial 68 finished with value: 0.7194623614578748 and parameters: {'n_estimators': 855, 'eta': 0.05449227338406686, 'max_depth': 12, 'alpha': 0.4566, 'lambda': 13.07734233733588, 'max_bin': 325}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:15:15,998] Trial 69 finished with value: 0.7176449604050663 and parameters: {'n_estimators': 622, 'eta': 0.04536644410247651, 'max_depth': 12, 'alpha': 0.5041, 'lambda': 9.572805446052673, 'max_bin': 335}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:15:26,692] Trial 70 finished with value: 0.7156630924079654 and parameters: {'n_estimators': 875, 'eta': 0.03997439641443517, 'max_depth': 9, 'alpha': 0.3844, 'lambda': 11.444283212980048, 'max_bin': 263}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:15:38,292] Trial 71 finished with value: 0.7137448305591059 and parameters: {'n_estimators': 794, 'eta': 0.0409587564774571, 'max_depth': 12, 'alpha': 0.3846, 'lambda': 6.761431826206034, 'max_bin': 313}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:15:51,078] Trial 72 finished with value: 0.71387544248227 and parameters: {'n_estimators': 805, 'eta': 0.036642207570617175, 'max_depth': 12, 'alpha': 0.3052, 'lambda': 9.88625695191333, 'max_bin': 295}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:16:01,286] Trial 73 finished with value: 0.7185245072536379 and parameters: {'n_estimators': 674, 'eta': 0.0503400363015834, 'max_depth': 11, 'alpha': 0.4378, 'lambda': 8.364310186631613, 'max_bin': 321}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:16:16,770] Trial 74 finished with value: 0.7184997703621623 and parameters: {'n_estimators': 828, 'eta': 0.03164035758013828, 'max_depth': 12, 'alpha': 0.3588, 'lambda': 12.060518974027612, 'max_bin': 278}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:16:28,674] Trial 75 finished with value: 0.7143281307469838 and parameters: {'n_estimators': 709, 'eta': 0.04482924856084577, 'max_depth': 11, 'alpha': 0.4829, 'lambda': 14.487845656768027, 'max_bin': 309}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:16:42,944] Trial 76 finished with value: 0.7169481333883214 and parameters: {'n_estimators': 895, 'eta': 0.03371420464969276, 'max_depth': 10, 'alpha': 0.4116, 'lambda': 17.626975485143575, 'max_bin': 333}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:16:54,032] Trial 77 finished with value: 0.7172783528565534 and parameters: {'n_estimators': 768, 'eta': 0.05871578676722318, 'max_depth': 12, 'alpha': 0.32180000000000003, 'lambda': 13.523186986247406, 'max_bin': 301}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:17:02,038] Trial 78 finished with value: 0.7032886466399572 and parameters: {'n_estimators': 865, 'eta': 0.05192973300924359, 'max_depth': 5, 'alpha': 0.278, 'lambda': 10.873228164781459, 'max_bin': 291}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:17:10,467] Trial 79 finished with value: 0.7140317316192215 and parameters: {'n_estimators': 302, 'eta': 0.047527011031549396, 'max_depth': 11, 'alpha': 0.5658000000000001, 'lambda': 15.992630197957752, 'max_bin': 363}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:17:19,594] Trial 80 finished with value: 0.717208784393476 and parameters: {'n_estimators': 388, 'eta': 0.04248157846344514, 'max_depth': 10, 'alpha': 0.521, 'lambda': 9.217305821361084, 'max_bin': 342}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:17:30,864] Trial 81 finished with value: 0.7165373307405826 and parameters: {'n_estimators': 666, 'eta': 0.039438697412959724, 'max_depth': 11, 'alpha': 0.8419000000000001, 'lambda': 12.514512239922471, 'max_bin': 317}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:17:43,852] Trial 82 finished with value: 0.7151423589411166 and parameters: {'n_estimators': 837, 'eta': 0.032008415522149364, 'max_depth': 12, 'alpha': 0.7943, 'lambda': 6.281203897018104, 'max_bin': 326}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:17:55,314] Trial 83 finished with value: 0.7176201911563376 and parameters: {'n_estimators': 733, 'eta': 0.03695123048383514, 'max_depth': 11, 'alpha': 0.9127000000000001, 'lambda': 8.084205554111719, 'max_bin': 352}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:18:08,746] Trial 84 finished with value: 0.7166634087580517 and parameters: {'n_estimators': 782, 'eta': 0.02871051641772669, 'max_depth': 10, 'alpha': 0.3927, 'lambda': 9.996352052794858, 'max_bin': 339}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:18:17,504] Trial 85 finished with value: 0.7104689179278381 and parameters: {'n_estimators': 726, 'eta': 0.04488757975752758, 'max_depth': 12, 'alpha': 0.4551, 'lambda': 3.609865633580905, 'max_bin': 304}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:18:31,259] Trial 86 finished with value: 0.714345932894123 and parameters: {'n_estimators': 757, 'eta': 0.034538968522352734, 'max_depth': 11, 'alpha': 0.051300000000000005, 'lambda': 11.583669111646644, 'max_bin': 346}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:18:43,413] Trial 87 finished with value: 0.7166942028692331 and parameters: {'n_estimators': 648, 'eta': 0.03861140487320992, 'max_depth': 10, 'alpha': 0.42460000000000003, 'lambda': 14.558241544466972, 'max_bin': 316}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:18:53,395] Trial 88 finished with value: 0.7176754798313001 and parameters: {'n_estimators': 695, 'eta': 0.05426826208964937, 'max_depth': 12, 'alpha': 0.3537, 'lambda': 8.935125588860576, 'max_bin': 323}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:19:01,571] Trial 89 finished with value: 0.7137735614536823 and parameters: {'n_estimators': 553, 'eta': 0.04919912109667021, 'max_depth': 9, 'alpha': 0.641, 'lambda': 5.872916784855334, 'max_bin': 309}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:19:04,505] Trial 90 finished with value: 0.6807566025183805 and parameters: {'n_estimators': 175, 'eta': 0.04133353372293369, 'max_depth': 6, 'alpha': 0.7603000000000001, 'lambda': 7.941860514625451, 'max_bin': 332}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:19:17,476] Trial 91 finished with value: 0.7197480550273148 and parameters: {'n_estimators': 810, 'eta': 0.047065997069694665, 'max_depth': 12, 'alpha': 0.30610000000000004, 'lambda': 13.40473673551974, 'max_bin': 300}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:19:29,709] Trial 92 finished with value: 0.7186169988055717 and parameters: {'n_estimators': 753, 'eta': 0.04280968708387973, 'max_depth': 12, 'alpha': 0.22390000000000002, 'lambda': 14.132220533348686, 'max_bin': 280}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:19:39,762] Trial 93 finished with value: 0.7213367423695446 and parameters: {'n_estimators': 597, 'eta': 0.05119699348789421, 'max_depth': 12, 'alpha': 0.15410000000000001, 'lambda': 12.216560804927362, 'max_bin': 291}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:19:53,192] Trial 94 finished with value: 0.715772535400392 and parameters: {'n_estimators': 842, 'eta': 0.04635999617567874, 'max_depth': 12, 'alpha': 0.34340000000000004, 'lambda': 15.304907288075261, 'max_bin': 304}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:20:08,586] Trial 95 finished with value: 0.7168282557123603 and parameters: {'n_estimators': 899, 'eta': 0.038723374865614536, 'max_depth': 11, 'alpha': 0.2641, 'lambda': 16.683313183315136, 'max_bin': 314}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:20:16,663] Trial 96 finished with value: 0.7198083182811519 and parameters: {'n_estimators': 769, 'eta': 0.06059833114480625, 'max_depth': 10, 'alpha': 0.3637, 'lambda': 11.05612789839831, 'max_bin': 329}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:20:25,413] Trial 97 finished with value: 0.7102689785193848 and parameters: {'n_estimators': 803, 'eta': 0.04943771094176516, 'max_depth': 8, 'alpha': 0.4035, 'lambda': 12.962284833450754, 'max_bin': 318}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:20:33,889] Trial 98 finished with value: 0.7166317010014495 and parameters: {'n_estimators': 518, 'eta': 0.05593428340670603, 'max_depth': 12, 'alpha': 0.9601000000000001, 'lambda': 10.298555066822681, 'max_bin': 295}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:20:43,328] Trial 99 finished with value: 0.718908341830659 and parameters: {'n_estimators': 460, 'eta': 0.05683463924691822, 'max_depth': 11, 'alpha': 0.2924, 'lambda': 15.192661631871509, 'max_bin': 288}. Best is trial 21 with value: 0.7296504000963024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7297\n",
      "\tBest params:\n",
      "\t\tn_estimators: 886\n",
      "\t\teta: 0.039303415262369676\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.3921\n",
      "\t\tlambda: 11.481057191444393\n",
      "\t\tmax_bin: 312\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.703752    0.746467\n",
      "1                    TP   34.000000   37.000000\n",
      "2                    TN  304.000000  305.000000\n",
      "3                    FP   12.000000   10.000000\n",
      "4                    FN   32.000000   30.000000\n",
      "5              Accuracy    0.884817    0.895288\n",
      "6             Precision    0.739130    0.787234\n",
      "7           Sensitivity    0.515152    0.552239\n",
      "8           Specificity    0.962000    0.968300\n",
      "9              F1 score    0.607143    0.649123\n",
      "10  F1 score (weighted)    0.876299    0.887714\n",
      "11     F1 score (macro)    0.769829    0.793792\n",
      "12    Balanced Accuracy    0.738588    0.760246\n",
      "13                  MCC    0.554302    0.602610\n",
      "14                  NPV    0.904800    0.910400\n",
      "15              ROC_AUC    0.738588    0.760246\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_xgb_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_1_cat = np.where(((y_pred_xgb_1 >= 2) | (y_pred_xgb_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 16:21:04,612] Trial 100 finished with value: 0.7201927137680045 and parameters: {'n_estimators': 879, 'eta': 0.036021897574469434, 'max_depth': 12, 'alpha': 0.4409, 'lambda': 17.14508514693859, 'max_bin': 340}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:21:21,269] Trial 101 finished with value: 0.721287874369007 and parameters: {'n_estimators': 860, 'eta': 0.04169670036789745, 'max_depth': 11, 'alpha': 0.32580000000000003, 'lambda': 20.76243743107996, 'max_bin': 267}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:21:23,587] Trial 102 finished with value: 0.6543810554656387 and parameters: {'n_estimators': 71, 'eta': 0.043685723401178274, 'max_depth': 10, 'alpha': 0.38520000000000004, 'lambda': 13.927028972722182, 'max_bin': 271}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:21:38,488] Trial 103 finished with value: 0.7174339014400204 and parameters: {'n_estimators': 838, 'eta': 0.04581797059429766, 'max_depth': 11, 'alpha': 0.47350000000000003, 'lambda': 15.86147642148112, 'max_bin': 250}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:21:53,789] Trial 104 finished with value: 0.7165678305532089 and parameters: {'n_estimators': 882, 'eta': 0.039981111288174644, 'max_depth': 11, 'alpha': 0.373, 'lambda': 11.310401510975318, 'max_bin': 309}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:22:06,636] Trial 105 finished with value: 0.7175516741655049 and parameters: {'n_estimators': 780, 'eta': 0.05274127669481334, 'max_depth': 12, 'alpha': 0.4182, 'lambda': 12.461119865392309, 'max_bin': 401}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:22:19,887] Trial 106 finished with value: 0.7182786367728433 and parameters: {'n_estimators': 712, 'eta': 0.03791223237226133, 'max_depth': 11, 'alpha': 0.2484, 'lambda': 9.017808835617899, 'max_bin': 322}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:22:33,508] Trial 107 finished with value: 0.7186634435111515 and parameters: {'n_estimators': 637, 'eta': 0.047870701189273834, 'max_depth': 12, 'alpha': 0.7004, 'lambda': 14.59292439715769, 'max_bin': 260}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:22:49,851] Trial 108 finished with value: 0.7161133851856333 and parameters: {'n_estimators': 822, 'eta': 0.03284482067774387, 'max_depth': 9, 'alpha': 0.0048000000000000004, 'lambda': 18.020168700000678, 'max_bin': 302}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:23:02,897] Trial 109 finished with value: 0.7177792988266998 and parameters: {'n_estimators': 734, 'eta': 0.042758333961115426, 'max_depth': 10, 'alpha': 0.3345, 'lambda': 9.862467471494089, 'max_bin': 312}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:23:17,351] Trial 110 finished with value: 0.71606103041724 and parameters: {'n_estimators': 899, 'eta': 0.03507599051679878, 'max_depth': 12, 'alpha': 0.4928, 'lambda': 7.249598665462559, 'max_bin': 257}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:23:29,813] Trial 111 finished with value: 0.7135838918254478 and parameters: {'n_estimators': 671, 'eta': 0.04996997651756404, 'max_depth': 12, 'alpha': 0.2167, 'lambda': 13.137906233394784, 'max_bin': 281}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:23:43,656] Trial 112 finished with value: 0.7165750095271155 and parameters: {'n_estimators': 698, 'eta': 0.04445625824510905, 'max_depth': 12, 'alpha': 0.2972, 'lambda': 11.858047237324351, 'max_bin': 296}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:23:56,435] Trial 113 finished with value: 0.7166087811233105 and parameters: {'n_estimators': 613, 'eta': 0.05313330549381542, 'max_depth': 12, 'alpha': 0.1459, 'lambda': 13.790025895374136, 'max_bin': 306}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:24:07,139] Trial 114 finished with value: 0.715154204414475 and parameters: {'n_estimators': 595, 'eta': 0.06199981651171216, 'max_depth': 11, 'alpha': 0.4489, 'lambda': 15.538469667484526, 'max_bin': 325}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:24:20,488] Trial 115 finished with value: 0.717062826926875 and parameters: {'n_estimators': 660, 'eta': 0.04606658866042444, 'max_depth': 12, 'alpha': 0.4053, 'lambda': 11.299381054272137, 'max_bin': 318}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:24:33,543] Trial 116 finished with value: 0.7195428818454918 and parameters: {'n_estimators': 683, 'eta': 0.05107577382916297, 'max_depth': 11, 'alpha': 0.36410000000000003, 'lambda': 16.825264865213704, 'max_bin': 336}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:24:44,957] Trial 117 finished with value: 0.716478044789854 and parameters: {'n_estimators': 626, 'eta': 0.05775977855269252, 'max_depth': 12, 'alpha': 0.41950000000000004, 'lambda': 12.601637084847592, 'max_bin': 312}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:24:56,386] Trial 118 finished with value: 0.7192887457028558 and parameters: {'n_estimators': 744, 'eta': 0.04164869750347813, 'max_depth': 10, 'alpha': 0.34750000000000003, 'lambda': 10.710662335837695, 'max_bin': 288}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:25:03,438] Trial 119 finished with value: 0.7112402945226235 and parameters: {'n_estimators': 853, 'eta': 0.06351436089552638, 'max_depth': 12, 'alpha': 0.5117, 'lambda': 1.0116913194512946, 'max_bin': 299}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:25:15,642] Trial 120 finished with value: 0.7170315012390311 and parameters: {'n_estimators': 873, 'eta': 0.054475618991144066, 'max_depth': 11, 'alpha': 0.201, 'lambda': 13.54076625792041, 'max_bin': 500}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:25:27,260] Trial 121 finished with value: 0.7202017045430298 and parameters: {'n_estimators': 646, 'eta': 0.06722923464657252, 'max_depth': 10, 'alpha': 0.544, 'lambda': 21.48936387838481, 'max_bin': 306}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:25:38,385] Trial 122 finished with value: 0.7208247530602315 and parameters: {'n_estimators': 572, 'eta': 0.06723321071320477, 'max_depth': 10, 'alpha': 0.4675, 'lambda': 22.495415941702735, 'max_bin': 291}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:25:49,937] Trial 123 finished with value: 0.7186469961522254 and parameters: {'n_estimators': 602, 'eta': 0.059258258918295705, 'max_depth': 10, 'alpha': 0.3914, 'lambda': 19.263860999163427, 'max_bin': 328}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:25:59,021] Trial 124 finished with value: 0.7160772071786048 and parameters: {'n_estimators': 723, 'eta': 0.0748158761992323, 'max_depth': 9, 'alpha': 0.3176, 'lambda': 14.677491352722207, 'max_bin': 299}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:26:14,943] Trial 125 finished with value: 0.7209772830433046 and parameters: {'n_estimators': 812, 'eta': 0.040224296720587846, 'max_depth': 10, 'alpha': 0.42600000000000005, 'lambda': 23.97881525577772, 'max_bin': 320}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:26:29,796] Trial 126 finished with value: 0.720983359419356 and parameters: {'n_estimators': 787, 'eta': 0.048328239892780185, 'max_depth': 12, 'alpha': 0.5746, 'lambda': 16.109265689721838, 'max_bin': 311}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:26:41,115] Trial 127 finished with value: 0.7157213814558261 and parameters: {'n_estimators': 409, 'eta': 0.03765452402406104, 'max_depth': 11, 'alpha': 0.5362, 'lambda': 12.037027673779617, 'max_bin': 294}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:26:55,551] Trial 128 finished with value: 0.7196443047351141 and parameters: {'n_estimators': 760, 'eta': 0.04305017065866609, 'max_depth': 12, 'alpha': 0.4867, 'lambda': 18.15019237536649, 'max_bin': 285}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:27:06,026] Trial 129 finished with value: 0.7195431336144723 and parameters: {'n_estimators': 488, 'eta': 0.04574312487130568, 'max_depth': 10, 'alpha': 0.4445, 'lambda': 9.917814732081032, 'max_bin': 303}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:27:19,620] Trial 130 finished with value: 0.7179313284288436 and parameters: {'n_estimators': 554, 'eta': 0.04839498314207142, 'max_depth': 12, 'alpha': 0.27240000000000003, 'lambda': 17.407098721843568, 'max_bin': 273}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:27:25,729] Trial 131 finished with value: 0.7109761548400124 and parameters: {'n_estimators': 227, 'eta': 0.055579340688836354, 'max_depth': 10, 'alpha': 0.2447, 'lambda': 15.005539522552588, 'max_bin': 265}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:27:33,339] Trial 132 finished with value: 0.7153893238106771 and parameters: {'n_estimators': 285, 'eta': 0.05143689463428445, 'max_depth': 10, 'alpha': 0.2644, 'lambda': 14.252289263774898, 'max_bin': 253}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:27:41,422] Trial 133 finished with value: 0.7132853238248151 and parameters: {'n_estimators': 354, 'eta': 0.05430026622322554, 'max_depth': 9, 'alpha': 0.17450000000000002, 'lambda': 13.204673426999582, 'max_bin': 257}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:27:45,896] Trial 134 finished with value: 0.7046770070283591 and parameters: {'n_estimators': 159, 'eta': 0.05738195893271193, 'max_depth': 10, 'alpha': 0.37170000000000003, 'lambda': 15.61515621326619, 'max_bin': 275}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:27:55,002] Trial 135 finished with value: 0.7160176490361287 and parameters: {'n_estimators': 296, 'eta': 0.0443500227254553, 'max_depth': 11, 'alpha': 0.10880000000000001, 'lambda': 25.064233689574877, 'max_bin': 453}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:28:03,539] Trial 136 finished with value: 0.7181101503009688 and parameters: {'n_estimators': 261, 'eta': 0.06079903406449157, 'max_depth': 12, 'alpha': 0.3408, 'lambda': 19.132126449437767, 'max_bin': 314}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:28:11,141] Trial 137 finished with value: 0.7182822425261168 and parameters: {'n_estimators': 321, 'eta': 0.07072495854705826, 'max_depth': 10, 'alpha': 0.3905, 'lambda': 16.668338925260997, 'max_bin': 281}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:28:27,504] Trial 138 finished with value: 0.7173734529373965 and parameters: {'n_estimators': 839, 'eta': 0.03386297195922283, 'max_depth': 11, 'alpha': 0.3088, 'lambda': 8.25719292097714, 'max_bin': 270}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:28:34,312] Trial 139 finished with value: 0.7090271453801685 and parameters: {'n_estimators': 201, 'eta': 0.04015763800250582, 'max_depth': 12, 'alpha': 0.23240000000000002, 'lambda': 12.578259896347356, 'max_bin': 304}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:28:47,649] Trial 140 finished with value: 0.7201968098459951 and parameters: {'n_estimators': 881, 'eta': 0.036202560431314916, 'max_depth': 8, 'alpha': 0.8466, 'lambda': 10.772383299628444, 'max_bin': 323}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:28:59,760] Trial 141 finished with value: 0.7169314786218826 and parameters: {'n_estimators': 839, 'eta': 0.04779353224807013, 'max_depth': 11, 'alpha': 0.43420000000000003, 'lambda': 9.585362791199945, 'max_bin': 328}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:29:12,311] Trial 142 finished with value: 0.7170152230642021 and parameters: {'n_estimators': 861, 'eta': 0.05027445169707829, 'max_depth': 11, 'alpha': 0.4607, 'lambda': 13.964461862153504, 'max_bin': 308}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:29:25,880] Trial 143 finished with value: 0.7182451169425804 and parameters: {'n_estimators': 827, 'eta': 0.047047783013369524, 'max_depth': 11, 'alpha': 0.5955, 'lambda': 11.653128843082767, 'max_bin': 318}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:29:36,390] Trial 144 finished with value: 0.7206307579941044 and parameters: {'n_estimators': 631, 'eta': 0.052279146233509176, 'max_depth': 10, 'alpha': 0.4051, 'lambda': 8.717421252658102, 'max_bin': 298}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:29:52,452] Trial 145 finished with value: 0.7223081361074587 and parameters: {'n_estimators': 867, 'eta': 0.0418041254096585, 'max_depth': 12, 'alpha': 0.3618, 'lambda': 12.045007111270547, 'max_bin': 333}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:30:04,606] Trial 146 finished with value: 0.7163483652848605 and parameters: {'n_estimators': 892, 'eta': 0.043826243461347114, 'max_depth': 11, 'alpha': 0.3246, 'lambda': 10.595624427741356, 'max_bin': 315}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:30:18,217] Trial 147 finished with value: 0.7190611445044512 and parameters: {'n_estimators': 655, 'eta': 0.049309650532485594, 'max_depth': 10, 'alpha': 0.3834, 'lambda': 15.130902799929178, 'max_bin': 345}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:30:31,533] Trial 148 finished with value: 0.7182007669823002 and parameters: {'n_estimators': 707, 'eta': 0.04554198230411542, 'max_depth': 12, 'alpha': 0.4253, 'lambda': 12.955469253619658, 'max_bin': 369}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:30:48,094] Trial 149 finished with value: 0.7190863967079119 and parameters: {'n_estimators': 801, 'eta': 0.039776642550340205, 'max_depth': 12, 'alpha': 0.4697, 'lambda': 13.776099297039309, 'max_bin': 308}. Best is trial 21 with value: 0.7296504000963024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7297\n",
      "\tBest params:\n",
      "\t\tn_estimators: 886\n",
      "\t\teta: 0.039303415262369676\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.3921\n",
      "\t\tlambda: 11.481057191444393\n",
      "\t\tmax_bin: 312\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.703752    0.746467    0.736627\n",
      "1                    TP   34.000000   37.000000   37.000000\n",
      "2                    TN  304.000000  305.000000  308.000000\n",
      "3                    FP   12.000000   10.000000    6.000000\n",
      "4                    FN   32.000000   30.000000   31.000000\n",
      "5              Accuracy    0.884817    0.895288    0.903141\n",
      "6             Precision    0.739130    0.787234    0.860465\n",
      "7           Sensitivity    0.515152    0.552239    0.544118\n",
      "8           Specificity    0.962000    0.968300    0.980900\n",
      "9              F1 score    0.607143    0.649123    0.666667\n",
      "10  F1 score (weighted)    0.876299    0.887714    0.894088\n",
      "11     F1 score (macro)    0.769829    0.793792    0.805003\n",
      "12    Balanced Accuracy    0.738588    0.760246    0.762505\n",
      "13                  MCC    0.554302    0.602610    0.635407\n",
      "14                  NPV    0.904800    0.910400    0.908600\n",
      "15              ROC_AUC    0.738588    0.760246    0.762505\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_xgb_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_2_cat = np.where(((y_pred_xgb_2 >= 2) | (y_pred_xgb_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 16:31:00,974] Trial 150 finished with value: 0.7269194639345007 and parameters: {'n_estimators': 611, 'eta': 0.05853101154193337, 'max_depth': 11, 'alpha': 0.501, 'lambda': 7.487954049074521, 'max_bin': 290}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:31:11,327] Trial 151 finished with value: 0.7192096246618392 and parameters: {'n_estimators': 576, 'eta': 0.059066058050076, 'max_depth': 11, 'alpha': 0.509, 'lambda': 9.195321189053578, 'max_bin': 288}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:31:20,152] Trial 152 finished with value: 0.7242539466756162 and parameters: {'n_estimators': 619, 'eta': 0.06443833797785972, 'max_depth': 11, 'alpha': 0.4928, 'lambda': 5.5962276423353705, 'max_bin': 324}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:31:30,805] Trial 153 finished with value: 0.7229010696054321 and parameters: {'n_estimators': 519, 'eta': 0.05636390475866544, 'max_depth': 11, 'alpha': 0.4461, 'lambda': 7.448994822141456, 'max_bin': 384}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:31:40,161] Trial 154 finished with value: 0.7235540080119753 and parameters: {'n_estimators': 582, 'eta': 0.06171134017177725, 'max_depth': 11, 'alpha': 0.2891, 'lambda': 6.252591741246831, 'max_bin': 296}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:31:51,893] Trial 155 finished with value: 0.7250696747211546 and parameters: {'n_estimators': 693, 'eta': 0.05268979830053856, 'max_depth': 10, 'alpha': 0.40690000000000004, 'lambda': 11.198066757775011, 'max_bin': 312}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:32:05,045] Trial 156 finished with value: 0.7250585518547186 and parameters: {'n_estimators': 671, 'eta': 0.057889922124075864, 'max_depth': 12, 'alpha': 0.3578, 'lambda': 14.61124092571726, 'max_bin': 302}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:32:19,831] Trial 157 finished with value: 0.7218859633792495 and parameters: {'n_estimators': 899, 'eta': 0.030865254497118903, 'max_depth': 10, 'alpha': 0.2112, 'lambda': 4.43680716150498, 'max_bin': 291}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:32:30,929] Trial 158 finished with value: 0.7255267418521641 and parameters: {'n_estimators': 857, 'eta': 0.05496587817497328, 'max_depth': 12, 'alpha': 0.7483000000000001, 'lambda': 10.167421278583474, 'max_bin': 307}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:32:46,533] Trial 159 finished with value: 0.727635386944481 and parameters: {'n_estimators': 605, 'eta': 0.03855023696117108, 'max_depth': 11, 'alpha': 0.252, 'lambda': 20.211814369725317, 'max_bin': 320}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:33:02,296] Trial 160 finished with value: 0.7268769983361871 and parameters: {'n_estimators': 605, 'eta': 0.037297113900158316, 'max_depth': 11, 'alpha': 0.1895, 'lambda': 21.37419064437305, 'max_bin': 318}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:33:19,134] Trial 161 finished with value: 0.7275230154753038 and parameters: {'n_estimators': 614, 'eta': 0.036806031506644306, 'max_depth': 11, 'alpha': 0.17700000000000002, 'lambda': 21.52588181550824, 'max_bin': 319}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:33:34,802] Trial 162 finished with value: 0.7279786803737496 and parameters: {'n_estimators': 615, 'eta': 0.03805393508605937, 'max_depth': 11, 'alpha': 0.1438, 'lambda': 19.99548835603551, 'max_bin': 320}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:33:50,646] Trial 163 finished with value: 0.7273951731159379 and parameters: {'n_estimators': 588, 'eta': 0.03781066966130847, 'max_depth': 11, 'alpha': 0.17420000000000002, 'lambda': 19.827742721173916, 'max_bin': 322}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:34:06,172] Trial 164 finished with value: 0.7256035372726185 and parameters: {'n_estimators': 539, 'eta': 0.03543448636100331, 'max_depth': 11, 'alpha': 0.1608, 'lambda': 20.7255098332578, 'max_bin': 328}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:34:21,279] Trial 165 finished with value: 0.7281968152143994 and parameters: {'n_estimators': 639, 'eta': 0.038459880985180134, 'max_depth': 11, 'alpha': 0.1087, 'lambda': 18.544994102724885, 'max_bin': 337}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:34:36,818] Trial 166 finished with value: 0.7270489233560888 and parameters: {'n_estimators': 649, 'eta': 0.037467515199593626, 'max_depth': 11, 'alpha': 0.10740000000000001, 'lambda': 19.987228419559305, 'max_bin': 335}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:34:52,269] Trial 167 finished with value: 0.7278521208823122 and parameters: {'n_estimators': 634, 'eta': 0.039190742793650854, 'max_depth': 11, 'alpha': 0.1854, 'lambda': 18.49100887159297, 'max_bin': 341}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:35:07,989] Trial 168 finished with value: 0.7278465030264709 and parameters: {'n_estimators': 629, 'eta': 0.0392910994522949, 'max_depth': 11, 'alpha': 0.1337, 'lambda': 19.613002120045813, 'max_bin': 348}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:35:24,872] Trial 169 finished with value: 0.7264277680402607 and parameters: {'n_estimators': 588, 'eta': 0.033681542096688805, 'max_depth': 11, 'alpha': 0.0592, 'lambda': 19.754529591961976, 'max_bin': 356}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:35:39,730] Trial 170 finished with value: 0.7275695012036639 and parameters: {'n_estimators': 633, 'eta': 0.038484607894661994, 'max_depth': 11, 'alpha': 0.1337, 'lambda': 18.1413656298579, 'max_bin': 351}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:35:56,009] Trial 171 finished with value: 0.7275249860014998 and parameters: {'n_estimators': 632, 'eta': 0.03733926857442437, 'max_depth': 11, 'alpha': 0.1206, 'lambda': 18.789004403187874, 'max_bin': 350}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:36:11,440] Trial 172 finished with value: 0.7285550151925202 and parameters: {'n_estimators': 625, 'eta': 0.03953261242635001, 'max_depth': 11, 'alpha': 0.11570000000000001, 'lambda': 18.435193014750208, 'max_bin': 354}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:36:26,638] Trial 173 finished with value: 0.7284621834570826 and parameters: {'n_estimators': 635, 'eta': 0.03951100869671391, 'max_depth': 11, 'alpha': 0.13040000000000002, 'lambda': 18.572311181700062, 'max_bin': 350}. Best is trial 21 with value: 0.7296504000963024.\n",
      "[I 2023-12-20 16:36:41,787] Trial 174 finished with value: 0.7303297461276881 and parameters: {'n_estimators': 644, 'eta': 0.03943097341037755, 'max_depth': 11, 'alpha': 0.1323, 'lambda': 18.59481239921997, 'max_bin': 353}. Best is trial 174 with value: 0.7303297461276881.\n",
      "[I 2023-12-20 16:36:56,220] Trial 175 finished with value: 0.7267891549085947 and parameters: {'n_estimators': 627, 'eta': 0.03975370625577594, 'max_depth': 11, 'alpha': 0.0876, 'lambda': 18.077975360189626, 'max_bin': 348}. Best is trial 174 with value: 0.7303297461276881.\n",
      "[I 2023-12-20 16:37:11,321] Trial 176 finished with value: 0.7283675741835952 and parameters: {'n_estimators': 644, 'eta': 0.04090143552446588, 'max_depth': 11, 'alpha': 0.13770000000000002, 'lambda': 18.628118656411104, 'max_bin': 360}. Best is trial 174 with value: 0.7303297461276881.\n",
      "[I 2023-12-20 16:37:26,352] Trial 177 finished with value: 0.7284494198742223 and parameters: {'n_estimators': 651, 'eta': 0.041529707833638504, 'max_depth': 11, 'alpha': 0.096, 'lambda': 18.955726954203417, 'max_bin': 359}. Best is trial 174 with value: 0.7303297461276881.\n",
      "[I 2023-12-20 16:37:41,529] Trial 178 finished with value: 0.7272597235993823 and parameters: {'n_estimators': 662, 'eta': 0.04186997578036167, 'max_depth': 11, 'alpha': 0.0674, 'lambda': 19.088530501807753, 'max_bin': 363}. Best is trial 174 with value: 0.7303297461276881.\n",
      "[I 2023-12-20 16:37:56,740] Trial 179 finished with value: 0.725915671832616 and parameters: {'n_estimators': 678, 'eta': 0.04120156564047713, 'max_depth': 11, 'alpha': 0.038700000000000005, 'lambda': 17.173328610209747, 'max_bin': 366}. Best is trial 174 with value: 0.7303297461276881.\n",
      "[I 2023-12-20 16:38:13,627] Trial 180 finished with value: 0.7273657165457795 and parameters: {'n_estimators': 644, 'eta': 0.03305592202570465, 'max_depth': 11, 'alpha': 0.1, 'lambda': 18.660494543757753, 'max_bin': 357}. Best is trial 174 with value: 0.7303297461276881.\n",
      "[I 2023-12-20 16:38:29,198] Trial 181 finished with value: 0.7277387165239254 and parameters: {'n_estimators': 656, 'eta': 0.03917804840481848, 'max_depth': 11, 'alpha': 0.1479, 'lambda': 17.462637896793243, 'max_bin': 343}. Best is trial 174 with value: 0.7303297461276881.\n",
      "[I 2023-12-20 16:38:43,844] Trial 182 finished with value: 0.7288536470717235 and parameters: {'n_estimators': 657, 'eta': 0.040598861632643986, 'max_depth': 11, 'alpha': 0.1419, 'lambda': 17.580417318492128, 'max_bin': 342}. Best is trial 174 with value: 0.7303297461276881.\n",
      "[I 2023-12-20 16:38:58,388] Trial 183 finished with value: 0.7295071268708819 and parameters: {'n_estimators': 656, 'eta': 0.042892219543694, 'max_depth': 11, 'alpha': 0.136, 'lambda': 18.17917329347634, 'max_bin': 343}. Best is trial 174 with value: 0.7303297461276881.\n",
      "[I 2023-12-20 16:39:14,141] Trial 184 finished with value: 0.7298178994006788 and parameters: {'n_estimators': 680, 'eta': 0.04294393003628156, 'max_depth': 11, 'alpha': 0.12890000000000001, 'lambda': 18.216519503834125, 'max_bin': 359}. Best is trial 174 with value: 0.7303297461276881.\n",
      "[I 2023-12-20 16:39:29,278] Trial 185 finished with value: 0.7295148491567309 and parameters: {'n_estimators': 681, 'eta': 0.04350131706070962, 'max_depth': 11, 'alpha': 0.12990000000000002, 'lambda': 18.319915702492963, 'max_bin': 359}. Best is trial 174 with value: 0.7303297461276881.\n",
      "[I 2023-12-20 16:39:44,000] Trial 186 finished with value: 0.7298745094489677 and parameters: {'n_estimators': 687, 'eta': 0.04298716119125434, 'max_depth': 11, 'alpha': 0.0852, 'lambda': 18.061197581031653, 'max_bin': 357}. Best is trial 174 with value: 0.7303297461276881.\n",
      "[I 2023-12-20 16:39:59,463] Trial 187 finished with value: 0.7306212017546259 and parameters: {'n_estimators': 685, 'eta': 0.043081219013907206, 'max_depth': 11, 'alpha': 0.0927, 'lambda': 17.82657861016001, 'max_bin': 357}. Best is trial 187 with value: 0.7306212017546259.\n",
      "[I 2023-12-20 16:40:14,933] Trial 188 finished with value: 0.7287402839882611 and parameters: {'n_estimators': 677, 'eta': 0.04336669974899882, 'max_depth': 11, 'alpha': 0.0845, 'lambda': 17.549533941093564, 'max_bin': 358}. Best is trial 187 with value: 0.7306212017546259.\n",
      "[I 2023-12-20 16:40:30,391] Trial 189 finished with value: 0.7286768699525232 and parameters: {'n_estimators': 685, 'eta': 0.04361326849407758, 'max_depth': 11, 'alpha': 0.08660000000000001, 'lambda': 17.55884172275782, 'max_bin': 372}. Best is trial 187 with value: 0.7306212017546259.\n",
      "[I 2023-12-20 16:40:44,308] Trial 190 finished with value: 0.7284476338053695 and parameters: {'n_estimators': 680, 'eta': 0.0445995264379704, 'max_depth': 11, 'alpha': 0.07, 'lambda': 16.494613018943483, 'max_bin': 360}. Best is trial 187 with value: 0.7306212017546259.\n",
      "[I 2023-12-20 16:40:59,213] Trial 191 finished with value: 0.7312920802069302 and parameters: {'n_estimators': 686, 'eta': 0.043404992941413346, 'max_depth': 11, 'alpha': 0.08270000000000001, 'lambda': 17.62920128007734, 'max_bin': 373}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:41:13,657] Trial 192 finished with value: 0.7284512984751526 and parameters: {'n_estimators': 682, 'eta': 0.04359235502315113, 'max_depth': 11, 'alpha': 0.0777, 'lambda': 17.61781542528716, 'max_bin': 376}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:41:27,851] Trial 193 finished with value: 0.7286344634010528 and parameters: {'n_estimators': 683, 'eta': 0.043292976823630445, 'max_depth': 11, 'alpha': 0.0799, 'lambda': 17.23030707828139, 'max_bin': 378}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:41:43,125] Trial 194 finished with value: 0.7293786127828742 and parameters: {'n_estimators': 679, 'eta': 0.042854134070197274, 'max_depth': 11, 'alpha': 0.0794, 'lambda': 17.658712915260534, 'max_bin': 377}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:41:57,914] Trial 195 finished with value: 0.729017657606443 and parameters: {'n_estimators': 690, 'eta': 0.04360543072330397, 'max_depth': 11, 'alpha': 0.0206, 'lambda': 17.331680198715677, 'max_bin': 379}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:42:13,817] Trial 196 finished with value: 0.7274716114846097 and parameters: {'n_estimators': 700, 'eta': 0.04379817131630192, 'max_depth': 11, 'alpha': 0.023700000000000002, 'lambda': 17.352383225118125, 'max_bin': 382}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:42:28,345] Trial 197 finished with value: 0.7261181534658971 and parameters: {'n_estimators': 710, 'eta': 0.042875716136894934, 'max_depth': 11, 'alpha': 0.0425, 'lambda': 16.819133374180694, 'max_bin': 388}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:42:42,521] Trial 198 finished with value: 0.727886293829763 and parameters: {'n_estimators': 695, 'eta': 0.045688228702900034, 'max_depth': 11, 'alpha': 0.0236, 'lambda': 17.9081562936995, 'max_bin': 376}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:42:58,038] Trial 199 finished with value: 0.7274957457911502 and parameters: {'n_estimators': 720, 'eta': 0.04260860779093799, 'max_depth': 11, 'alpha': 0.0821, 'lambda': 16.583358645417295, 'max_bin': 371}. Best is trial 191 with value: 0.7312920802069302.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7313\n",
      "\tBest params:\n",
      "\t\tn_estimators: 686\n",
      "\t\teta: 0.043404992941413346\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.08270000000000001\n",
      "\t\tlambda: 17.62920128007734\n",
      "\t\tmax_bin: 373\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.703752    0.746467    0.736627    0.749621\n",
      "1                    TP   34.000000   37.000000   37.000000   34.000000\n",
      "2                    TN  304.000000  305.000000  308.000000  307.000000\n",
      "3                    FP   12.000000   10.000000    6.000000    9.000000\n",
      "4                    FN   32.000000   30.000000   31.000000   32.000000\n",
      "5              Accuracy    0.884817    0.895288    0.903141    0.892670\n",
      "6             Precision    0.739130    0.787234    0.860465    0.790698\n",
      "7           Sensitivity    0.515152    0.552239    0.544118    0.515152\n",
      "8           Specificity    0.962000    0.968300    0.980900    0.971500\n",
      "9              F1 score    0.607143    0.649123    0.666667    0.623853\n",
      "10  F1 score (weighted)    0.876299    0.887714    0.894088    0.883231\n",
      "11     F1 score (macro)    0.769829    0.793792    0.805003    0.780629\n",
      "12    Balanced Accuracy    0.738588    0.760246    0.762505    0.743335\n",
      "13                  MCC    0.554302    0.602610    0.635407    0.582125\n",
      "14                  NPV    0.904800    0.910400    0.908600    0.905600\n",
      "15              ROC_AUC    0.738588    0.760246    0.762505    0.743335\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_xgb_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_3_cat = np.where(((y_pred_xgb_3 >= 2) | (y_pred_xgb_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 16:43:12,701] Trial 200 finished with value: 0.7111831672945054 and parameters: {'n_estimators': 669, 'eta': 0.04575057078957632, 'max_depth': 11, 'alpha': 0.11720000000000001, 'lambda': 17.709565618035402, 'max_bin': 371}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:43:25,237] Trial 201 finished with value: 0.7082667427112693 and parameters: {'n_estimators': 687, 'eta': 0.043434278099131876, 'max_depth': 11, 'alpha': 0.0809, 'lambda': 17.77914562795708, 'max_bin': 392}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:43:38,038] Trial 202 finished with value: 0.7083317242545897 and parameters: {'n_estimators': 672, 'eta': 0.043690505406312766, 'max_depth': 11, 'alpha': 0.0579, 'lambda': 17.077201043207634, 'max_bin': 381}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:43:51,007] Trial 203 finished with value: 0.7107097806338305 and parameters: {'n_estimators': 687, 'eta': 0.041233910434299345, 'max_depth': 11, 'alpha': 0.0906, 'lambda': 16.261498150004286, 'max_bin': 373}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:44:04,807] Trial 204 finished with value: 0.707558455011072 and parameters: {'n_estimators': 670, 'eta': 0.0448248103043563, 'max_depth': 11, 'alpha': 0.0665, 'lambda': 17.84318236855423, 'max_bin': 367}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:44:17,999] Trial 205 finished with value: 0.7068945375493437 and parameters: {'n_estimators': 700, 'eta': 0.04256858218780722, 'max_depth': 11, 'alpha': 0.1012, 'lambda': 17.227129540046636, 'max_bin': 378}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:44:29,573] Trial 206 finished with value: 0.7071641081302882 and parameters: {'n_estimators': 727, 'eta': 0.04712511846602842, 'max_depth': 11, 'alpha': 0.0017000000000000001, 'lambda': 15.796611061816076, 'max_bin': 355}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:44:41,794] Trial 207 finished with value: 0.7100771454233563 and parameters: {'n_estimators': 683, 'eta': 0.04489015116289497, 'max_depth': 11, 'alpha': 0.11910000000000001, 'lambda': 19.1161014784671, 'max_bin': 376}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:44:54,840] Trial 208 finished with value: 0.7083407695447911 and parameters: {'n_estimators': 659, 'eta': 0.041574038248531495, 'max_depth': 11, 'alpha': 0.0806, 'lambda': 17.937980968499506, 'max_bin': 364}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:45:09,185] Trial 209 finished with value: 0.7077018722730544 and parameters: {'n_estimators': 712, 'eta': 0.04681513990342334, 'max_depth': 11, 'alpha': 0.1593, 'lambda': 16.268442130133185, 'max_bin': 398}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:45:24,126] Trial 210 finished with value: 0.7100782672492404 and parameters: {'n_estimators': 684, 'eta': 0.04054241320675493, 'max_depth': 11, 'alpha': 0.12430000000000001, 'lambda': 18.506305170964637, 'max_bin': 368}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:45:37,639] Trial 211 finished with value: 0.7088205656699545 and parameters: {'n_estimators': 659, 'eta': 0.04237136417829395, 'max_depth': 11, 'alpha': 0.09190000000000001, 'lambda': 19.124863102831046, 'max_bin': 354}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:45:51,582] Trial 212 finished with value: 0.7089667521020944 and parameters: {'n_estimators': 659, 'eta': 0.04367103072626716, 'max_depth': 11, 'alpha': 0.0478, 'lambda': 17.142197558224055, 'max_bin': 357}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:46:06,486] Trial 213 finished with value: 0.7114238346752888 and parameters: {'n_estimators': 699, 'eta': 0.04093624803590314, 'max_depth': 11, 'alpha': 0.1047, 'lambda': 19.386947200173324, 'max_bin': 361}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:46:19,751] Trial 214 finished with value: 0.7072455718029353 and parameters: {'n_estimators': 674, 'eta': 0.04450746513747798, 'max_depth': 11, 'alpha': 0.06860000000000001, 'lambda': 18.559478959705995, 'max_bin': 387}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:46:33,477] Trial 215 finished with value: 0.7087928495654221 and parameters: {'n_estimators': 655, 'eta': 0.04148621579405557, 'max_depth': 11, 'alpha': 0.026500000000000003, 'lambda': 17.72977411228494, 'max_bin': 373}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:46:47,315] Trial 216 finished with value: 0.7097549255175343 and parameters: {'n_estimators': 686, 'eta': 0.04022097707907311, 'max_depth': 11, 'alpha': 0.1291, 'lambda': 16.86479401653978, 'max_bin': 352}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:46:59,641] Trial 217 finished with value: 0.7092926852611553 and parameters: {'n_estimators': 721, 'eta': 0.04717668293825139, 'max_depth': 11, 'alpha': 0.0994, 'lambda': 19.236200904064823, 'max_bin': 363}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:47:12,850] Trial 218 finished with value: 0.7068347282389109 and parameters: {'n_estimators': 741, 'eta': 0.04264281422266602, 'max_depth': 11, 'alpha': 0.076, 'lambda': 18.052072062208595, 'max_bin': 367}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:47:25,634] Trial 219 finished with value: 0.7072719991914234 and parameters: {'n_estimators': 703, 'eta': 0.04566497126451308, 'max_depth': 11, 'alpha': 0.15910000000000002, 'lambda': 15.882230527326199, 'max_bin': 378}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:47:36,736] Trial 220 finished with value: 0.700266950989206 and parameters: {'n_estimators': 646, 'eta': 0.03588374906680962, 'max_depth': 7, 'alpha': 0.054, 'lambda': 18.466251554904495, 'max_bin': 352}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:47:49,643] Trial 221 finished with value: 0.7092611120158256 and parameters: {'n_estimators': 677, 'eta': 0.044433786188065816, 'max_depth': 11, 'alpha': 0.0874, 'lambda': 16.455812444629416, 'max_bin': 358}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:48:02,251] Trial 222 finished with value: 0.7075918873577866 and parameters: {'n_estimators': 664, 'eta': 0.043612721432248265, 'max_depth': 11, 'alpha': 0.0685, 'lambda': 16.993877409619856, 'max_bin': 361}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:48:16,440] Trial 223 finished with value: 0.7090735583899731 and parameters: {'n_estimators': 678, 'eta': 0.04004496039746297, 'max_depth': 11, 'alpha': 0.11750000000000001, 'lambda': 17.63004264410584, 'max_bin': 359}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:48:28,344] Trial 224 finished with value: 0.7059133012844858 and parameters: {'n_estimators': 692, 'eta': 0.048577877253324186, 'max_depth': 11, 'alpha': 0.098, 'lambda': 16.281760817342313, 'max_bin': 349}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:48:41,163] Trial 225 finished with value: 0.7083284990114205 and parameters: {'n_estimators': 649, 'eta': 0.04566520892738002, 'max_depth': 11, 'alpha': 0.0373, 'lambda': 19.374579510996217, 'max_bin': 373}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:48:53,745] Trial 226 finished with value: 0.7058806069124134 and parameters: {'n_estimators': 712, 'eta': 0.04234198093382318, 'max_depth': 11, 'alpha': 0.1163, 'lambda': 15.566218873684242, 'max_bin': 383}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:49:08,661] Trial 227 finished with value: 0.7101982336690039 and parameters: {'n_estimators': 671, 'eta': 0.040378069213613955, 'max_depth': 11, 'alpha': 0.14, 'lambda': 18.307447833313205, 'max_bin': 364}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:49:21,789] Trial 228 finished with value: 0.7096425728953333 and parameters: {'n_estimators': 695, 'eta': 0.044101440177526624, 'max_depth': 11, 'alpha': 0.0733, 'lambda': 17.283353966165425, 'max_bin': 356}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:49:35,429] Trial 229 finished with value: 0.7086669209757528 and parameters: {'n_estimators': 682, 'eta': 0.04219756645914777, 'max_depth': 11, 'alpha': 0.095, 'lambda': 20.317186419569875, 'max_bin': 344}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:49:43,973] Trial 230 finished with value: 0.6915334632871298 and parameters: {'n_estimators': 668, 'eta': 0.04515723957067205, 'max_depth': 6, 'alpha': 0.0539, 'lambda': 16.739323765423777, 'max_bin': 370}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:49:57,358] Trial 231 finished with value: 0.7092303412761731 and parameters: {'n_estimators': 640, 'eta': 0.03920130992355391, 'max_depth': 11, 'alpha': 0.1331, 'lambda': 17.623766249474457, 'max_bin': 346}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:50:08,917] Trial 232 finished with value: 0.7063247114226512 and parameters: {'n_estimators': 436, 'eta': 0.04748182585280574, 'max_depth': 11, 'alpha': 0.075, 'lambda': 16.164290289651007, 'max_bin': 351}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:50:22,997] Trial 233 finished with value: 0.7078953492948182 and parameters: {'n_estimators': 651, 'eta': 0.04299624691396262, 'max_depth': 11, 'alpha': 0.1124, 'lambda': 18.78526657828746, 'max_bin': 355}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:50:36,958] Trial 234 finished with value: 0.7084775200301282 and parameters: {'n_estimators': 621, 'eta': 0.04054722357434729, 'max_depth': 11, 'alpha': 0.015300000000000001, 'lambda': 15.367991791107107, 'max_bin': 362}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:50:51,058] Trial 235 finished with value: 0.7115882667197861 and parameters: {'n_estimators': 688, 'eta': 0.0358553504657984, 'max_depth': 11, 'alpha': 0.0881, 'lambda': 16.771069865264977, 'max_bin': 341}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:51:04,121] Trial 236 finished with value: 0.7067375467841094 and parameters: {'n_estimators': 661, 'eta': 0.04621654462741171, 'max_depth': 11, 'alpha': 0.041800000000000004, 'lambda': 18.404820830019986, 'max_bin': 378}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:51:14,516] Trial 237 finished with value: 0.7085571709699776 and parameters: {'n_estimators': 710, 'eta': 0.06538374042521922, 'max_depth': 11, 'alpha': 0.1522, 'lambda': 17.56008266768248, 'max_bin': 367}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:51:28,883] Trial 238 finished with value: 0.711343274492483 and parameters: {'n_estimators': 635, 'eta': 0.04213055716335577, 'max_depth': 11, 'alpha': 0.1218, 'lambda': 19.41257343491673, 'max_bin': 360}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:51:40,515] Trial 239 finished with value: 0.707195500278281 and parameters: {'n_estimators': 731, 'eta': 0.049195106037816225, 'max_depth': 11, 'alpha': 0.09820000000000001, 'lambda': 18.19490969359854, 'max_bin': 347}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:51:53,891] Trial 240 finished with value: 0.7087788562226349 and parameters: {'n_estimators': 675, 'eta': 0.0440824825480398, 'max_depth': 11, 'alpha': 0.058800000000000005, 'lambda': 16.12421281070968, 'max_bin': 354}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:52:08,719] Trial 241 finished with value: 0.7106728525169201 and parameters: {'n_estimators': 651, 'eta': 0.04040375342954032, 'max_depth': 11, 'alpha': 0.1346, 'lambda': 18.749040379344617, 'max_bin': 358}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:52:19,103] Trial 242 finished with value: 0.706382617206079 and parameters: {'n_estimators': 641, 'eta': 0.03823613945796753, 'max_depth': 11, 'alpha': 0.1302, 'lambda': 3.6292638123834573, 'max_bin': 374}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:52:33,407] Trial 243 finished with value: 0.7090820709028467 and parameters: {'n_estimators': 698, 'eta': 0.041434125076998334, 'max_depth': 11, 'alpha': 0.10250000000000001, 'lambda': 17.229023544183377, 'max_bin': 361}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:52:44,193] Trial 244 finished with value: 0.7063808194058492 and parameters: {'n_estimators': 621, 'eta': 0.06351892492165388, 'max_depth': 11, 'alpha': 0.166, 'lambda': 18.174511055146496, 'max_bin': 367}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:52:59,026] Trial 245 finished with value: 0.7081711049069819 and parameters: {'n_estimators': 666, 'eta': 0.043978896737823577, 'max_depth': 11, 'alpha': 0.1383, 'lambda': 18.946629582761595, 'max_bin': 353}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:53:10,625] Trial 246 finished with value: 0.708270598126454 and parameters: {'n_estimators': 651, 'eta': 0.040744831858802885, 'max_depth': 11, 'alpha': 0.0784, 'lambda': 6.607009109616094, 'max_bin': 348}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:53:25,372] Trial 247 finished with value: 0.710882329631137 and parameters: {'n_estimators': 686, 'eta': 0.03878956202501575, 'max_depth': 11, 'alpha': 0.14980000000000002, 'lambda': 17.247657267063037, 'max_bin': 387}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:53:38,163] Trial 248 finished with value: 0.7081257280269277 and parameters: {'n_estimators': 642, 'eta': 0.046063398222897844, 'max_depth': 11, 'alpha': 0.1121, 'lambda': 19.821388300884454, 'max_bin': 364}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:53:51,148] Trial 249 finished with value: 0.7084127667224163 and parameters: {'n_estimators': 673, 'eta': 0.04221586141773043, 'max_depth': 11, 'alpha': 0.0801, 'lambda': 16.612621690234416, 'max_bin': 359}. Best is trial 191 with value: 0.7312920802069302.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7313\n",
      "\tBest params:\n",
      "\t\tn_estimators: 686\n",
      "\t\teta: 0.043404992941413346\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.08270000000000001\n",
      "\t\tlambda: 17.62920128007734\n",
      "\t\tmax_bin: 373\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.703752    0.746467    0.736627    0.749621   \n",
      "1                    TP   34.000000   37.000000   37.000000   34.000000   \n",
      "2                    TN  304.000000  305.000000  308.000000  307.000000   \n",
      "3                    FP   12.000000   10.000000    6.000000    9.000000   \n",
      "4                    FN   32.000000   30.000000   31.000000   32.000000   \n",
      "5              Accuracy    0.884817    0.895288    0.903141    0.892670   \n",
      "6             Precision    0.739130    0.787234    0.860465    0.790698   \n",
      "7           Sensitivity    0.515152    0.552239    0.544118    0.515152   \n",
      "8           Specificity    0.962000    0.968300    0.980900    0.971500   \n",
      "9              F1 score    0.607143    0.649123    0.666667    0.623853   \n",
      "10  F1 score (weighted)    0.876299    0.887714    0.894088    0.883231   \n",
      "11     F1 score (macro)    0.769829    0.793792    0.805003    0.780629   \n",
      "12    Balanced Accuracy    0.738588    0.760246    0.762505    0.743335   \n",
      "13                  MCC    0.554302    0.602610    0.635407    0.582125   \n",
      "14                  NPV    0.904800    0.910400    0.908600    0.905600   \n",
      "15              ROC_AUC    0.738588    0.760246    0.762505    0.743335   \n",
      "\n",
      "          Set4  \n",
      "0     0.804255  \n",
      "1    42.000000  \n",
      "2   308.000000  \n",
      "3     8.000000  \n",
      "4    24.000000  \n",
      "5     0.916230  \n",
      "6     0.840000  \n",
      "7     0.636364  \n",
      "8     0.974700  \n",
      "9     0.724138  \n",
      "10    0.911487  \n",
      "11    0.837378  \n",
      "12    0.805524  \n",
      "13    0.684914  \n",
      "14    0.927700  \n",
      "15    0.805524  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_xgb_4)\n",
    "# now convert the resuls to binary with cutoff 6.4\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_4_cat = np.where(((y_pred_xgb_4 >= 2) | (y_pred_xgb_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 16:54:09,126] Trial 250 finished with value: 0.7307417290627367 and parameters: {'n_estimators': 707, 'eta': 0.04420412997020361, 'max_depth': 11, 'alpha': 0.0551, 'lambda': 20.73532198426768, 'max_bin': 382}. Best is trial 191 with value: 0.7312920802069302.\n",
      "[I 2023-12-20 16:54:23,305] Trial 251 finished with value: 0.7325607760163457 and parameters: {'n_estimators': 715, 'eta': 0.04466963492347199, 'max_depth': 11, 'alpha': 0.0379, 'lambda': 15.225874440267374, 'max_bin': 379}. Best is trial 251 with value: 0.7325607760163457.\n",
      "[I 2023-12-20 16:54:37,989] Trial 252 finished with value: 0.7315778666491745 and parameters: {'n_estimators': 721, 'eta': 0.04491110358191759, 'max_depth': 11, 'alpha': 0.030500000000000003, 'lambda': 15.048374769723617, 'max_bin': 381}. Best is trial 251 with value: 0.7325607760163457.\n",
      "[I 2023-12-20 16:54:51,703] Trial 253 finished with value: 0.7320036717979029 and parameters: {'n_estimators': 722, 'eta': 0.047633664155340545, 'max_depth': 11, 'alpha': 0.024300000000000002, 'lambda': 14.773923773452477, 'max_bin': 379}. Best is trial 251 with value: 0.7325607760163457.\n",
      "[I 2023-12-20 16:55:06,935] Trial 254 finished with value: 0.7328107635291472 and parameters: {'n_estimators': 740, 'eta': 0.04814065085547304, 'max_depth': 11, 'alpha': 0.022500000000000003, 'lambda': 14.94097169492104, 'max_bin': 380}. Best is trial 254 with value: 0.7328107635291472.\n",
      "[I 2023-12-20 16:55:22,051] Trial 255 finished with value: 0.7325248049434196 and parameters: {'n_estimators': 745, 'eta': 0.04894898099989487, 'max_depth': 11, 'alpha': 0.0112, 'lambda': 14.919836519934211, 'max_bin': 382}. Best is trial 254 with value: 0.7328107635291472.\n",
      "[I 2023-12-20 16:55:35,555] Trial 256 finished with value: 0.7299743923383346 and parameters: {'n_estimators': 726, 'eta': 0.04976272241858335, 'max_depth': 11, 'alpha': 0.0091, 'lambda': 15.170496334421937, 'max_bin': 394}. Best is trial 254 with value: 0.7328107635291472.\n",
      "[I 2023-12-20 16:55:48,596] Trial 257 finished with value: 0.73042724056906 and parameters: {'n_estimators': 730, 'eta': 0.05050020588621975, 'max_depth': 11, 'alpha': 0.015300000000000001, 'lambda': 15.029006431820543, 'max_bin': 392}. Best is trial 254 with value: 0.7328107635291472.\n",
      "[I 2023-12-20 16:56:01,283] Trial 258 finished with value: 0.7319050038523327 and parameters: {'n_estimators': 754, 'eta': 0.04890950294677066, 'max_depth': 11, 'alpha': 0.0106, 'lambda': 14.772123740057628, 'max_bin': 394}. Best is trial 254 with value: 0.7328107635291472.\n",
      "[I 2023-12-20 16:56:14,303] Trial 259 finished with value: 0.7294314175211892 and parameters: {'n_estimators': 751, 'eta': 0.05042383029615631, 'max_depth': 11, 'alpha': 0.0, 'lambda': 14.591793962761061, 'max_bin': 397}. Best is trial 254 with value: 0.7328107635291472.\n",
      "[I 2023-12-20 16:56:30,036] Trial 260 finished with value: 0.7300489055241784 and parameters: {'n_estimators': 750, 'eta': 0.05016803844433427, 'max_depth': 11, 'alpha': 0.0018000000000000002, 'lambda': 14.677162594650866, 'max_bin': 405}. Best is trial 254 with value: 0.7328107635291472.\n",
      "[I 2023-12-20 16:56:43,814] Trial 261 finished with value: 0.7322186473830368 and parameters: {'n_estimators': 746, 'eta': 0.050551368729856966, 'max_depth': 11, 'alpha': 0.014400000000000001, 'lambda': 14.863747677418292, 'max_bin': 404}. Best is trial 254 with value: 0.7328107635291472.\n",
      "[I 2023-12-20 16:56:57,421] Trial 262 finished with value: 0.7309162741378092 and parameters: {'n_estimators': 751, 'eta': 0.05104528012276646, 'max_depth': 11, 'alpha': 0.0004, 'lambda': 14.549002468111643, 'max_bin': 401}. Best is trial 254 with value: 0.7328107635291472.\n",
      "[I 2023-12-20 16:57:12,186] Trial 263 finished with value: 0.7311097331666752 and parameters: {'n_estimators': 752, 'eta': 0.04937795606319663, 'max_depth': 11, 'alpha': 0.0033, 'lambda': 14.87417071273674, 'max_bin': 411}. Best is trial 254 with value: 0.7328107635291472.\n",
      "[I 2023-12-20 16:57:25,685] Trial 264 finished with value: 0.7288464372667904 and parameters: {'n_estimators': 749, 'eta': 0.05076376904076291, 'max_depth': 11, 'alpha': 0.0025, 'lambda': 14.722956646477511, 'max_bin': 409}. Best is trial 254 with value: 0.7328107635291472.\n",
      "[I 2023-12-20 16:57:40,501] Trial 265 finished with value: 0.7306337328132761 and parameters: {'n_estimators': 754, 'eta': 0.05020095749698102, 'max_depth': 11, 'alpha': 0.006900000000000001, 'lambda': 14.349145923278964, 'max_bin': 409}. Best is trial 254 with value: 0.7328107635291472.\n",
      "[I 2023-12-20 16:57:55,175] Trial 266 finished with value: 0.734436427190172 and parameters: {'n_estimators': 763, 'eta': 0.05057363757927622, 'max_depth': 11, 'alpha': 0.001, 'lambda': 14.263066723554317, 'max_bin': 412}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 16:58:10,033] Trial 267 finished with value: 0.7323083549456035 and parameters: {'n_estimators': 762, 'eta': 0.05042451551640045, 'max_depth': 11, 'alpha': 0.0048000000000000004, 'lambda': 14.376244944843188, 'max_bin': 414}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 16:58:23,610] Trial 268 finished with value: 0.7302434626952193 and parameters: {'n_estimators': 771, 'eta': 0.05202134935045512, 'max_depth': 11, 'alpha': 0.0152, 'lambda': 14.23343660427907, 'max_bin': 414}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 16:58:37,341] Trial 269 finished with value: 0.7301600016099171 and parameters: {'n_estimators': 769, 'eta': 0.05268533141915113, 'max_depth': 11, 'alpha': 0.019200000000000002, 'lambda': 14.08707228046529, 'max_bin': 417}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 16:58:51,840] Trial 270 finished with value: 0.7299477256859073 and parameters: {'n_estimators': 771, 'eta': 0.04996249057705827, 'max_depth': 11, 'alpha': 0.021400000000000002, 'lambda': 13.879642609994571, 'max_bin': 417}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 16:59:05,412] Trial 271 finished with value: 0.732109428783416 and parameters: {'n_estimators': 775, 'eta': 0.05304634101123122, 'max_depth': 11, 'alpha': 0.0239, 'lambda': 14.074778160891414, 'max_bin': 422}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 16:59:18,177] Trial 272 finished with value: 0.7331894457307135 and parameters: {'n_estimators': 766, 'eta': 0.05230769486338281, 'max_depth': 11, 'alpha': 0.028200000000000003, 'lambda': 14.079263689194928, 'max_bin': 423}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 16:59:30,498] Trial 273 finished with value: 0.7321345572492579 and parameters: {'n_estimators': 770, 'eta': 0.05232769707518342, 'max_depth': 11, 'alpha': 0.0194, 'lambda': 14.174727233009381, 'max_bin': 416}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 16:59:43,567] Trial 274 finished with value: 0.7292802697329998 and parameters: {'n_estimators': 772, 'eta': 0.053590851116104, 'max_depth': 11, 'alpha': 0.020200000000000003, 'lambda': 14.191231714568818, 'max_bin': 421}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 16:59:57,310] Trial 275 finished with value: 0.7312599117784531 and parameters: {'n_estimators': 763, 'eta': 0.05226222065594478, 'max_depth': 11, 'alpha': 0.0223, 'lambda': 13.679291985885982, 'max_bin': 409}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:00:11,356] Trial 276 finished with value: 0.7324934969457432 and parameters: {'n_estimators': 760, 'eta': 0.05177280757276328, 'max_depth': 11, 'alpha': 0.0028, 'lambda': 14.407783265959319, 'max_bin': 410}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:00:24,350] Trial 277 finished with value: 0.7285569313087947 and parameters: {'n_estimators': 759, 'eta': 0.05251905056838222, 'max_depth': 11, 'alpha': 0.032100000000000004, 'lambda': 13.652991240272055, 'max_bin': 409}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:00:37,665] Trial 278 finished with value: 0.7304795190596437 and parameters: {'n_estimators': 775, 'eta': 0.052157203307556896, 'max_depth': 11, 'alpha': 0.0002, 'lambda': 14.43028369388542, 'max_bin': 416}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:00:52,355] Trial 279 finished with value: 0.7321743104582492 and parameters: {'n_estimators': 792, 'eta': 0.052270023116904554, 'max_depth': 11, 'alpha': 0.0313, 'lambda': 14.020141019399496, 'max_bin': 429}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:01:04,714] Trial 280 finished with value: 0.729227521362011 and parameters: {'n_estimators': 789, 'eta': 0.05372804237742994, 'max_depth': 11, 'alpha': 0.0356, 'lambda': 13.175493545648482, 'max_bin': 430}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:01:19,600] Trial 281 finished with value: 0.7294048791956906 and parameters: {'n_estimators': 786, 'eta': 0.05147622730019404, 'max_depth': 11, 'alpha': 0.0008, 'lambda': 14.856423567325997, 'max_bin': 412}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:01:35,064] Trial 282 finished with value: 0.7311456206382585 and parameters: {'n_estimators': 742, 'eta': 0.05167335897358565, 'max_depth': 11, 'alpha': 0.0316, 'lambda': 14.312232387964997, 'max_bin': 424}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:01:48,609] Trial 283 finished with value: 0.7315430627309476 and parameters: {'n_estimators': 739, 'eta': 0.04900133625904943, 'max_depth': 11, 'alpha': 0.0379, 'lambda': 13.321286552258377, 'max_bin': 423}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:02:01,628] Trial 284 finished with value: 0.7321868594917682 and parameters: {'n_estimators': 747, 'eta': 0.05163281901186673, 'max_depth': 11, 'alpha': 0.039, 'lambda': 13.37614859624107, 'max_bin': 423}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:02:16,387] Trial 285 finished with value: 0.7293179524945762 and parameters: {'n_estimators': 748, 'eta': 0.054914143473059125, 'max_depth': 11, 'alpha': 0.0385, 'lambda': 13.413603871811066, 'max_bin': 427}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:02:30,407] Trial 286 finished with value: 0.7321450994800098 and parameters: {'n_estimators': 759, 'eta': 0.04885553595516507, 'max_depth': 11, 'alpha': 0.0346, 'lambda': 13.008684641298547, 'max_bin': 434}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:02:41,994] Trial 287 finished with value: 0.7311190638416818 and parameters: {'n_estimators': 741, 'eta': 0.048778366650516325, 'max_depth': 11, 'alpha': 0.038, 'lambda': 13.502521455704304, 'max_bin': 438}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:02:56,126] Trial 288 finished with value: 0.7314813651739429 and parameters: {'n_estimators': 741, 'eta': 0.04867882214724444, 'max_depth': 11, 'alpha': 0.038400000000000004, 'lambda': 13.2397453307548, 'max_bin': 437}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:03:08,865] Trial 289 finished with value: 0.7302952029288596 and parameters: {'n_estimators': 737, 'eta': 0.04876611917162648, 'max_depth': 11, 'alpha': 0.040400000000000005, 'lambda': 12.919103992707926, 'max_bin': 442}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:03:22,695] Trial 290 finished with value: 0.7308193334205938 and parameters: {'n_estimators': 759, 'eta': 0.048300714335176576, 'max_depth': 11, 'alpha': 0.0332, 'lambda': 13.40034946640779, 'max_bin': 437}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:03:36,758] Trial 291 finished with value: 0.7325218203241517 and parameters: {'n_estimators': 761, 'eta': 0.04872289520025273, 'max_depth': 11, 'alpha': 0.0337, 'lambda': 13.315943050889853, 'max_bin': 435}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:03:48,734] Trial 292 finished with value: 0.7291529979886973 and parameters: {'n_estimators': 794, 'eta': 0.05392795768966595, 'max_depth': 11, 'alpha': 0.028200000000000003, 'lambda': 12.662444630429412, 'max_bin': 423}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:04:03,542] Trial 293 finished with value: 0.7309295618699571 and parameters: {'n_estimators': 743, 'eta': 0.04865503923585665, 'max_depth': 11, 'alpha': 0.042300000000000004, 'lambda': 13.586716517440998, 'max_bin': 433}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:04:17,474] Trial 294 finished with value: 0.734106512762986 and parameters: {'n_estimators': 737, 'eta': 0.04859103526716105, 'max_depth': 11, 'alpha': 0.040600000000000004, 'lambda': 13.299523735724742, 'max_bin': 434}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:04:30,986] Trial 295 finished with value: 0.7324718151289406 and parameters: {'n_estimators': 782, 'eta': 0.04870474675043082, 'max_depth': 11, 'alpha': 0.0258, 'lambda': 12.534398660641902, 'max_bin': 441}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:04:44,333] Trial 296 finished with value: 0.7297366411684515 and parameters: {'n_estimators': 783, 'eta': 0.0527584127491529, 'max_depth': 11, 'alpha': 0.031, 'lambda': 12.263268871311949, 'max_bin': 438}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:04:57,323] Trial 297 finished with value: 0.7319135326619753 and parameters: {'n_estimators': 807, 'eta': 0.0478394092389275, 'max_depth': 11, 'alpha': 0.0487, 'lambda': 12.538641223561775, 'max_bin': 451}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:05:10,737] Trial 298 finished with value: 0.7298294585402199 and parameters: {'n_estimators': 804, 'eta': 0.05149999261377423, 'max_depth': 11, 'alpha': 0.0494, 'lambda': 12.654653241251383, 'max_bin': 444}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:05:25,056] Trial 299 finished with value: 0.7319511615578963 and parameters: {'n_estimators': 769, 'eta': 0.04753096088735577, 'max_depth': 11, 'alpha': 0.026500000000000003, 'lambda': 12.88619276690634, 'max_bin': 424}. Best is trial 266 with value: 0.734436427190172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7344\n",
      "\tBest params:\n",
      "\t\tn_estimators: 763\n",
      "\t\teta: 0.05057363757927622\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.001\n",
      "\t\tlambda: 14.263066723554317\n",
      "\t\tmax_bin: 412\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.703752    0.746467    0.736627    0.749621   \n",
      "1                    TP   34.000000   37.000000   37.000000   34.000000   \n",
      "2                    TN  304.000000  305.000000  308.000000  307.000000   \n",
      "3                    FP   12.000000   10.000000    6.000000    9.000000   \n",
      "4                    FN   32.000000   30.000000   31.000000   32.000000   \n",
      "5              Accuracy    0.884817    0.895288    0.903141    0.892670   \n",
      "6             Precision    0.739130    0.787234    0.860465    0.790698   \n",
      "7           Sensitivity    0.515152    0.552239    0.544118    0.515152   \n",
      "8           Specificity    0.962000    0.968300    0.980900    0.971500   \n",
      "9              F1 score    0.607143    0.649123    0.666667    0.623853   \n",
      "10  F1 score (weighted)    0.876299    0.887714    0.894088    0.883231   \n",
      "11     F1 score (macro)    0.769829    0.793792    0.805003    0.780629   \n",
      "12    Balanced Accuracy    0.738588    0.760246    0.762505    0.743335   \n",
      "13                  MCC    0.554302    0.602610    0.635407    0.582125   \n",
      "14                  NPV    0.904800    0.910400    0.908600    0.905600   \n",
      "15              ROC_AUC    0.738588    0.760246    0.762505    0.743335   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.804255    0.737585  \n",
      "1    42.000000   36.000000  \n",
      "2   308.000000  303.000000  \n",
      "3     8.000000   12.000000  \n",
      "4    24.000000   31.000000  \n",
      "5     0.916230    0.887435  \n",
      "6     0.840000    0.750000  \n",
      "7     0.636364    0.537313  \n",
      "8     0.974700    0.961900  \n",
      "9     0.724138    0.626087  \n",
      "10    0.911487    0.879783  \n",
      "11    0.837378    0.779916  \n",
      "12    0.805524    0.749609  \n",
      "13    0.684914    0.572782  \n",
      "14    0.927700    0.907200  \n",
      "15    0.805524    0.749609  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_xgb_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_5_cat = np.where(((y_pred_xgb_5 >= 2) | (y_pred_xgb_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:05:42,371] Trial 300 finished with value: 0.7252660742122058 and parameters: {'n_estimators': 785, 'eta': 0.04834733717562532, 'max_depth': 11, 'alpha': 0.0511, 'lambda': 12.874287648656466, 'max_bin': 451}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:05:57,263] Trial 301 finished with value: 0.7257532515816232 and parameters: {'n_estimators': 769, 'eta': 0.048041335765277, 'max_depth': 11, 'alpha': 0.0263, 'lambda': 12.288019414615764, 'max_bin': 430}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:06:11,428] Trial 302 finished with value: 0.7251084121682491 and parameters: {'n_estimators': 804, 'eta': 0.05519710257304939, 'max_depth': 11, 'alpha': 0.023700000000000002, 'lambda': 13.517977855237053, 'max_bin': 421}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:06:25,765] Trial 303 finished with value: 0.7256429317888669 and parameters: {'n_estimators': 765, 'eta': 0.04747582409331089, 'max_depth': 11, 'alpha': 0.052000000000000005, 'lambda': 11.762767366060707, 'max_bin': 448}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:06:41,251] Trial 304 finished with value: 0.7259009097842064 and parameters: {'n_estimators': 784, 'eta': 0.05350082373194424, 'max_depth': 11, 'alpha': 0.0218, 'lambda': 12.953153901764725, 'max_bin': 433}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:06:57,965] Trial 305 finished with value: 0.7272755290183321 and parameters: {'n_estimators': 818, 'eta': 0.050431012899216275, 'max_depth': 11, 'alpha': 0.0519, 'lambda': 15.430200964021477, 'max_bin': 423}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:07:13,827] Trial 306 finished with value: 0.724778175123972 and parameters: {'n_estimators': 729, 'eta': 0.047617561441934166, 'max_depth': 11, 'alpha': 0.0284, 'lambda': 13.861183779317757, 'max_bin': 462}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:07:28,597] Trial 307 finished with value: 0.7264628552027542 and parameters: {'n_estimators': 766, 'eta': 0.05117793341183469, 'max_depth': 11, 'alpha': 0.0199, 'lambda': 13.263332038617015, 'max_bin': 427}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:07:42,101] Trial 308 finished with value: 0.7244117842328447 and parameters: {'n_estimators': 793, 'eta': 0.055771115800102466, 'max_depth': 11, 'alpha': 0.0506, 'lambda': 12.183355158607396, 'max_bin': 405}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:07:55,740] Trial 309 finished with value: 0.7264606429451151 and parameters: {'n_estimators': 733, 'eta': 0.0490848383887344, 'max_depth': 11, 'alpha': 0.0396, 'lambda': 13.78286939819196, 'max_bin': 419}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:08:12,293] Trial 310 finished with value: 0.7261286679351555 and parameters: {'n_estimators': 766, 'eta': 0.053830707459815386, 'max_depth': 11, 'alpha': 0.0171, 'lambda': 15.343825344398114, 'max_bin': 442}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:08:26,955] Trial 311 finished with value: 0.7261501614975445 and parameters: {'n_estimators': 757, 'eta': 0.046799033789109816, 'max_depth': 11, 'alpha': 0.0541, 'lambda': 12.825481629538416, 'max_bin': 433}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:08:41,098] Trial 312 finished with value: 0.7260951495021342 and parameters: {'n_estimators': 735, 'eta': 0.049642034261158485, 'max_depth': 11, 'alpha': 0.0182, 'lambda': 14.028383641725219, 'max_bin': 430}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:08:56,087] Trial 313 finished with value: 0.7281800778626262 and parameters: {'n_estimators': 782, 'eta': 0.05238571204507759, 'max_depth': 11, 'alpha': 0.039400000000000004, 'lambda': 15.259783602720471, 'max_bin': 424}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:09:10,572] Trial 314 finished with value: 0.724388759468064 and parameters: {'n_estimators': 725, 'eta': 0.04722208726392688, 'max_depth': 11, 'alpha': 0.0158, 'lambda': 11.675730494243407, 'max_bin': 415}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:09:25,153] Trial 315 finished with value: 0.7280201886890172 and parameters: {'n_estimators': 805, 'eta': 0.051048849741658166, 'max_depth': 11, 'alpha': 0.0009000000000000001, 'lambda': 14.056405268118443, 'max_bin': 460}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:09:39,559] Trial 316 finished with value: 0.7260896722754977 and parameters: {'n_estimators': 758, 'eta': 0.04875982883641668, 'max_depth': 11, 'alpha': 0.058600000000000006, 'lambda': 12.884019367742908, 'max_bin': 407}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:09:57,082] Trial 317 finished with value: 0.7252486809579006 and parameters: {'n_estimators': 778, 'eta': 0.05446186844664596, 'max_depth': 11, 'alpha': 0.0342, 'lambda': 15.33873034058411, 'max_bin': 435}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:10:13,782] Trial 318 finished with value: 0.7273312799364035 and parameters: {'n_estimators': 751, 'eta': 0.05648040374884882, 'max_depth': 11, 'alpha': 0.0193, 'lambda': 14.763048670736167, 'max_bin': 418}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:10:25,093] Trial 319 finished with value: 0.7239071024023419 and parameters: {'n_estimators': 736, 'eta': 0.05176826319302881, 'max_depth': 11, 'alpha': 0.0562, 'lambda': 12.26708362098873, 'max_bin': 401}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:10:39,777] Trial 320 finished with value: 0.7230157593802872 and parameters: {'n_estimators': 800, 'eta': 0.049711073788434236, 'max_depth': 11, 'alpha': 0.038200000000000005, 'lambda': 13.491680327276603, 'max_bin': 427}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:10:56,480] Trial 321 finished with value: 0.726197595577977 and parameters: {'n_estimators': 769, 'eta': 0.04708428754587948, 'max_depth': 11, 'alpha': 0.017400000000000002, 'lambda': 14.280396952771834, 'max_bin': 443}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:11:10,737] Trial 322 finished with value: 0.7250980820398804 and parameters: {'n_estimators': 719, 'eta': 0.052798541688307006, 'max_depth': 11, 'alpha': 0.0037, 'lambda': 13.187704792938597, 'max_bin': 425}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:11:25,014] Trial 323 finished with value: 0.7248555082327608 and parameters: {'n_estimators': 744, 'eta': 0.04946221627890798, 'max_depth': 11, 'alpha': 0.0403, 'lambda': 15.46627304799486, 'max_bin': 456}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:11:38,040] Trial 324 finished with value: 0.7254640472401086 and parameters: {'n_estimators': 821, 'eta': 0.04709554714500523, 'max_depth': 11, 'alpha': 0.056900000000000006, 'lambda': 11.60639324935227, 'max_bin': 412}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:11:53,589] Trial 325 finished with value: 0.7248227161648026 and parameters: {'n_estimators': 781, 'eta': 0.0510408675568488, 'max_depth': 11, 'alpha': 0.0007, 'lambda': 14.664560503499459, 'max_bin': 446}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:12:08,955] Trial 326 finished with value: 0.7248781167532601 and parameters: {'n_estimators': 762, 'eta': 0.054379718595807265, 'max_depth': 11, 'alpha': 0.029400000000000003, 'lambda': 13.946483686982862, 'max_bin': 438}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:12:22,879] Trial 327 finished with value: 0.7256475156614576 and parameters: {'n_estimators': 722, 'eta': 0.04855878724881036, 'max_depth': 11, 'alpha': 0.061900000000000004, 'lambda': 12.75741436253006, 'max_bin': 421}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:12:37,388] Trial 328 finished with value: 0.7260179425082642 and parameters: {'n_estimators': 743, 'eta': 0.04658640952364189, 'max_depth': 11, 'alpha': 0.0257, 'lambda': 13.647266985685217, 'max_bin': 431}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:12:49,907] Trial 329 finished with value: 0.7253936520190762 and parameters: {'n_estimators': 793, 'eta': 0.05683052654182554, 'max_depth': 11, 'alpha': 0.039, 'lambda': 12.273156052697848, 'max_bin': 401}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:12:57,737] Trial 330 finished with value: 0.7109344189648426 and parameters: {'n_estimators': 759, 'eta': 0.05237278268362261, 'max_depth': 5, 'alpha': 0.0158, 'lambda': 14.596493903984651, 'max_bin': 415}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:13:11,635] Trial 331 finished with value: 0.7261614761897254 and parameters: {'n_estimators': 777, 'eta': 0.050737756982430444, 'max_depth': 11, 'alpha': 0.0007, 'lambda': 15.805820144325937, 'max_bin': 391}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:13:24,450] Trial 332 finished with value: 0.7247926776011957 and parameters: {'n_estimators': 736, 'eta': 0.04846382227568026, 'max_depth': 11, 'alpha': 0.0439, 'lambda': 13.27944357855349, 'max_bin': 386}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:13:39,726] Trial 333 finished with value: 0.7265149684013255 and parameters: {'n_estimators': 761, 'eta': 0.0536978061581704, 'max_depth': 11, 'alpha': 0.060000000000000005, 'lambda': 15.007472269251135, 'max_bin': 421}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:13:54,394] Trial 334 finished with value: 0.7274053392447625 and parameters: {'n_estimators': 725, 'eta': 0.04668917913545053, 'max_depth': 11, 'alpha': 0.0233, 'lambda': 11.206496537227512, 'max_bin': 440}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:14:08,350] Trial 335 finished with value: 0.7242911065993315 and parameters: {'n_estimators': 794, 'eta': 0.05028212044452219, 'max_depth': 11, 'alpha': 0.0386, 'lambda': 14.170212919294364, 'max_bin': 429}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:14:33,579] Trial 336 finished with value: 0.7205364366550219 and parameters: {'n_estimators': 814, 'eta': 0.012207599465814345, 'max_depth': 11, 'alpha': 0.062, 'lambda': 12.26849122502892, 'max_bin': 404}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:14:43,104] Trial 337 finished with value: 0.7139392920696654 and parameters: {'n_estimators': 749, 'eta': 0.05215519348367329, 'max_depth': 6, 'alpha': 0.019, 'lambda': 15.251139723616696, 'max_bin': 412}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:14:56,182] Trial 338 finished with value: 0.7253875506314553 and parameters: {'n_estimators': 773, 'eta': 0.049762198769005506, 'max_depth': 11, 'alpha': 0.042, 'lambda': 13.520210270102483, 'max_bin': 434}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:15:07,898] Trial 339 finished with value: 0.7254010252390051 and parameters: {'n_estimators': 717, 'eta': 0.055412733103167244, 'max_depth': 8, 'alpha': 0.002, 'lambda': 14.276088404943641, 'max_bin': 481}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:15:24,659] Trial 340 finished with value: 0.7259673440967929 and parameters: {'n_estimators': 741, 'eta': 0.04646608765892495, 'max_depth': 11, 'alpha': 0.0, 'lambda': 15.664306403490926, 'max_bin': 418}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:15:40,276] Trial 341 finished with value: 0.7271837504917775 and parameters: {'n_estimators': 763, 'eta': 0.04835216503983085, 'max_depth': 11, 'alpha': 0.025900000000000003, 'lambda': 12.722008191043928, 'max_bin': 425}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:15:55,889] Trial 342 finished with value: 0.7268564249454805 and parameters: {'n_estimators': 785, 'eta': 0.051612780408415576, 'max_depth': 11, 'alpha': 0.055600000000000004, 'lambda': 14.810040430500482, 'max_bin': 396}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:16:09,447] Trial 343 finished with value: 0.7248537590354626 and parameters: {'n_estimators': 750, 'eta': 0.053708530584202864, 'max_depth': 11, 'alpha': 0.0291, 'lambda': 13.592991362098017, 'max_bin': 407}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:16:23,177] Trial 344 finished with value: 0.7259477916382568 and parameters: {'n_estimators': 728, 'eta': 0.04797850736609853, 'max_depth': 11, 'alpha': 0.0653, 'lambda': 11.852306763090022, 'max_bin': 447}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:16:39,998] Trial 345 finished with value: 0.7264576908737765 and parameters: {'n_estimators': 799, 'eta': 0.046091838405657226, 'max_depth': 11, 'alpha': 0.042300000000000004, 'lambda': 12.97067663877817, 'max_bin': 435}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:16:54,456] Trial 346 finished with value: 0.7260690924533546 and parameters: {'n_estimators': 774, 'eta': 0.049969794227071734, 'max_depth': 11, 'alpha': 0.0182, 'lambda': 14.138071824794002, 'max_bin': 419}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:17:12,114] Trial 347 finished with value: 0.7332745742410643 and parameters: {'n_estimators': 739, 'eta': 0.056738787469381484, 'max_depth': 11, 'alpha': 0.0193, 'lambda': 32.670365199541024, 'max_bin': 427}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:17:28,053] Trial 348 finished with value: 0.7266388909507089 and parameters: {'n_estimators': 720, 'eta': 0.054805826851998835, 'max_depth': 11, 'alpha': 0.0453, 'lambda': 31.02678202471726, 'max_bin': 426}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:17:46,887] Trial 349 finished with value: 0.724673605778284 and parameters: {'n_estimators': 739, 'eta': 0.0461807699738693, 'max_depth': 11, 'alpha': 0.0183, 'lambda': 38.835997891960325, 'max_bin': 431}. Best is trial 266 with value: 0.734436427190172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7344\n",
      "\tBest params:\n",
      "\t\tn_estimators: 763\n",
      "\t\teta: 0.05057363757927622\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.001\n",
      "\t\tlambda: 14.263066723554317\n",
      "\t\tmax_bin: 412\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.703752    0.746467    0.736627    0.749621   \n",
      "1                    TP   34.000000   37.000000   37.000000   34.000000   \n",
      "2                    TN  304.000000  305.000000  308.000000  307.000000   \n",
      "3                    FP   12.000000   10.000000    6.000000    9.000000   \n",
      "4                    FN   32.000000   30.000000   31.000000   32.000000   \n",
      "5              Accuracy    0.884817    0.895288    0.903141    0.892670   \n",
      "6             Precision    0.739130    0.787234    0.860465    0.790698   \n",
      "7           Sensitivity    0.515152    0.552239    0.544118    0.515152   \n",
      "8           Specificity    0.962000    0.968300    0.980900    0.971500   \n",
      "9              F1 score    0.607143    0.649123    0.666667    0.623853   \n",
      "10  F1 score (weighted)    0.876299    0.887714    0.894088    0.883231   \n",
      "11     F1 score (macro)    0.769829    0.793792    0.805003    0.780629   \n",
      "12    Balanced Accuracy    0.738588    0.760246    0.762505    0.743335   \n",
      "13                  MCC    0.554302    0.602610    0.635407    0.582125   \n",
      "14                  NPV    0.904800    0.910400    0.908600    0.905600   \n",
      "15              ROC_AUC    0.738588    0.760246    0.762505    0.743335   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.804255    0.737585    0.763038  \n",
      "1    42.000000   36.000000   37.000000  \n",
      "2   308.000000  303.000000  307.000000  \n",
      "3     8.000000   12.000000    9.000000  \n",
      "4    24.000000   31.000000   29.000000  \n",
      "5     0.916230    0.887435    0.900524  \n",
      "6     0.840000    0.750000    0.804348  \n",
      "7     0.636364    0.537313    0.560606  \n",
      "8     0.974700    0.961900    0.971500  \n",
      "9     0.724138    0.626087    0.660714  \n",
      "10    0.911487    0.879783    0.893167  \n",
      "11    0.837378    0.779916    0.801216  \n",
      "12    0.805524    0.749609    0.766063  \n",
      "13    0.684914    0.572782    0.618131  \n",
      "14    0.927700    0.907200    0.913700  \n",
      "15    0.805524    0.749609    0.766063  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_xgb_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_6_cat = np.where(((y_pred_xgb_6 >= 2) | (y_pred_xgb_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:18:01,633] Trial 350 finished with value: 0.694402896567323 and parameters: {'n_estimators': 751, 'eta': 0.05772952605761577, 'max_depth': 11, 'alpha': 0.0656, 'lambda': 27.124866709708247, 'max_bin': 383}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:18:12,222] Trial 351 finished with value: 0.6938558608505768 and parameters: {'n_estimators': 735, 'eta': 0.057115503028259434, 'max_depth': 11, 'alpha': 0.0015, 'lambda': 15.782265450770794, 'max_bin': 439}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:18:28,266] Trial 352 finished with value: 0.692543596028539 and parameters: {'n_estimators': 716, 'eta': 0.0487151928926239, 'max_depth': 11, 'alpha': 0.039400000000000004, 'lambda': 37.69485506628514, 'max_bin': 428}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:18:44,600] Trial 353 finished with value: 0.6933173565876096 and parameters: {'n_estimators': 784, 'eta': 0.05062480824709879, 'max_depth': 11, 'alpha': 0.0201, 'lambda': 31.928397438072906, 'max_bin': 388}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:18:55,533] Trial 354 finished with value: 0.6965506374652791 and parameters: {'n_estimators': 759, 'eta': 0.0861698359462824, 'max_depth': 11, 'alpha': 0.049100000000000005, 'lambda': 34.349009846748864, 'max_bin': 421}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:19:08,508] Trial 355 finished with value: 0.6911464593590907 and parameters: {'n_estimators': 813, 'eta': 0.06019832966748122, 'max_depth': 11, 'alpha': 0.0636, 'lambda': 24.874465889229803, 'max_bin': 416}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:19:23,443] Trial 356 finished with value: 0.6939705975425883 and parameters: {'n_estimators': 741, 'eta': 0.05326817999955867, 'max_depth': 11, 'alpha': 0.0339, 'lambda': 26.63460252210431, 'max_bin': 434}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:19:39,233] Trial 357 finished with value: 0.6943083097755987 and parameters: {'n_estimators': 774, 'eta': 0.05540977096959344, 'max_depth': 11, 'alpha': 0.0017000000000000001, 'lambda': 29.669475561628953, 'max_bin': 452}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:19:51,815] Trial 358 finished with value: 0.6915458698201187 and parameters: {'n_estimators': 714, 'eta': 0.04610564243014064, 'max_depth': 11, 'alpha': 0.0211, 'lambda': 14.909688209603546, 'max_bin': 423}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:20:04,243] Trial 359 finished with value: 0.6947165934850305 and parameters: {'n_estimators': 754, 'eta': 0.0488315807512279, 'max_depth': 11, 'alpha': 0.0417, 'lambda': 11.005492154693206, 'max_bin': 430}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:20:18,352] Trial 360 finished with value: 0.6925518068393838 and parameters: {'n_estimators': 831, 'eta': 0.05098507182778349, 'max_depth': 11, 'alpha': 0.0613, 'lambda': 22.66963591277024, 'max_bin': 439}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:20:30,584] Trial 361 finished with value: 0.6893753096036273 and parameters: {'n_estimators': 791, 'eta': 0.04675528835272266, 'max_depth': 11, 'alpha': 0.019200000000000002, 'lambda': 15.739186008923037, 'max_bin': 382}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:20:45,138] Trial 362 finished with value: 0.6939064250492694 and parameters: {'n_estimators': 735, 'eta': 0.05266393095741596, 'max_depth': 11, 'alpha': 0.0, 'lambda': 36.10427931397803, 'max_bin': 413}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:20:55,239] Trial 363 finished with value: 0.6902935773397837 and parameters: {'n_estimators': 768, 'eta': 0.04824590959693561, 'max_depth': 11, 'alpha': 0.0347, 'lambda': 12.505187505200162, 'max_bin': 446}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:21:12,150] Trial 364 finished with value: 0.695439397523781 and parameters: {'n_estimators': 752, 'eta': 0.0283589946551262, 'max_depth': 11, 'alpha': 0.0684, 'lambda': 14.4372886788714, 'max_bin': 423}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:21:22,592] Trial 365 finished with value: 0.6922192268969422 and parameters: {'n_estimators': 802, 'eta': 0.05057264562020328, 'max_depth': 11, 'alpha': 0.0414, 'lambda': 13.246873598607046, 'max_bin': 426}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:21:34,761] Trial 366 finished with value: 0.6938521699251654 and parameters: {'n_estimators': 729, 'eta': 0.05782687021877361, 'max_depth': 11, 'alpha': 0.019200000000000002, 'lambda': 14.988757964048979, 'max_bin': 392}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:21:45,418] Trial 367 finished with value: 0.692207229509172 and parameters: {'n_estimators': 780, 'eta': 0.056055211605686356, 'max_depth': 11, 'alpha': 0.0, 'lambda': 14.089533570391781, 'max_bin': 415}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:21:57,336] Trial 368 finished with value: 0.6939682520871489 and parameters: {'n_estimators': 710, 'eta': 0.04931869147165899, 'max_depth': 11, 'alpha': 0.0509, 'lambda': 11.816573012603955, 'max_bin': 402}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:22:09,329] Trial 369 finished with value: 0.6921878785126184 and parameters: {'n_estimators': 763, 'eta': 0.04554900096234116, 'max_depth': 11, 'alpha': 0.029, 'lambda': 12.905126237087186, 'max_bin': 435}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:22:20,077] Trial 370 finished with value: 0.6914277944676522 and parameters: {'n_estimators': 741, 'eta': 0.05365867438503996, 'max_depth': 11, 'alpha': 0.0205, 'lambda': 15.539982046392867, 'max_bin': 430}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:22:36,060] Trial 371 finished with value: 0.6932532989390834 and parameters: {'n_estimators': 779, 'eta': 0.047879379294764324, 'max_depth': 11, 'alpha': 0.06770000000000001, 'lambda': 33.466590907162235, 'max_bin': 417}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:22:57,164] Trial 372 finished with value: 0.6929190677382328 and parameters: {'n_estimators': 723, 'eta': 0.023971856227398742, 'max_depth': 11, 'alpha': 0.0463, 'lambda': 28.603681132840265, 'max_bin': 385}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:23:21,599] Trial 373 finished with value: 0.5857695259526118 and parameters: {'n_estimators': 753, 'eta': 0.002743226428967148, 'max_depth': 10, 'alpha': 0.0189, 'lambda': 13.568755685190157, 'max_bin': 397}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:23:32,682] Trial 374 finished with value: 0.6954395497538954 and parameters: {'n_estimators': 811, 'eta': 0.051199779839547224, 'max_depth': 11, 'alpha': 0.0373, 'lambda': 14.48713285070981, 'max_bin': 409}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:23:43,302] Trial 375 finished with value: 0.6952959397206231 and parameters: {'n_estimators': 766, 'eta': 0.05240869548441935, 'max_depth': 11, 'alpha': 0.054200000000000005, 'lambda': 12.162305784955251, 'max_bin': 442}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:23:53,725] Trial 376 finished with value: 0.6911055510861319 and parameters: {'n_estimators': 792, 'eta': 0.049469453639778976, 'max_depth': 8, 'alpha': 0.0162, 'lambda': 16.044091996245875, 'max_bin': 471}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:24:02,409] Trial 377 finished with value: 0.6905538384502268 and parameters: {'n_estimators': 739, 'eta': 0.09806216858097, 'max_depth': 11, 'alpha': 0.0015, 'lambda': 36.140150801629595, 'max_bin': 427}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:24:13,507] Trial 378 finished with value: 0.6960498307743224 and parameters: {'n_estimators': 721, 'eta': 0.04583384813179694, 'max_depth': 11, 'alpha': 0.0683, 'lambda': 10.709244327692055, 'max_bin': 379}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:24:24,579] Trial 379 finished with value: 0.6928893612133911 and parameters: {'n_estimators': 749, 'eta': 0.059147228639860876, 'max_depth': 11, 'alpha': 0.0322, 'lambda': 14.861838429998677, 'max_bin': 420}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:24:36,798] Trial 380 finished with value: 0.6942005666738564 and parameters: {'n_estimators': 772, 'eta': 0.047949517934609436, 'max_depth': 11, 'alpha': 0.0463, 'lambda': 13.048034917993794, 'max_bin': 433}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:24:49,272] Trial 381 finished with value: 0.693411887655769 and parameters: {'n_estimators': 734, 'eta': 0.05481486122983854, 'max_depth': 11, 'alpha': 0.0167, 'lambda': 13.947301834085165, 'max_bin': 413}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:25:01,517] Trial 382 finished with value: 0.6930142570030704 and parameters: {'n_estimators': 711, 'eta': 0.05067969231984797, 'max_depth': 11, 'alpha': 0.031, 'lambda': 15.809128086441074, 'max_bin': 438}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:25:13,068] Trial 383 finished with value: 0.6922608295268882 and parameters: {'n_estimators': 794, 'eta': 0.04575908004325881, 'max_depth': 10, 'alpha': 0.0008, 'lambda': 12.45100728265283, 'max_bin': 457}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:25:23,647] Trial 384 finished with value: 0.6955677214052133 and parameters: {'n_estimators': 759, 'eta': 0.05240422734302103, 'max_depth': 11, 'alpha': 0.0694, 'lambda': 11.519263107751918, 'max_bin': 405}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:25:40,609] Trial 385 finished with value: 0.6948330002532709 and parameters: {'n_estimators': 780, 'eta': 0.04813864323421928, 'max_depth': 11, 'alpha': 0.0534, 'lambda': 37.49268207497954, 'max_bin': 423}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:25:52,602] Trial 386 finished with value: 0.6915799515981791 and parameters: {'n_estimators': 748, 'eta': 0.05562119736194256, 'max_depth': 11, 'alpha': 0.0325, 'lambda': 22.231811092921156, 'max_bin': 448}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:26:03,651] Trial 387 finished with value: 0.6914827605635969 and parameters: {'n_estimators': 726, 'eta': 0.049823987460096086, 'max_depth': 11, 'alpha': 0.017, 'lambda': 14.052829142601873, 'max_bin': 390}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:26:15,652] Trial 388 finished with value: 0.6909559042041347 and parameters: {'n_estimators': 825, 'eta': 0.04716131981843715, 'max_depth': 11, 'alpha': 0.047900000000000005, 'lambda': 15.23937258993633, 'max_bin': 429}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:26:24,224] Trial 389 finished with value: 0.6866134416811323 and parameters: {'n_estimators': 762, 'eta': 0.05327760416290898, 'max_depth': 7, 'alpha': 0.0002, 'lambda': 13.444666355102969, 'max_bin': 420}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:26:44,733] Trial 390 finished with value: 0.6927704306073148 and parameters: {'n_estimators': 707, 'eta': 0.01859331832695047, 'max_depth': 11, 'alpha': 0.0318, 'lambda': 14.596879133647809, 'max_bin': 379}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:26:59,227] Trial 391 finished with value: 0.6942101278741436 and parameters: {'n_estimators': 777, 'eta': 0.05143654101871593, 'max_depth': 11, 'alpha': 0.07100000000000001, 'lambda': 26.11174495559488, 'max_bin': 412}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:27:15,774] Trial 392 finished with value: 0.6952158551533942 and parameters: {'n_estimators': 804, 'eta': 0.04567630650612483, 'max_depth': 11, 'alpha': 0.0182, 'lambda': 39.45304099434598, 'max_bin': 374}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:27:26,855] Trial 393 finished with value: 0.6945425156087497 and parameters: {'n_estimators': 742, 'eta': 0.04886083953218551, 'max_depth': 11, 'alpha': 0.0485, 'lambda': 12.841084947367033, 'max_bin': 435}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:27:40,841] Trial 394 finished with value: 0.691691545509113 and parameters: {'n_estimators': 762, 'eta': 0.05036853132503007, 'max_depth': 11, 'alpha': 0.0371, 'lambda': 28.128448102698734, 'max_bin': 384}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:27:52,113] Trial 395 finished with value: 0.6898653370020457 and parameters: {'n_estimators': 732, 'eta': 0.05387553244248237, 'max_depth': 11, 'alpha': 0.0198, 'lambda': 16.098534147471735, 'max_bin': 426}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:28:07,894] Trial 396 finished with value: 0.6941609483509275 and parameters: {'n_estimators': 779, 'eta': 0.04754283505868495, 'max_depth': 11, 'alpha': 0.067, 'lambda': 23.846468644603206, 'max_bin': 441}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:28:18,041] Trial 397 finished with value: 0.6909038748560458 and parameters: {'n_estimators': 745, 'eta': 0.057014331974056136, 'max_depth': 11, 'alpha': 0.0001, 'lambda': 13.83745697543001, 'max_bin': 419}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:28:28,433] Trial 398 finished with value: 0.6926583832091879 and parameters: {'n_estimators': 797, 'eta': 0.051997218079585114, 'max_depth': 11, 'alpha': 0.0333, 'lambda': 15.00028870761948, 'max_bin': 431}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:28:40,874] Trial 399 finished with value: 0.6943031174407703 and parameters: {'n_estimators': 719, 'eta': 0.045325428250819706, 'max_depth': 11, 'alpha': 0.0524, 'lambda': 11.458442657011789, 'max_bin': 399}. Best is trial 266 with value: 0.734436427190172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7344\n",
      "\tBest params:\n",
      "\t\tn_estimators: 763\n",
      "\t\teta: 0.05057363757927622\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.001\n",
      "\t\tlambda: 14.263066723554317\n",
      "\t\tmax_bin: 412\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.703752    0.746467    0.736627    0.749621   \n",
      "1                    TP   34.000000   37.000000   37.000000   34.000000   \n",
      "2                    TN  304.000000  305.000000  308.000000  307.000000   \n",
      "3                    FP   12.000000   10.000000    6.000000    9.000000   \n",
      "4                    FN   32.000000   30.000000   31.000000   32.000000   \n",
      "5              Accuracy    0.884817    0.895288    0.903141    0.892670   \n",
      "6             Precision    0.739130    0.787234    0.860465    0.790698   \n",
      "7           Sensitivity    0.515152    0.552239    0.544118    0.515152   \n",
      "8           Specificity    0.962000    0.968300    0.980900    0.971500   \n",
      "9              F1 score    0.607143    0.649123    0.666667    0.623853   \n",
      "10  F1 score (weighted)    0.876299    0.887714    0.894088    0.883231   \n",
      "11     F1 score (macro)    0.769829    0.793792    0.805003    0.780629   \n",
      "12    Balanced Accuracy    0.738588    0.760246    0.762505    0.743335   \n",
      "13                  MCC    0.554302    0.602610    0.635407    0.582125   \n",
      "14                  NPV    0.904800    0.910400    0.908600    0.905600   \n",
      "15              ROC_AUC    0.738588    0.760246    0.762505    0.743335   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.804255    0.737585    0.763038    0.783537  \n",
      "1    42.000000   36.000000   37.000000   39.000000  \n",
      "2   308.000000  303.000000  307.000000  308.000000  \n",
      "3     8.000000   12.000000    9.000000    7.000000  \n",
      "4    24.000000   31.000000   29.000000   28.000000  \n",
      "5     0.916230    0.887435    0.900524    0.908377  \n",
      "6     0.840000    0.750000    0.804348    0.847826  \n",
      "7     0.636364    0.537313    0.560606    0.582090  \n",
      "8     0.974700    0.961900    0.971500    0.977800  \n",
      "9     0.724138    0.626087    0.660714    0.690265  \n",
      "10    0.911487    0.879783    0.893167    0.901341  \n",
      "11    0.837378    0.779916    0.801216    0.818251  \n",
      "12    0.805524    0.749609    0.766063    0.779934  \n",
      "13    0.684914    0.572782    0.618131    0.654228  \n",
      "14    0.927700    0.907200    0.913700    0.916700  \n",
      "15    0.805524    0.749609    0.766063    0.779934  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_xgb_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_7_cat = np.where(((y_pred_xgb_7 >= 2) | (y_pred_xgb_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:28:58,065] Trial 400 finished with value: 0.7278087402653546 and parameters: {'n_estimators': 766, 'eta': 0.04899580289052901, 'max_depth': 11, 'alpha': 0.0193, 'lambda': 24.873358388907214, 'max_bin': 394}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:29:11,596] Trial 401 finished with value: 0.7244850389515971 and parameters: {'n_estimators': 751, 'eta': 0.05043545619807492, 'max_depth': 11, 'alpha': 0.0806, 'lambda': 13.199281622000232, 'max_bin': 408}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:29:27,451] Trial 402 finished with value: 0.7334463683032388 and parameters: {'n_estimators': 788, 'eta': 0.05455089039521898, 'max_depth': 11, 'alpha': 0.6356, 'lambda': 32.66276627248261, 'max_bin': 416}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:29:41,224] Trial 403 finished with value: 0.7318414136965179 and parameters: {'n_estimators': 816, 'eta': 0.061271182833108076, 'max_depth': 11, 'alpha': 0.7773, 'lambda': 32.985076717451236, 'max_bin': 415}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:29:55,885] Trial 404 finished with value: 0.7277699678783375 and parameters: {'n_estimators': 821, 'eta': 0.060600808374705606, 'max_depth': 11, 'alpha': 0.7123, 'lambda': 23.63517090243245, 'max_bin': 414}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:30:10,786] Trial 405 finished with value: 0.730574758073404 and parameters: {'n_estimators': 843, 'eta': 0.06722940468259338, 'max_depth': 11, 'alpha': 0.6432, 'lambda': 32.18107908176482, 'max_bin': 417}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:30:24,007] Trial 406 finished with value: 0.7296304599761531 and parameters: {'n_estimators': 810, 'eta': 0.062411459878040774, 'max_depth': 11, 'alpha': 0.7185, 'lambda': 31.140525505969666, 'max_bin': 410}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:30:38,569] Trial 407 finished with value: 0.7321554364164199 and parameters: {'n_estimators': 793, 'eta': 0.06209468766429424, 'max_depth': 11, 'alpha': 0.9571000000000001, 'lambda': 33.699703559119136, 'max_bin': 403}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:30:49,467] Trial 408 finished with value: 0.7307257020148804 and parameters: {'n_estimators': 818, 'eta': 0.06441518051135624, 'max_depth': 10, 'alpha': 0.676, 'lambda': 32.862926098393835, 'max_bin': 404}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:31:04,223] Trial 409 finished with value: 0.7319627373900628 and parameters: {'n_estimators': 796, 'eta': 0.0593888734136414, 'max_depth': 11, 'alpha': 0.5918, 'lambda': 34.03108081127659, 'max_bin': 411}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:31:18,934] Trial 410 finished with value: 0.7330663616018865 and parameters: {'n_estimators': 838, 'eta': 0.05723669474392325, 'max_depth': 11, 'alpha': 0.7603000000000001, 'lambda': 33.33033228383898, 'max_bin': 404}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:31:33,615] Trial 411 finished with value: 0.7335115706390157 and parameters: {'n_estimators': 796, 'eta': 0.05872214851527787, 'max_depth': 11, 'alpha': 0.5835, 'lambda': 35.02079104320388, 'max_bin': 401}. Best is trial 266 with value: 0.734436427190172.\n",
      "[I 2023-12-20 17:31:48,514] Trial 412 finished with value: 0.7363592711203093 and parameters: {'n_estimators': 841, 'eta': 0.0593546986294894, 'max_depth': 11, 'alpha': 0.8138000000000001, 'lambda': 34.19294262674558, 'max_bin': 405}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:32:04,238] Trial 413 finished with value: 0.7343586610944819 and parameters: {'n_estimators': 850, 'eta': 0.05986774338351438, 'max_depth': 11, 'alpha': 0.8774000000000001, 'lambda': 34.25792303793202, 'max_bin': 402}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:32:20,124] Trial 414 finished with value: 0.7336016489143868 and parameters: {'n_estimators': 850, 'eta': 0.05987026492263799, 'max_depth': 11, 'alpha': 0.8351000000000001, 'lambda': 36.177590531086715, 'max_bin': 401}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:32:35,890] Trial 415 finished with value: 0.7324963194874302 and parameters: {'n_estimators': 855, 'eta': 0.059927238509738655, 'max_depth': 11, 'alpha': 0.8224, 'lambda': 35.16766020854011, 'max_bin': 400}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:32:50,999] Trial 416 finished with value: 0.7329546519170803 and parameters: {'n_estimators': 839, 'eta': 0.05918478745032517, 'max_depth': 11, 'alpha': 0.8296, 'lambda': 34.8911108710911, 'max_bin': 399}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:33:06,182] Trial 417 finished with value: 0.7348075130251072 and parameters: {'n_estimators': 853, 'eta': 0.05923102138977904, 'max_depth': 11, 'alpha': 0.8336, 'lambda': 34.85414178352238, 'max_bin': 402}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:33:22,783] Trial 418 finished with value: 0.7336884341156568 and parameters: {'n_estimators': 858, 'eta': 0.058869126366002486, 'max_depth': 10, 'alpha': 0.8233, 'lambda': 35.43697756812693, 'max_bin': 401}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:33:36,739] Trial 419 finished with value: 0.7354075560223174 and parameters: {'n_estimators': 861, 'eta': 0.059946145217411595, 'max_depth': 9, 'alpha': 0.8130000000000001, 'lambda': 33.955511393615296, 'max_bin': 401}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:33:51,526] Trial 420 finished with value: 0.7342514002478774 and parameters: {'n_estimators': 871, 'eta': 0.0589284394905774, 'max_depth': 9, 'alpha': 0.8255, 'lambda': 35.48668655764205, 'max_bin': 399}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:34:05,090] Trial 421 finished with value: 0.7352847696288517 and parameters: {'n_estimators': 863, 'eta': 0.06061078121968845, 'max_depth': 9, 'alpha': 0.8694000000000001, 'lambda': 35.18395809801484, 'max_bin': 399}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:34:20,020] Trial 422 finished with value: 0.7316265472919025 and parameters: {'n_estimators': 864, 'eta': 0.05901252765783504, 'max_depth': 9, 'alpha': 0.8119000000000001, 'lambda': 35.4324295191497, 'max_bin': 399}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:34:34,463] Trial 423 finished with value: 0.7347162376580627 and parameters: {'n_estimators': 854, 'eta': 0.059068071268569826, 'max_depth': 9, 'alpha': 0.8679, 'lambda': 35.43589437453987, 'max_bin': 397}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:34:48,298] Trial 424 finished with value: 0.7347571789410586 and parameters: {'n_estimators': 848, 'eta': 0.060825800824142136, 'max_depth': 9, 'alpha': 0.8694000000000001, 'lambda': 34.993267648427164, 'max_bin': 397}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:35:04,130] Trial 425 finished with value: 0.7353822944305011 and parameters: {'n_estimators': 854, 'eta': 0.06037187047265694, 'max_depth': 9, 'alpha': 0.8629, 'lambda': 35.035706320479655, 'max_bin': 397}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:35:18,567] Trial 426 finished with value: 0.7346320092651009 and parameters: {'n_estimators': 851, 'eta': 0.060309255474609705, 'max_depth': 9, 'alpha': 0.8775000000000001, 'lambda': 35.2193655870569, 'max_bin': 395}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:35:32,950] Trial 427 finished with value: 0.7336655015726468 and parameters: {'n_estimators': 851, 'eta': 0.06024651457794968, 'max_depth': 9, 'alpha': 0.8757, 'lambda': 34.79400662340244, 'max_bin': 399}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:35:47,574] Trial 428 finished with value: 0.7357070849004094 and parameters: {'n_estimators': 881, 'eta': 0.06113466111771109, 'max_depth': 9, 'alpha': 0.8776, 'lambda': 34.7777653945583, 'max_bin': 396}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:36:01,896] Trial 429 finished with value: 0.7338314784407751 and parameters: {'n_estimators': 881, 'eta': 0.061689550541848395, 'max_depth': 9, 'alpha': 0.8689, 'lambda': 34.75043920648228, 'max_bin': 396}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:36:15,873] Trial 430 finished with value: 0.734903471658046 and parameters: {'n_estimators': 879, 'eta': 0.06158514619882459, 'max_depth': 9, 'alpha': 0.8792000000000001, 'lambda': 34.65913837838647, 'max_bin': 396}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:36:31,425] Trial 431 finished with value: 0.7347781172801318 and parameters: {'n_estimators': 880, 'eta': 0.06257740388453277, 'max_depth': 9, 'alpha': 0.8811, 'lambda': 34.83246681989706, 'max_bin': 396}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:36:45,732] Trial 432 finished with value: 0.7313103427858915 and parameters: {'n_estimators': 879, 'eta': 0.06247717205080902, 'max_depth': 9, 'alpha': 0.8777, 'lambda': 34.67585160032069, 'max_bin': 396}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:36:59,634] Trial 433 finished with value: 0.7334935008869842 and parameters: {'n_estimators': 873, 'eta': 0.06084030963923953, 'max_depth': 9, 'alpha': 0.8581000000000001, 'lambda': 34.930436388776485, 'max_bin': 397}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:37:13,172] Trial 434 finished with value: 0.7341874522039834 and parameters: {'n_estimators': 876, 'eta': 0.06120993117390258, 'max_depth': 9, 'alpha': 0.9061, 'lambda': 34.54101765690791, 'max_bin': 394}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:37:26,394] Trial 435 finished with value: 0.7333192880208712 and parameters: {'n_estimators': 878, 'eta': 0.0636707133790079, 'max_depth': 9, 'alpha': 0.9215000000000001, 'lambda': 35.47599151888313, 'max_bin': 394}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:37:41,085] Trial 436 finished with value: 0.7340245458181773 and parameters: {'n_estimators': 882, 'eta': 0.06326124069155065, 'max_depth': 9, 'alpha': 0.9204, 'lambda': 35.217768249195316, 'max_bin': 394}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:37:54,658] Trial 437 finished with value: 0.733064969822767 and parameters: {'n_estimators': 882, 'eta': 0.06360531549700181, 'max_depth': 9, 'alpha': 0.9126000000000001, 'lambda': 35.43768191125072, 'max_bin': 394}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:38:07,716] Trial 438 finished with value: 0.7320953462335882 and parameters: {'n_estimators': 864, 'eta': 0.06091870352472041, 'max_depth': 9, 'alpha': 0.8585, 'lambda': 34.73844726169101, 'max_bin': 392}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:38:21,856] Trial 439 finished with value: 0.734014414625124 and parameters: {'n_estimators': 890, 'eta': 0.062350896565693065, 'max_depth': 9, 'alpha': 0.8903000000000001, 'lambda': 34.16844443081291, 'max_bin': 396}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:38:34,864] Trial 440 finished with value: 0.7325550410840125 and parameters: {'n_estimators': 899, 'eta': 0.06137731107602569, 'max_depth': 9, 'alpha': 0.8827, 'lambda': 33.910564769206566, 'max_bin': 398}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:38:49,272] Trial 441 finished with value: 0.7338445063731707 and parameters: {'n_estimators': 852, 'eta': 0.06559645782356734, 'max_depth': 9, 'alpha': 0.8560000000000001, 'lambda': 34.41709158404162, 'max_bin': 399}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:39:02,159] Trial 442 finished with value: 0.7312873434596223 and parameters: {'n_estimators': 856, 'eta': 0.06623248084166376, 'max_depth': 9, 'alpha': 0.8572000000000001, 'lambda': 34.31807722042591, 'max_bin': 390}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:39:17,164] Trial 443 finished with value: 0.733776423652009 and parameters: {'n_estimators': 872, 'eta': 0.06506138394596343, 'max_depth': 9, 'alpha': 0.8959, 'lambda': 35.832206045192, 'max_bin': 396}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:39:32,346] Trial 444 finished with value: 0.7335089647961361 and parameters: {'n_estimators': 851, 'eta': 0.06458399950081181, 'max_depth': 9, 'alpha': 0.8931, 'lambda': 36.17832617206276, 'max_bin': 400}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:39:45,864] Trial 445 finished with value: 0.7341402716323918 and parameters: {'n_estimators': 884, 'eta': 0.06284094531486865, 'max_depth': 9, 'alpha': 0.9055000000000001, 'lambda': 35.70199102473298, 'max_bin': 389}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:40:01,257] Trial 446 finished with value: 0.7327599447550984 and parameters: {'n_estimators': 889, 'eta': 0.0626441141286805, 'max_depth': 9, 'alpha': 0.9228000000000001, 'lambda': 36.24006065983164, 'max_bin': 390}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:40:16,898] Trial 447 finished with value: 0.7348635789895611 and parameters: {'n_estimators': 872, 'eta': 0.0656916679934953, 'max_depth': 9, 'alpha': 0.9028, 'lambda': 34.12050217680561, 'max_bin': 396}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:40:30,535] Trial 448 finished with value: 0.7330674705368141 and parameters: {'n_estimators': 868, 'eta': 0.06539125771393439, 'max_depth': 9, 'alpha': 0.9059, 'lambda': 34.08636504592015, 'max_bin': 395}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:40:44,615] Trial 449 finished with value: 0.7357338989733964 and parameters: {'n_estimators': 886, 'eta': 0.06392470771154796, 'max_depth': 9, 'alpha': 0.9346000000000001, 'lambda': 34.00754300269656, 'max_bin': 388}. Best is trial 412 with value: 0.7363592711203093.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7364\n",
      "\tBest params:\n",
      "\t\tn_estimators: 841\n",
      "\t\teta: 0.0593546986294894\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.8138000000000001\n",
      "\t\tlambda: 34.19294262674558\n",
      "\t\tmax_bin: 405\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.703752    0.746467    0.736627    0.749621   \n",
      "1                    TP   34.000000   37.000000   37.000000   34.000000   \n",
      "2                    TN  304.000000  305.000000  308.000000  307.000000   \n",
      "3                    FP   12.000000   10.000000    6.000000    9.000000   \n",
      "4                    FN   32.000000   30.000000   31.000000   32.000000   \n",
      "5              Accuracy    0.884817    0.895288    0.903141    0.892670   \n",
      "6             Precision    0.739130    0.787234    0.860465    0.790698   \n",
      "7           Sensitivity    0.515152    0.552239    0.544118    0.515152   \n",
      "8           Specificity    0.962000    0.968300    0.980900    0.971500   \n",
      "9              F1 score    0.607143    0.649123    0.666667    0.623853   \n",
      "10  F1 score (weighted)    0.876299    0.887714    0.894088    0.883231   \n",
      "11     F1 score (macro)    0.769829    0.793792    0.805003    0.780629   \n",
      "12    Balanced Accuracy    0.738588    0.760246    0.762505    0.743335   \n",
      "13                  MCC    0.554302    0.602610    0.635407    0.582125   \n",
      "14                  NPV    0.904800    0.910400    0.908600    0.905600   \n",
      "15              ROC_AUC    0.738588    0.760246    0.762505    0.743335   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.804255    0.737585    0.763038    0.783537    0.694418  \n",
      "1    42.000000   36.000000   37.000000   39.000000   38.000000  \n",
      "2   308.000000  303.000000  307.000000  308.000000  304.000000  \n",
      "3     8.000000   12.000000    9.000000    7.000000   10.000000  \n",
      "4    24.000000   31.000000   29.000000   28.000000   30.000000  \n",
      "5     0.916230    0.887435    0.900524    0.908377    0.895288  \n",
      "6     0.840000    0.750000    0.804348    0.847826    0.791667  \n",
      "7     0.636364    0.537313    0.560606    0.582090    0.558824  \n",
      "8     0.974700    0.961900    0.971500    0.977800    0.968200  \n",
      "9     0.724138    0.626087    0.660714    0.690265    0.655172  \n",
      "10    0.911487    0.879783    0.893167    0.901341    0.887877  \n",
      "11    0.837378    0.779916    0.801216    0.818251    0.796722  \n",
      "12    0.805524    0.749609    0.766063    0.779934    0.763488  \n",
      "13    0.684914    0.572782    0.618131    0.654228    0.608158  \n",
      "14    0.927700    0.907200    0.913700    0.916700    0.910200  \n",
      "15    0.805524    0.749609    0.766063    0.779934    0.763488  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_xgb_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_8_cat = np.where(((y_pred_xgb_8 >= 2) | (y_pred_xgb_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:40:59,078] Trial 450 finished with value: 0.7268020240057134 and parameters: {'n_estimators': 900, 'eta': 0.06536776110843123, 'max_depth': 9, 'alpha': 0.9843000000000001, 'lambda': 34.25616890902637, 'max_bin': 389}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:41:12,867] Trial 451 finished with value: 0.7267599540839949 and parameters: {'n_estimators': 882, 'eta': 0.06319257426969639, 'max_depth': 9, 'alpha': 0.8055, 'lambda': 35.5367183849785, 'max_bin': 392}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:41:27,678] Trial 452 finished with value: 0.7258132086242918 and parameters: {'n_estimators': 873, 'eta': 0.06702071699698284, 'max_depth': 9, 'alpha': 0.9331, 'lambda': 33.81079160469248, 'max_bin': 389}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:41:40,387] Trial 453 finished with value: 0.7228154567917471 and parameters: {'n_estimators': 869, 'eta': 0.06272766944493312, 'max_depth': 9, 'alpha': 0.8953000000000001, 'lambda': 36.84892958530951, 'max_bin': 394}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:41:53,410] Trial 454 finished with value: 0.7248961377698674 and parameters: {'n_estimators': 886, 'eta': 0.06474474694221587, 'max_depth': 9, 'alpha': 0.8498, 'lambda': 35.64925286439213, 'max_bin': 395}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:42:05,566] Trial 455 finished with value: 0.7249712025956073 and parameters: {'n_estimators': 858, 'eta': 0.0682721381933319, 'max_depth': 9, 'alpha': 0.8701000000000001, 'lambda': 34.52471421034936, 'max_bin': 404}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:42:18,002] Trial 456 finished with value: 0.7215244900680415 and parameters: {'n_estimators': 889, 'eta': 0.06190323968938513, 'max_depth': 9, 'alpha': 0.9001, 'lambda': 36.80850650304359, 'max_bin': 388}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:42:31,272] Trial 457 finished with value: 0.7245379526216154 and parameters: {'n_estimators': 865, 'eta': 0.06485011124620134, 'max_depth': 9, 'alpha': 0.8775000000000001, 'lambda': 33.79218535521152, 'max_bin': 397}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:42:45,127] Trial 458 finished with value: 0.7268357148917193 and parameters: {'n_estimators': 844, 'eta': 0.06178711922902809, 'max_depth': 9, 'alpha': 0.9486, 'lambda': 35.30021114696874, 'max_bin': 405}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:42:58,260] Trial 459 finished with value: 0.7258430939378718 and parameters: {'n_estimators': 882, 'eta': 0.06359071736386487, 'max_depth': 9, 'alpha': 0.8350000000000001, 'lambda': 34.679444210884604, 'max_bin': 396}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:43:10,317] Trial 460 finished with value: 0.7253640104145734 and parameters: {'n_estimators': 867, 'eta': 0.06956314958479601, 'max_depth': 9, 'alpha': 0.8915000000000001, 'lambda': 33.53196874049072, 'max_bin': 388}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:43:23,567] Trial 461 finished with value: 0.7239437528893339 and parameters: {'n_estimators': 843, 'eta': 0.06633126375474498, 'max_depth': 9, 'alpha': 0.8647, 'lambda': 35.72207447912074, 'max_bin': 402}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:43:37,074] Trial 462 finished with value: 0.7230485908854225 and parameters: {'n_estimators': 900, 'eta': 0.061308715107828836, 'max_depth': 9, 'alpha': 0.797, 'lambda': 36.70881811018608, 'max_bin': 393}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:43:50,390] Trial 463 finished with value: 0.7217931804716751 and parameters: {'n_estimators': 875, 'eta': 0.060045920222892256, 'max_depth': 9, 'alpha': 0.8462000000000001, 'lambda': 34.585248355229744, 'max_bin': 398}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:44:03,871] Trial 464 finished with value: 0.7249253823929169 and parameters: {'n_estimators': 854, 'eta': 0.06384883263665411, 'max_depth': 9, 'alpha': 0.9263, 'lambda': 35.45574598383748, 'max_bin': 404}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:44:17,916] Trial 465 finished with value: 0.7244598891547195 and parameters: {'n_estimators': 835, 'eta': 0.05859006214658596, 'max_depth': 9, 'alpha': 0.9085000000000001, 'lambda': 34.12232755949447, 'max_bin': 392}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:44:31,971] Trial 466 finished with value: 0.7266259136357098 and parameters: {'n_estimators': 884, 'eta': 0.0660894341523894, 'max_depth': 9, 'alpha': 0.8882, 'lambda': 33.28728531482281, 'max_bin': 387}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:44:45,463] Trial 467 finished with value: 0.7257566008961411 and parameters: {'n_estimators': 863, 'eta': 0.062429545501970794, 'max_depth': 9, 'alpha': 0.8162, 'lambda': 35.914765708715166, 'max_bin': 397}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:44:56,565] Trial 468 finished with value: 0.7240489272521662 and parameters: {'n_estimators': 849, 'eta': 0.06877215707682184, 'max_depth': 8, 'alpha': 0.8625, 'lambda': 34.97832042234047, 'max_bin': 405}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:45:11,412] Trial 469 finished with value: 0.724227317796065 and parameters: {'n_estimators': 900, 'eta': 0.060619323068919796, 'max_depth': 9, 'alpha': 0.9392, 'lambda': 36.79941217371873, 'max_bin': 399}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:45:24,922] Trial 470 finished with value: 0.7230682947867288 and parameters: {'n_estimators': 873, 'eta': 0.05858827218841633, 'max_depth': 9, 'alpha': 0.8391000000000001, 'lambda': 34.27523230414853, 'max_bin': 391}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:45:37,799] Trial 471 finished with value: 0.7257351792329079 and parameters: {'n_estimators': 836, 'eta': 0.06339054527699572, 'max_depth': 9, 'alpha': 0.8753000000000001, 'lambda': 35.28266699224515, 'max_bin': 401}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:45:51,539] Trial 472 finished with value: 0.7272847188295957 and parameters: {'n_estimators': 859, 'eta': 0.061281827070713855, 'max_depth': 9, 'alpha': 0.9088, 'lambda': 33.36952630753368, 'max_bin': 395}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:46:05,104] Trial 473 finished with value: 0.724188756109408 and parameters: {'n_estimators': 886, 'eta': 0.05853587106602818, 'max_depth': 9, 'alpha': 0.9792000000000001, 'lambda': 35.98790458618889, 'max_bin': 406}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:46:19,023] Trial 474 finished with value: 0.7247766221838121 and parameters: {'n_estimators': 874, 'eta': 0.06534884128517009, 'max_depth': 9, 'alpha': 0.8844000000000001, 'lambda': 34.3827918799725, 'max_bin': 389}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:46:30,222] Trial 475 finished with value: 0.721080742252463 and parameters: {'n_estimators': 849, 'eta': 0.0677764526699997, 'max_depth': 9, 'alpha': 0.8476, 'lambda': 37.574567252238026, 'max_bin': 396}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:46:43,881] Trial 476 finished with value: 0.7266916134531544 and parameters: {'n_estimators': 887, 'eta': 0.06264099354452339, 'max_depth': 9, 'alpha': 0.9006000000000001, 'lambda': 35.093631887794096, 'max_bin': 402}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:46:56,677] Trial 477 finished with value: 0.7213808180278439 and parameters: {'n_estimators': 862, 'eta': 0.06002285412651874, 'max_depth': 9, 'alpha': 0.7876000000000001, 'lambda': 36.248760204084704, 'max_bin': 387}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:47:10,185] Trial 478 finished with value: 0.7274876154601861 and parameters: {'n_estimators': 832, 'eta': 0.0644439677064503, 'max_depth': 9, 'alpha': 0.8665, 'lambda': 33.66751801956351, 'max_bin': 394}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:47:23,628] Trial 479 finished with value: 0.7258546761480371 and parameters: {'n_estimators': 873, 'eta': 0.058537228415247264, 'max_depth': 8, 'alpha': 0.9355, 'lambda': 32.81348178836983, 'max_bin': 406}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:47:37,887] Trial 480 finished with value: 0.7243108208848471 and parameters: {'n_estimators': 899, 'eta': 0.06056530594671118, 'max_depth': 9, 'alpha': 0.8281000000000001, 'lambda': 35.036501588724846, 'max_bin': 399}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:47:51,862] Trial 481 finished with value: 0.724580716299299 and parameters: {'n_estimators': 855, 'eta': 0.06594362519378306, 'max_depth': 9, 'alpha': 0.9192, 'lambda': 35.7560972331848, 'max_bin': 385}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:48:06,142] Trial 482 finished with value: 0.7246534723628759 and parameters: {'n_estimators': 842, 'eta': 0.06223576960671934, 'max_depth': 9, 'alpha': 0.8896000000000001, 'lambda': 36.97357153230383, 'max_bin': 392}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:48:19,475] Trial 483 finished with value: 0.7231556173925806 and parameters: {'n_estimators': 880, 'eta': 0.057600009120679495, 'max_depth': 9, 'alpha': 0.8516, 'lambda': 34.320956111290116, 'max_bin': 401}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:48:30,432] Trial 484 finished with value: 0.7232508376009787 and parameters: {'n_estimators': 864, 'eta': 0.07114544275476264, 'max_depth': 9, 'alpha': 0.8139000000000001, 'lambda': 33.56275277586611, 'max_bin': 407}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:48:42,973] Trial 485 finished with value: 0.724900809662154 and parameters: {'n_estimators': 833, 'eta': 0.06404151783586016, 'max_depth': 9, 'alpha': 0.8738, 'lambda': 35.92739488007978, 'max_bin': 397}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:48:56,143] Trial 486 finished with value: 0.72530020833127 and parameters: {'n_estimators': 886, 'eta': 0.061017922744257755, 'max_depth': 9, 'alpha': 0.9583, 'lambda': 34.77228341687389, 'max_bin': 392}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:49:08,406] Trial 487 finished with value: 0.7248042196350883 and parameters: {'n_estimators': 867, 'eta': 0.06667533052124475, 'max_depth': 9, 'alpha': 0.9034000000000001, 'lambda': 36.51698982750001, 'max_bin': 401}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:49:21,315] Trial 488 finished with value: 0.7233917239098392 and parameters: {'n_estimators': 849, 'eta': 0.06282646289764957, 'max_depth': 9, 'alpha': 0.8392000000000001, 'lambda': 32.594325907387024, 'max_bin': 396}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:49:35,252] Trial 489 finished with value: 0.7252156552952325 and parameters: {'n_estimators': 889, 'eta': 0.05856186532642491, 'max_depth': 9, 'alpha': 0.8747, 'lambda': 35.38403874065393, 'max_bin': 387}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:49:49,039] Trial 490 finished with value: 0.7288684631166205 and parameters: {'n_estimators': 868, 'eta': 0.06881770314071499, 'max_depth': 9, 'alpha': 0.8565, 'lambda': 34.005071766468625, 'max_bin': 404}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:50:04,394] Trial 491 finished with value: 0.726209184173704 and parameters: {'n_estimators': 900, 'eta': 0.059813099588257854, 'max_depth': 9, 'alpha': 0.9151, 'lambda': 38.05555054040898, 'max_bin': 393}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:50:16,051] Trial 492 finished with value: 0.7240874367335781 and parameters: {'n_estimators': 845, 'eta': 0.061647328627193014, 'max_depth': 8, 'alpha': 0.8976000000000001, 'lambda': 36.88405287875324, 'max_bin': 399}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:50:28,737] Trial 493 finished with value: 0.7231154032798944 and parameters: {'n_estimators': 830, 'eta': 0.06456553635287407, 'max_depth': 9, 'alpha': 0.8279000000000001, 'lambda': 34.6206689840138, 'max_bin': 385}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:50:43,843] Trial 494 finished with value: 0.725252467978424 and parameters: {'n_estimators': 876, 'eta': 0.05760723113270941, 'max_depth': 9, 'alpha': 0.9353, 'lambda': 35.66541845610822, 'max_bin': 408}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:50:55,531] Trial 495 finished with value: 0.722281270627643 and parameters: {'n_estimators': 857, 'eta': 0.06319738866816424, 'max_depth': 9, 'alpha': 0.8666, 'lambda': 33.444652976163624, 'max_bin': 391}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:51:08,237] Trial 496 finished with value: 0.7238461713389943 and parameters: {'n_estimators': 884, 'eta': 0.060026395170030486, 'max_depth': 9, 'alpha': 0.7984, 'lambda': 31.795018117907638, 'max_bin': 401}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:51:21,969] Trial 497 finished with value: 0.7269773894137168 and parameters: {'n_estimators': 860, 'eta': 0.06686982718049066, 'max_depth': 9, 'alpha': 0.8816, 'lambda': 36.24358755999589, 'max_bin': 396}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:51:35,095] Trial 498 finished with value: 0.7239608907635922 and parameters: {'n_estimators': 835, 'eta': 0.06132984846461794, 'max_depth': 9, 'alpha': 0.8497, 'lambda': 34.877156785392565, 'max_bin': 407}. Best is trial 412 with value: 0.7363592711203093.\n",
      "[I 2023-12-20 17:51:50,460] Trial 499 finished with value: 0.7271611821633938 and parameters: {'n_estimators': 871, 'eta': 0.05727084089110534, 'max_depth': 9, 'alpha': 0.901, 'lambda': 33.22051103680953, 'max_bin': 390}. Best is trial 412 with value: 0.7363592711203093.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7364\n",
      "\tBest params:\n",
      "\t\tn_estimators: 841\n",
      "\t\teta: 0.0593546986294894\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.8138000000000001\n",
      "\t\tlambda: 34.19294262674558\n",
      "\t\tmax_bin: 405\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.703752    0.746467    0.736627    0.749621   \n",
      "1                    TP   34.000000   37.000000   37.000000   34.000000   \n",
      "2                    TN  304.000000  305.000000  308.000000  307.000000   \n",
      "3                    FP   12.000000   10.000000    6.000000    9.000000   \n",
      "4                    FN   32.000000   30.000000   31.000000   32.000000   \n",
      "5              Accuracy    0.884817    0.895288    0.903141    0.892670   \n",
      "6             Precision    0.739130    0.787234    0.860465    0.790698   \n",
      "7           Sensitivity    0.515152    0.552239    0.544118    0.515152   \n",
      "8           Specificity    0.962000    0.968300    0.980900    0.971500   \n",
      "9              F1 score    0.607143    0.649123    0.666667    0.623853   \n",
      "10  F1 score (weighted)    0.876299    0.887714    0.894088    0.883231   \n",
      "11     F1 score (macro)    0.769829    0.793792    0.805003    0.780629   \n",
      "12    Balanced Accuracy    0.738588    0.760246    0.762505    0.743335   \n",
      "13                  MCC    0.554302    0.602610    0.635407    0.582125   \n",
      "14                  NPV    0.904800    0.910400    0.908600    0.905600   \n",
      "15              ROC_AUC    0.738588    0.760246    0.762505    0.743335   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.804255    0.737585    0.763038    0.783537    0.694418    0.759055  \n",
      "1    42.000000   36.000000   37.000000   39.000000   38.000000   40.000000  \n",
      "2   308.000000  303.000000  307.000000  308.000000  304.000000  304.000000  \n",
      "3     8.000000   12.000000    9.000000    7.000000   10.000000    9.000000  \n",
      "4    24.000000   31.000000   29.000000   28.000000   30.000000   29.000000  \n",
      "5     0.916230    0.887435    0.900524    0.908377    0.895288    0.900524  \n",
      "6     0.840000    0.750000    0.804348    0.847826    0.791667    0.816327  \n",
      "7     0.636364    0.537313    0.560606    0.582090    0.558824    0.579710  \n",
      "8     0.974700    0.961900    0.971500    0.977800    0.968200    0.971200  \n",
      "9     0.724138    0.626087    0.660714    0.690265    0.655172    0.677966  \n",
      "10    0.911487    0.879783    0.893167    0.901341    0.887877    0.893633  \n",
      "11    0.837378    0.779916    0.801216    0.818251    0.796722    0.809571  \n",
      "12    0.805524    0.749609    0.766063    0.779934    0.763488    0.775478  \n",
      "13    0.684914    0.572782    0.618131    0.654228    0.608158    0.633860  \n",
      "14    0.927700    0.907200    0.913700    0.916700    0.910200    0.912900  \n",
      "15    0.805524    0.749609    0.766063    0.779934    0.763488    0.775478  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_xgb_9)\n",
    "# now convert the resuls to binary with cutoff 6.9\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_9_cat = np.where(((y_pred_xgb_9 >= 2) | (y_pred_xgb_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.746640</td>\n",
       "      <td>0.040852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>3.431877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>153.700000</td>\n",
       "      <td>2.540779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>2.183270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>15.400000</td>\n",
       "      <td>3.864367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.898953</td>\n",
       "      <td>0.027489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.820845</td>\n",
       "      <td>0.100459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.540216</td>\n",
       "      <td>0.109472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.975240</td>\n",
       "      <td>0.013923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.648519</td>\n",
       "      <td>0.102997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.889664</td>\n",
       "      <td>0.031609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.794742</td>\n",
       "      <td>0.059302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.757728</td>\n",
       "      <td>0.058378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.612309</td>\n",
       "      <td>0.113155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.909170</td>\n",
       "      <td>0.021679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.757728</td>\n",
       "      <td>0.058378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.746640     0.040852\n",
       "1                    TP        18.000000     3.431877\n",
       "2                    TN       153.700000     2.540779\n",
       "3                    FP         3.900000     2.183270\n",
       "4                    FN        15.400000     3.864367\n",
       "5              Accuracy         0.898953     0.027489\n",
       "6             Precision         0.820845     0.100459\n",
       "7           Sensitivity         0.540216     0.109472\n",
       "8           Specificity         0.975240     0.013923\n",
       "9              F1 score         0.648519     0.102997\n",
       "10  F1 score (weighted)         0.889664     0.031609\n",
       "11     F1 score (macro)         0.794742     0.059302\n",
       "12    Balanced Accuracy         0.757728     0.058378\n",
       "13                  MCC         0.612309     0.113155\n",
       "14                  NPV         0.909170     0.021679\n",
       "15              ROC_AUC         0.757728     0.058378"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.703752</td>\n",
       "      <td>0.746467</td>\n",
       "      <td>0.736627</td>\n",
       "      <td>0.749621</td>\n",
       "      <td>0.804255</td>\n",
       "      <td>0.737585</td>\n",
       "      <td>0.763038</td>\n",
       "      <td>0.783537</td>\n",
       "      <td>0.694418</td>\n",
       "      <td>0.759055</td>\n",
       "      <td>0.747836</td>\n",
       "      <td>0.033078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>37.400000</td>\n",
       "      <td>2.503331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>305.800000</td>\n",
       "      <td>1.988858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>1.932184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>2.366432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.884817</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.916230</td>\n",
       "      <td>0.887435</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.908377</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.898429</td>\n",
       "      <td>0.009463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.802769</td>\n",
       "      <td>0.039828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.537313</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.558157</td>\n",
       "      <td>0.035786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.968300</td>\n",
       "      <td>0.980900</td>\n",
       "      <td>0.971500</td>\n",
       "      <td>0.974700</td>\n",
       "      <td>0.961900</td>\n",
       "      <td>0.971500</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.968200</td>\n",
       "      <td>0.971200</td>\n",
       "      <td>0.970800</td>\n",
       "      <td>0.006119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.623853</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.626087</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.658113</td>\n",
       "      <td>0.034567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.876299</td>\n",
       "      <td>0.887714</td>\n",
       "      <td>0.894088</td>\n",
       "      <td>0.883231</td>\n",
       "      <td>0.911487</td>\n",
       "      <td>0.879783</td>\n",
       "      <td>0.893167</td>\n",
       "      <td>0.901341</td>\n",
       "      <td>0.887877</td>\n",
       "      <td>0.893633</td>\n",
       "      <td>0.890862</td>\n",
       "      <td>0.010384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.769829</td>\n",
       "      <td>0.793792</td>\n",
       "      <td>0.805003</td>\n",
       "      <td>0.780629</td>\n",
       "      <td>0.837378</td>\n",
       "      <td>0.779916</td>\n",
       "      <td>0.801216</td>\n",
       "      <td>0.818251</td>\n",
       "      <td>0.796722</td>\n",
       "      <td>0.809571</td>\n",
       "      <td>0.799231</td>\n",
       "      <td>0.019946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.738588</td>\n",
       "      <td>0.760246</td>\n",
       "      <td>0.762505</td>\n",
       "      <td>0.743335</td>\n",
       "      <td>0.805524</td>\n",
       "      <td>0.749609</td>\n",
       "      <td>0.766063</td>\n",
       "      <td>0.779934</td>\n",
       "      <td>0.763488</td>\n",
       "      <td>0.775478</td>\n",
       "      <td>0.764477</td>\n",
       "      <td>0.019460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.554302</td>\n",
       "      <td>0.602610</td>\n",
       "      <td>0.635407</td>\n",
       "      <td>0.582125</td>\n",
       "      <td>0.684914</td>\n",
       "      <td>0.572782</td>\n",
       "      <td>0.618131</td>\n",
       "      <td>0.654228</td>\n",
       "      <td>0.608158</td>\n",
       "      <td>0.633860</td>\n",
       "      <td>0.614652</td>\n",
       "      <td>0.039378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.904800</td>\n",
       "      <td>0.910400</td>\n",
       "      <td>0.908600</td>\n",
       "      <td>0.905600</td>\n",
       "      <td>0.927700</td>\n",
       "      <td>0.907200</td>\n",
       "      <td>0.913700</td>\n",
       "      <td>0.916700</td>\n",
       "      <td>0.910200</td>\n",
       "      <td>0.912900</td>\n",
       "      <td>0.911780</td>\n",
       "      <td>0.006708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.738588</td>\n",
       "      <td>0.760246</td>\n",
       "      <td>0.762505</td>\n",
       "      <td>0.743335</td>\n",
       "      <td>0.805524</td>\n",
       "      <td>0.749609</td>\n",
       "      <td>0.766063</td>\n",
       "      <td>0.779934</td>\n",
       "      <td>0.763488</td>\n",
       "      <td>0.775478</td>\n",
       "      <td>0.764477</td>\n",
       "      <td>0.019460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.703752    0.746467    0.736627    0.749621   \n",
       "1                    TP   34.000000   37.000000   37.000000   34.000000   \n",
       "2                    TN  304.000000  305.000000  308.000000  307.000000   \n",
       "3                    FP   12.000000   10.000000    6.000000    9.000000   \n",
       "4                    FN   32.000000   30.000000   31.000000   32.000000   \n",
       "5              Accuracy    0.884817    0.895288    0.903141    0.892670   \n",
       "6             Precision    0.739130    0.787234    0.860465    0.790698   \n",
       "7           Sensitivity    0.515152    0.552239    0.544118    0.515152   \n",
       "8           Specificity    0.962000    0.968300    0.980900    0.971500   \n",
       "9              F1 score    0.607143    0.649123    0.666667    0.623853   \n",
       "10  F1 score (weighted)    0.876299    0.887714    0.894088    0.883231   \n",
       "11     F1 score (macro)    0.769829    0.793792    0.805003    0.780629   \n",
       "12    Balanced Accuracy    0.738588    0.760246    0.762505    0.743335   \n",
       "13                  MCC    0.554302    0.602610    0.635407    0.582125   \n",
       "14                  NPV    0.904800    0.910400    0.908600    0.905600   \n",
       "15              ROC_AUC    0.738588    0.760246    0.762505    0.743335   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.804255    0.737585    0.763038    0.783537    0.694418    0.759055   \n",
       "1    42.000000   36.000000   37.000000   39.000000   38.000000   40.000000   \n",
       "2   308.000000  303.000000  307.000000  308.000000  304.000000  304.000000   \n",
       "3     8.000000   12.000000    9.000000    7.000000   10.000000    9.000000   \n",
       "4    24.000000   31.000000   29.000000   28.000000   30.000000   29.000000   \n",
       "5     0.916230    0.887435    0.900524    0.908377    0.895288    0.900524   \n",
       "6     0.840000    0.750000    0.804348    0.847826    0.791667    0.816327   \n",
       "7     0.636364    0.537313    0.560606    0.582090    0.558824    0.579710   \n",
       "8     0.974700    0.961900    0.971500    0.977800    0.968200    0.971200   \n",
       "9     0.724138    0.626087    0.660714    0.690265    0.655172    0.677966   \n",
       "10    0.911487    0.879783    0.893167    0.901341    0.887877    0.893633   \n",
       "11    0.837378    0.779916    0.801216    0.818251    0.796722    0.809571   \n",
       "12    0.805524    0.749609    0.766063    0.779934    0.763488    0.775478   \n",
       "13    0.684914    0.572782    0.618131    0.654228    0.608158    0.633860   \n",
       "14    0.927700    0.907200    0.913700    0.916700    0.910200    0.912900   \n",
       "15    0.805524    0.749609    0.766063    0.779934    0.763488    0.775478   \n",
       "\n",
       "           ave       std  \n",
       "0     0.747836  0.033078  \n",
       "1    37.400000  2.503331  \n",
       "2   305.800000  1.988858  \n",
       "3     9.200000  1.932184  \n",
       "4    29.600000  2.366432  \n",
       "5     0.898429  0.009463  \n",
       "6     0.802769  0.039828  \n",
       "7     0.558157  0.035786  \n",
       "8     0.970800  0.006119  \n",
       "9     0.658113  0.034567  \n",
       "10    0.890862  0.010384  \n",
       "11    0.799231  0.019946  \n",
       "12    0.764477  0.019460  \n",
       "13    0.614652  0.039378  \n",
       "14    0.911780  0.006708  \n",
       "15    0.764477  0.019460  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test['std'] = mat_met_xgb_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.743689</td>\n",
       "      <td>0.042574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.901257</td>\n",
       "      <td>0.018951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.826090</td>\n",
       "      <td>0.075087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.555207</td>\n",
       "      <td>0.089177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.974622</td>\n",
       "      <td>0.013132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.660056</td>\n",
       "      <td>0.074731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.892834</td>\n",
       "      <td>0.021895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.801119</td>\n",
       "      <td>0.042481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.764908</td>\n",
       "      <td>0.044864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.623880</td>\n",
       "      <td>0.078597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.912050</td>\n",
       "      <td>0.016409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.764908</td>\n",
       "      <td>0.044864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.743689     0.042574\n",
       "1              Accuracy         0.901257     0.018951\n",
       "2             Precision         0.826090     0.075087\n",
       "3           Sensitivity         0.555207     0.089177\n",
       "4           Specificity         0.974622     0.013132\n",
       "5              F1 score         0.660056     0.074731\n",
       "6   F1 score (weighted)         0.892834     0.021895\n",
       "7      F1 score (macro)         0.801119     0.042481\n",
       "8     Balanced Accuracy         0.764908     0.044864\n",
       "9                   MCC         0.623880     0.078597\n",
       "10                  NPV         0.912050     0.016409\n",
       "11              ROC_AUC         0.764908     0.044864"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"rmse\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_xgb_cat = np.where(((y_pred_optimized_xgb >= 2) | (y_pred_optimized_xgb <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_xgb_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_xgb))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "xgb_run0 = data_xgb[['y_test_idx0', 'y_test0', 'y_pred_xgb0']]\n",
    "xgb_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "xgb_run0.reset_index(inplace=True, drop=True)\n",
    "xgb_run1 = data_xgb[['y_test_idx1', 'y_test1', 'y_pred_xgb1']]\n",
    "xgb_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "xgb_run1.reset_index(inplace=True, drop=True)\n",
    "xgb_run2 = data_xgb[['y_test_idx2', 'y_test2', 'y_pred_xgb2']]\n",
    "xgb_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "xgb_run2.reset_index(inplace=True, drop=True)\n",
    "xgb_run3 = data_xgb[['y_test_idx3', 'y_test3', 'y_pred_xgb3']]\n",
    "xgb_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "xgb_run3.reset_index(inplace=True, drop=True)\n",
    "xgb_run4 = data_xgb[['y_test_idx4', 'y_test4', 'y_pred_xgb4']]\n",
    "xgb_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "xgb_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "xgb_5preds = pd.concat([chembl_id,xgb_run0, xgb_run1, xgb_run2, xgb_run3, xgb_run4], axis=1)\n",
    "xgb_5preds = xgb_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_xgb0', 'y_pred_xgb1', 'y_pred_xgb2', 'y_pred_xgb3', 'y_pred_xgb4']]\n",
    "xgb_5preds['y_pred_xgb_ave'] = xgb_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "xgb_5preds['y_pred_xgb_std'] = xgb_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "\n",
    "mat_met_optimized_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "02aaad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b19aca7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt  \n",
    "# retrieve performance metrics\n",
    "results = optimized_xgb_0.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "    \n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "pyplot.show()\n",
    "\n",
    " # plot classification error\n",
    "#fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "#ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "#ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "#ax.legend()\n",
    "    \n",
    "#pyplot.ylabel('Classification Error')\n",
    "#pyplot.title('XGBoost Classification Error')\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost baseline model r2_score 0.7119 with a standard deviation of 0.0570\n",
      "XGBoost optimized model r2_score 0.7465 with a standard deviation of 0.0520\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_reg, X, Y, cv=10, scoring=\"r2\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_xgb_withSemiSel.joblib']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_reg, \"OUTPUT/xgb_reg_withSemiSel.joblib\")\n",
    "joblib.dump(optimizedCV_xgb, \"OUTPUT/optimizedCV_xgb_withSemiSel.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.697252     0.057759\n",
      "1                    TP        19.100000     4.012481\n",
      "2                    TN       152.000000     2.905933\n",
      "3                    FP         5.600000     2.547330\n",
      "4                    FN        14.300000     4.423423\n",
      "5              Accuracy         0.895812     0.025821\n",
      "6             Precision         0.779388     0.100840\n",
      "7           Sensitivity         0.573288     0.128191\n",
      "8           Specificity         0.964440     0.016255\n",
      "9              F1 score         0.652873     0.099429\n",
      "10  F1 score (weighted)         0.888489     0.029751\n",
      "11     F1 score (macro)         0.795743     0.056707\n",
      "12    Balanced Accuracy         0.768868     0.063890\n",
      "13                  MCC         0.607964     0.105142\n",
      "14                  NPV         0.914520     0.024477\n",
      "15              ROC_AUC         0.768868     0.063890\n",
      "CPU times: user 1.17 s, sys: 3.11 s, total: 4.28 s\n",
      "Wall time: 141 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    \n",
    "    knn_reg.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=4)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=4)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:54:16,929] A new study created in memory with name: KNNregressor\n",
      "[I 2023-12-20 17:54:17,276] Trial 0 finished with value: 0.5984275411858129 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 0 with value: 0.5984275411858129.\n",
      "[I 2023-12-20 17:54:17,438] Trial 1 finished with value: 0.6293071359177043 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 78}. Best is trial 1 with value: 0.6293071359177043.\n",
      "[I 2023-12-20 17:54:17,592] Trial 2 finished with value: 0.6395460895824618 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 35}. Best is trial 2 with value: 0.6395460895824618.\n",
      "[I 2023-12-20 17:54:17,883] Trial 3 finished with value: 0.575478217639919 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 2 with value: 0.6395460895824618.\n",
      "[I 2023-12-20 17:54:18,037] Trial 4 finished with value: 0.5220521427795439 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 91}. Best is trial 2 with value: 0.6395460895824618.\n",
      "[I 2023-12-20 17:54:18,362] Trial 5 finished with value: 0.6221862807735282 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 2 with value: 0.6395460895824618.\n",
      "[I 2023-12-20 17:54:18,512] Trial 6 finished with value: 0.5882980639083973 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 50}. Best is trial 2 with value: 0.6395460895824618.\n",
      "[I 2023-12-20 17:54:18,660] Trial 7 finished with value: 0.5109621276546555 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 67}. Best is trial 2 with value: 0.6395460895824618.\n",
      "[I 2023-12-20 17:54:18,809] Trial 8 finished with value: 0.6365146388626626 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 85}. Best is trial 2 with value: 0.6395460895824618.\n",
      "[I 2023-12-20 17:54:19,119] Trial 9 finished with value: 0.6221862807735282 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 2 with value: 0.6395460895824618.\n",
      "[I 2023-12-20 17:54:19,273] Trial 10 finished with value: 0.6778214961865662 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 37}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:19,430] Trial 11 finished with value: 0.6778214961865662 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 37}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:19,591] Trial 12 finished with value: 0.6778214961865662 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 44}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:19,752] Trial 13 finished with value: 0.6778214961865662 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 60}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:19,908] Trial 14 finished with value: 0.6395460895824618 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:20,068] Trial 15 finished with value: 0.6491732222939162 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:20,226] Trial 16 finished with value: 0.5138264069412524 and parameters: {'n_neighbors': 22, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:20,386] Trial 17 finished with value: 0.6559814180832177 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 52}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:20,548] Trial 18 finished with value: 0.5986425676963734 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:20,709] Trial 19 finished with value: 0.6280710011208487 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 62}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:20,871] Trial 20 finished with value: 0.6778214961865662 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:21,031] Trial 21 finished with value: 0.6778214961865662 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:21,192] Trial 22 finished with value: 0.6683758699094327 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 37}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:21,354] Trial 23 finished with value: 0.6183408987248047 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 47}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:21,515] Trial 24 finished with value: 0.6559814180832177 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 36}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:21,677] Trial 25 finished with value: 0.575478217639919 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 57}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:21,841] Trial 26 finished with value: 0.6559814180832177 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:22,003] Trial 27 finished with value: 0.523213206514982 and parameters: {'n_neighbors': 21, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 45}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:22,307] Trial 28 finished with value: 0.6280710011208487 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:22,469] Trial 29 finished with value: 0.575478217639919 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 21}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:22,799] Trial 30 finished with value: 0.5009969388037069 and parameters: {'n_neighbors': 24, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:22,954] Trial 31 finished with value: 0.6778214961865662 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 64}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:23,113] Trial 32 finished with value: 0.6778214961865662 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 70}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:23,270] Trial 33 finished with value: 0.6491732222939162 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 40}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:23,432] Trial 34 finished with value: 0.6559814180832177 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 79}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:23,594] Trial 35 finished with value: 0.6395460895824618 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 33}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:23,757] Trial 36 finished with value: 0.6683758699094327 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 48}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:23,920] Trial 37 finished with value: 0.6442944569398882 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:24,222] Trial 38 finished with value: 0.49348030515293984 and parameters: {'n_neighbors': 25, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:24,382] Trial 39 finished with value: 0.6663509733088437 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 75}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:24,545] Trial 40 finished with value: 0.6683758699094327 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 53}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:24,708] Trial 41 finished with value: 0.6778214961865662 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:24,872] Trial 42 finished with value: 0.6778214961865662 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 31}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:25,033] Trial 43 finished with value: 0.6395460895824618 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:25,196] Trial 44 finished with value: 0.6683758699094327 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 35}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:25,359] Trial 45 finished with value: 0.6663509733088437 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:25,523] Trial 46 finished with value: 0.6559814180832177 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 49}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:25,825] Trial 47 finished with value: 0.5986425676963734 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:25,987] Trial 48 finished with value: 0.573168111570597 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 34}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:26,148] Trial 49 finished with value: 0.6280710011208487 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 10 with value: 0.6778214961865662.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6778\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: uniform\n",
      "\t\tmetric: minkowski\n",
      "\t\tleaf_size: 37\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNregressor\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.685239\n",
      "1                    TP   40.000000\n",
      "2                    TN  307.000000\n",
      "3                    FP    9.000000\n",
      "4                    FN   26.000000\n",
      "5              Accuracy    0.908377\n",
      "6             Precision    0.816327\n",
      "7           Sensitivity    0.606061\n",
      "8           Specificity    0.971500\n",
      "9              F1 score    0.695652\n",
      "10  F1 score (weighted)    0.902805\n",
      "11     F1 score (macro)    0.820862\n",
      "12    Balanced Accuracy    0.788790\n",
      "13                  MCC    0.652991\n",
      "14                  NPV    0.921900\n",
      "15              ROC_AUC    0.788790\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_knn_0)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_knn_0_cat = np.where(((y_pred_knn_0 >= 2) | (y_pred_knn_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_knn_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:54:26,373] Trial 50 finished with value: 0.667983204403327 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:26,550] Trial 51 finished with value: 0.6707498392668896 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 44}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:26,726] Trial 52 finished with value: 0.6707498392668896 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 39}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:26,901] Trial 53 finished with value: 0.6430792565257057 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 45}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:27,077] Trial 54 finished with value: 0.667983204403327 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 51}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:27,255] Trial 55 finished with value: 0.4730407339984442 and parameters: {'n_neighbors': 30, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:27,432] Trial 56 finished with value: 0.6553558462357313 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 38}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:27,610] Trial 57 finished with value: 0.5757760343374972 and parameters: {'n_neighbors': 16, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:27,947] Trial 58 finished with value: 0.6384403391203148 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 10 with value: 0.6778214961865662.\n",
      "[I 2023-12-20 17:54:28,118] Trial 59 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:28,295] Trial 60 finished with value: 0.6765787514081429 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:28,472] Trial 61 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:28,649] Trial 62 finished with value: 0.685632886551979 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:28,825] Trial 63 finished with value: 0.685632886551979 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:29,003] Trial 64 finished with value: 0.685632886551979 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:29,180] Trial 65 finished with value: 0.685632886551979 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:29,351] Trial 66 finished with value: 0.685632886551979 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:29,515] Trial 67 finished with value: 0.6534024248593381 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:29,683] Trial 68 finished with value: 0.685632886551979 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:29,845] Trial 69 finished with value: 0.6648290938739496 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:30,012] Trial 70 finished with value: 0.667850939883352 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:30,180] Trial 71 finished with value: 0.685632886551979 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:30,348] Trial 72 finished with value: 0.6765787514081429 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:30,516] Trial 73 finished with value: 0.685632886551979 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:30,678] Trial 74 finished with value: 0.685632886551979 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:30,850] Trial 75 finished with value: 0.5451153378811286 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:31,019] Trial 76 finished with value: 0.667850939883352 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:31,188] Trial 77 finished with value: 0.6593885284420524 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:31,355] Trial 78 finished with value: 0.6765787514081429 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:31,520] Trial 79 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:31,685] Trial 80 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:31,849] Trial 81 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:32,015] Trial 82 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:32,181] Trial 83 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:32,349] Trial 84 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:32,514] Trial 85 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:32,678] Trial 86 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 21}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:32,848] Trial 87 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:33,015] Trial 88 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:33,183] Trial 89 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:33,351] Trial 90 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:33,517] Trial 91 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:33,686] Trial 92 finished with value: 0.6765787514081429 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:33,855] Trial 93 finished with value: 0.6765787514081429 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:34,022] Trial 94 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:34,185] Trial 95 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:34,351] Trial 96 finished with value: 0.667850939883352 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:34,513] Trial 97 finished with value: 0.6765787514081429 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:34,679] Trial 98 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 21}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:34,842] Trial 99 finished with value: 0.6862975887785467 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 59 with value: 0.6862975887785467.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6863\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 27\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.685239    0.696281\n",
      "1                    TP   40.000000   35.000000\n",
      "2                    TN  307.000000  307.000000\n",
      "3                    FP    9.000000    8.000000\n",
      "4                    FN   26.000000   32.000000\n",
      "5              Accuracy    0.908377    0.895288\n",
      "6             Precision    0.816327    0.813953\n",
      "7           Sensitivity    0.606061    0.522388\n",
      "8           Specificity    0.971500    0.974600\n",
      "9              F1 score    0.695652    0.636364\n",
      "10  F1 score (weighted)    0.902805    0.885786\n",
      "11     F1 score (macro)    0.820862    0.787601\n",
      "12    Balanced Accuracy    0.788790    0.748496\n",
      "13                  MCC    0.652991    0.598008\n",
      "14                  NPV    0.921900    0.905600\n",
      "15              ROC_AUC    0.788790    0.748496\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_knn_1)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet1_cat = np.where(((Y_testSet1>=2) | (Y_testSet1<=-2)), 1, 0) \n",
    "y_pred_knn_1_cat = np.where(((y_pred_knn_1 >= 2) | (y_pred_knn_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_knn_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:54:35,070] Trial 100 finished with value: 0.6625311112821212 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:35,243] Trial 101 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:35,414] Trial 102 finished with value: 0.6679170072979996 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:35,580] Trial 103 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:35,748] Trial 104 finished with value: 0.6679170072979996 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:35,914] Trial 105 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:36,085] Trial 106 finished with value: 0.6679170072979996 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:36,386] Trial 107 finished with value: 0.6745984331973066 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:36,554] Trial 108 finished with value: 0.6217111643161524 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:36,726] Trial 109 finished with value: 0.6679170072979996 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:36,896] Trial 110 finished with value: 0.5993950971947927 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 90}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:37,062] Trial 111 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:37,233] Trial 112 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:37,402] Trial 113 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 21}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:37,571] Trial 114 finished with value: 0.6679170072979996 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:37,733] Trial 115 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:37,904] Trial 116 finished with value: 0.6592882212916888 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:38,073] Trial 117 finished with value: 0.5478632546313269 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:38,241] Trial 118 finished with value: 0.6679170072979996 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:38,412] Trial 119 finished with value: 0.6625311112821212 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:38,736] Trial 120 finished with value: 0.6792674900337122 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:38,902] Trial 121 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:39,073] Trial 122 finished with value: 0.6679170072979996 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:39,243] Trial 123 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:39,413] Trial 124 finished with value: 0.6625311112821212 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:39,575] Trial 125 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:39,742] Trial 126 finished with value: 0.6679170072979996 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:39,912] Trial 127 finished with value: 0.6679170072979996 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:40,083] Trial 128 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:40,254] Trial 129 finished with value: 0.6625311112821212 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:40,426] Trial 130 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 21}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:40,596] Trial 131 finished with value: 0.6679170072979996 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:40,766] Trial 132 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:40,937] Trial 133 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:41,106] Trial 134 finished with value: 0.6679170072979996 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:41,273] Trial 135 finished with value: 0.5833768136401122 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:41,444] Trial 136 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:41,616] Trial 137 finished with value: 0.6679170072979996 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:41,788] Trial 138 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:41,960] Trial 139 finished with value: 0.6625311112821212 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:42,260] Trial 140 finished with value: 0.6780434064921076 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:42,432] Trial 141 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:42,603] Trial 142 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:42,774] Trial 143 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:42,946] Trial 144 finished with value: 0.6679170072979996 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:43,119] Trial 145 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:43,291] Trial 146 finished with value: 0.6679170072979996 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:43,463] Trial 147 finished with value: 0.6625311112821212 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:43,634] Trial 148 finished with value: 0.6679170072979996 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 21}. Best is trial 59 with value: 0.6862975887785467.\n",
      "[I 2023-12-20 17:54:43,806] Trial 149 finished with value: 0.6705518114270208 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 59 with value: 0.6862975887785467.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6863\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 27\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.685239    0.696281    0.725667\n",
      "1                    TP   40.000000   35.000000   37.000000\n",
      "2                    TN  307.000000  307.000000  306.000000\n",
      "3                    FP    9.000000    8.000000    8.000000\n",
      "4                    FN   26.000000   32.000000   31.000000\n",
      "5              Accuracy    0.908377    0.895288    0.897906\n",
      "6             Precision    0.816327    0.813953    0.822222\n",
      "7           Sensitivity    0.606061    0.522388    0.544118\n",
      "8           Specificity    0.971500    0.974600    0.974500\n",
      "9              F1 score    0.695652    0.636364    0.654867\n",
      "10  F1 score (weighted)    0.902805    0.885786    0.889319\n",
      "11     F1 score (macro)    0.820862    0.787601    0.797480\n",
      "12    Balanced Accuracy    0.788790    0.748496    0.759320\n",
      "13                  MCC    0.652991    0.598008    0.615409\n",
      "14                  NPV    0.921900    0.905600    0.908000\n",
      "15              ROC_AUC    0.788790    0.748496    0.759320\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_knn_2)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet2_cat = np.where(((Y_testSet2>=2) | (Y_testSet2<=-2)), 1, 0) \n",
    "y_pred_knn_2_cat = np.where(((y_pred_knn_2 >= 2) | (y_pred_knn_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_knn_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:54:44,039] Trial 150 finished with value: 0.6868899983749874 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:44,201] Trial 151 finished with value: 0.6868899983749874 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:44,372] Trial 152 finished with value: 0.6868899983749874 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:44,543] Trial 153 finished with value: 0.6868899983749874 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:44,715] Trial 154 finished with value: 0.6814629209957228 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 91}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:44,885] Trial 155 finished with value: 0.6868899983749874 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:45,056] Trial 156 finished with value: 0.6814629209957228 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:45,227] Trial 157 finished with value: 0.6868899983749874 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:45,397] Trial 158 finished with value: 0.6732942449049029 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:45,560] Trial 159 finished with value: 0.6868899983749874 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:45,729] Trial 160 finished with value: 0.6814629209957228 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:45,893] Trial 161 finished with value: 0.6868899983749874 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:46,064] Trial 162 finished with value: 0.6868899983749874 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:46,234] Trial 163 finished with value: 0.6868899983749874 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:46,405] Trial 164 finished with value: 0.6868899983749874 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:46,576] Trial 165 finished with value: 0.6814629209957228 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:46,747] Trial 166 finished with value: 0.6868899983749874 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:46,916] Trial 167 finished with value: 0.643593254294038 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:47,079] Trial 168 finished with value: 0.6814629209957228 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 150 with value: 0.6868899983749874.\n",
      "[I 2023-12-20 17:54:47,398] Trial 169 finished with value: 0.6951910825697657 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:47,741] Trial 170 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:48,105] Trial 171 finished with value: 0.6951910825697657 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:48,472] Trial 172 finished with value: 0.6951910825697657 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:48,845] Trial 173 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:49,208] Trial 174 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:49,556] Trial 175 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:49,925] Trial 176 finished with value: 0.6867933310538403 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:50,310] Trial 177 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:50,679] Trial 178 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:51,043] Trial 179 finished with value: 0.6823454325177924 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:51,429] Trial 180 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:51,806] Trial 181 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:52,182] Trial 182 finished with value: 0.6867933310538403 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:52,567] Trial 183 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:52,891] Trial 184 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:53,256] Trial 185 finished with value: 0.6867933310538403 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:53,601] Trial 186 finished with value: 0.6867933310538403 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:53,936] Trial 187 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:54,312] Trial 188 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:54,643] Trial 189 finished with value: 0.6823454325177924 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:54,976] Trial 190 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:55,311] Trial 191 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:55,684] Trial 192 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:56,069] Trial 193 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:56,446] Trial 194 finished with value: 0.6867933310538403 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:56,820] Trial 195 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:57,177] Trial 196 finished with value: 0.6823454325177924 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:57,563] Trial 197 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:57,933] Trial 198 finished with value: 0.6867933310538403 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:58,279] Trial 199 finished with value: 0.6923252058977476 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 169 with value: 0.6951910825697657.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6952\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 96\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.685239    0.696281    0.725667    0.713837\n",
      "1                    TP   40.000000   35.000000   37.000000   40.000000\n",
      "2                    TN  307.000000  307.000000  306.000000  306.000000\n",
      "3                    FP    9.000000    8.000000    8.000000   10.000000\n",
      "4                    FN   26.000000   32.000000   31.000000   26.000000\n",
      "5              Accuracy    0.908377    0.895288    0.897906    0.905759\n",
      "6             Precision    0.816327    0.813953    0.822222    0.800000\n",
      "7           Sensitivity    0.606061    0.522388    0.544118    0.606061\n",
      "8           Specificity    0.971500    0.974600    0.974500    0.968400\n",
      "9              F1 score    0.695652    0.636364    0.654867    0.689655\n",
      "10  F1 score (weighted)    0.902805    0.885786    0.889319    0.900423\n",
      "11     F1 score (macro)    0.820862    0.787601    0.797480    0.817050\n",
      "12    Balanced Accuracy    0.788790    0.748496    0.759320    0.787208\n",
      "13                  MCC    0.652991    0.598008    0.615409    0.643854\n",
      "14                  NPV    0.921900    0.905600    0.908000    0.921700\n",
      "15              ROC_AUC    0.788790    0.748496    0.759320    0.787208\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_knn_3)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet3_cat = np.where(((Y_testSet3>=2) | (Y_testSet3<=-2)), 1, 0) \n",
    "y_pred_knn_3_cat = np.where(((y_pred_knn_3 >= 2) | (y_pred_knn_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_knn_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:54:58,764] Trial 200 finished with value: 0.6785602554463452 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:59,141] Trial 201 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:59,533] Trial 202 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:54:59,916] Trial 203 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:00,296] Trial 204 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:00,673] Trial 205 finished with value: 0.6785602554463452 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:01,032] Trial 206 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:01,433] Trial 207 finished with value: 0.6785602554463452 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:01,816] Trial 208 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:02,188] Trial 209 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:02,580] Trial 210 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:02,946] Trial 211 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:03,346] Trial 212 finished with value: 0.6785602554463452 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:03,724] Trial 213 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:04,075] Trial 214 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:04,507] Trial 215 finished with value: 0.6785602554463452 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:04,878] Trial 216 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:05,256] Trial 217 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:05,661] Trial 218 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:06,072] Trial 219 finished with value: 0.6785602554463452 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:06,476] Trial 220 finished with value: 0.6724688374501395 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:06,865] Trial 221 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:07,252] Trial 222 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:07,622] Trial 223 finished with value: 0.6785602554463452 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:08,014] Trial 224 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:08,435] Trial 225 finished with value: 0.5660220070327064 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:08,813] Trial 226 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:09,185] Trial 227 finished with value: 0.6785602554463452 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:09,585] Trial 228 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:09,978] Trial 229 finished with value: 0.6785602554463452 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:10,394] Trial 230 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:10,785] Trial 231 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:11,174] Trial 232 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:11,577] Trial 233 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:11,888] Trial 234 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:12,293] Trial 235 finished with value: 0.6785602554463452 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:12,670] Trial 236 finished with value: 0.682972404068817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:13,013] Trial 237 finished with value: 0.599559225907601 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:13,409] Trial 238 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:13,800] Trial 239 finished with value: 0.682972404068817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:14,177] Trial 240 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:14,575] Trial 241 finished with value: 0.682972404068817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:14,932] Trial 242 finished with value: 0.682972404068817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:15,316] Trial 243 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:15,698] Trial 244 finished with value: 0.682972404068817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:16,046] Trial 245 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:16,426] Trial 246 finished with value: 0.6242442361414319 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:16,774] Trial 247 finished with value: 0.6785602554463452 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:17,180] Trial 248 finished with value: 0.682972404068817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 169 with value: 0.6951910825697657.\n",
      "[I 2023-12-20 17:55:17,585] Trial 249 finished with value: 0.6783538774436153 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 169 with value: 0.6951910825697657.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6952\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 96\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.685239    0.696281    0.725667    0.713837   \n",
      "1                    TP   40.000000   35.000000   37.000000   40.000000   \n",
      "2                    TN  307.000000  307.000000  306.000000  306.000000   \n",
      "3                    FP    9.000000    8.000000    8.000000   10.000000   \n",
      "4                    FN   26.000000   32.000000   31.000000   26.000000   \n",
      "5              Accuracy    0.908377    0.895288    0.897906    0.905759   \n",
      "6             Precision    0.816327    0.813953    0.822222    0.800000   \n",
      "7           Sensitivity    0.606061    0.522388    0.544118    0.606061   \n",
      "8           Specificity    0.971500    0.974600    0.974500    0.968400   \n",
      "9              F1 score    0.695652    0.636364    0.654867    0.689655   \n",
      "10  F1 score (weighted)    0.902805    0.885786    0.889319    0.900423   \n",
      "11     F1 score (macro)    0.820862    0.787601    0.797480    0.817050   \n",
      "12    Balanced Accuracy    0.788790    0.748496    0.759320    0.787208   \n",
      "13                  MCC    0.652991    0.598008    0.615409    0.643854   \n",
      "14                  NPV    0.921900    0.905600    0.908000    0.921700   \n",
      "15              ROC_AUC    0.788790    0.748496    0.759320    0.787208   \n",
      "\n",
      "          Set4  \n",
      "0     0.785704  \n",
      "1    42.000000  \n",
      "2   306.000000  \n",
      "3    10.000000  \n",
      "4    24.000000  \n",
      "5     0.910995  \n",
      "6     0.807692  \n",
      "7     0.636364  \n",
      "8     0.968400  \n",
      "9     0.711864  \n",
      "10    0.906679  \n",
      "11    0.829616  \n",
      "12    0.802359  \n",
      "13    0.666668  \n",
      "14    0.927300  \n",
      "15    0.802359  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_knn_4)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet4_cat = np.where(((Y_testSet4>=2) | (Y_testSet4<=-2)), 1, 0) \n",
    "y_pred_knn_4_cat = np.where(((y_pred_knn_4 >= 2) | (y_pred_knn_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_knn_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:55:18,058] Trial 250 finished with value: 0.6970573093210177 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 250 with value: 0.6970573093210177.\n",
      "[I 2023-12-20 17:55:18,419] Trial 251 finished with value: 0.6973312697872558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:18,792] Trial 252 finished with value: 0.6973312697872558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:19,147] Trial 253 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:19,546] Trial 254 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:19,942] Trial 255 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:20,370] Trial 256 finished with value: 0.688341961413849 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:20,771] Trial 257 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:21,147] Trial 258 finished with value: 0.688341961413849 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:21,558] Trial 259 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:21,947] Trial 260 finished with value: 0.6821328511662066 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:22,363] Trial 261 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:22,737] Trial 262 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:23,099] Trial 263 finished with value: 0.688341961413849 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:23,482] Trial 264 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:23,875] Trial 265 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:24,252] Trial 266 finished with value: 0.688341961413849 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:24,603] Trial 267 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:24,974] Trial 268 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:25,323] Trial 269 finished with value: 0.688341961413849 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:25,682] Trial 270 finished with value: 0.6821328511662066 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:26,051] Trial 271 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:26,438] Trial 272 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:26,777] Trial 273 finished with value: 0.688341961413849 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:27,136] Trial 274 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:27,552] Trial 275 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:27,945] Trial 276 finished with value: 0.688341961413849 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:28,309] Trial 277 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:28,663] Trial 278 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:29,019] Trial 279 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:29,415] Trial 280 finished with value: 0.688341961413849 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:29,799] Trial 281 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:30,184] Trial 282 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:30,580] Trial 283 finished with value: 0.6451149708780932 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:30,989] Trial 284 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:31,418] Trial 285 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:31,814] Trial 286 finished with value: 0.688341961413849 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:32,232] Trial 287 finished with value: 0.6821328511662066 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:32,640] Trial 288 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:33,056] Trial 289 finished with value: 0.688341961413849 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:33,453] Trial 290 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:33,840] Trial 291 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:34,191] Trial 292 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:34,525] Trial 293 finished with value: 0.688341961413849 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:34,914] Trial 294 finished with value: 0.688341961413849 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:35,300] Trial 295 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:35,699] Trial 296 finished with value: 0.6821328511662066 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:36,110] Trial 297 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:36,495] Trial 298 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:36,887] Trial 299 finished with value: 0.692933179363802 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 251 with value: 0.6973312697872558.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6973\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 97\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.685239    0.696281    0.725667    0.713837   \n",
      "1                    TP   40.000000   35.000000   37.000000   40.000000   \n",
      "2                    TN  307.000000  307.000000  306.000000  306.000000   \n",
      "3                    FP    9.000000    8.000000    8.000000   10.000000   \n",
      "4                    FN   26.000000   32.000000   31.000000   26.000000   \n",
      "5              Accuracy    0.908377    0.895288    0.897906    0.905759   \n",
      "6             Precision    0.816327    0.813953    0.822222    0.800000   \n",
      "7           Sensitivity    0.606061    0.522388    0.544118    0.606061   \n",
      "8           Specificity    0.971500    0.974600    0.974500    0.968400   \n",
      "9              F1 score    0.695652    0.636364    0.654867    0.689655   \n",
      "10  F1 score (weighted)    0.902805    0.885786    0.889319    0.900423   \n",
      "11     F1 score (macro)    0.820862    0.787601    0.797480    0.817050   \n",
      "12    Balanced Accuracy    0.788790    0.748496    0.759320    0.787208   \n",
      "13                  MCC    0.652991    0.598008    0.615409    0.643854   \n",
      "14                  NPV    0.921900    0.905600    0.908000    0.921700   \n",
      "15              ROC_AUC    0.788790    0.748496    0.759320    0.787208   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.785704    0.713344  \n",
      "1    42.000000   38.000000  \n",
      "2   306.000000  305.000000  \n",
      "3    10.000000   10.000000  \n",
      "4    24.000000   29.000000  \n",
      "5     0.910995    0.897906  \n",
      "6     0.807692    0.791667  \n",
      "7     0.636364    0.567164  \n",
      "8     0.968400    0.968300  \n",
      "9     0.711864    0.660870  \n",
      "10    0.906679    0.890966  \n",
      "11    0.829616    0.800389  \n",
      "12    0.802359    0.767709  \n",
      "13    0.666668    0.614316  \n",
      "14    0.927300    0.913200  \n",
      "15    0.802359    0.767709  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_knn_5)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet5_cat = np.where(((Y_testSet5>=2) | (Y_testSet5<=-2)), 1, 0) \n",
    "y_pred_knn_5_cat = np.where(((y_pred_knn_5 >= 2) | (y_pred_knn_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_knn_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:55:37,386] Trial 300 finished with value: 0.6816402731286116 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:37,732] Trial 301 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:38,080] Trial 302 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:38,458] Trial 303 finished with value: 0.6374197533147428 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:38,806] Trial 304 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:39,203] Trial 305 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:39,610] Trial 306 finished with value: 0.6816402731286116 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:39,971] Trial 307 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:40,302] Trial 308 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:40,666] Trial 309 finished with value: 0.6816402731286116 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:41,037] Trial 310 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:41,440] Trial 311 finished with value: 0.6816402731286116 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:41,829] Trial 312 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:42,202] Trial 313 finished with value: 0.6769697811050456 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:42,580] Trial 314 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:42,958] Trial 315 finished with value: 0.6816402731286116 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:43,366] Trial 316 finished with value: 0.5875136209158504 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:43,744] Trial 317 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:44,131] Trial 318 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:44,509] Trial 319 finished with value: 0.6577019765944903 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:44,886] Trial 320 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:45,272] Trial 321 finished with value: 0.6374197533147428 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:45,622] Trial 322 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:45,944] Trial 323 finished with value: 0.6769697811050456 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:46,312] Trial 324 finished with value: 0.6816402731286116 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:46,690] Trial 325 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:47,065] Trial 326 finished with value: 0.6816402731286116 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:47,247] Trial 327 finished with value: 0.6705868137571324 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:47,583] Trial 328 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:47,950] Trial 329 finished with value: 0.6816402731286116 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:48,327] Trial 330 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:48,718] Trial 331 finished with value: 0.5558968478871249 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:49,105] Trial 332 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:49,472] Trial 333 finished with value: 0.6816402731286116 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:49,839] Trial 334 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:50,227] Trial 335 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:50,585] Trial 336 finished with value: 0.6816402731286116 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:50,942] Trial 337 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:51,311] Trial 338 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:51,667] Trial 339 finished with value: 0.6769697811050456 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:52,044] Trial 340 finished with value: 0.6165912490563829 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:52,408] Trial 341 finished with value: 0.6816402731286116 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:52,747] Trial 342 finished with value: 0.6437750441131812 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:53,115] Trial 343 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:53,494] Trial 344 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:53,851] Trial 345 finished with value: 0.6816402731286116 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:54,187] Trial 346 finished with value: 0.6195831768588638 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:54,545] Trial 347 finished with value: 0.6851246090752852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:54,727] Trial 348 finished with value: 0.6705868137571324 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:55,070] Trial 349 finished with value: 0.6816402731286116 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 251 with value: 0.6973312697872558.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6973\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 97\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.685239    0.696281    0.725667    0.713837   \n",
      "1                    TP   40.000000   35.000000   37.000000   40.000000   \n",
      "2                    TN  307.000000  307.000000  306.000000  306.000000   \n",
      "3                    FP    9.000000    8.000000    8.000000   10.000000   \n",
      "4                    FN   26.000000   32.000000   31.000000   26.000000   \n",
      "5              Accuracy    0.908377    0.895288    0.897906    0.905759   \n",
      "6             Precision    0.816327    0.813953    0.822222    0.800000   \n",
      "7           Sensitivity    0.606061    0.522388    0.544118    0.606061   \n",
      "8           Specificity    0.971500    0.974600    0.974500    0.968400   \n",
      "9              F1 score    0.695652    0.636364    0.654867    0.689655   \n",
      "10  F1 score (weighted)    0.902805    0.885786    0.889319    0.900423   \n",
      "11     F1 score (macro)    0.820862    0.787601    0.797480    0.817050   \n",
      "12    Balanced Accuracy    0.788790    0.748496    0.759320    0.787208   \n",
      "13                  MCC    0.652991    0.598008    0.615409    0.643854   \n",
      "14                  NPV    0.921900    0.905600    0.908000    0.921700   \n",
      "15              ROC_AUC    0.788790    0.748496    0.759320    0.787208   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.785704    0.713344    0.753293  \n",
      "1    42.000000   38.000000   34.000000  \n",
      "2   306.000000  305.000000  307.000000  \n",
      "3    10.000000   10.000000    9.000000  \n",
      "4    24.000000   29.000000   32.000000  \n",
      "5     0.910995    0.897906    0.892670  \n",
      "6     0.807692    0.791667    0.790698  \n",
      "7     0.636364    0.567164    0.515152  \n",
      "8     0.968400    0.968300    0.971500  \n",
      "9     0.711864    0.660870    0.623853  \n",
      "10    0.906679    0.890966    0.883231  \n",
      "11    0.829616    0.800389    0.780629  \n",
      "12    0.802359    0.767709    0.743335  \n",
      "13    0.666668    0.614316    0.582125  \n",
      "14    0.927300    0.913200    0.905600  \n",
      "15    0.802359    0.767709    0.743335  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_knn_6)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet6_cat = np.where(((Y_testSet6>=2) | (Y_testSet6<=-2)), 1, 0) \n",
    "y_pred_knn_6_cat = np.where(((y_pred_knn_6 >= 2) | (y_pred_knn_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_knn_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:55:55,551] Trial 350 finished with value: 0.6662667190732022 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:55,936] Trial 351 finished with value: 0.6089011367952779 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:56,326] Trial 352 finished with value: 0.6613483023486222 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:56,726] Trial 353 finished with value: 0.6662667190732022 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:57,131] Trial 354 finished with value: 0.6662667190732022 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:57,545] Trial 355 finished with value: 0.6613483023486222 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:57,962] Trial 356 finished with value: 0.5559754184505343 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:58,390] Trial 357 finished with value: 0.6726545891214066 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:58,784] Trial 358 finished with value: 0.6662667190732022 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:59,180] Trial 359 finished with value: 0.6439961548962809 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:59,587] Trial 360 finished with value: 0.6585547175837462 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:55:59,977] Trial 361 finished with value: 0.6662667190732022 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:00,395] Trial 362 finished with value: 0.6613483023486222 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:00,782] Trial 363 finished with value: 0.6726545891214066 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:01,178] Trial 364 finished with value: 0.5830467901796516 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:01,537] Trial 365 finished with value: 0.6662667190732022 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:01,937] Trial 366 finished with value: 0.6739902248707668 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:02,318] Trial 367 finished with value: 0.5919433329616293 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:02,744] Trial 368 finished with value: 0.6662667190732022 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:02,931] Trial 369 finished with value: 0.6612233478554244 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:03,249] Trial 370 finished with value: 0.6613483023486222 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:03,625] Trial 371 finished with value: 0.6662667190732022 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:03,989] Trial 372 finished with value: 0.6726545891214066 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:04,370] Trial 373 finished with value: 0.6662667190732022 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:04,744] Trial 374 finished with value: 0.6613483023486222 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:05,128] Trial 375 finished with value: 0.6739902248707668 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:05,522] Trial 376 finished with value: 0.6468588543159209 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:05,915] Trial 377 finished with value: 0.6726545891214066 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:06,321] Trial 378 finished with value: 0.6299449450789987 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:06,702] Trial 379 finished with value: 0.6613483023486222 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:07,093] Trial 380 finished with value: 0.6726545891214066 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:07,506] Trial 381 finished with value: 0.6739902248707668 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:07,865] Trial 382 finished with value: 0.6662667190732022 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:08,259] Trial 383 finished with value: 0.6726545891214066 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:08,680] Trial 384 finished with value: 0.6585547175837462 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:09,073] Trial 385 finished with value: 0.6613483023486222 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:09,449] Trial 386 finished with value: 0.6662667190732022 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:09,842] Trial 387 finished with value: 0.6662667190732022 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:10,249] Trial 388 finished with value: 0.6726545891214066 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:10,635] Trial 389 finished with value: 0.6739902248707668 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:10,829] Trial 390 finished with value: 0.5365107433946813 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:11,158] Trial 391 finished with value: 0.6613483023486222 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:11,520] Trial 392 finished with value: 0.6726545891214066 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:11,914] Trial 393 finished with value: 0.6662667190732022 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:12,300] Trial 394 finished with value: 0.6662667190732022 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:12,677] Trial 395 finished with value: 0.6739902248707668 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:13,052] Trial 396 finished with value: 0.6726545891214066 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:13,449] Trial 397 finished with value: 0.6662667190732022 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:13,809] Trial 398 finished with value: 0.6180331976514987 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:14,195] Trial 399 finished with value: 0.6726545891214066 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 251 with value: 0.6973312697872558.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6973\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 97\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.685239    0.696281    0.725667    0.713837   \n",
      "1                    TP   40.000000   35.000000   37.000000   40.000000   \n",
      "2                    TN  307.000000  307.000000  306.000000  306.000000   \n",
      "3                    FP    9.000000    8.000000    8.000000   10.000000   \n",
      "4                    FN   26.000000   32.000000   31.000000   26.000000   \n",
      "5              Accuracy    0.908377    0.895288    0.897906    0.905759   \n",
      "6             Precision    0.816327    0.813953    0.822222    0.800000   \n",
      "7           Sensitivity    0.606061    0.522388    0.544118    0.606061   \n",
      "8           Specificity    0.971500    0.974600    0.974500    0.968400   \n",
      "9              F1 score    0.695652    0.636364    0.654867    0.689655   \n",
      "10  F1 score (weighted)    0.902805    0.885786    0.889319    0.900423   \n",
      "11     F1 score (macro)    0.820862    0.787601    0.797480    0.817050   \n",
      "12    Balanced Accuracy    0.788790    0.748496    0.759320    0.787208   \n",
      "13                  MCC    0.652991    0.598008    0.615409    0.643854   \n",
      "14                  NPV    0.921900    0.905600    0.908000    0.921700   \n",
      "15              ROC_AUC    0.788790    0.748496    0.759320    0.787208   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.785704    0.713344    0.753293    0.741681  \n",
      "1    42.000000   38.000000   34.000000   42.000000  \n",
      "2   306.000000  305.000000  307.000000  306.000000  \n",
      "3    10.000000   10.000000    9.000000    9.000000  \n",
      "4    24.000000   29.000000   32.000000   25.000000  \n",
      "5     0.910995    0.897906    0.892670    0.910995  \n",
      "6     0.807692    0.791667    0.790698    0.823529  \n",
      "7     0.636364    0.567164    0.515152    0.626866  \n",
      "8     0.968400    0.968300    0.971500    0.971400  \n",
      "9     0.711864    0.660870    0.623853    0.711864  \n",
      "10    0.906679    0.890966    0.883231    0.906063  \n",
      "11    0.829616    0.800389    0.780629    0.829616  \n",
      "12    0.802359    0.767709    0.743335    0.799147  \n",
      "13    0.666668    0.614316    0.582125    0.668973  \n",
      "14    0.927300    0.913200    0.905600    0.924500  \n",
      "15    0.802359    0.767709    0.743335    0.799147  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_knn_7)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet7_cat = np.where(((Y_testSet7>=2) | (Y_testSet7<=-2)), 1, 0) \n",
    "y_pred_knn_7_cat = np.where(((y_pred_knn_7 >= 2) | (y_pred_knn_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_knn_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:56:14,700] Trial 400 finished with value: 0.5887496779240705 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 251 with value: 0.6973312697872558.\n",
      "[I 2023-12-20 17:56:15,120] Trial 401 finished with value: 0.7077316489716907 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 401 with value: 0.7077316489716907.\n",
      "[I 2023-12-20 17:56:15,531] Trial 402 finished with value: 0.7133560567201337 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 402 with value: 0.7133560567201337.\n",
      "[I 2023-12-20 17:56:15,904] Trial 403 finished with value: 0.7153585358923547 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:16,307] Trial 404 finished with value: 0.7153585358923547 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:16,704] Trial 405 finished with value: 0.7153585358923547 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:17,104] Trial 406 finished with value: 0.7153585358923547 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:17,518] Trial 407 finished with value: 0.7153585358923547 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:17,915] Trial 408 finished with value: 0.7153585358923547 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:18,303] Trial 409 finished with value: 0.7153585358923547 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:18,688] Trial 410 finished with value: 0.7153585358923547 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:19,071] Trial 411 finished with value: 0.7153585358923547 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:19,450] Trial 412 finished with value: 0.7153585358923547 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:19,645] Trial 413 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:19,833] Trial 414 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:20,022] Trial 415 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:20,211] Trial 416 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:20,395] Trial 417 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:20,564] Trial 418 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:20,732] Trial 419 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:20,912] Trial 420 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:21,090] Trial 421 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:21,270] Trial 422 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:21,439] Trial 423 finished with value: 0.6864400198487559 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:21,619] Trial 424 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:21,800] Trial 425 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:21,979] Trial 426 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:22,148] Trial 427 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:22,324] Trial 428 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:22,504] Trial 429 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:22,684] Trial 430 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:22,862] Trial 431 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:23,047] Trial 432 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:23,227] Trial 433 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:23,407] Trial 434 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:23,587] Trial 435 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:23,768] Trial 436 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:23,949] Trial 437 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:24,128] Trial 438 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:24,309] Trial 439 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:24,489] Trial 440 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:24,669] Trial 441 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:24,849] Trial 442 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:25,031] Trial 443 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:25,209] Trial 444 finished with value: 0.6864400198487559 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:25,388] Trial 445 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:25,568] Trial 446 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:25,748] Trial 447 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:25,929] Trial 448 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:26,110] Trial 449 finished with value: 0.7062621597652635 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7154\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.685239    0.696281    0.725667    0.713837   \n",
      "1                    TP   40.000000   35.000000   37.000000   40.000000   \n",
      "2                    TN  307.000000  307.000000  306.000000  306.000000   \n",
      "3                    FP    9.000000    8.000000    8.000000   10.000000   \n",
      "4                    FN   26.000000   32.000000   31.000000   26.000000   \n",
      "5              Accuracy    0.908377    0.895288    0.897906    0.905759   \n",
      "6             Precision    0.816327    0.813953    0.822222    0.800000   \n",
      "7           Sensitivity    0.606061    0.522388    0.544118    0.606061   \n",
      "8           Specificity    0.971500    0.974600    0.974500    0.968400   \n",
      "9              F1 score    0.695652    0.636364    0.654867    0.689655   \n",
      "10  F1 score (weighted)    0.902805    0.885786    0.889319    0.900423   \n",
      "11     F1 score (macro)    0.820862    0.787601    0.797480    0.817050   \n",
      "12    Balanced Accuracy    0.788790    0.748496    0.759320    0.787208   \n",
      "13                  MCC    0.652991    0.598008    0.615409    0.643854   \n",
      "14                  NPV    0.921900    0.905600    0.908000    0.921700   \n",
      "15              ROC_AUC    0.788790    0.748496    0.759320    0.787208   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.785704    0.713344    0.753293    0.741681    0.683498  \n",
      "1    42.000000   38.000000   34.000000   42.000000   39.000000  \n",
      "2   306.000000  305.000000  307.000000  306.000000  304.000000  \n",
      "3    10.000000   10.000000    9.000000    9.000000   10.000000  \n",
      "4    24.000000   29.000000   32.000000   25.000000   29.000000  \n",
      "5     0.910995    0.897906    0.892670    0.910995    0.897906  \n",
      "6     0.807692    0.791667    0.790698    0.823529    0.795918  \n",
      "7     0.636364    0.567164    0.515152    0.626866    0.573529  \n",
      "8     0.968400    0.968300    0.971500    0.971400    0.968200  \n",
      "9     0.711864    0.660870    0.623853    0.711864    0.666667  \n",
      "10    0.906679    0.890966    0.883231    0.906063    0.891115  \n",
      "11    0.829616    0.800389    0.780629    0.829616    0.803194  \n",
      "12    0.802359    0.767709    0.743335    0.799147    0.770841  \n",
      "13    0.666668    0.614316    0.582125    0.668973    0.619646  \n",
      "14    0.927300    0.913200    0.905600    0.924500    0.912900  \n",
      "15    0.802359    0.767709    0.743335    0.799147    0.770841  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_knn_8)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet8_cat = np.where(((Y_testSet8>=2) | (Y_testSet8<=-2)), 1, 0) \n",
    "y_pred_knn_8_cat = np.where(((y_pred_knn_8 >= 2) | (y_pred_knn_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_knn_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:56:26,509] Trial 450 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:26,698] Trial 451 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:26,884] Trial 452 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:27,072] Trial 453 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:27,260] Trial 454 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:27,448] Trial 455 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:27,637] Trial 456 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:27,825] Trial 457 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:28,013] Trial 458 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:28,202] Trial 459 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:28,391] Trial 460 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:28,580] Trial 461 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:28,769] Trial 462 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:28,958] Trial 463 finished with value: 0.6749051081065836 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:29,137] Trial 464 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:29,316] Trial 465 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:29,495] Trial 466 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:29,673] Trial 467 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:29,856] Trial 468 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:30,035] Trial 469 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:30,214] Trial 470 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:30,392] Trial 471 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:30,559] Trial 472 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:30,728] Trial 473 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:30,907] Trial 474 finished with value: 0.6824717056261038 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:31,087] Trial 475 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:31,265] Trial 476 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 67}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:31,432] Trial 477 finished with value: 0.6824717056261038 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:31,601] Trial 478 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:31,773] Trial 479 finished with value: 0.6824717056261038 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:31,952] Trial 480 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:32,120] Trial 481 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:32,299] Trial 482 finished with value: 0.6824717056261038 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:32,478] Trial 483 finished with value: 0.6749051081065836 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:32,658] Trial 484 finished with value: 0.6824717056261038 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:32,840] Trial 485 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:33,018] Trial 486 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:33,187] Trial 487 finished with value: 0.6824717056261038 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:33,359] Trial 488 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:33,538] Trial 489 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:33,719] Trial 490 finished with value: 0.6824717056261038 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:33,895] Trial 491 finished with value: 0.626605112063855 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:34,075] Trial 492 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:34,254] Trial 493 finished with value: 0.6824717056261038 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:34,432] Trial 494 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:34,612] Trial 495 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:34,794] Trial 496 finished with value: 0.6824717056261038 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:34,975] Trial 497 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:35,155] Trial 498 finished with value: 0.6824717056261038 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 403 with value: 0.7153585358923547.\n",
      "[I 2023-12-20 17:56:35,335] Trial 499 finished with value: 0.6865237626104144 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 403 with value: 0.7153585358923547.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7154\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 100\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.685239    0.696281    0.725667    0.713837   \n",
      "1                    TP   40.000000   35.000000   37.000000   40.000000   \n",
      "2                    TN  307.000000  307.000000  306.000000  306.000000   \n",
      "3                    FP    9.000000    8.000000    8.000000   10.000000   \n",
      "4                    FN   26.000000   32.000000   31.000000   26.000000   \n",
      "5              Accuracy    0.908377    0.895288    0.897906    0.905759   \n",
      "6             Precision    0.816327    0.813953    0.822222    0.800000   \n",
      "7           Sensitivity    0.606061    0.522388    0.544118    0.606061   \n",
      "8           Specificity    0.971500    0.974600    0.974500    0.968400   \n",
      "9              F1 score    0.695652    0.636364    0.654867    0.689655   \n",
      "10  F1 score (weighted)    0.902805    0.885786    0.889319    0.900423   \n",
      "11     F1 score (macro)    0.820862    0.787601    0.797480    0.817050   \n",
      "12    Balanced Accuracy    0.788790    0.748496    0.759320    0.787208   \n",
      "13                  MCC    0.652991    0.598008    0.615409    0.643854   \n",
      "14                  NPV    0.921900    0.905600    0.908000    0.921700   \n",
      "15              ROC_AUC    0.788790    0.748496    0.759320    0.787208   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.785704    0.713344    0.753293    0.741681    0.683498    0.712557  \n",
      "1    42.000000   38.000000   34.000000   42.000000   39.000000   41.000000  \n",
      "2   306.000000  305.000000  307.000000  306.000000  304.000000  306.000000  \n",
      "3    10.000000   10.000000    9.000000    9.000000   10.000000    7.000000  \n",
      "4    24.000000   29.000000   32.000000   25.000000   29.000000   28.000000  \n",
      "5     0.910995    0.897906    0.892670    0.910995    0.897906    0.908377  \n",
      "6     0.807692    0.791667    0.790698    0.823529    0.795918    0.854167  \n",
      "7     0.636364    0.567164    0.515152    0.626866    0.573529    0.594203  \n",
      "8     0.968400    0.968300    0.971500    0.971400    0.968200    0.977600  \n",
      "9     0.711864    0.660870    0.623853    0.711864    0.666667    0.700855  \n",
      "10    0.906679    0.890966    0.883231    0.906063    0.891115    0.901641  \n",
      "11    0.829616    0.800389    0.780629    0.829616    0.803194    0.823379  \n",
      "12    0.802359    0.767709    0.743335    0.799147    0.770841    0.785919  \n",
      "13    0.666668    0.614316    0.582125    0.668973    0.619646    0.663707  \n",
      "14    0.927300    0.913200    0.905600    0.924500    0.912900    0.916200  \n",
      "15    0.802359    0.767709    0.743335    0.799147    0.770841    0.785919  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_knn_9)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet9_cat = np.where(((Y_testSet9>=2) | (Y_testSet9<=-2)), 1, 0) \n",
    "y_pred_knn_9_cat = np.where(((y_pred_knn_9 >= 2) | (y_pred_knn_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_knn_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.715725</td>\n",
       "      <td>0.059646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>2.836273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>152.200000</td>\n",
       "      <td>2.936362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>2.590581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>3.326660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.904712</td>\n",
       "      <td>0.021058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.799669</td>\n",
       "      <td>0.089643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.618062</td>\n",
       "      <td>0.094033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.965720</td>\n",
       "      <td>0.016518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.692387</td>\n",
       "      <td>0.070095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.899501</td>\n",
       "      <td>0.022869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.817978</td>\n",
       "      <td>0.040942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.791890</td>\n",
       "      <td>0.046867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.647679</td>\n",
       "      <td>0.079513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>0.018879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.791890</td>\n",
       "      <td>0.046867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.715725     0.059646\n",
       "1                    TP        20.600000     2.836273\n",
       "2                    TN       152.200000     2.936362\n",
       "3                    FP         5.400000     2.590581\n",
       "4                    FN        12.800000     3.326660\n",
       "5              Accuracy         0.904712     0.021058\n",
       "6             Precision         0.799669     0.089643\n",
       "7           Sensitivity         0.618062     0.094033\n",
       "8           Specificity         0.965720     0.016518\n",
       "9              F1 score         0.692387     0.070095\n",
       "10  F1 score (weighted)         0.899501     0.022869\n",
       "11     F1 score (macro)         0.817978     0.040942\n",
       "12    Balanced Accuracy         0.791890     0.046867\n",
       "13                  MCC         0.647679     0.079513\n",
       "14                  NPV         0.922700     0.018879\n",
       "15              ROC_AUC         0.791890     0.046867"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.685239</td>\n",
       "      <td>0.696281</td>\n",
       "      <td>0.725667</td>\n",
       "      <td>0.713837</td>\n",
       "      <td>0.785704</td>\n",
       "      <td>0.713344</td>\n",
       "      <td>0.753293</td>\n",
       "      <td>0.741681</td>\n",
       "      <td>0.683498</td>\n",
       "      <td>0.712557</td>\n",
       "      <td>0.721110</td>\n",
       "      <td>0.031896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>38.800000</td>\n",
       "      <td>2.780887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>0.942809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.054093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>28.200000</td>\n",
       "      <td>2.898275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.908377</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.905759</td>\n",
       "      <td>0.910995</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.910995</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.908377</td>\n",
       "      <td>0.902618</td>\n",
       "      <td>0.006959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.811617</td>\n",
       "      <td>0.019221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.579190</td>\n",
       "      <td>0.042092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.971500</td>\n",
       "      <td>0.974600</td>\n",
       "      <td>0.974500</td>\n",
       "      <td>0.968400</td>\n",
       "      <td>0.968400</td>\n",
       "      <td>0.968300</td>\n",
       "      <td>0.971500</td>\n",
       "      <td>0.971400</td>\n",
       "      <td>0.968200</td>\n",
       "      <td>0.977600</td>\n",
       "      <td>0.971440</td>\n",
       "      <td>0.003268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.654867</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.660870</td>\n",
       "      <td>0.623853</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.700855</td>\n",
       "      <td>0.675251</td>\n",
       "      <td>0.031288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.902805</td>\n",
       "      <td>0.885786</td>\n",
       "      <td>0.889319</td>\n",
       "      <td>0.900423</td>\n",
       "      <td>0.906679</td>\n",
       "      <td>0.890966</td>\n",
       "      <td>0.883231</td>\n",
       "      <td>0.906063</td>\n",
       "      <td>0.891115</td>\n",
       "      <td>0.901641</td>\n",
       "      <td>0.895803</td>\n",
       "      <td>0.008653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.820862</td>\n",
       "      <td>0.787601</td>\n",
       "      <td>0.797480</td>\n",
       "      <td>0.817050</td>\n",
       "      <td>0.829616</td>\n",
       "      <td>0.800389</td>\n",
       "      <td>0.780629</td>\n",
       "      <td>0.829616</td>\n",
       "      <td>0.803194</td>\n",
       "      <td>0.823379</td>\n",
       "      <td>0.808982</td>\n",
       "      <td>0.017530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.788790</td>\n",
       "      <td>0.748496</td>\n",
       "      <td>0.759320</td>\n",
       "      <td>0.787208</td>\n",
       "      <td>0.802359</td>\n",
       "      <td>0.767709</td>\n",
       "      <td>0.743335</td>\n",
       "      <td>0.799147</td>\n",
       "      <td>0.770841</td>\n",
       "      <td>0.785919</td>\n",
       "      <td>0.775312</td>\n",
       "      <td>0.020576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.652991</td>\n",
       "      <td>0.598008</td>\n",
       "      <td>0.615409</td>\n",
       "      <td>0.643854</td>\n",
       "      <td>0.666668</td>\n",
       "      <td>0.614316</td>\n",
       "      <td>0.582125</td>\n",
       "      <td>0.668973</td>\n",
       "      <td>0.619646</td>\n",
       "      <td>0.663707</td>\n",
       "      <td>0.632570</td>\n",
       "      <td>0.030796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.921900</td>\n",
       "      <td>0.905600</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.921700</td>\n",
       "      <td>0.927300</td>\n",
       "      <td>0.913200</td>\n",
       "      <td>0.905600</td>\n",
       "      <td>0.924500</td>\n",
       "      <td>0.912900</td>\n",
       "      <td>0.916200</td>\n",
       "      <td>0.915690</td>\n",
       "      <td>0.007917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.788790</td>\n",
       "      <td>0.748496</td>\n",
       "      <td>0.759320</td>\n",
       "      <td>0.787208</td>\n",
       "      <td>0.802359</td>\n",
       "      <td>0.767709</td>\n",
       "      <td>0.743335</td>\n",
       "      <td>0.799147</td>\n",
       "      <td>0.770841</td>\n",
       "      <td>0.785919</td>\n",
       "      <td>0.775312</td>\n",
       "      <td>0.020576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.685239    0.696281    0.725667    0.713837   \n",
       "1                    TP   40.000000   35.000000   37.000000   40.000000   \n",
       "2                    TN  307.000000  307.000000  306.000000  306.000000   \n",
       "3                    FP    9.000000    8.000000    8.000000   10.000000   \n",
       "4                    FN   26.000000   32.000000   31.000000   26.000000   \n",
       "5              Accuracy    0.908377    0.895288    0.897906    0.905759   \n",
       "6             Precision    0.816327    0.813953    0.822222    0.800000   \n",
       "7           Sensitivity    0.606061    0.522388    0.544118    0.606061   \n",
       "8           Specificity    0.971500    0.974600    0.974500    0.968400   \n",
       "9              F1 score    0.695652    0.636364    0.654867    0.689655   \n",
       "10  F1 score (weighted)    0.902805    0.885786    0.889319    0.900423   \n",
       "11     F1 score (macro)    0.820862    0.787601    0.797480    0.817050   \n",
       "12    Balanced Accuracy    0.788790    0.748496    0.759320    0.787208   \n",
       "13                  MCC    0.652991    0.598008    0.615409    0.643854   \n",
       "14                  NPV    0.921900    0.905600    0.908000    0.921700   \n",
       "15              ROC_AUC    0.788790    0.748496    0.759320    0.787208   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.785704    0.713344    0.753293    0.741681    0.683498    0.712557   \n",
       "1    42.000000   38.000000   34.000000   42.000000   39.000000   41.000000   \n",
       "2   306.000000  305.000000  307.000000  306.000000  304.000000  306.000000   \n",
       "3    10.000000   10.000000    9.000000    9.000000   10.000000    7.000000   \n",
       "4    24.000000   29.000000   32.000000   25.000000   29.000000   28.000000   \n",
       "5     0.910995    0.897906    0.892670    0.910995    0.897906    0.908377   \n",
       "6     0.807692    0.791667    0.790698    0.823529    0.795918    0.854167   \n",
       "7     0.636364    0.567164    0.515152    0.626866    0.573529    0.594203   \n",
       "8     0.968400    0.968300    0.971500    0.971400    0.968200    0.977600   \n",
       "9     0.711864    0.660870    0.623853    0.711864    0.666667    0.700855   \n",
       "10    0.906679    0.890966    0.883231    0.906063    0.891115    0.901641   \n",
       "11    0.829616    0.800389    0.780629    0.829616    0.803194    0.823379   \n",
       "12    0.802359    0.767709    0.743335    0.799147    0.770841    0.785919   \n",
       "13    0.666668    0.614316    0.582125    0.668973    0.619646    0.663707   \n",
       "14    0.927300    0.913200    0.905600    0.924500    0.912900    0.916200   \n",
       "15    0.802359    0.767709    0.743335    0.799147    0.770841    0.785919   \n",
       "\n",
       "           ave       std  \n",
       "0     0.721110  0.031896  \n",
       "1    38.800000  2.780887  \n",
       "2   306.000000  0.942809  \n",
       "3     9.000000  1.054093  \n",
       "4    28.200000  2.898275  \n",
       "5     0.902618  0.006959  \n",
       "6     0.811617  0.019221  \n",
       "7     0.579190  0.042092  \n",
       "8     0.971440  0.003268  \n",
       "9     0.675251  0.031288  \n",
       "10    0.895803  0.008653  \n",
       "11    0.808982  0.017530  \n",
       "12    0.775312  0.020576  \n",
       "13    0.632570  0.030796  \n",
       "14    0.915690  0.007917  \n",
       "15    0.775312  0.020576  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test['std'] = mat_met_knn_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.716345</td>\n",
       "      <td>0.044662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.907539</td>\n",
       "      <td>0.019493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.807920</td>\n",
       "      <td>0.071567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.622207</td>\n",
       "      <td>0.098350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.968032</td>\n",
       "      <td>0.013990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.698581</td>\n",
       "      <td>0.074945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.902197</td>\n",
       "      <td>0.022024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.821957</td>\n",
       "      <td>0.042803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.795116</td>\n",
       "      <td>0.048878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.655744</td>\n",
       "      <td>0.080368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.923960</td>\n",
       "      <td>0.018327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.795116</td>\n",
       "      <td>0.048878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.716345     0.044662\n",
       "1              Accuracy         0.907539     0.019493\n",
       "2             Precision         0.807920     0.071567\n",
       "3           Sensitivity         0.622207     0.098350\n",
       "4           Specificity         0.968032     0.013990\n",
       "5              F1 score         0.698581     0.074945\n",
       "6   F1 score (weighted)         0.902197     0.022024\n",
       "7      F1 score (macro)         0.821957     0.042803\n",
       "8     Balanced Accuracy         0.795116     0.048878\n",
       "9                   MCC         0.655744     0.080368\n",
       "10                  NPV         0.923960     0.018327\n",
       "11              ROC_AUC         0.795116     0.048878"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_knn_cat = np.where(((y_pred_optimized_knn >= 2) | (y_pred_optimized_knn <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_knn_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_knn))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "knn_run0 = data_knn[['y_test_idx0', 'y_test0', 'y_pred_knn0']]\n",
    "knn_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "knn_run0.reset_index(inplace=True, drop=True)\n",
    "knn_run1 = data_knn[['y_test_idx1', 'y_test1', 'y_pred_knn1']]\n",
    "knn_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "knn_run1.reset_index(inplace=True, drop=True)\n",
    "knn_run2 = data_knn[['y_test_idx2', 'y_test2', 'y_pred_knn2']]\n",
    "knn_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "knn_run2.reset_index(inplace=True, drop=True)\n",
    "knn_run3 = data_knn[['y_test_idx3', 'y_test3', 'y_pred_knn3']]\n",
    "knn_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "knn_run3.reset_index(inplace=True, drop=True)\n",
    "knn_run4 = data_knn[['y_test_idx4', 'y_test4', 'y_pred_knn4']]\n",
    "knn_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "knn_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "knn_5preds = pd.concat([chembl_id,knn_run0, knn_run1, knn_run2, knn_run3, knn_run4], axis=1)\n",
    "knn_5preds = knn_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_knn0', 'y_pred_knn1', 'y_pred_knn2', 'y_pred_knn3', 'y_pred_knn4']]\n",
    "knn_5preds['y_pred_knn_ave'] = knn_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "knn_5preds['y_pred_knn_std'] = knn_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "\n",
    "mat_met_optimized_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0bc43db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model r2_score 0.6998 with a standard deviation of 0.0551\n",
      "KNN optimized model r2_score 0.7178 with a standard deviation of 0.0508\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"KNN baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_knn_withSemiSel.joblib']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_reg, \"OUTPUT/knn_reg_withSemiSel.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"OUTPUT/optimizedCV_knn_withSemiSel.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.709571     0.045627\n",
      "1                    TP        13.800000     2.859681\n",
      "2                    TN       155.200000     1.475730\n",
      "3                    FP         2.400000     1.173788\n",
      "4                    FN        19.600000     3.533962\n",
      "5              Accuracy         0.884817     0.018304\n",
      "6             Precision         0.856817     0.072007\n",
      "7           Sensitivity         0.414856     0.094215\n",
      "8           Specificity         0.984770     0.007458\n",
      "9              F1 score         0.552734     0.088583\n",
      "10  F1 score (weighted)         0.866973     0.024921\n",
      "11     F1 score (macro)         0.743297     0.049138\n",
      "12    Balanced Accuracy         0.699815     0.046517\n",
      "13                  MCC         0.542525     0.076567\n",
      "14                  NPV         0.888140     0.018340\n",
      "15              ROC_AUC         0.699815     0.046517\n",
      "CPU times: user 3.53 s, sys: 4 ms, total: 3.53 s\n",
      "Wall time: 3.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    \n",
    "    svm_reg.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:56:44,232] A new study created in memory with name: SVM_regressor_CV\n",
      "[I 2023-12-20 17:56:47,042] Trial 0 finished with value: 0.26271054034863967 and parameters: {'C': 2.0, 'gamma': 0.125}. Best is trial 0 with value: 0.26271054034863967.\n",
      "[I 2023-12-20 17:56:49,662] Trial 1 finished with value: 0.11194540511629972 and parameters: {'C': 0.125, 'gamma': 0.00048828125}. Best is trial 0 with value: 0.26271054034863967.\n",
      "[I 2023-12-20 17:56:52,127] Trial 2 finished with value: 0.5953023024641206 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 2 with value: 0.5953023024641206.\n",
      "[I 2023-12-20 17:56:54,728] Trial 3 finished with value: 0.4782182445944068 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 2 with value: 0.5953023024641206.\n",
      "[I 2023-12-20 17:56:57,346] Trial 4 finished with value: 0.5945589748025306 and parameters: {'C': 0.5, 'gamma': 0.0078125}. Best is trial 2 with value: 0.5953023024641206.\n",
      "[I 2023-12-20 17:57:00,178] Trial 5 finished with value: 0.015138070938915648 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 2 with value: 0.5953023024641206.\n",
      "[I 2023-12-20 17:57:03,469] Trial 6 finished with value: 0.014845812105935363 and parameters: {'C': 16.0, 'gamma': 8.0}. Best is trial 2 with value: 0.5953023024641206.\n",
      "[I 2023-12-20 17:57:06,207] Trial 7 finished with value: 0.029755834882765165 and parameters: {'C': 0.03125, 'gamma': 0.00048828125}. Best is trial 2 with value: 0.5953023024641206.\n",
      "[I 2023-12-20 17:57:09,492] Trial 8 finished with value: 0.01484578603872001 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 2 with value: 0.5953023024641206.\n",
      "[I 2023-12-20 17:57:12,133] Trial 9 finished with value: 0.5498113802552635 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 2 with value: 0.5953023024641206.\n",
      "[I 2023-12-20 17:57:14,803] Trial 10 finished with value: 0.3921441205038171 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 2 with value: 0.5953023024641206.\n",
      "[I 2023-12-20 17:57:17,417] Trial 11 finished with value: 0.5945589748025306 and parameters: {'C': 0.5, 'gamma': 0.0078125}. Best is trial 2 with value: 0.5953023024641206.\n",
      "[I 2023-12-20 17:57:19,958] Trial 12 finished with value: 0.6055452041914491 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 12 with value: 0.6055452041914491.\n",
      "[I 2023-12-20 17:57:22,529] Trial 13 finished with value: 0.6926303959964503 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:57:25,099] Trial 14 finished with value: 0.6926303959964503 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:57:27,707] Trial 15 finished with value: 0.6926303959964503 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:57:30,309] Trial 16 finished with value: 0.6771063684761452 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:57:32,920] Trial 17 finished with value: 0.12168750460597244 and parameters: {'C': 0.0625, 'gamma': 0.0625}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:57:35,640] Trial 18 finished with value: -0.0034575195360087908 and parameters: {'C': 0.0078125, 'gamma': 3.0517578125e-05}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:57:39,046] Trial 19 finished with value: 0.6339757945813487 and parameters: {'C': 64.0, 'gamma': 0.001953125}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:57:41,672] Trial 20 finished with value: 0.4779184740472019 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:57:44,291] Trial 21 finished with value: 0.6926303959964503 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:57:46,856] Trial 22 finished with value: 0.6926303959964503 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:57:49,560] Trial 23 finished with value: -0.0027078306238578985 and parameters: {'C': 0.015625, 'gamma': 0.5}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:57:52,051] Trial 24 finished with value: 0.6648748080513485 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:57:54,880] Trial 25 finished with value: 0.07122496528525109 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:57:57,445] Trial 26 finished with value: 0.5973080108268609 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:58:00,291] Trial 27 finished with value: 0.022817225174495935 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:58:02,932] Trial 28 finished with value: 0.17674077984568926 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:58:05,719] Trial 29 finished with value: 0.26271054034863967 and parameters: {'C': 2.0, 'gamma': 0.125}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:58:08,375] Trial 30 finished with value: 0.387086557739239 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:58:11,043] Trial 31 finished with value: 0.017188886591486284 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:58:13,609] Trial 32 finished with value: 0.6926303959964503 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:58:16,159] Trial 33 finished with value: 0.6926303959964503 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:58:18,801] Trial 34 finished with value: 0.6897631532803796 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:58:21,430] Trial 35 finished with value: 0.0005603325172591211 and parameters: {'C': 0.015625, 'gamma': 0.0001220703125}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:58:24,195] Trial 36 finished with value: 0.015138070938915648 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:58:26,690] Trial 37 finished with value: 0.2703736427303956 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:58:29,898] Trial 38 finished with value: 0.6286746101087748 and parameters: {'C': 64.0, 'gamma': 0.0009765625}. Best is trial 13 with value: 0.6926303959964503.\n",
      "[I 2023-12-20 17:58:32,517] Trial 39 finished with value: 0.7083467362631212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:58:35,164] Trial 40 finished with value: 0.7083467362631212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:58:37,810] Trial 41 finished with value: 0.7083467362631212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:58:40,453] Trial 42 finished with value: 0.7083467362631212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:58:43,095] Trial 43 finished with value: 0.7083467362631212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:58:45,739] Trial 44 finished with value: 0.7083467362631212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:58:48,385] Trial 45 finished with value: 0.7083467362631212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:58:51,031] Trial 46 finished with value: 0.7083467362631212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:58:53,675] Trial 47 finished with value: 0.7083467362631212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:58:56,320] Trial 48 finished with value: 0.7083467362631212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:58:58,966] Trial 49 finished with value: 0.7083467362631212 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.7083\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_regressor_CV\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.709863\n",
      "1                    TP   33.000000\n",
      "2                    TN  309.000000\n",
      "3                    FP    7.000000\n",
      "4                    FN   33.000000\n",
      "5              Accuracy    0.895288\n",
      "6             Precision    0.825000\n",
      "7           Sensitivity    0.500000\n",
      "8           Specificity    0.977800\n",
      "9              F1 score    0.622642\n",
      "10  F1 score (weighted)    0.884515\n",
      "11     F1 score (macro)    0.780926\n",
      "12    Balanced Accuracy    0.738924\n",
      "13                  MCC    0.590014\n",
      "14                  NPV    0.903500\n",
      "15              ROC_AUC    0.738924\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_svm_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_svm_0_cat = np.where(((y_pred_svm_0 >= 2) | (y_pred_svm_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_svm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 17:59:01,914] Trial 50 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:04,409] Trial 51 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:06,908] Trial 52 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:09,414] Trial 53 finished with value: 0.031332534639814835 and parameters: {'C': 0.25, 'gamma': 6.103515625e-05}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:11,959] Trial 54 finished with value: 0.636413509533387 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:14,429] Trial 55 finished with value: 0.7019156841782396 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:16,900] Trial 56 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:19,417] Trial 57 finished with value: 0.04844430002204533 and parameters: {'C': 0.0078125, 'gamma': 0.00390625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:22,111] Trial 58 finished with value: 0.682084463927759 and parameters: {'C': 32.0, 'gamma': 0.0078125}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:25,120] Trial 59 finished with value: 0.017407561185100208 and parameters: {'C': 1.0, 'gamma': 8.0}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:27,567] Trial 60 finished with value: 0.4800127401733089 and parameters: {'C': 0.5, 'gamma': 0.001953125}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:30,064] Trial 61 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:32,551] Trial 62 finished with value: 0.58953814167051 and parameters: {'C': 32.0, 'gamma': 0.0625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:35,038] Trial 63 finished with value: 0.4879198571027392 and parameters: {'C': 32.0, 'gamma': 3.0517578125e-05}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:37,462] Trial 64 finished with value: 0.3977170382079473 and parameters: {'C': 2.0, 'gamma': 0.000244140625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:39,962] Trial 65 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:42,324] Trial 66 finished with value: 0.4528155536876465 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:44,900] Trial 67 finished with value: -0.002509373929837122 and parameters: {'C': 0.03125, 'gamma': 2.0}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:47,506] Trial 68 finished with value: 0.0394920136043108 and parameters: {'C': 128.0, 'gamma': 0.5}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:50,092] Trial 69 finished with value: 0.027890852779759577 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:52,697] Trial 70 finished with value: 0.08265172518812905 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:55,164] Trial 71 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 17:59:57,632] Trial 72 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:00,102] Trial 73 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:02,758] Trial 74 finished with value: 0.3073111712240424 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:05,252] Trial 75 finished with value: 0.06329825924741797 and parameters: {'C': 0.25, 'gamma': 0.0001220703125}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:07,710] Trial 76 finished with value: 0.3354515596762791 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:10,334] Trial 77 finished with value: 0.019719295598846987 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:12,810] Trial 78 finished with value: 0.6976721451393112 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:15,319] Trial 79 finished with value: 0.08157452599718841 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:17,879] Trial 80 finished with value: -0.0019410649271997872 and parameters: {'C': 0.015625, 'gamma': 6.103515625e-05}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:20,347] Trial 81 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:22,815] Trial 82 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:25,282] Trial 83 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:27,836] Trial 84 finished with value: 0.636413509533387 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:30,324] Trial 85 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:32,650] Trial 86 finished with value: 0.6105698896980003 and parameters: {'C': 1.0, 'gamma': 0.00390625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:35,386] Trial 87 finished with value: 0.6401683142294421 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:38,328] Trial 88 finished with value: 0.013481541548421905 and parameters: {'C': 0.5, 'gamma': 8.0}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:41,014] Trial 89 finished with value: 0.682084463927759 and parameters: {'C': 32.0, 'gamma': 0.0078125}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:43,383] Trial 90 finished with value: 0.5862924697193448 and parameters: {'C': 2.0, 'gamma': 0.0625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:45,871] Trial 91 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:48,358] Trial 92 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:50,716] Trial 93 finished with value: 0.4528155536876465 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:53,198] Trial 94 finished with value: 0.4879198571027392 and parameters: {'C': 32.0, 'gamma': 3.0517578125e-05}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:55,728] Trial 95 finished with value: 0.10475428595496225 and parameters: {'C': 0.03125, 'gamma': 0.001953125}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:00:58,218] Trial 96 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:01:01,039] Trial 97 finished with value: 0.6214925147163965 and parameters: {'C': 128.0, 'gamma': 0.000244140625}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:01:03,629] Trial 98 finished with value: 0.03949207246568037 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:01:06,107] Trial 99 finished with value: 0.6989863514415173 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 39 with value: 0.7083467362631212.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7083\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.709863    0.750816\n",
      "1                    TP   33.000000   32.000000\n",
      "2                    TN  309.000000  310.000000\n",
      "3                    FP    7.000000    5.000000\n",
      "4                    FN   33.000000   35.000000\n",
      "5              Accuracy    0.895288    0.895288\n",
      "6             Precision    0.825000    0.864865\n",
      "7           Sensitivity    0.500000    0.477612\n",
      "8           Specificity    0.977800    0.984100\n",
      "9              F1 score    0.622642    0.615385\n",
      "10  F1 score (weighted)    0.884515    0.882565\n",
      "11     F1 score (macro)    0.780926    0.777389\n",
      "12    Balanced Accuracy    0.738924    0.730869\n",
      "13                  MCC    0.590014    0.593716\n",
      "14                  NPV    0.903500    0.898600\n",
      "15              ROC_AUC    0.738924    0.730869\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_svm_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet1_cat = np.where(((Y_testSet1>=2) | (Y_testSet1<=-2)), 1, 0) \n",
    "y_pred_svm_1_cat = np.where(((y_pred_svm_1 >= 2) | (y_pred_svm_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_svm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 18:01:09,334] Trial 100 finished with value: 0.07390165603529497 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 39 with value: 0.7083467362631212.\n",
      "[I 2023-12-20 18:01:12,013] Trial 101 finished with value: 0.7085548695296222 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:14,719] Trial 102 finished with value: 0.7085548695296222 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:17,401] Trial 103 finished with value: 0.7085548695296222 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:20,107] Trial 104 finished with value: 0.7085548695296222 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:22,899] Trial 105 finished with value: 0.025591198933192318 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:25,624] Trial 106 finished with value: -0.007999745208031562 and parameters: {'C': 0.015625, 'gamma': 2.0}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:28,298] Trial 107 finished with value: 0.7085548695296222 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:30,836] Trial 108 finished with value: 0.5524794996563386 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:33,630] Trial 109 finished with value: 0.28066454210868474 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:36,265] Trial 110 finished with value: 0.3298123782655483 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:38,942] Trial 111 finished with value: 0.7085548695296222 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:41,654] Trial 112 finished with value: 0.7085548695296222 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:44,326] Trial 113 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:47,050] Trial 114 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:49,726] Trial 115 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:52,429] Trial 116 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:55,105] Trial 117 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:01:57,981] Trial 118 finished with value: 0.01927707420488951 and parameters: {'C': 64.0, 'gamma': 4.0}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:00,545] Trial 119 finished with value: 0.6219085214938455 and parameters: {'C': 64.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:03,248] Trial 120 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:05,922] Trial 121 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:08,625] Trial 122 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:11,302] Trial 123 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:14,004] Trial 124 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:16,680] Trial 125 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:19,383] Trial 126 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:21,898] Trial 127 finished with value: 0.5897021303768173 and parameters: {'C': 64.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:24,808] Trial 128 finished with value: 0.6134753356454032 and parameters: {'C': 64.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:28,027] Trial 129 finished with value: 0.6228921082788041 and parameters: {'C': 64.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:31,204] Trial 130 finished with value: 0.6659030897038654 and parameters: {'C': 64.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:33,878] Trial 131 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:36,584] Trial 132 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:39,263] Trial 133 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:41,967] Trial 134 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:44,831] Trial 135 finished with value: 0.6928204539451261 and parameters: {'C': 64.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:48,101] Trial 136 finished with value: 0.019106231682824028 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:50,741] Trial 137 finished with value: 0.07284076882238553 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:53,304] Trial 138 finished with value: 0.5772876724479843 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:55,896] Trial 139 finished with value: 0.5453938441367794 and parameters: {'C': 1.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:02:58,531] Trial 140 finished with value: 0.025272048442998584 and parameters: {'C': 0.5, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:03:01,208] Trial 141 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:03:03,910] Trial 142 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:03:06,587] Trial 143 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:03:09,153] Trial 144 finished with value: 0.7049946158449882 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:03:11,829] Trial 145 finished with value: 0.7085378182851589 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:03:14,455] Trial 146 finished with value: 0.055072961406997176 and parameters: {'C': 0.125, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:03:17,108] Trial 147 finished with value: 0.22235839330072577 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:03:19,882] Trial 148 finished with value: 0.07390165603529489 and parameters: {'C': 64.0, 'gamma': 0.25}. Best is trial 101 with value: 0.7085548695296222.\n",
      "[I 2023-12-20 18:03:22,616] Trial 149 finished with value: 0.7105296178065524 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 149 with value: 0.7105296178065524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7105\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.709863    0.750816    0.745380\n",
      "1                    TP   33.000000   32.000000   31.000000\n",
      "2                    TN  309.000000  310.000000  308.000000\n",
      "3                    FP    7.000000    5.000000    6.000000\n",
      "4                    FN   33.000000   35.000000   37.000000\n",
      "5              Accuracy    0.895288    0.895288    0.887435\n",
      "6             Precision    0.825000    0.864865    0.837838\n",
      "7           Sensitivity    0.500000    0.477612    0.455882\n",
      "8           Specificity    0.977800    0.984100    0.980900\n",
      "9              F1 score    0.622642    0.615385    0.590476\n",
      "10  F1 score (weighted)    0.884515    0.882565    0.873465\n",
      "11     F1 score (macro)    0.780926    0.777389    0.762613\n",
      "12    Balanced Accuracy    0.738924    0.730869    0.718387\n",
      "13                  MCC    0.590014    0.593716    0.564892\n",
      "14                  NPV    0.903500    0.898600    0.892800\n",
      "15              ROC_AUC    0.738924    0.730869    0.718387\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_svm_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet2_cat = np.where(((Y_testSet2>=2) | (Y_testSet2<=-2)), 1, 0) \n",
    "y_pred_svm_2_cat = np.where(((y_pred_svm_2 >= 2) | (y_pred_svm_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_svm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 18:03:25,704] Trial 150 finished with value: 0.03171475913068539 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 149 with value: 0.7105296178065524.\n",
      "[I 2023-12-20 18:03:28,217] Trial 151 finished with value: 0.7154202798435894 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.7154202798435894.\n",
      "[I 2023-12-20 18:03:30,876] Trial 152 finished with value: 0.04284142021240957 and parameters: {'C': 128.0, 'gamma': 0.5}. Best is trial 151 with value: 0.7154202798435894.\n",
      "[I 2023-12-20 18:03:33,432] Trial 153 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:03:35,986] Trial 154 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:03:38,608] Trial 155 finished with value: 0.02614136658325352 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:03:41,166] Trial 156 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:03:43,739] Trial 157 finished with value: 0.29139255569338346 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:03:46,293] Trial 158 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:03:48,846] Trial 159 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:03:51,251] Trial 160 finished with value: 0.5354990925644983 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:03:53,806] Trial 161 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:03:56,361] Trial 162 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:03:58,917] Trial 163 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:01,473] Trial 164 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:04,026] Trial 165 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:06,675] Trial 166 finished with value: 0.02413146352749086 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:09,231] Trial 167 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:11,786] Trial 168 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:14,338] Trial 169 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:16,893] Trial 170 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:19,446] Trial 171 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:22,001] Trial 172 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:24,555] Trial 173 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:27,111] Trial 174 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:29,666] Trial 175 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:32,053] Trial 176 finished with value: 0.4707023308219725 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:34,607] Trial 177 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:37,129] Trial 178 finished with value: 0.6349897063130677 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:39,561] Trial 179 finished with value: 0.6202333289270502 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:42,115] Trial 180 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:44,669] Trial 181 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:47,226] Trial 182 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:49,801] Trial 183 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:52,354] Trial 184 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:55,160] Trial 185 finished with value: 0.6783622786330622 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:04:57,718] Trial 186 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:00,751] Trial 187 finished with value: 0.02383315415751005 and parameters: {'C': 16.0, 'gamma': 8.0}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:03,305] Trial 188 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:05,861] Trial 189 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:08,313] Trial 190 finished with value: 0.5857209980384953 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:10,870] Trial 191 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:13,423] Trial 192 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:16,121] Trial 193 finished with value: 0.7070285430635564 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:18,677] Trial 194 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:21,384] Trial 195 finished with value: 0.6541003412293117 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:23,939] Trial 196 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:26,356] Trial 197 finished with value: 0.3808839785174282 and parameters: {'C': 16.0, 'gamma': 3.0517578125e-05}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:28,912] Trial 198 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:31,465] Trial 199 finished with value: 0.7183314166899264 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7183\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.709863    0.750816    0.745380    0.730434\n",
      "1                    TP   33.000000   32.000000   31.000000   29.000000\n",
      "2                    TN  309.000000  310.000000  308.000000  308.000000\n",
      "3                    FP    7.000000    5.000000    6.000000    8.000000\n",
      "4                    FN   33.000000   35.000000   37.000000   37.000000\n",
      "5              Accuracy    0.895288    0.895288    0.887435    0.882199\n",
      "6             Precision    0.825000    0.864865    0.837838    0.783784\n",
      "7           Sensitivity    0.500000    0.477612    0.455882    0.439394\n",
      "8           Specificity    0.977800    0.984100    0.980900    0.974700\n",
      "9              F1 score    0.622642    0.615385    0.590476    0.563107\n",
      "10  F1 score (weighted)    0.884515    0.882565    0.873465    0.868199\n",
      "11     F1 score (macro)    0.780926    0.777389    0.762613    0.747514\n",
      "12    Balanced Accuracy    0.738924    0.730869    0.718387    0.707039\n",
      "13                  MCC    0.590014    0.593716    0.564892    0.529282\n",
      "14                  NPV    0.903500    0.898600    0.892800    0.892800\n",
      "15              ROC_AUC    0.738924    0.730869    0.718387    0.707039\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_svm_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet3_cat = np.where(((Y_testSet3>=2) | (Y_testSet3<=-2)), 1, 0) \n",
    "y_pred_svm_3_cat = np.where(((y_pred_svm_3 >= 2) | (y_pred_svm_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_svm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 18:05:34,567] Trial 200 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:37,210] Trial 201 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:39,849] Trial 202 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:42,491] Trial 203 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:45,129] Trial 204 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:47,605] Trial 205 finished with value: 0.5876606351217999 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:50,243] Trial 206 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:52,889] Trial 207 finished with value: 0.6808862102025616 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:55,734] Trial 208 finished with value: 0.04150450582778013 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:05:58,578] Trial 209 finished with value: 0.08136299307305521 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:01,421] Trial 210 finished with value: 0.026545406540028328 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:04,064] Trial 211 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:06,709] Trial 212 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:09,352] Trial 213 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:12,242] Trial 214 finished with value: 0.031005595641552884 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:14,882] Trial 215 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:17,526] Trial 216 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:20,166] Trial 217 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:22,821] Trial 218 finished with value: 0.3119353911923811 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:25,481] Trial 219 finished with value: 0.0826460694206041 and parameters: {'C': 0.25, 'gamma': 0.125}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:28,124] Trial 220 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:30,764] Trial 221 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:33,405] Trial 222 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:36,015] Trial 223 finished with value: 0.1297471391621184 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:38,658] Trial 224 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:41,211] Trial 225 finished with value: 0.5381355917651846 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:43,852] Trial 226 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:46,493] Trial 227 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:49,314] Trial 228 finished with value: 0.025095038562476836 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:51,958] Trial 229 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:54,602] Trial 230 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:57,249] Trial 231 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:06:59,893] Trial 232 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:02,578] Trial 233 finished with value: 0.07174435558069707 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:05,220] Trial 234 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:07,867] Trial 235 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:10,408] Trial 236 finished with value: 0.46971974204427197 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:12,910] Trial 237 finished with value: 0.5378716246798682 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:15,634] Trial 238 finished with value: 0.6292984910269197 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:18,283] Trial 239 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:20,841] Trial 240 finished with value: 0.6721950001976874 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:23,481] Trial 241 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:26,119] Trial 242 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:28,645] Trial 243 finished with value: 0.6198387715059684 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:31,620] Trial 244 finished with value: 0.6559301725945222 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:34,169] Trial 245 finished with value: 0.6948727062430979 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:36,815] Trial 246 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:40,025] Trial 247 finished with value: 0.024883252326743554 and parameters: {'C': 16.0, 'gamma': 8.0}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:42,550] Trial 248 finished with value: 0.40230887575038476 and parameters: {'C': 0.125, 'gamma': 0.0078125}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:45,191] Trial 249 finished with value: 0.6988577116974718 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 153 with value: 0.7183314166899264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7183\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.709863    0.750816    0.745380    0.730434   \n",
      "1                    TP   33.000000   32.000000   31.000000   29.000000   \n",
      "2                    TN  309.000000  310.000000  308.000000  308.000000   \n",
      "3                    FP    7.000000    5.000000    6.000000    8.000000   \n",
      "4                    FN   33.000000   35.000000   37.000000   37.000000   \n",
      "5              Accuracy    0.895288    0.895288    0.887435    0.882199   \n",
      "6             Precision    0.825000    0.864865    0.837838    0.783784   \n",
      "7           Sensitivity    0.500000    0.477612    0.455882    0.439394   \n",
      "8           Specificity    0.977800    0.984100    0.980900    0.974700   \n",
      "9              F1 score    0.622642    0.615385    0.590476    0.563107   \n",
      "10  F1 score (weighted)    0.884515    0.882565    0.873465    0.868199   \n",
      "11     F1 score (macro)    0.780926    0.777389    0.762613    0.747514   \n",
      "12    Balanced Accuracy    0.738924    0.730869    0.718387    0.707039   \n",
      "13                  MCC    0.590014    0.593716    0.564892    0.529282   \n",
      "14                  NPV    0.903500    0.898600    0.892800    0.892800   \n",
      "15              ROC_AUC    0.738924    0.730869    0.718387    0.707039   \n",
      "\n",
      "          Set4  \n",
      "0     0.799827  \n",
      "1    37.000000  \n",
      "2   313.000000  \n",
      "3     3.000000  \n",
      "4    29.000000  \n",
      "5     0.916230  \n",
      "6     0.925000  \n",
      "7     0.560606  \n",
      "8     0.990500  \n",
      "9     0.698113  \n",
      "10    0.907612  \n",
      "11    0.824740  \n",
      "12    0.775556  \n",
      "13    0.680476  \n",
      "14    0.915200  \n",
      "15    0.775556  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_svm_4)\n",
    "# now convert the resuls to binary with cutoff 6.4\n",
    "Y_testSet4_cat = np.where(((Y_testSet4>=2) | (Y_testSet4<=-2)), 1, 0) \n",
    "y_pred_svm_4_cat = np.where(((y_pred_svm_4 >= 2) | (y_pred_svm_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_svm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 18:07:48,079] Trial 250 finished with value: 0.5766214141085217 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 153 with value: 0.7183314166899264.\n",
      "[I 2023-12-20 18:07:50,517] Trial 251 finished with value: 0.7197732873701657 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:07:52,958] Trial 252 finished with value: 0.7197732873701657 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:07:55,462] Trial 253 finished with value: 0.10199732515107277 and parameters: {'C': 0.03125, 'gamma': 0.001953125}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:07:57,905] Trial 254 finished with value: 0.38087134211045603 and parameters: {'C': 16.0, 'gamma': 3.0517578125e-05}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:00,344] Trial 255 finished with value: 0.7197732873701657 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:02,788] Trial 256 finished with value: 0.7197732873701657 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:05,227] Trial 257 finished with value: 0.7197732873701657 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:07,619] Trial 258 finished with value: 0.5929287078657379 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:10,063] Trial 259 finished with value: 0.7197732873701657 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:12,501] Trial 260 finished with value: 0.7197732873701657 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:14,915] Trial 261 finished with value: 0.26189566126819414 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:17,362] Trial 262 finished with value: 0.7197732873701657 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:20,011] Trial 263 finished with value: 0.04692058165996079 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:22,674] Trial 264 finished with value: 0.08575428810918942 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:25,089] Trial 265 finished with value: 0.5406550715301892 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:27,532] Trial 266 finished with value: 0.7197732873701657 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:30,234] Trial 267 finished with value: -0.004236024583856424 and parameters: {'C': 0.015625, 'gamma': 1.0}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:32,679] Trial 268 finished with value: 0.7197732873701657 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:35,118] Trial 269 finished with value: 0.7197732873701657 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:37,566] Trial 270 finished with value: 0.7197732873701657 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.7197732873701657.\n",
      "[I 2023-12-20 18:08:40,011] Trial 271 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:08:42,456] Trial 272 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:08:45,056] Trial 273 finished with value: 0.28560488585310645 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:08:47,484] Trial 274 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:08:49,936] Trial 275 finished with value: 0.46944231035116885 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:08:52,618] Trial 276 finished with value: 0.027822418474856125 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:08:55,059] Trial 277 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:08:57,506] Trial 278 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:08:59,957] Trial 279 finished with value: 0.71870512929459 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:02,401] Trial 280 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:04,845] Trial 281 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:07,293] Trial 282 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:09,996] Trial 283 finished with value: 0.02505971652079183 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:12,424] Trial 284 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:14,848] Trial 285 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:17,258] Trial 286 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:19,630] Trial 287 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:22,012] Trial 288 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:24,326] Trial 289 finished with value: 0.637759220364557 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:26,699] Trial 290 finished with value: 0.3805599357603101 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:29,086] Trial 291 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:31,459] Trial 292 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:33,842] Trial 293 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:36,228] Trial 294 finished with value: 0.594054999083359 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:38,639] Trial 295 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:41,042] Trial 296 finished with value: 0.7208909422150429 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:43,459] Trial 297 finished with value: 0.5767173729321453 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:46,023] Trial 298 finished with value: 0.6925579935637733 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:48,577] Trial 299 finished with value: 0.7126038679811201 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 271 with value: 0.7208909422150429.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7209\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.709863    0.750816    0.745380    0.730434   \n",
      "1                    TP   33.000000   32.000000   31.000000   29.000000   \n",
      "2                    TN  309.000000  310.000000  308.000000  308.000000   \n",
      "3                    FP    7.000000    5.000000    6.000000    8.000000   \n",
      "4                    FN   33.000000   35.000000   37.000000   37.000000   \n",
      "5              Accuracy    0.895288    0.895288    0.887435    0.882199   \n",
      "6             Precision    0.825000    0.864865    0.837838    0.783784   \n",
      "7           Sensitivity    0.500000    0.477612    0.455882    0.439394   \n",
      "8           Specificity    0.977800    0.984100    0.980900    0.974700   \n",
      "9              F1 score    0.622642    0.615385    0.590476    0.563107   \n",
      "10  F1 score (weighted)    0.884515    0.882565    0.873465    0.868199   \n",
      "11     F1 score (macro)    0.780926    0.777389    0.762613    0.747514   \n",
      "12    Balanced Accuracy    0.738924    0.730869    0.718387    0.707039   \n",
      "13                  MCC    0.590014    0.593716    0.564892    0.529282   \n",
      "14                  NPV    0.903500    0.898600    0.892800    0.892800   \n",
      "15              ROC_AUC    0.738924    0.730869    0.718387    0.707039   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.799827    0.742039  \n",
      "1    37.000000   34.000000  \n",
      "2   313.000000  303.000000  \n",
      "3     3.000000   12.000000  \n",
      "4    29.000000   33.000000  \n",
      "5     0.916230    0.882199  \n",
      "6     0.925000    0.739130  \n",
      "7     0.560606    0.507463  \n",
      "8     0.990500    0.961900  \n",
      "9     0.698113    0.601770  \n",
      "10    0.907612    0.873153  \n",
      "11    0.824740    0.766323  \n",
      "12    0.775556    0.734684  \n",
      "13    0.680476    0.548475  \n",
      "14    0.915200    0.901800  \n",
      "15    0.775556    0.734684  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_svm_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "Y_testSet5_cat = np.where(((Y_testSet5>=2) | (Y_testSet5<=-2)), 1, 0) \n",
    "y_pred_svm_5_cat = np.where(((y_pred_svm_5 >= 2) | (y_pred_svm_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_svm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 18:09:51,598] Trial 300 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:54,817] Trial 301 finished with value: 0.016778433972125095 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:57,396] Trial 302 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:09:59,979] Trial 303 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:02,596] Trial 304 finished with value: 0.6397757475976467 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:05,110] Trial 305 finished with value: 0.27693336195856 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:07,688] Trial 306 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:10,266] Trial 307 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:12,846] Trial 308 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:15,378] Trial 309 finished with value: 0.5282463424701367 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:17,958] Trial 310 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:20,750] Trial 311 finished with value: 0.036106879742205736 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:23,504] Trial 312 finished with value: 0.06982938476644208 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:26,088] Trial 313 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:28,571] Trial 314 finished with value: 0.6904641600822891 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:31,146] Trial 315 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:33,903] Trial 316 finished with value: 0.01936268903117293 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:36,477] Trial 317 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:39,244] Trial 318 finished with value: 0.02543467784463116 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:41,830] Trial 319 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:44,401] Trial 320 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:47,092] Trial 321 finished with value: 0.271896279357722 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:49,673] Trial 322 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:52,256] Trial 323 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:54,844] Trial 324 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:10:57,439] Trial 325 finished with value: 0.4677884953957376 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:00,319] Trial 326 finished with value: 0.017108325542209903 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:02,954] Trial 327 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:05,587] Trial 328 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:08,226] Trial 329 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:10,861] Trial 330 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:13,496] Trial 331 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:16,128] Trial 332 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:18,768] Trial 333 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:21,490] Trial 334 finished with value: 0.06677264897033218 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:24,118] Trial 335 finished with value: 0.102875770037128 and parameters: {'C': 1.0, 'gamma': 6.103515625e-05}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:26,685] Trial 336 finished with value: 0.6114565091259558 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:29,334] Trial 337 finished with value: 0.2719264122736556 and parameters: {'C': 0.5, 'gamma': 0.00048828125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:31,937] Trial 338 finished with value: 0.7154625590558188 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:34,649] Trial 339 finished with value: 0.6730969854526891 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:37,235] Trial 340 finished with value: 0.4329091416134876 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:40,505] Trial 341 finished with value: 0.019213568441752392 and parameters: {'C': 2.0, 'gamma': 8.0}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:43,148] Trial 342 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:45,752] Trial 343 finished with value: 0.19549305002513376 and parameters: {'C': 0.03125, 'gamma': 0.0078125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:48,392] Trial 344 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:50,992] Trial 345 finished with value: 0.5673129801362029 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:53,672] Trial 346 finished with value: 0.6397757475976467 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:56,301] Trial 347 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:11:58,877] Trial 348 finished with value: 0.27693336195856 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:01,508] Trial 349 finished with value: 0.7180085166206758 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7209\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.709863    0.750816    0.745380    0.730434   \n",
      "1                    TP   33.000000   32.000000   31.000000   29.000000   \n",
      "2                    TN  309.000000  310.000000  308.000000  308.000000   \n",
      "3                    FP    7.000000    5.000000    6.000000    8.000000   \n",
      "4                    FN   33.000000   35.000000   37.000000   37.000000   \n",
      "5              Accuracy    0.895288    0.895288    0.887435    0.882199   \n",
      "6             Precision    0.825000    0.864865    0.837838    0.783784   \n",
      "7           Sensitivity    0.500000    0.477612    0.455882    0.439394   \n",
      "8           Specificity    0.977800    0.984100    0.980900    0.974700   \n",
      "9              F1 score    0.622642    0.615385    0.590476    0.563107   \n",
      "10  F1 score (weighted)    0.884515    0.882565    0.873465    0.868199   \n",
      "11     F1 score (macro)    0.780926    0.777389    0.762613    0.747514   \n",
      "12    Balanced Accuracy    0.738924    0.730869    0.718387    0.707039   \n",
      "13                  MCC    0.590014    0.593716    0.564892    0.529282   \n",
      "14                  NPV    0.903500    0.898600    0.892800    0.892800   \n",
      "15              ROC_AUC    0.738924    0.730869    0.718387    0.707039   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.799827    0.742039    0.758361  \n",
      "1    37.000000   34.000000   31.000000  \n",
      "2   313.000000  303.000000  307.000000  \n",
      "3     3.000000   12.000000    9.000000  \n",
      "4    29.000000   33.000000   35.000000  \n",
      "5     0.916230    0.882199    0.884817  \n",
      "6     0.925000    0.739130    0.775000  \n",
      "7     0.560606    0.507463    0.469697  \n",
      "8     0.990500    0.961900    0.971500  \n",
      "9     0.698113    0.601770    0.584906  \n",
      "10    0.907612    0.873153    0.872966  \n",
      "11    0.824740    0.766323    0.759018  \n",
      "12    0.775556    0.734684    0.720608  \n",
      "13    0.680476    0.548475    0.544783  \n",
      "14    0.915200    0.901800    0.897700  \n",
      "15    0.775556    0.734684    0.720608  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_svm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "Y_testSet6_cat = np.where(((Y_testSet6>=2) | (Y_testSet6<=-2)), 1, 0) \n",
    "y_pred_svm_6_cat = np.where(((y_pred_svm_6 >= 2) | (y_pred_svm_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_svm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 18:12:04,452] Trial 350 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:06,977] Trial 351 finished with value: 0.11349975145186371 and parameters: {'C': 0.25, 'gamma': 0.000244140625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:09,407] Trial 352 finished with value: 0.26949897009730034 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:11,897] Trial 353 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:14,383] Trial 354 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:17,011] Trial 355 finished with value: 0.04295822603218952 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:19,608] Trial 356 finished with value: -0.0034273146855829605 and parameters: {'C': 0.015625, 'gamma': 0.25}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:22,103] Trial 357 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:24,579] Trial 358 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:27,062] Trial 359 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:29,733] Trial 360 finished with value: 0.02747189281537362 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:32,407] Trial 361 finished with value: 0.03288633807160753 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:34,874] Trial 362 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:37,441] Trial 363 finished with value: -0.002465990740262336 and parameters: {'C': 0.0078125, 'gamma': 0.125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:39,774] Trial 364 finished with value: 0.666247537794105 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:42,278] Trial 365 finished with value: 0.6929796138689897 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:44,747] Trial 366 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:47,142] Trial 367 finished with value: 0.46658735538843754 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:49,561] Trial 368 finished with value: 0.6157330578455645 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:52,037] Trial 369 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:54,646] Trial 370 finished with value: -0.0035674703556252017 and parameters: {'C': 0.03125, 'gamma': 4.0}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:57,092] Trial 371 finished with value: 0.6903303346353714 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:12:59,570] Trial 372 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:02,046] Trial 373 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:04,487] Trial 374 finished with value: 0.44137820089861685 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:06,965] Trial 375 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:09,465] Trial 376 finished with value: 0.38888305356714853 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:11,938] Trial 377 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:14,413] Trial 378 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:16,794] Trial 379 finished with value: 0.5654294999319001 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:19,268] Trial 380 finished with value: 0.2797664515415374 and parameters: {'C': 0.25, 'gamma': 0.0009765625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:21,685] Trial 381 finished with value: 0.24801747751606648 and parameters: {'C': 0.0625, 'gamma': 0.00390625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:24,154] Trial 382 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:26,618] Trial 383 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:29,093] Trial 384 finished with value: 0.1408517924365782 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:32,180] Trial 385 finished with value: 0.02470741787831987 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:34,781] Trial 386 finished with value: 0.6764049261748635 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:37,262] Trial 387 finished with value: 0.5696050595037476 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:39,732] Trial 388 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:43,350] Trial 389 finished with value: 0.5783902728282323 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:45,827] Trial 390 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:48,376] Trial 391 finished with value: -0.005226636880176027 and parameters: {'C': 0.0078125, 'gamma': 3.0517578125e-05}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:50,853] Trial 392 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:53,356] Trial 393 finished with value: 0.6929796138689897 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:55,829] Trial 394 finished with value: 0.691964644059552 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:13:58,247] Trial 395 finished with value: 0.6157330578455645 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:14:00,600] Trial 396 finished with value: 0.666247537794105 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:14:03,205] Trial 397 finished with value: 0.04295822603218952 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:14:05,866] Trial 398 finished with value: 0.3871395178728071 and parameters: {'C': 2.0, 'gamma': 0.000244140625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:14:08,353] Trial 399 finished with value: 0.6775063786391962 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 271 with value: 0.7208909422150429.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7209\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.709863    0.750816    0.745380    0.730434   \n",
      "1                    TP   33.000000   32.000000   31.000000   29.000000   \n",
      "2                    TN  309.000000  310.000000  308.000000  308.000000   \n",
      "3                    FP    7.000000    5.000000    6.000000    8.000000   \n",
      "4                    FN   33.000000   35.000000   37.000000   37.000000   \n",
      "5              Accuracy    0.895288    0.895288    0.887435    0.882199   \n",
      "6             Precision    0.825000    0.864865    0.837838    0.783784   \n",
      "7           Sensitivity    0.500000    0.477612    0.455882    0.439394   \n",
      "8           Specificity    0.977800    0.984100    0.980900    0.974700   \n",
      "9              F1 score    0.622642    0.615385    0.590476    0.563107   \n",
      "10  F1 score (weighted)    0.884515    0.882565    0.873465    0.868199   \n",
      "11     F1 score (macro)    0.780926    0.777389    0.762613    0.747514   \n",
      "12    Balanced Accuracy    0.738924    0.730869    0.718387    0.707039   \n",
      "13                  MCC    0.590014    0.593716    0.564892    0.529282   \n",
      "14                  NPV    0.903500    0.898600    0.892800    0.892800   \n",
      "15              ROC_AUC    0.738924    0.730869    0.718387    0.707039   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.799827    0.742039    0.758361    0.792121  \n",
      "1    37.000000   34.000000   31.000000   35.000000  \n",
      "2   313.000000  303.000000  307.000000  310.000000  \n",
      "3     3.000000   12.000000    9.000000    5.000000  \n",
      "4    29.000000   33.000000   35.000000   32.000000  \n",
      "5     0.916230    0.882199    0.884817    0.903141  \n",
      "6     0.925000    0.739130    0.775000    0.875000  \n",
      "7     0.560606    0.507463    0.469697    0.522388  \n",
      "8     0.990500    0.961900    0.971500    0.984100  \n",
      "9     0.698113    0.601770    0.584906    0.654206  \n",
      "10    0.907612    0.873153    0.872966    0.892911  \n",
      "11    0.824740    0.766323    0.759018    0.798945  \n",
      "12    0.775556    0.734684    0.720608    0.753258  \n",
      "13    0.680476    0.548475    0.544783    0.629132  \n",
      "14    0.915200    0.901800    0.897700    0.906400  \n",
      "15    0.775556    0.734684    0.720608    0.753258  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_svm_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "Y_testSet7_cat = np.where(((Y_testSet7>=2) | (Y_testSet7<=-2)), 1, 0) \n",
    "y_pred_svm_7_cat = np.where(((y_pred_svm_7 >= 2) | (y_pred_svm_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_svm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 18:14:11,416] Trial 400 finished with value: 0.4461812526083535 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 271 with value: 0.7208909422150429.\n",
      "[I 2023-12-20 18:14:14,006] Trial 401 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:16,734] Trial 402 finished with value: 0.09198727957475542 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:19,536] Trial 403 finished with value: 0.03988025160610159 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:22,130] Trial 404 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:24,849] Trial 405 finished with value: 0.03320750247344263 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:27,444] Trial 406 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:30,136] Trial 407 finished with value: 0.3004707595981871 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:32,725] Trial 408 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:35,313] Trial 409 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:37,903] Trial 410 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:40,492] Trial 411 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:43,097] Trial 412 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:45,634] Trial 413 finished with value: 0.48021172816729607 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:48,228] Trial 414 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:50,819] Trial 415 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:53,571] Trial 416 finished with value: 0.03054778694004945 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:56,162] Trial 417 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:14:58,760] Trial 418 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:01,360] Trial 419 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:03,948] Trial 420 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:06,541] Trial 421 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:09,134] Trial 422 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:11,727] Trial 423 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:14,374] Trial 424 finished with value: 0.390864159902769 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:16,969] Trial 425 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:19,491] Trial 426 finished with value: 0.632683753523409 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:21,950] Trial 427 finished with value: 0.5969909421768451 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:24,544] Trial 428 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:27,139] Trial 429 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:29,731] Trial 430 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:32,467] Trial 431 finished with value: 0.6965430541904223 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:35,057] Trial 432 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:37,651] Trial 433 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:40,836] Trial 434 finished with value: 0.03016026701812634 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:43,429] Trial 435 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:46,021] Trial 436 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:48,734] Trial 437 finished with value: 0.7154461400213331 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:51,253] Trial 438 finished with value: 0.5988430870037663 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:53,816] Trial 439 finished with value: 0.22254334894507463 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:56,408] Trial 440 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:15:59,000] Trial 441 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:01,589] Trial 442 finished with value: 0.6621108509203457 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:04,177] Trial 443 finished with value: 0.28516910491463027 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:06,768] Trial 444 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:09,363] Trial 445 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:11,951] Trial 446 finished with value: 0.7271381912270376 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:14,479] Trial 447 finished with value: 0.26801080774932784 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:17,223] Trial 448 finished with value: 0.05220137900884132 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:19,766] Trial 449 finished with value: 0.13852102824163331 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7271\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.709863    0.750816    0.745380    0.730434   \n",
      "1                    TP   33.000000   32.000000   31.000000   29.000000   \n",
      "2                    TN  309.000000  310.000000  308.000000  308.000000   \n",
      "3                    FP    7.000000    5.000000    6.000000    8.000000   \n",
      "4                    FN   33.000000   35.000000   37.000000   37.000000   \n",
      "5              Accuracy    0.895288    0.895288    0.887435    0.882199   \n",
      "6             Precision    0.825000    0.864865    0.837838    0.783784   \n",
      "7           Sensitivity    0.500000    0.477612    0.455882    0.439394   \n",
      "8           Specificity    0.977800    0.984100    0.980900    0.974700   \n",
      "9              F1 score    0.622642    0.615385    0.590476    0.563107   \n",
      "10  F1 score (weighted)    0.884515    0.882565    0.873465    0.868199   \n",
      "11     F1 score (macro)    0.780926    0.777389    0.762613    0.747514   \n",
      "12    Balanced Accuracy    0.738924    0.730869    0.718387    0.707039   \n",
      "13                  MCC    0.590014    0.593716    0.564892    0.529282   \n",
      "14                  NPV    0.903500    0.898600    0.892800    0.892800   \n",
      "15              ROC_AUC    0.738924    0.730869    0.718387    0.707039   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.799827    0.742039    0.758361    0.792121    0.691504  \n",
      "1    37.000000   34.000000   31.000000   35.000000   33.000000  \n",
      "2   313.000000  303.000000  307.000000  310.000000  307.000000  \n",
      "3     3.000000   12.000000    9.000000    5.000000    7.000000  \n",
      "4    29.000000   33.000000   35.000000   32.000000   35.000000  \n",
      "5     0.916230    0.882199    0.884817    0.903141    0.890052  \n",
      "6     0.925000    0.739130    0.775000    0.875000    0.825000  \n",
      "7     0.560606    0.507463    0.469697    0.522388    0.485294  \n",
      "8     0.990500    0.961900    0.971500    0.984100    0.977700  \n",
      "9     0.698113    0.601770    0.584906    0.654206    0.611111  \n",
      "10    0.907612    0.873153    0.872966    0.892911    0.878146  \n",
      "11    0.824740    0.766323    0.759018    0.798945    0.773543  \n",
      "12    0.775556    0.734684    0.720608    0.753258    0.731501  \n",
      "13    0.680476    0.548475    0.544783    0.629132    0.578440  \n",
      "14    0.915200    0.901800    0.897700    0.906400    0.897700  \n",
      "15    0.775556    0.734684    0.720608    0.753258    0.731501  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_svm_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "Y_testSet8_cat = np.where(((Y_testSet8>=2) | (Y_testSet8<=-2)), 1, 0) \n",
    "y_pred_svm_8_cat = np.where(((y_pred_svm_8 >= 2) | (y_pred_svm_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_svm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 18:16:22,695] Trial 450 finished with value: 0.11506752061759914 and parameters: {'C': 0.25, 'gamma': 0.000244140625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:25,352] Trial 451 finished with value: 0.09177846277316039 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:27,830] Trial 452 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:30,333] Trial 453 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:33,057] Trial 454 finished with value: 0.0402199483924226 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:35,522] Trial 455 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:38,151] Trial 456 finished with value: 0.032673319613496364 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:40,625] Trial 457 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:43,191] Trial 458 finished with value: 0.29485522801477404 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:45,697] Trial 459 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:48,164] Trial 460 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:50,606] Trial 461 finished with value: 0.07999122257340081 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:53,085] Trial 462 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:55,693] Trial 463 finished with value: 0.026853436501574214 and parameters: {'C': 1.0, 'gamma': 4.0}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:16:58,102] Trial 464 finished with value: 0.7157939434583925 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:00,577] Trial 465 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:02,910] Trial 466 finished with value: 0.4861555203554696 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:05,381] Trial 467 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:07,756] Trial 468 finished with value: 0.4529382903753433 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:10,240] Trial 469 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:12,650] Trial 470 finished with value: 0.6382028487575061 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:14,944] Trial 471 finished with value: 0.5520276568119314 and parameters: {'C': 2.0, 'gamma': 0.0009765625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:17,445] Trial 472 finished with value: -0.0008343951933660665 and parameters: {'C': 0.03125, 'gamma': 6.103515625e-05}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:19,927] Trial 473 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:22,258] Trial 474 finished with value: 0.5971462761841788 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:24,736] Trial 475 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:27,197] Trial 476 finished with value: 0.6798537155729476 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:29,685] Trial 477 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:32,176] Trial 478 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:34,660] Trial 479 finished with value: 0.70351920105168 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:37,743] Trial 480 finished with value: 0.029416245889736348 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:40,172] Trial 481 finished with value: 0.5805851792673293 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:42,670] Trial 482 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:45,095] Trial 483 finished with value: 0.5610508729010311 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:47,569] Trial 484 finished with value: 0.1762807387816613 and parameters: {'C': 0.0625, 'gamma': 0.001953125}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:50,049] Trial 485 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:52,534] Trial 486 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:55,102] Trial 487 finished with value: -0.004439178381452269 and parameters: {'C': 0.015625, 'gamma': 3.0517578125e-05}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:57,581] Trial 488 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:17:59,872] Trial 489 finished with value: 0.5518426684183202 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:18:02,344] Trial 490 finished with value: 0.7117842535573105 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:18:04,716] Trial 491 finished with value: 0.6953793668247162 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:18:07,335] Trial 492 finished with value: 0.05281460750171322 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:18:09,828] Trial 493 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:18:12,485] Trial 494 finished with value: 0.09177846277316039 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:18:14,977] Trial 495 finished with value: 0.7169619073779826 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:18:17,446] Trial 496 finished with value: 0.07999122257340081 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:18:20,072] Trial 497 finished with value: 0.032673319613496364 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:18:22,391] Trial 498 finished with value: 0.687740113408065 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n",
      "[I 2023-12-20 18:18:24,808] Trial 499 finished with value: 0.7157939434583925 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.7271381912270376.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7271\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.709863    0.750816    0.745380    0.730434   \n",
      "1                    TP   33.000000   32.000000   31.000000   29.000000   \n",
      "2                    TN  309.000000  310.000000  308.000000  308.000000   \n",
      "3                    FP    7.000000    5.000000    6.000000    8.000000   \n",
      "4                    FN   33.000000   35.000000   37.000000   37.000000   \n",
      "5              Accuracy    0.895288    0.895288    0.887435    0.882199   \n",
      "6             Precision    0.825000    0.864865    0.837838    0.783784   \n",
      "7           Sensitivity    0.500000    0.477612    0.455882    0.439394   \n",
      "8           Specificity    0.977800    0.984100    0.980900    0.974700   \n",
      "9              F1 score    0.622642    0.615385    0.590476    0.563107   \n",
      "10  F1 score (weighted)    0.884515    0.882565    0.873465    0.868199   \n",
      "11     F1 score (macro)    0.780926    0.777389    0.762613    0.747514   \n",
      "12    Balanced Accuracy    0.738924    0.730869    0.718387    0.707039   \n",
      "13                  MCC    0.590014    0.593716    0.564892    0.529282   \n",
      "14                  NPV    0.903500    0.898600    0.892800    0.892800   \n",
      "15              ROC_AUC    0.738924    0.730869    0.718387    0.707039   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.799827    0.742039    0.758361    0.792121    0.691504    0.752298  \n",
      "1    37.000000   34.000000   31.000000   35.000000   33.000000   36.000000  \n",
      "2   313.000000  303.000000  307.000000  310.000000  307.000000  311.000000  \n",
      "3     3.000000   12.000000    9.000000    5.000000    7.000000    2.000000  \n",
      "4    29.000000   33.000000   35.000000   32.000000   35.000000   33.000000  \n",
      "5     0.916230    0.882199    0.884817    0.903141    0.890052    0.908377  \n",
      "6     0.925000    0.739130    0.775000    0.875000    0.825000    0.947368  \n",
      "7     0.560606    0.507463    0.469697    0.522388    0.485294    0.521739  \n",
      "8     0.990500    0.961900    0.971500    0.984100    0.977700    0.993600  \n",
      "9     0.698113    0.601770    0.584906    0.654206    0.611111    0.672897  \n",
      "10    0.907612    0.873153    0.872966    0.892911    0.878146    0.897266  \n",
      "11    0.824740    0.766323    0.759018    0.798945    0.773543    0.809812  \n",
      "12    0.775556    0.734684    0.720608    0.753258    0.731501    0.757675  \n",
      "13    0.680476    0.548475    0.544783    0.629132    0.578440    0.662411  \n",
      "14    0.915200    0.901800    0.897700    0.906400    0.897700    0.904100  \n",
      "15    0.775556    0.734684    0.720608    0.753258    0.731501    0.757675  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_svm_9)\n",
    "# now convert the resuls to binary with cutoff 6.9\n",
    "Y_testSet9_cat = np.where(((Y_testSet9>=2) | (Y_testSet9<=-2)), 1, 0) \n",
    "y_pred_svm_9_cat = np.where(((y_pred_svm_9 >= 2) | (y_pred_svm_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_svm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7b0e56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7271\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "#plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.743302</td>\n",
       "      <td>0.042489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>2.756810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>154.400000</td>\n",
       "      <td>2.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>1.873796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.091206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.894241</td>\n",
       "      <td>0.019868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.839048</td>\n",
       "      <td>0.083610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.491798</td>\n",
       "      <td>0.086992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.979700</td>\n",
       "      <td>0.011860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.616641</td>\n",
       "      <td>0.082395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.882221</td>\n",
       "      <td>0.023916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.777632</td>\n",
       "      <td>0.046619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.735746</td>\n",
       "      <td>0.044707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.589333</td>\n",
       "      <td>0.085474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.901020</td>\n",
       "      <td>0.016609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.735746</td>\n",
       "      <td>0.044707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.743302     0.042489\n",
       "1                    TP        16.400000     2.756810\n",
       "2                    TN       154.400000     2.118700\n",
       "3                    FP         3.200000     1.873796\n",
       "4                    FN        17.000000     3.091206\n",
       "5              Accuracy         0.894241     0.019868\n",
       "6             Precision         0.839048     0.083610\n",
       "7           Sensitivity         0.491798     0.086992\n",
       "8           Specificity         0.979700     0.011860\n",
       "9              F1 score         0.616641     0.082395\n",
       "10  F1 score (weighted)         0.882221     0.023916\n",
       "11     F1 score (macro)         0.777632     0.046619\n",
       "12    Balanced Accuracy         0.735746     0.044707\n",
       "13                  MCC         0.589333     0.085474\n",
       "14                  NPV         0.901020     0.016609\n",
       "15              ROC_AUC         0.735746     0.044707"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.709863</td>\n",
       "      <td>0.750816</td>\n",
       "      <td>0.745380</td>\n",
       "      <td>0.730434</td>\n",
       "      <td>0.799827</td>\n",
       "      <td>0.742039</td>\n",
       "      <td>0.758361</td>\n",
       "      <td>0.792121</td>\n",
       "      <td>0.691504</td>\n",
       "      <td>0.752298</td>\n",
       "      <td>0.747264</td>\n",
       "      <td>0.032928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>33.100000</td>\n",
       "      <td>2.469818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>308.600000</td>\n",
       "      <td>2.716207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>2.913570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.900000</td>\n",
       "      <td>2.424413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.887435</td>\n",
       "      <td>0.882199</td>\n",
       "      <td>0.916230</td>\n",
       "      <td>0.882199</td>\n",
       "      <td>0.884817</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.908377</td>\n",
       "      <td>0.894503</td>\n",
       "      <td>0.011580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.839799</td>\n",
       "      <td>0.065498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.469697</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.494008</td>\n",
       "      <td>0.035899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.984100</td>\n",
       "      <td>0.980900</td>\n",
       "      <td>0.974700</td>\n",
       "      <td>0.990500</td>\n",
       "      <td>0.961900</td>\n",
       "      <td>0.971500</td>\n",
       "      <td>0.984100</td>\n",
       "      <td>0.977700</td>\n",
       "      <td>0.993600</td>\n",
       "      <td>0.979680</td>\n",
       "      <td>0.009230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.590476</td>\n",
       "      <td>0.563107</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.601770</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.654206</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.672897</td>\n",
       "      <td>0.621461</td>\n",
       "      <td>0.041937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.884515</td>\n",
       "      <td>0.882565</td>\n",
       "      <td>0.873465</td>\n",
       "      <td>0.868199</td>\n",
       "      <td>0.907612</td>\n",
       "      <td>0.873153</td>\n",
       "      <td>0.872966</td>\n",
       "      <td>0.892911</td>\n",
       "      <td>0.878146</td>\n",
       "      <td>0.897266</td>\n",
       "      <td>0.883080</td>\n",
       "      <td>0.012654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.780926</td>\n",
       "      <td>0.777389</td>\n",
       "      <td>0.762613</td>\n",
       "      <td>0.747514</td>\n",
       "      <td>0.824740</td>\n",
       "      <td>0.766323</td>\n",
       "      <td>0.759018</td>\n",
       "      <td>0.798945</td>\n",
       "      <td>0.773543</td>\n",
       "      <td>0.809812</td>\n",
       "      <td>0.780082</td>\n",
       "      <td>0.024230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.738924</td>\n",
       "      <td>0.730869</td>\n",
       "      <td>0.718387</td>\n",
       "      <td>0.707039</td>\n",
       "      <td>0.775556</td>\n",
       "      <td>0.734684</td>\n",
       "      <td>0.720608</td>\n",
       "      <td>0.753258</td>\n",
       "      <td>0.731501</td>\n",
       "      <td>0.757675</td>\n",
       "      <td>0.736850</td>\n",
       "      <td>0.020461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.590014</td>\n",
       "      <td>0.593716</td>\n",
       "      <td>0.564892</td>\n",
       "      <td>0.529282</td>\n",
       "      <td>0.680476</td>\n",
       "      <td>0.548475</td>\n",
       "      <td>0.544783</td>\n",
       "      <td>0.629132</td>\n",
       "      <td>0.578440</td>\n",
       "      <td>0.662411</td>\n",
       "      <td>0.592162</td>\n",
       "      <td>0.050714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.898600</td>\n",
       "      <td>0.892800</td>\n",
       "      <td>0.892800</td>\n",
       "      <td>0.915200</td>\n",
       "      <td>0.901800</td>\n",
       "      <td>0.897700</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>0.897700</td>\n",
       "      <td>0.904100</td>\n",
       "      <td>0.901060</td>\n",
       "      <td>0.006744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.738924</td>\n",
       "      <td>0.730869</td>\n",
       "      <td>0.718387</td>\n",
       "      <td>0.707039</td>\n",
       "      <td>0.775556</td>\n",
       "      <td>0.734684</td>\n",
       "      <td>0.720608</td>\n",
       "      <td>0.753258</td>\n",
       "      <td>0.731501</td>\n",
       "      <td>0.757675</td>\n",
       "      <td>0.736850</td>\n",
       "      <td>0.020461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.709863    0.750816    0.745380    0.730434   \n",
       "1                    TP   33.000000   32.000000   31.000000   29.000000   \n",
       "2                    TN  309.000000  310.000000  308.000000  308.000000   \n",
       "3                    FP    7.000000    5.000000    6.000000    8.000000   \n",
       "4                    FN   33.000000   35.000000   37.000000   37.000000   \n",
       "5              Accuracy    0.895288    0.895288    0.887435    0.882199   \n",
       "6             Precision    0.825000    0.864865    0.837838    0.783784   \n",
       "7           Sensitivity    0.500000    0.477612    0.455882    0.439394   \n",
       "8           Specificity    0.977800    0.984100    0.980900    0.974700   \n",
       "9              F1 score    0.622642    0.615385    0.590476    0.563107   \n",
       "10  F1 score (weighted)    0.884515    0.882565    0.873465    0.868199   \n",
       "11     F1 score (macro)    0.780926    0.777389    0.762613    0.747514   \n",
       "12    Balanced Accuracy    0.738924    0.730869    0.718387    0.707039   \n",
       "13                  MCC    0.590014    0.593716    0.564892    0.529282   \n",
       "14                  NPV    0.903500    0.898600    0.892800    0.892800   \n",
       "15              ROC_AUC    0.738924    0.730869    0.718387    0.707039   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.799827    0.742039    0.758361    0.792121    0.691504    0.752298   \n",
       "1    37.000000   34.000000   31.000000   35.000000   33.000000   36.000000   \n",
       "2   313.000000  303.000000  307.000000  310.000000  307.000000  311.000000   \n",
       "3     3.000000   12.000000    9.000000    5.000000    7.000000    2.000000   \n",
       "4    29.000000   33.000000   35.000000   32.000000   35.000000   33.000000   \n",
       "5     0.916230    0.882199    0.884817    0.903141    0.890052    0.908377   \n",
       "6     0.925000    0.739130    0.775000    0.875000    0.825000    0.947368   \n",
       "7     0.560606    0.507463    0.469697    0.522388    0.485294    0.521739   \n",
       "8     0.990500    0.961900    0.971500    0.984100    0.977700    0.993600   \n",
       "9     0.698113    0.601770    0.584906    0.654206    0.611111    0.672897   \n",
       "10    0.907612    0.873153    0.872966    0.892911    0.878146    0.897266   \n",
       "11    0.824740    0.766323    0.759018    0.798945    0.773543    0.809812   \n",
       "12    0.775556    0.734684    0.720608    0.753258    0.731501    0.757675   \n",
       "13    0.680476    0.548475    0.544783    0.629132    0.578440    0.662411   \n",
       "14    0.915200    0.901800    0.897700    0.906400    0.897700    0.904100   \n",
       "15    0.775556    0.734684    0.720608    0.753258    0.731501    0.757675   \n",
       "\n",
       "           ave       std  \n",
       "0     0.747264  0.032928  \n",
       "1    33.100000  2.469818  \n",
       "2   308.600000  2.716207  \n",
       "3     6.400000  2.913570  \n",
       "4    33.900000  2.424413  \n",
       "5     0.894503  0.011580  \n",
       "6     0.839799  0.065498  \n",
       "7     0.494008  0.035899  \n",
       "8     0.979680  0.009230  \n",
       "9     0.621461  0.041937  \n",
       "10    0.883080  0.012654  \n",
       "11    0.780082  0.024230  \n",
       "12    0.736850  0.020461  \n",
       "13    0.592162  0.050714  \n",
       "14    0.901060  0.006744  \n",
       "15    0.736850  0.020461  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test['std'] = mat_met_svm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.737969</td>\n",
       "      <td>0.042929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.895707</td>\n",
       "      <td>0.016786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.837089</td>\n",
       "      <td>0.066949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.503539</td>\n",
       "      <td>0.089100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.978802</td>\n",
       "      <td>0.010659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.624209</td>\n",
       "      <td>0.077589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.884279</td>\n",
       "      <td>0.021087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.781799</td>\n",
       "      <td>0.043168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.741168</td>\n",
       "      <td>0.043995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.595718</td>\n",
       "      <td>0.075089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.903212</td>\n",
       "      <td>0.015687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.741168</td>\n",
       "      <td>0.043995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.737969     0.042929\n",
       "1              Accuracy         0.895707     0.016786\n",
       "2             Precision         0.837089     0.066949\n",
       "3           Sensitivity         0.503539     0.089100\n",
       "4           Specificity         0.978802     0.010659\n",
       "5              F1 score         0.624209     0.077589\n",
       "6   F1 score (weighted)         0.884279     0.021087\n",
       "7      F1 score (macro)         0.781799     0.043168\n",
       "8     Balanced Accuracy         0.741168     0.043995\n",
       "9                   MCC         0.595718     0.075089\n",
       "10                  NPV         0.903212     0.015687\n",
       "11              ROC_AUC         0.741168     0.043995"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_svm_cat = np.where(((y_pred_optimized_svm >= 2) | (y_pred_optimized_svm <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_svm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_svm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "svm_run0 = data_svm[['y_test_idx0', 'y_test0', 'y_pred_svm0']]\n",
    "svm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "svm_run0.reset_index(inplace=True, drop=True)\n",
    "svm_run1 = data_svm[['y_test_idx1', 'y_test1', 'y_pred_svm1']]\n",
    "svm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "svm_run1.reset_index(inplace=True, drop=True)\n",
    "svm_run2 = data_svm[['y_test_idx2', 'y_test2', 'y_pred_svm2']]\n",
    "svm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "svm_run2.reset_index(inplace=True, drop=True)\n",
    "svm_run3 = data_svm[['y_test_idx3', 'y_test3', 'y_pred_svm3']]\n",
    "svm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "svm_run3.reset_index(inplace=True, drop=True)\n",
    "svm_run4 = data_svm[['y_test_idx4', 'y_test4', 'y_pred_svm4']]\n",
    "svm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "svm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "svm_5preds = pd.concat([chembl_id,svm_run0, svm_run1, svm_run2, svm_run3, svm_run4], axis=1)\n",
    "svm_5preds = svm_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_svm0', 'y_pred_svm1', 'y_pred_svm2', 'y_pred_svm3', 'y_pred_svm4']]\n",
    "svm_5preds['y_pred_svm_ave'] = svm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "svm_5preds['y_pred_svm_std'] = svm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "\n",
    "mat_met_optimized_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2869d8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model r2_score 0.7105 with a standard deviation of 0.0516\n",
      "SVM optimized model r2_score 0.7445 with a standard deviation of 0.0468\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVR \n",
    "svm_baseline_CVscore = cross_val_score(svm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"SVM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_svm_withSemiSel.joblib']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_reg, \"OUTPUT/svm_reg_withSemiSel.joblib\")\n",
    "joblib.dump(optimizedCV_svm, \"OUTPUT/optimizedCV_svm_withSemiSel.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2f79bd46-edc3-436d-8645-d799fe5d49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of test set \n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/TestSet_EvaluationResults_withSemiSel.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_rf_test.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_lgbm_test.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_xgb_test.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_knn_test.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_svm_test.to_excel(writer, sheet_name=\"SVM\", )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "84941152-089d-4746-a005-cb43e7de4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/EvaluationResults_withSemiSel.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_optimized_rf.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_optimized_lgbm.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_optimized_xgb.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_optimized_knn.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_optimized_svm.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1c758d92-e3b4-4361-96f2-aaabf7965382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/PredResults_withSemiSel.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    rf_5preds.to_excel(writer, sheet_name=\"RF\", )\n",
    "    lgbm_5preds.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    xgb_5preds.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    knn_5preds.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    svm_5preds.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
