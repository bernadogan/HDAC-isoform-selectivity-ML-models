{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berna\\anaconda3\\envs\\teachopencadd\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "HDAC1and6 = Path(HERE).resolve().parents[1]/'input'\n",
    "output = HERE/'OUTPUT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL1091474</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[2620858, 10511, 137380, 5988811, 4030911, 184...</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4442777</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1229720, 3913535, 6718756, 838678, 6615052, 6...</td>\n",
       "      <td>3.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3955013</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2962587, 2100111, 3601163, 6502576, 5971385, ...</td>\n",
       "      <td>-0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3800394</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...</td>\n",
       "      <td>[12283267, 6435124, 8033062, 9391761, 1042149,...</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3775662</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2502218, 3190883, 8634031, 7125875, 24941683,...</td>\n",
       "      <td>-0.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL1091474  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL4442777  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3955013  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL3800394  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL3775662  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...   \n",
       "2  [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  SelectivityWindow  \n",
       "0  [2620858, 10511, 137380, 5988811, 4030911, 184...               0.58  \n",
       "1  [1229720, 3913535, 6718756, 838678, 6615052, 6...               3.12  \n",
       "2  [2962587, 2100111, 3601163, 6502576, 5971385, ...              -0.88  \n",
       "3  [12283267, 6435124, 8033062, 9391761, 1042149,...               1.21  \n",
       "4  [2502218, 3190883, 8634031, 7125875, 24941683,...              -0.37  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC1and6/\"HDAC1and6_SemiSel_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>type_HDAC1</th>\n",
       "      <th>Standard_Value_HDAC1</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "      <th>type_HDAC6</th>\n",
       "      <th>Standard_Value_HDAC6</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "      <th>SelectivityRatio</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4098975</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>109.647820</td>\n",
       "      <td>6.96</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.141254</td>\n",
       "      <td>9.85</td>\n",
       "      <td>776.247117</td>\n",
       "      <td>2.89</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3912061</td>\n",
       "      <td>CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12</td>\n",
       "      <td>IC50</td>\n",
       "      <td>616.595002</td>\n",
       "      <td>6.21</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.169824</td>\n",
       "      <td>9.77</td>\n",
       "      <td>3630.780548</td>\n",
       "      <td>3.56</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4243347</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1.995262</td>\n",
       "      <td>8.70</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.199526</td>\n",
       "      <td>9.70</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4247128</td>\n",
       "      <td>C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>83.176377</td>\n",
       "      <td>7.08</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.251189</td>\n",
       "      <td>9.60</td>\n",
       "      <td>331.131122</td>\n",
       "      <td>2.52</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4126811</td>\n",
       "      <td>CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>436.515832</td>\n",
       "      <td>6.36</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.331131</td>\n",
       "      <td>9.48</td>\n",
       "      <td>1318.256739</td>\n",
       "      <td>3.12</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>CHEMBL4278591</td>\n",
       "      <td>CC(=O)Nc1ccc(-c2ccnc(Nc3cccc(OCCCCCCCC(=O)NO)c...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1047.128548</td>\n",
       "      <td>5.98</td>\n",
       "      <td>IC50</td>\n",
       "      <td>6.760830</td>\n",
       "      <td>8.17</td>\n",
       "      <td>154.881662</td>\n",
       "      <td>2.19</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>CHEMBL4649511</td>\n",
       "      <td>O=C(CCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO</td>\n",
       "      <td>IC50</td>\n",
       "      <td>3.311311</td>\n",
       "      <td>8.48</td>\n",
       "      <td>IC50</td>\n",
       "      <td>12.882496</td>\n",
       "      <td>7.89</td>\n",
       "      <td>0.257040</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>CHEMBL4291781</td>\n",
       "      <td>CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(OCCCCCCCC(=O)NO)cc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1698.243652</td>\n",
       "      <td>5.77</td>\n",
       "      <td>IC50</td>\n",
       "      <td>14.125375</td>\n",
       "      <td>7.85</td>\n",
       "      <td>120.226444</td>\n",
       "      <td>2.08</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>CHEMBL3215861</td>\n",
       "      <td>CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC</td>\n",
       "      <td>Ki</td>\n",
       "      <td>28.183829</td>\n",
       "      <td>7.55</td>\n",
       "      <td>Ki</td>\n",
       "      <td>245.470892</td>\n",
       "      <td>6.61</td>\n",
       "      <td>0.114815</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>CHEMBL3233708</td>\n",
       "      <td>O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>354.813389</td>\n",
       "      <td>6.45</td>\n",
       "      <td>IC50</td>\n",
       "      <td>295.120923</td>\n",
       "      <td>6.53</td>\n",
       "      <td>1.202264</td>\n",
       "      <td>0.08</td>\n",
       "      <td>Non-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1339 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                             smiles  \\\n",
       "0         CHEMBL4098975  O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...   \n",
       "1         CHEMBL3912061     CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12   \n",
       "2         CHEMBL4243347  O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...   \n",
       "3         CHEMBL4247128  C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...   \n",
       "4         CHEMBL4126811  CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...   \n",
       "...                 ...                                                ...   \n",
       "1334      CHEMBL4278591  CC(=O)Nc1ccc(-c2ccnc(Nc3cccc(OCCCCCCCC(=O)NO)c...   \n",
       "1335      CHEMBL4649511        O=C(CCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO   \n",
       "1336      CHEMBL4291781  CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(OCCCCCCCC(=O)NO)cc...   \n",
       "1337      CHEMBL3215861           CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC   \n",
       "1338      CHEMBL3233708  O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...   \n",
       "\n",
       "     type_HDAC1  Standard_Value_HDAC1  pChEMBL_HDAC1 type_HDAC6  \\\n",
       "0          IC50            109.647820           6.96       IC50   \n",
       "1          IC50            616.595002           6.21       IC50   \n",
       "2          IC50              1.995262           8.70       IC50   \n",
       "3          IC50             83.176377           7.08       IC50   \n",
       "4          IC50            436.515832           6.36       IC50   \n",
       "...         ...                   ...            ...        ...   \n",
       "1334       IC50           1047.128548           5.98       IC50   \n",
       "1335       IC50              3.311311           8.48       IC50   \n",
       "1336       IC50           1698.243652           5.77       IC50   \n",
       "1337         Ki             28.183829           7.55         Ki   \n",
       "1338       IC50            354.813389           6.45       IC50   \n",
       "\n",
       "      Standard_Value_HDAC6  pChEMBL_HDAC6  SelectivityRatio  \\\n",
       "0                 0.141254           9.85        776.247117   \n",
       "1                 0.169824           9.77       3630.780548   \n",
       "2                 0.199526           9.70         10.000000   \n",
       "3                 0.251189           9.60        331.131122   \n",
       "4                 0.331131           9.48       1318.256739   \n",
       "...                    ...            ...               ...   \n",
       "1334              6.760830           8.17        154.881662   \n",
       "1335             12.882496           7.89          0.257040   \n",
       "1336             14.125375           7.85        120.226444   \n",
       "1337            245.470892           6.61          0.114815   \n",
       "1338            295.120923           6.53          1.202264   \n",
       "\n",
       "      SelectivityWindow            label  \n",
       "0                  2.89  HDAC6-selective  \n",
       "1                  3.56  HDAC6-selective  \n",
       "2                  1.00      Dual-binder  \n",
       "3                  2.52  HDAC6-selective  \n",
       "4                  3.12  HDAC6-selective  \n",
       "...                 ...              ...  \n",
       "1334               2.19  HDAC6-selective  \n",
       "1335              -0.59      Dual-binder  \n",
       "1336               2.08  HDAC6-selective  \n",
       "1337              -0.94      Dual-binder  \n",
       "1338               0.08       Non-binder  \n",
       "\n",
       "[1339 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC1and6/\"HDAC1and6_SemiSel_dataset.csv\", )\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL1091474</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[2620858, 10511, 137380, 5988811, 4030911, 184...</td>\n",
       "      <td>0.58</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4442777</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1229720, 3913535, 6718756, 838678, 6615052, 6...</td>\n",
       "      <td>3.12</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3955013</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2962587, 2100111, 3601163, 6502576, 5971385, ...</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>Non-binder</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3800394</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...</td>\n",
       "      <td>[12283267, 6435124, 8033062, 9391761, 1042149,...</td>\n",
       "      <td>1.21</td>\n",
       "      <td>Non-binder</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL1091474  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL4442777  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3955013  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL3800394  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...   \n",
       "2  [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...   \n",
       "\n",
       "                                             fp_MAP4  SelectivityWindow  \\\n",
       "0  [2620858, 10511, 137380, 5988811, 4030911, 184...               0.58   \n",
       "1  [1229720, 3913535, 6718756, 838678, 6615052, 6...               3.12   \n",
       "2  [2962587, 2100111, 3601163, 6502576, 5971385, ...              -0.88   \n",
       "3  [12283267, 6435124, 8033062, 9391761, 1042149,...               1.21   \n",
       "\n",
       "             label  Class  \n",
       "0      Dual-binder    3.0  \n",
       "1  HDAC6-selective    0.0  \n",
       "2       Non-binder    4.0  \n",
       "3       Non-binder    4.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'hDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'hDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'hDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "df.loc[df[df.label == 'Semi-selective'].index, \"Class\"] = 5.0\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for selectivity\n",
    "df[\"selectivity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as selective if SelectivityWindow is >=2 or >=-2, 0 otherwise\n",
    "df.loc[df[df.SelectivityWindow >= 2.0].index, \"selectivity\"] = 1.0\n",
    "df.loc[df[df.SelectivityWindow <= -2.0].index, \"selectivity\"] = 1.0\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "indices =  np.array(df.index)\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y = df[\"SelectivityWindow\"].values\n",
    "Y_cat =  df[\"selectivity\"].values\n",
    "Y_class = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['SelectivityWindow'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_cat\"] = np.array(list(df.iloc[index_tr_all[i]]['selectivity'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['SelectivityWindow'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_cat\"] = np.array(list(df.iloc[index_te_all[i]]['selectivity'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.715459     0.050072\n",
      "1                    TP        16.700000     2.002776\n",
      "2                    TN       154.400000     2.065591\n",
      "3                    FP         3.200000     1.475730\n",
      "4                    FN        16.700000     1.337494\n",
      "5              Accuracy         0.895812     0.009701\n",
      "6             Precision         0.843191     0.059710\n",
      "7           Sensitivity         0.499124     0.047246\n",
      "8           Specificity         0.979670     0.009423\n",
      "9              F1 score         0.625257     0.040074\n",
      "10  F1 score (weighted)         0.884618     0.010628\n",
      "11     F1 score (macro)         0.782354     0.022213\n",
      "12    Balanced Accuracy         0.739401     0.023050\n",
      "13                  MCC         0.596765     0.042694\n",
      "14                  NPV         0.902450     0.006902\n",
      "15              ROC_AUC         0.739401     0.023050\n",
      "CPU times: user 2min, sys: 52.9 ms, total: 2min\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_reg =  RandomForestRegressor(random_state=1121218, max_features = None, n_jobs=4,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_reg.fit(x_train, y_train)\n",
    "        y_pred = rf_reg.predict(x_test)  \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=4, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=4, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 15:21:47,003]\u001b[0m A new study created in memory with name: RFRegressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:23:01,776]\u001b[0m Trial 0 finished with value: 0.6919603213991465 and parameters: {'n_estimators': 309}. Best is trial 0 with value: 0.6919603213991465.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:23:44,507]\u001b[0m Trial 1 finished with value: 0.6900531723482798 and parameters: {'n_estimators': 174}. Best is trial 0 with value: 0.6919603213991465.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:26:56,388]\u001b[0m Trial 2 finished with value: 0.6928610616919311 and parameters: {'n_estimators': 786}. Best is trial 2 with value: 0.6928610616919311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:27:25,388]\u001b[0m Trial 3 finished with value: 0.688791064655251 and parameters: {'n_estimators': 119}. Best is trial 2 with value: 0.6928610616919311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:28:42,185]\u001b[0m Trial 4 finished with value: 0.6920066612879057 and parameters: {'n_estimators': 313}. Best is trial 2 with value: 0.6928610616919311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:31:44,904]\u001b[0m Trial 5 finished with value: 0.6929333549867676 and parameters: {'n_estimators': 750}. Best is trial 5 with value: 0.6929333549867676.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:32:12,312]\u001b[0m Trial 6 finished with value: 0.6873072682695869 and parameters: {'n_estimators': 111}. Best is trial 5 with value: 0.6929333549867676.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:36:16,012]\u001b[0m Trial 7 finished with value: 0.6929665571710336 and parameters: {'n_estimators': 996}. Best is trial 7 with value: 0.6929665571710336.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:40:22,043]\u001b[0m Trial 8 finished with value: 0.6928530524942199 and parameters: {'n_estimators': 985}. Best is trial 7 with value: 0.6929665571710336.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:43:20,290]\u001b[0m Trial 9 finished with value: 0.6931106253986571 and parameters: {'n_estimators': 712}. Best is trial 9 with value: 0.6931106253986571.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:45:51,005]\u001b[0m Trial 10 finished with value: 0.693242923985548 and parameters: {'n_estimators': 602}. Best is trial 10 with value: 0.693242923985548.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:48:15,330]\u001b[0m Trial 11 finished with value: 0.6934146468018787 and parameters: {'n_estimators': 574}. Best is trial 11 with value: 0.6934146468018787.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:50:30,557]\u001b[0m Trial 12 finished with value: 0.6931494250933192 and parameters: {'n_estimators': 537}. Best is trial 11 with value: 0.6934146468018787.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:52:45,184]\u001b[0m Trial 13 finished with value: 0.6932003885899507 and parameters: {'n_estimators': 536}. Best is trial 11 with value: 0.6934146468018787.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:55:27,412]\u001b[0m Trial 14 finished with value: 0.693023711688239 and parameters: {'n_estimators': 642}. Best is trial 11 with value: 0.6934146468018787.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:57:19,379]\u001b[0m Trial 15 finished with value: 0.6932168198306089 and parameters: {'n_estimators': 438}. Best is trial 11 with value: 0.6934146468018787.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:59:04,517]\u001b[0m Trial 16 finished with value: 0.6929618985225605 and parameters: {'n_estimators': 415}. Best is trial 11 with value: 0.6934146468018787.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:02:34,191]\u001b[0m Trial 17 finished with value: 0.6929356279321592 and parameters: {'n_estimators': 834}. Best is trial 11 with value: 0.6934146468018787.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:05:11,286]\u001b[0m Trial 18 finished with value: 0.6930758199180745 and parameters: {'n_estimators': 626}. Best is trial 11 with value: 0.6934146468018787.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:08:56,246]\u001b[0m Trial 19 finished with value: 0.6930754752936474 and parameters: {'n_estimators': 893}. Best is trial 11 with value: 0.6934146468018787.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:10:48,216]\u001b[0m Trial 20 finished with value: 0.6931125664644139 and parameters: {'n_estimators': 443}. Best is trial 11 with value: 0.6934146468018787.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:12:34,699]\u001b[0m Trial 21 finished with value: 0.6931109832754583 and parameters: {'n_estimators': 419}. Best is trial 11 with value: 0.6934146468018787.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:15:20,626]\u001b[0m Trial 22 finished with value: 0.6931274673104515 and parameters: {'n_estimators': 657}. Best is trial 11 with value: 0.6934146468018787.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:17:22,535]\u001b[0m Trial 23 finished with value: 0.693384699863041 and parameters: {'n_estimators': 482}. Best is trial 11 with value: 0.6934146468018787.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:19:43,736]\u001b[0m Trial 24 finished with value: 0.6934635286960982 and parameters: {'n_estimators': 557}. Best is trial 24 with value: 0.6934635286960982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:21:54,768]\u001b[0m Trial 25 finished with value: 0.6932540958280006 and parameters: {'n_estimators': 519}. Best is trial 24 with value: 0.6934635286960982.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:23:58,430]\u001b[0m Trial 26 finished with value: 0.693493103830716 and parameters: {'n_estimators': 487}. Best is trial 26 with value: 0.693493103830716.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:25:18,669]\u001b[0m Trial 27 finished with value: 0.6919714252135488 and parameters: {'n_estimators': 316}. Best is trial 26 with value: 0.693493103830716.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:27:45,381]\u001b[0m Trial 28 finished with value: 0.6933992995331568 and parameters: {'n_estimators': 578}. Best is trial 26 with value: 0.693493103830716.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:30:42,046]\u001b[0m Trial 29 finished with value: 0.693163261766724 and parameters: {'n_estimators': 700}. Best is trial 26 with value: 0.693493103830716.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:32:10,711]\u001b[0m Trial 30 finished with value: 0.692504026213625 and parameters: {'n_estimators': 350}. Best is trial 26 with value: 0.693493103830716.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:34:41,624]\u001b[0m Trial 31 finished with value: 0.6934541506957012 and parameters: {'n_estimators': 587}. Best is trial 26 with value: 0.693493103830716.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:35:40,665]\u001b[0m Trial 32 finished with value: 0.6902672943696879 and parameters: {'n_estimators': 235}. Best is trial 26 with value: 0.693493103830716.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:37:42,804]\u001b[0m Trial 33 finished with value: 0.693493103830716 and parameters: {'n_estimators': 487}. Best is trial 26 with value: 0.693493103830716.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:39:17,066]\u001b[0m Trial 34 finished with value: 0.6928768619777953 and parameters: {'n_estimators': 374}. Best is trial 26 with value: 0.693493103830716.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:41:19,273]\u001b[0m Trial 35 finished with value: 0.6933640880430738 and parameters: {'n_estimators': 481}. Best is trial 26 with value: 0.693493103830716.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:42:20,766]\u001b[0m Trial 36 finished with value: 0.6905737982873833 and parameters: {'n_estimators': 241}. Best is trial 26 with value: 0.693493103830716.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:44:24,191]\u001b[0m Trial 37 finished with value: 0.6935264385848763 and parameters: {'n_estimators': 488}. Best is trial 37 with value: 0.6935264385848763.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:46:28,190]\u001b[0m Trial 38 finished with value: 0.6935515155771638 and parameters: {'n_estimators': 490}. Best is trial 38 with value: 0.6935515155771638.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:48:30,243]\u001b[0m Trial 39 finished with value: 0.6933927668150021 and parameters: {'n_estimators': 483}. Best is trial 38 with value: 0.6935515155771638.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:50:04,650]\u001b[0m Trial 40 finished with value: 0.69295291961363 and parameters: {'n_estimators': 372}. Best is trial 38 with value: 0.6935515155771638.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:52:18,654]\u001b[0m Trial 41 finished with value: 0.6933379244928546 and parameters: {'n_estimators': 525}. Best is trial 38 with value: 0.6935515155771638.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:54:18,804]\u001b[0m Trial 42 finished with value: 0.6933111400670053 and parameters: {'n_estimators': 473}. Best is trial 38 with value: 0.6935515155771638.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:55:59,339]\u001b[0m Trial 43 finished with value: 0.6929328748078586 and parameters: {'n_estimators': 399}. Best is trial 38 with value: 0.6935515155771638.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:58:53,599]\u001b[0m Trial 44 finished with value: 0.6932384322332232 and parameters: {'n_estimators': 687}. Best is trial 38 with value: 0.6935515155771638.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:01:12,891]\u001b[0m Trial 45 finished with value: 0.6933652868699303 and parameters: {'n_estimators': 556}. Best is trial 38 with value: 0.6935515155771638.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 17:03:23,234]\u001b[0m Trial 46 finished with value: 0.6932922280103834 and parameters: {'n_estimators': 515}. Best is trial 38 with value: 0.6935515155771638.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:04:35,319]\u001b[0m Trial 47 finished with value: 0.6911264151216219 and parameters: {'n_estimators': 284}. Best is trial 38 with value: 0.6935515155771638.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:07:44,026]\u001b[0m Trial 48 finished with value: 0.6929231229182249 and parameters: {'n_estimators': 751}. Best is trial 38 with value: 0.6935515155771638.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:10:18,599]\u001b[0m Trial 49 finished with value: 0.6930526088238136 and parameters: {'n_estimators': 615}. Best is trial 38 with value: 0.6935515155771638.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6936\n",
      "\tBest params:\n",
      "\t\tn_estimators: 490\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFRegressor\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.686187\n",
      "1                    TP   34.000000\n",
      "2                    TN  306.000000\n",
      "3                    FP    7.000000\n",
      "4                    FN   35.000000\n",
      "5              Accuracy    0.890052\n",
      "6             Precision    0.829268\n",
      "7           Sensitivity    0.492754\n",
      "8           Specificity    0.977600\n",
      "9              F1 score    0.618182\n",
      "10  F1 score (weighted)    0.878413\n",
      "11     F1 score (macro)    0.776981\n",
      "12    Balanced Accuracy    0.735195\n",
      "13                  MCC    0.584635\n",
      "14                  NPV    0.897400\n",
      "15              ROC_AUC    0.735195\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_rf_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet0 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_0_cat = np.where(((y_pred_rf_0 >= 2) | (y_pred_rf_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_rf_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 17:12:29,234]\u001b[0m Trial 50 finished with value: 0.6989284069020538 and parameters: {'n_estimators': 468}. Best is trial 50 with value: 0.6989284069020538.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:14:26,439]\u001b[0m Trial 51 finished with value: 0.6988438605930399 and parameters: {'n_estimators': 472}. Best is trial 50 with value: 0.6989284069020538.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:16:21,364]\u001b[0m Trial 52 finished with value: 0.6989996062019701 and parameters: {'n_estimators': 460}. Best is trial 52 with value: 0.6989996062019701.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:18:12,251]\u001b[0m Trial 53 finished with value: 0.6992415200623231 and parameters: {'n_estimators': 446}. Best is trial 53 with value: 0.6992415200623231.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:20:03,773]\u001b[0m Trial 54 finished with value: 0.6991754480129573 and parameters: {'n_estimators': 447}. Best is trial 53 with value: 0.6992415200623231.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:21:53,442]\u001b[0m Trial 55 finished with value: 0.6994027800042317 and parameters: {'n_estimators': 439}. Best is trial 55 with value: 0.6994027800042317.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:23:42,998]\u001b[0m Trial 56 finished with value: 0.6994027800042317 and parameters: {'n_estimators': 439}. Best is trial 55 with value: 0.6994027800042317.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:25:04,586]\u001b[0m Trial 57 finished with value: 0.6988940805542256 and parameters: {'n_estimators': 325}. Best is trial 55 with value: 0.6994027800042317.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:26:55,594]\u001b[0m Trial 58 finished with value: 0.6992442755070586 and parameters: {'n_estimators': 441}. Best is trial 55 with value: 0.6994027800042317.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:28:45,098]\u001b[0m Trial 59 finished with value: 0.6994177737668117 and parameters: {'n_estimators': 437}. Best is trial 59 with value: 0.6994177737668117.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:30:25,854]\u001b[0m Trial 60 finished with value: 0.6989672478688835 and parameters: {'n_estimators': 399}. Best is trial 59 with value: 0.6994177737668117.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:32:15,473]\u001b[0m Trial 61 finished with value: 0.6994177737668117 and parameters: {'n_estimators': 437}. Best is trial 59 with value: 0.6994177737668117.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:33:59,228]\u001b[0m Trial 62 finished with value: 0.6990545682182681 and parameters: {'n_estimators': 411}. Best is trial 59 with value: 0.6994177737668117.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:35:29,675]\u001b[0m Trial 63 finished with value: 0.6991489552895366 and parameters: {'n_estimators': 358}. Best is trial 59 with value: 0.6994177737668117.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:37:18,886]\u001b[0m Trial 64 finished with value: 0.6993208833866993 and parameters: {'n_estimators': 434}. Best is trial 59 with value: 0.6994177737668117.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:39:07,920]\u001b[0m Trial 65 finished with value: 0.6995129182358003 and parameters: {'n_estimators': 431}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:40:19,107]\u001b[0m Trial 66 finished with value: 0.6991938593296023 and parameters: {'n_estimators': 282}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:42:07,012]\u001b[0m Trial 67 finished with value: 0.6993473265396666 and parameters: {'n_estimators': 428}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:43:31,659]\u001b[0m Trial 68 finished with value: 0.6991601048249836 and parameters: {'n_estimators': 335}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:45:09,139]\u001b[0m Trial 69 finished with value: 0.6992123291164551 and parameters: {'n_estimators': 388}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:46:54,545]\u001b[0m Trial 70 finished with value: 0.6990637115981546 and parameters: {'n_estimators': 419}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:48:43,193]\u001b[0m Trial 71 finished with value: 0.6995129182358003 and parameters: {'n_estimators': 431}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:50:51,354]\u001b[0m Trial 72 finished with value: 0.6991935150052191 and parameters: {'n_estimators': 511}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:52:37,977]\u001b[0m Trial 73 finished with value: 0.6991165372426213 and parameters: {'n_estimators': 426}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:54:13,630]\u001b[0m Trial 74 finished with value: 0.6992581026967171 and parameters: {'n_estimators': 382}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:55:41,896]\u001b[0m Trial 75 finished with value: 0.6993321086474011 and parameters: {'n_estimators': 344}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:56:56,873]\u001b[0m Trial 76 finished with value: 0.6989529354251495 and parameters: {'n_estimators': 294}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:58:25,449]\u001b[0m Trial 77 finished with value: 0.6992107447077844 and parameters: {'n_estimators': 347}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:59:08,080]\u001b[0m Trial 78 finished with value: 0.6977797227591352 and parameters: {'n_estimators': 165}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:01:28,381]\u001b[0m Trial 79 finished with value: 0.6995070214312541 and parameters: {'n_estimators': 550}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:03:47,102]\u001b[0m Trial 80 finished with value: 0.699423892920319 and parameters: {'n_estimators': 544}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:06:05,400]\u001b[0m Trial 81 finished with value: 0.6994889441392346 and parameters: {'n_estimators': 543}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:08:25,785]\u001b[0m Trial 82 finished with value: 0.69947309826519 and parameters: {'n_estimators': 549}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:10:56,924]\u001b[0m Trial 83 finished with value: 0.699457530468222 and parameters: {'n_estimators': 591}. Best is trial 65 with value: 0.6995129182358003.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:13:41,339]\u001b[0m Trial 84 finished with value: 0.6996136718836088 and parameters: {'n_estimators': 646}. Best is trial 84 with value: 0.6996136718836088.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:16:30,993]\u001b[0m Trial 85 finished with value: 0.6995601239555397 and parameters: {'n_estimators': 665}. Best is trial 84 with value: 0.6996136718836088.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:19:22,267]\u001b[0m Trial 86 finished with value: 0.6995467438968407 and parameters: {'n_estimators': 672}. Best is trial 84 with value: 0.6996136718836088.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:22:14,264]\u001b[0m Trial 87 finished with value: 0.6994961090661918 and parameters: {'n_estimators': 678}. Best is trial 84 with value: 0.6996136718836088.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:25:02,282]\u001b[0m Trial 88 finished with value: 0.6996776389495076 and parameters: {'n_estimators': 660}. Best is trial 88 with value: 0.6996776389495076.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:27:49,270]\u001b[0m Trial 89 finished with value: 0.6997443390527953 and parameters: {'n_estimators': 658}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:30:37,336]\u001b[0m Trial 90 finished with value: 0.6996285270838049 and parameters: {'n_estimators': 662}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:33:25,541]\u001b[0m Trial 91 finished with value: 0.6996285270838049 and parameters: {'n_estimators': 662}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:36:31,090]\u001b[0m Trial 92 finished with value: 0.6991334556964465 and parameters: {'n_estimators': 730}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:39:20,130]\u001b[0m Trial 93 finished with value: 0.6996187379303163 and parameters: {'n_estimators': 661}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:42:08,594]\u001b[0m Trial 94 finished with value: 0.6996776389495076 and parameters: {'n_estimators': 660}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:44:53,583]\u001b[0m Trial 95 finished with value: 0.6996147243918858 and parameters: {'n_estimators': 648}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:47:36,546]\u001b[0m Trial 96 finished with value: 0.6995711451853784 and parameters: {'n_estimators': 643}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 18:50:17,078]\u001b[0m Trial 97 finished with value: 0.6996512230616675 and parameters: {'n_estimators': 636}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:52:59,037]\u001b[0m Trial 98 finished with value: 0.6995506667556286 and parameters: {'n_estimators': 639}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:56:01,608]\u001b[0m Trial 99 finished with value: 0.6992052560501227 and parameters: {'n_estimators': 723}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6997\n",
      "\tBest params:\n",
      "\t\tn_estimators: 658\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.686187    0.735183\n",
      "1                    TP   34.000000   36.000000\n",
      "2                    TN  306.000000  309.000000\n",
      "3                    FP    7.000000    5.000000\n",
      "4                    FN   35.000000   32.000000\n",
      "5              Accuracy    0.890052    0.903141\n",
      "6             Precision    0.829268    0.878049\n",
      "7           Sensitivity    0.492754    0.529412\n",
      "8           Specificity    0.977600    0.984100\n",
      "9              F1 score    0.618182    0.660550\n",
      "10  F1 score (weighted)    0.878413    0.893141\n",
      "11     F1 score (macro)    0.776981    0.802031\n",
      "12    Balanced Accuracy    0.735195    0.756744\n",
      "13                  MCC    0.584635    0.634572\n",
      "14                  NPV    0.897400    0.906200\n",
      "15              ROC_AUC    0.735195    0.756744\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_rf_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_1_cat = np.where(((y_pred_rf_1 >= 2) | (y_pred_rf_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_rf_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 18:59:33,831]\u001b[0m Trial 100 finished with value: 0.6898171606147638 and parameters: {'n_estimators': 759}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:02:17,979]\u001b[0m Trial 101 finished with value: 0.6899515566271202 and parameters: {'n_estimators': 645}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:04:55,328]\u001b[0m Trial 102 finished with value: 0.6901816810111373 and parameters: {'n_estimators': 615}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:07:53,239]\u001b[0m Trial 103 finished with value: 0.689903238619365 and parameters: {'n_estimators': 700}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:11:16,570]\u001b[0m Trial 104 finished with value: 0.6899779069672403 and parameters: {'n_estimators': 798}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:14:05,233]\u001b[0m Trial 105 finished with value: 0.6900894582924609 and parameters: {'n_estimators': 659}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:16:45,284]\u001b[0m Trial 106 finished with value: 0.6900817649739512 and parameters: {'n_estimators': 626}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:19:43,142]\u001b[0m Trial 107 finished with value: 0.6899722209067637 and parameters: {'n_estimators': 698}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:22:16,208]\u001b[0m Trial 108 finished with value: 0.6900494438004945 and parameters: {'n_estimators': 603}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:24:42,447]\u001b[0m Trial 109 finished with value: 0.6903469254338134 and parameters: {'n_estimators': 573}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:27:29,538]\u001b[0m Trial 110 finished with value: 0.6900119032786145 and parameters: {'n_estimators': 654}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:30:10,311]\u001b[0m Trial 111 finished with value: 0.6900448643446694 and parameters: {'n_estimators': 632}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:33:00,971]\u001b[0m Trial 112 finished with value: 0.6900892576518697 and parameters: {'n_estimators': 670}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:36:03,306]\u001b[0m Trial 113 finished with value: 0.6899174261934518 and parameters: {'n_estimators': 717}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:38:46,564]\u001b[0m Trial 114 finished with value: 0.6900609401521672 and parameters: {'n_estimators': 641}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:41:55,042]\u001b[0m Trial 115 finished with value: 0.6899989624418279 and parameters: {'n_estimators': 740}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:45:10,583]\u001b[0m Trial 116 finished with value: 0.6897203537339754 and parameters: {'n_estimators': 768}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:48:06,434]\u001b[0m Trial 117 finished with value: 0.6900294677436328 and parameters: {'n_estimators': 693}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:50:43,217]\u001b[0m Trial 118 finished with value: 0.6901448402058524 and parameters: {'n_estimators': 616}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:53:15,816]\u001b[0m Trial 119 finished with value: 0.6899994352395808 and parameters: {'n_estimators': 597}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:56:04,152]\u001b[0m Trial 120 finished with value: 0.6901475998178384 and parameters: {'n_estimators': 662}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:58:58,123]\u001b[0m Trial 121 finished with value: 0.6900454908753136 and parameters: {'n_estimators': 683}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:01:49,207]\u001b[0m Trial 122 finished with value: 0.6900502302860607 and parameters: {'n_estimators': 672}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:04:15,390]\u001b[0m Trial 123 finished with value: 0.6903469254338134 and parameters: {'n_estimators': 573}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:06:57,892]\u001b[0m Trial 124 finished with value: 0.6901221667344875 and parameters: {'n_estimators': 639}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:09:45,346]\u001b[0m Trial 125 finished with value: 0.6900201922599895 and parameters: {'n_estimators': 652}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:12:47,911]\u001b[0m Trial 126 finished with value: 0.689948599025149 and parameters: {'n_estimators': 715}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:15:23,751]\u001b[0m Trial 127 finished with value: 0.6901745011120506 and parameters: {'n_estimators': 609}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:17:56,557]\u001b[0m Trial 128 finished with value: 0.6899927636702515 and parameters: {'n_estimators': 630}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:20:37,783]\u001b[0m Trial 129 finished with value: 0.6900464434851858 and parameters: {'n_estimators': 671}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:23:23,752]\u001b[0m Trial 130 finished with value: 0.6900851555516483 and parameters: {'n_estimators': 688}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:26:14,649]\u001b[0m Trial 131 finished with value: 0.6898434519723394 and parameters: {'n_estimators': 708}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:28:52,027]\u001b[0m Trial 132 finished with value: 0.6900201922599895 and parameters: {'n_estimators': 652}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:31:11,873]\u001b[0m Trial 133 finished with value: 0.6902493654528861 and parameters: {'n_estimators': 579}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:33:42,460]\u001b[0m Trial 134 finished with value: 0.6902351785034209 and parameters: {'n_estimators': 624}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:36:24,211]\u001b[0m Trial 135 finished with value: 0.6900464434851858 and parameters: {'n_estimators': 671}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:39:20,631]\u001b[0m Trial 136 finished with value: 0.6899857609155434 and parameters: {'n_estimators': 734}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:41:55,418]\u001b[0m Trial 137 finished with value: 0.6899565345083927 and parameters: {'n_estimators': 644}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:44:18,060]\u001b[0m Trial 138 finished with value: 0.6900097087955203 and parameters: {'n_estimators': 594}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:47:02,586]\u001b[0m Trial 139 finished with value: 0.6900881590919071 and parameters: {'n_estimators': 687}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:50:09,010]\u001b[0m Trial 140 finished with value: 0.6897280512598909 and parameters: {'n_estimators': 776}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:52:48,136]\u001b[0m Trial 141 finished with value: 0.6901475998178381 and parameters: {'n_estimators': 662}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:55:38,345]\u001b[0m Trial 142 finished with value: 0.6899032325867763 and parameters: {'n_estimators': 707}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:58:09,998]\u001b[0m Trial 143 finished with value: 0.6900448643446694 and parameters: {'n_estimators': 632}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:01:49,414]\u001b[0m Trial 144 finished with value: 0.6898340381689634 and parameters: {'n_estimators': 913}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:04:16,200]\u001b[0m Trial 145 finished with value: 0.6901745011120506 and parameters: {'n_estimators': 609}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 21:07:00,795]\u001b[0m Trial 146 finished with value: 0.6900454908753136 and parameters: {'n_estimators': 683}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:09:37,541]\u001b[0m Trial 147 finished with value: 0.6899232726718683 and parameters: {'n_estimators': 649}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:12:07,001]\u001b[0m Trial 148 finished with value: 0.6902113196364073 and parameters: {'n_estimators': 621}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:14:47,119]\u001b[0m Trial 149 finished with value: 0.6901499299136871 and parameters: {'n_estimators': 665}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6997\n",
      "\tBest params:\n",
      "\t\tn_estimators: 658\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.686187    0.735183    0.712420\n",
      "1                    TP   34.000000   36.000000   32.000000\n",
      "2                    TN  306.000000  309.000000  303.000000\n",
      "3                    FP    7.000000    5.000000   11.000000\n",
      "4                    FN   35.000000   32.000000   36.000000\n",
      "5              Accuracy    0.890052    0.903141    0.876963\n",
      "6             Precision    0.829268    0.878049    0.744186\n",
      "7           Sensitivity    0.492754    0.529412    0.470588\n",
      "8           Specificity    0.977600    0.984100    0.965000\n",
      "9              F1 score    0.618182    0.660550    0.576577\n",
      "10  F1 score (weighted)    0.878413    0.893141    0.865463\n",
      "11     F1 score (macro)    0.776981    0.802031    0.752301\n",
      "12    Balanced Accuracy    0.735195    0.756744    0.717778\n",
      "13                  MCC    0.584635    0.634572    0.527144\n",
      "14                  NPV    0.897400    0.906200    0.893800\n",
      "15              ROC_AUC    0.735195    0.756744    0.717778\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_rf_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_2_cat = np.where(((y_pred_rf_2 >= 2) | (y_pred_rf_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_rf_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 21:17:59,405]\u001b[0m Trial 150 finished with value: 0.6823106322233012 and parameters: {'n_estimators': 703}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:20:04,102]\u001b[0m Trial 151 finished with value: 0.681777899968196 and parameters: {'n_estimators': 500}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:22:24,514]\u001b[0m Trial 152 finished with value: 0.6816795418271718 and parameters: {'n_estimators': 560}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:25:04,245]\u001b[0m Trial 153 finished with value: 0.6815480578134874 and parameters: {'n_estimators': 632}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:27:34,797]\u001b[0m Trial 154 finished with value: 0.6813909687630362 and parameters: {'n_estimators': 599}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:30:27,917]\u001b[0m Trial 155 finished with value: 0.6820642473362771 and parameters: {'n_estimators': 680}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:33:03,659]\u001b[0m Trial 156 finished with value: 0.6815950571743505 and parameters: {'n_estimators': 646}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:35:42,574]\u001b[0m Trial 157 finished with value: 0.6824846275957122 and parameters: {'n_estimators': 729}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:38:08,953]\u001b[0m Trial 158 finished with value: 0.6817274808210794 and parameters: {'n_estimators': 662}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:40:42,959]\u001b[0m Trial 159 finished with value: 0.6821957256555884 and parameters: {'n_estimators': 694}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:42:54,278]\u001b[0m Trial 160 finished with value: 0.6813785707835308 and parameters: {'n_estimators': 587}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:45:25,575]\u001b[0m Trial 161 finished with value: 0.6818233135474773 and parameters: {'n_estimators': 675}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:47:43,685]\u001b[0m Trial 162 finished with value: 0.6815757550027837 and parameters: {'n_estimators': 615}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:50:07,034]\u001b[0m Trial 163 finished with value: 0.6814662392055256 and parameters: {'n_estimators': 643}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:52:37,559]\u001b[0m Trial 164 finished with value: 0.6818343400324104 and parameters: {'n_estimators': 674}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:55:03,468]\u001b[0m Trial 165 finished with value: 0.6816436722203778 and parameters: {'n_estimators': 653}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:57:23,626]\u001b[0m Trial 166 finished with value: 0.6815985648358409 and parameters: {'n_estimators': 625}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:00:02,762]\u001b[0m Trial 167 finished with value: 0.6824331663822137 and parameters: {'n_estimators': 713}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:02:36,542]\u001b[0m Trial 168 finished with value: 0.6820806026538138 and parameters: {'n_estimators': 685}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:05:03,885]\u001b[0m Trial 169 finished with value: 0.6816208583281599 and parameters: {'n_estimators': 656}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:07:52,421]\u001b[0m Trial 170 finished with value: 0.6826828818755369 and parameters: {'n_estimators': 753}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:10:13,990]\u001b[0m Trial 171 finished with value: 0.6815211662739883 and parameters: {'n_estimators': 634}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:12:19,823]\u001b[0m Trial 172 finished with value: 0.6816753130749944 and parameters: {'n_estimators': 564}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:14:54,495]\u001b[0m Trial 173 finished with value: 0.6821531034830008 and parameters: {'n_estimators': 695}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:17:11,242]\u001b[0m Trial 174 finished with value: 0.6813993109044675 and parameters: {'n_estimators': 610}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:19:40,707]\u001b[0m Trial 175 finished with value: 0.6818229216890265 and parameters: {'n_estimators': 667}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:21:23,331]\u001b[0m Trial 176 finished with value: 0.6815369649439204 and parameters: {'n_estimators': 455}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:23:45,870]\u001b[0m Trial 177 finished with value: 0.6815211662739883 and parameters: {'n_estimators': 634}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:25:17,969]\u001b[0m Trial 178 finished with value: 0.6810313002677884 and parameters: {'n_estimators': 407}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:27:45,294]\u001b[0m Trial 179 finished with value: 0.6816208583281599 and parameters: {'n_estimators': 656}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:29:39,902]\u001b[0m Trial 180 finished with value: 0.6816292334894227 and parameters: {'n_estimators': 513}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:31:39,874]\u001b[0m Trial 181 finished with value: 0.6817860815965948 and parameters: {'n_estimators': 536}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:33:41,717]\u001b[0m Trial 182 finished with value: 0.6817270940652794 and parameters: {'n_estimators': 541}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:35:54,109]\u001b[0m Trial 183 finished with value: 0.6813122335972588 and parameters: {'n_estimators': 591}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:38:26,850]\u001b[0m Trial 184 finished with value: 0.6820642473362771 and parameters: {'n_estimators': 680}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:40:49,963]\u001b[0m Trial 185 finished with value: 0.6814115169547443 and parameters: {'n_estimators': 642}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:42:46,798]\u001b[0m Trial 186 finished with value: 0.6818220102916344 and parameters: {'n_estimators': 521}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:45:22,747]\u001b[0m Trial 187 finished with value: 0.6823793681212266 and parameters: {'n_estimators': 705}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:47:27,287]\u001b[0m Trial 188 finished with value: 0.6817163101649505 and parameters: {'n_estimators': 558}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:49:43,037]\u001b[0m Trial 189 finished with value: 0.6813881522812035 and parameters: {'n_estimators': 612}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:52:09,543]\u001b[0m Trial 190 finished with value: 0.6817824820900709 and parameters: {'n_estimators': 665}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:54:20,122]\u001b[0m Trial 191 finished with value: 0.6813479576809975 and parameters: {'n_estimators': 588}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:56:28,692]\u001b[0m Trial 192 finished with value: 0.681465887220576 and parameters: {'n_estimators': 578}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:58:47,406]\u001b[0m Trial 193 finished with value: 0.6815861360434579 and parameters: {'n_estimators': 626}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:00:44,048]\u001b[0m Trial 194 finished with value: 0.6817134751167505 and parameters: {'n_estimators': 538}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:03:05,959]\u001b[0m Trial 195 finished with value: 0.6814662392055257 and parameters: {'n_estimators': 643}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 23:05:19,239]\u001b[0m Trial 196 finished with value: 0.6814854904414611 and parameters: {'n_estimators': 597}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:07:50,474]\u001b[0m Trial 197 finished with value: 0.681950612337745 and parameters: {'n_estimators': 679}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:10:06,065]\u001b[0m Trial 198 finished with value: 0.6815993518264412 and parameters: {'n_estimators': 618}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:12:30,235]\u001b[0m Trial 199 finished with value: 0.6816208583281599 and parameters: {'n_estimators': 656}. Best is trial 89 with value: 0.6997443390527953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6997\n",
      "\tBest params:\n",
      "\t\tn_estimators: 658\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.686187    0.735183    0.712420    0.725186\n",
      "1                    TP   34.000000   36.000000   32.000000   35.000000\n",
      "2                    TN  306.000000  309.000000  303.000000  304.000000\n",
      "3                    FP    7.000000    5.000000   11.000000   11.000000\n",
      "4                    FN   35.000000   32.000000   36.000000   32.000000\n",
      "5              Accuracy    0.890052    0.903141    0.876963    0.887435\n",
      "6             Precision    0.829268    0.878049    0.744186    0.760870\n",
      "7           Sensitivity    0.492754    0.529412    0.470588    0.522388\n",
      "8           Specificity    0.977600    0.984100    0.965000    0.965100\n",
      "9              F1 score    0.618182    0.660550    0.576577    0.619469\n",
      "10  F1 score (weighted)    0.878413    0.893141    0.865463    0.878791\n",
      "11     F1 score (macro)    0.776981    0.802031    0.752301    0.776708\n",
      "12    Balanced Accuracy    0.735195    0.756744    0.717778    0.743734\n",
      "13                  MCC    0.584635    0.634572    0.527144    0.569626\n",
      "14                  NPV    0.897400    0.906200    0.893800    0.904800\n",
      "15              ROC_AUC    0.735195    0.756744    0.717778    0.743734\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_rf_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_3_cat = np.where(((y_pred_rf_3 >= 2) | (y_pred_rf_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_rf_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 23:14:45,925]\u001b[0m Trial 200 finished with value: 0.7126520697731722 and parameters: {'n_estimators': 553}. Best is trial 200 with value: 0.7126520697731722.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:16:49,114]\u001b[0m Trial 201 finished with value: 0.7124637235297874 and parameters: {'n_estimators': 570}. Best is trial 200 with value: 0.7126520697731722.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:18:49,943]\u001b[0m Trial 202 finished with value: 0.7125444169057558 and parameters: {'n_estimators': 561}. Best is trial 200 with value: 0.7126520697731722.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:20:36,451]\u001b[0m Trial 203 finished with value: 0.712923377978599 and parameters: {'n_estimators': 497}. Best is trial 203 with value: 0.712923377978599.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:22:23,641]\u001b[0m Trial 204 finished with value: 0.7128906984661062 and parameters: {'n_estimators': 502}. Best is trial 203 with value: 0.712923377978599.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:24:10,364]\u001b[0m Trial 205 finished with value: 0.7129939862138195 and parameters: {'n_estimators': 496}. Best is trial 205 with value: 0.7129939862138195.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:25:51,600]\u001b[0m Trial 206 finished with value: 0.7130219357802193 and parameters: {'n_estimators': 472}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:27:32,774]\u001b[0m Trial 207 finished with value: 0.7129610270481314 and parameters: {'n_estimators': 473}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:29:18,622]\u001b[0m Trial 208 finished with value: 0.7130035980593934 and parameters: {'n_estimators': 492}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:31:04,550]\u001b[0m Trial 209 finished with value: 0.7130035980593934 and parameters: {'n_estimators': 492}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:32:50,794]\u001b[0m Trial 210 finished with value: 0.7129939862138195 and parameters: {'n_estimators': 496}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:34:33,081]\u001b[0m Trial 211 finished with value: 0.7129325586916442 and parameters: {'n_estimators': 476}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:36:14,186]\u001b[0m Trial 212 finished with value: 0.7130219357802193 and parameters: {'n_estimators': 472}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:37:59,342]\u001b[0m Trial 213 finished with value: 0.7128574981205765 and parameters: {'n_estimators': 485}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:39:46,441]\u001b[0m Trial 214 finished with value: 0.712928129432482 and parameters: {'n_estimators': 493}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:41:32,004]\u001b[0m Trial 215 finished with value: 0.7130035980593934 and parameters: {'n_estimators': 492}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:43:15,248]\u001b[0m Trial 216 finished with value: 0.7129325586916442 and parameters: {'n_estimators': 476}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:45:01,744]\u001b[0m Trial 217 finished with value: 0.7129483470213859 and parameters: {'n_estimators': 494}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:46:47,404]\u001b[0m Trial 218 finished with value: 0.7130170599533732 and parameters: {'n_estimators': 491}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:48:34,637]\u001b[0m Trial 219 finished with value: 0.712948347021386 and parameters: {'n_estimators': 494}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:50:21,531]\u001b[0m Trial 220 finished with value: 0.7129483470213859 and parameters: {'n_estimators': 494}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:52:08,302]\u001b[0m Trial 221 finished with value: 0.712923377978599 and parameters: {'n_estimators': 497}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:53:51,231]\u001b[0m Trial 222 finished with value: 0.7129272679238783 and parameters: {'n_estimators': 474}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:55:36,973]\u001b[0m Trial 223 finished with value: 0.7130093483919733 and parameters: {'n_estimators': 495}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:57:20,266]\u001b[0m Trial 224 finished with value: 0.7130113579869627 and parameters: {'n_estimators': 480}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:59:07,100]\u001b[0m Trial 225 finished with value: 0.7129939862138195 and parameters: {'n_estimators': 496}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:00:50,108]\u001b[0m Trial 226 finished with value: 0.7128515670281946 and parameters: {'n_estimators': 475}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:02:36,077]\u001b[0m Trial 227 finished with value: 0.7130035980593934 and parameters: {'n_estimators': 492}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:04:23,579]\u001b[0m Trial 228 finished with value: 0.712923377978599 and parameters: {'n_estimators': 497}. Best is trial 206 with value: 0.7130219357802193.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:06:04,205]\u001b[0m Trial 229 finished with value: 0.7130333583771761 and parameters: {'n_estimators': 461}. Best is trial 229 with value: 0.7130333583771761.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:07:43,035]\u001b[0m Trial 230 finished with value: 0.7128965357349795 and parameters: {'n_estimators': 456}. Best is trial 229 with value: 0.7130333583771761.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:09:29,422]\u001b[0m Trial 231 finished with value: 0.7130035980593934 and parameters: {'n_estimators': 492}. Best is trial 229 with value: 0.7130333583771761.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:11:11,130]\u001b[0m Trial 232 finished with value: 0.7130163372570154 and parameters: {'n_estimators': 471}. Best is trial 229 with value: 0.7130333583771761.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:12:51,755]\u001b[0m Trial 233 finished with value: 0.712898605625285 and parameters: {'n_estimators': 466}. Best is trial 229 with value: 0.7130333583771761.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:14:35,114]\u001b[0m Trial 234 finished with value: 0.7128515670281946 and parameters: {'n_estimators': 475}. Best is trial 229 with value: 0.7130333583771761.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:16:18,427]\u001b[0m Trial 235 finished with value: 0.7130113579869627 and parameters: {'n_estimators': 480}. Best is trial 229 with value: 0.7130333583771761.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:17:58,662]\u001b[0m Trial 236 finished with value: 0.7130598708794997 and parameters: {'n_estimators': 464}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:19:49,947]\u001b[0m Trial 237 finished with value: 0.7128623341975501 and parameters: {'n_estimators': 520}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:21:27,740]\u001b[0m Trial 238 finished with value: 0.7129074081350302 and parameters: {'n_estimators': 454}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:23:12,535]\u001b[0m Trial 239 finished with value: 0.7128574981205765 and parameters: {'n_estimators': 485}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:25:02,722]\u001b[0m Trial 240 finished with value: 0.7128712359855236 and parameters: {'n_estimators': 509}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:26:44,154]\u001b[0m Trial 241 finished with value: 0.7129525785760287 and parameters: {'n_estimators': 469}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:28:24,504]\u001b[0m Trial 242 finished with value: 0.7130598708794997 and parameters: {'n_estimators': 464}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:30:06,143]\u001b[0m Trial 243 finished with value: 0.7129659958227228 and parameters: {'n_estimators': 470}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:31:46,084]\u001b[0m Trial 244 finished with value: 0.7130085611760981 and parameters: {'n_estimators': 462}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:33:24,285]\u001b[0m Trial 245 finished with value: 0.7128580121590324 and parameters: {'n_estimators': 455}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 00:35:03,401]\u001b[0m Trial 246 finished with value: 0.7129313165383715 and parameters: {'n_estimators': 459}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:36:54,375]\u001b[0m Trial 247 finished with value: 0.7128162768448718 and parameters: {'n_estimators': 515}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:38:34,712]\u001b[0m Trial 248 finished with value: 0.7129836239960647 and parameters: {'n_estimators': 465}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:40:08,917]\u001b[0m Trial 249 finished with value: 0.7128960574972302 and parameters: {'n_estimators': 439}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7131\n",
      "\tBest params:\n",
      "\t\tn_estimators: 464\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77894dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686187    0.735183    0.712420    0.725186   \n",
      "1                    TP   34.000000   36.000000   32.000000   35.000000   \n",
      "2                    TN  306.000000  309.000000  303.000000  304.000000   \n",
      "3                    FP    7.000000    5.000000   11.000000   11.000000   \n",
      "4                    FN   35.000000   32.000000   36.000000   32.000000   \n",
      "5              Accuracy    0.890052    0.903141    0.876963    0.887435   \n",
      "6             Precision    0.829268    0.878049    0.744186    0.760870   \n",
      "7           Sensitivity    0.492754    0.529412    0.470588    0.522388   \n",
      "8           Specificity    0.977600    0.984100    0.965000    0.965100   \n",
      "9              F1 score    0.618182    0.660550    0.576577    0.619469   \n",
      "10  F1 score (weighted)    0.878413    0.893141    0.865463    0.878791   \n",
      "11     F1 score (macro)    0.776981    0.802031    0.752301    0.776708   \n",
      "12    Balanced Accuracy    0.735195    0.756744    0.717778    0.743734   \n",
      "13                  MCC    0.584635    0.634572    0.527144    0.569626   \n",
      "14                  NPV    0.897400    0.906200    0.893800    0.904800   \n",
      "15              ROC_AUC    0.735195    0.756744    0.717778    0.743734   \n",
      "\n",
      "          Set4  \n",
      "0     0.632286  \n",
      "1    31.000000  \n",
      "2   306.000000  \n",
      "3     9.000000  \n",
      "4    36.000000  \n",
      "5     0.882199  \n",
      "6     0.775000  \n",
      "7     0.462687  \n",
      "8     0.971400  \n",
      "9     0.579439  \n",
      "10    0.869757  \n",
      "11    0.755473  \n",
      "12    0.717058  \n",
      "13    0.539206  \n",
      "14    0.894700  \n",
      "15    0.717058  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_rf_4)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_4_cat = np.where(((y_pred_rf_4 >= 2) | (y_pred_rf_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_rf_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 00:41:58,598]\u001b[0m Trial 250 finished with value: 0.6891278806431 and parameters: {'n_estimators': 463}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:43:42,535]\u001b[0m Trial 251 finished with value: 0.6894053324325069 and parameters: {'n_estimators': 487}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:45:13,475]\u001b[0m Trial 252 finished with value: 0.6890352870370359 and parameters: {'n_estimators': 423}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:46:48,047]\u001b[0m Trial 253 finished with value: 0.6892049124381407 and parameters: {'n_estimators': 443}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:48:38,618]\u001b[0m Trial 254 finished with value: 0.6897526565350275 and parameters: {'n_estimators': 524}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:50:19,630]\u001b[0m Trial 255 finished with value: 0.6891981491947977 and parameters: {'n_estimators': 469}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:52:06,975]\u001b[0m Trial 256 finished with value: 0.6896994231758352 and parameters: {'n_estimators': 509}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:53:50,234]\u001b[0m Trial 257 finished with value: 0.6893452183765193 and parameters: {'n_estimators': 484}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:55:28,198]\u001b[0m Trial 258 finished with value: 0.6889859774746039 and parameters: {'n_estimators': 459}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:57:15,414]\u001b[0m Trial 259 finished with value: 0.689639023842568 and parameters: {'n_estimators': 502}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:58:55,714]\u001b[0m Trial 260 finished with value: 0.6892114072350759 and parameters: {'n_estimators': 472}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:00:30,417]\u001b[0m Trial 261 finished with value: 0.6891850342322294 and parameters: {'n_estimators': 445}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:02:13,868]\u001b[0m Trial 262 finished with value: 0.6894452776683703 and parameters: {'n_estimators': 488}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:04:04,945]\u001b[0m Trial 263 finished with value: 0.6895870730123859 and parameters: {'n_estimators': 522}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:05:44,009]\u001b[0m Trial 264 finished with value: 0.689101717626553 and parameters: {'n_estimators': 464}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:07:30,507]\u001b[0m Trial 265 finished with value: 0.689639023842568 and parameters: {'n_estimators': 502}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:09:00,141]\u001b[0m Trial 266 finished with value: 0.6890352870370359 and parameters: {'n_estimators': 423}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:10:43,417]\u001b[0m Trial 267 finished with value: 0.6893210301969184 and parameters: {'n_estimators': 483}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:12:18,119]\u001b[0m Trial 268 finished with value: 0.6891396544575852 and parameters: {'n_estimators': 446}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:13:58,241]\u001b[0m Trial 269 finished with value: 0.6891981491947977 and parameters: {'n_estimators': 469}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:15:50,734]\u001b[0m Trial 270 finished with value: 0.68977971959826 and parameters: {'n_estimators': 528}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:17:38,651]\u001b[0m Trial 271 finished with value: 0.6897005446281473 and parameters: {'n_estimators': 508}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:19:21,585]\u001b[0m Trial 272 finished with value: 0.6893856635704998 and parameters: {'n_estimators': 485}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:20:57,799]\u001b[0m Trial 273 finished with value: 0.6889840569108736 and parameters: {'n_estimators': 452}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:22:42,554]\u001b[0m Trial 274 finished with value: 0.6893668107332314 and parameters: {'n_estimators': 492}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:24:22,146]\u001b[0m Trial 275 finished with value: 0.689089841387102 and parameters: {'n_estimators': 468}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:26:11,205]\u001b[0m Trial 276 finished with value: 0.6896299961329124 and parameters: {'n_estimators': 507}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:27:43,271]\u001b[0m Trial 277 finished with value: 0.6892096835858688 and parameters: {'n_estimators': 429}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:29:24,914]\u001b[0m Trial 278 finished with value: 0.6892597673541153 and parameters: {'n_estimators': 481}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:31:00,251]\u001b[0m Trial 279 finished with value: 0.6891023811659227 and parameters: {'n_estimators': 449}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:32:51,707]\u001b[0m Trial 280 finished with value: 0.689746638294705 and parameters: {'n_estimators': 526}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:34:37,638]\u001b[0m Trial 281 finished with value: 0.6894897735759435 and parameters: {'n_estimators': 497}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:36:16,870]\u001b[0m Trial 282 finished with value: 0.6891111649915277 and parameters: {'n_estimators': 467}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:37:59,110]\u001b[0m Trial 283 finished with value: 0.6892597673541153 and parameters: {'n_estimators': 481}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:39:25,970]\u001b[0m Trial 284 finished with value: 0.6889816686456081 and parameters: {'n_estimators': 407}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:41:13,810]\u001b[0m Trial 285 finished with value: 0.6896994231758352 and parameters: {'n_estimators': 509}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:42:46,802]\u001b[0m Trial 286 finished with value: 0.6892303964845465 and parameters: {'n_estimators': 436}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:44:25,014]\u001b[0m Trial 287 finished with value: 0.6889859774746039 and parameters: {'n_estimators': 459}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:46:09,256]\u001b[0m Trial 288 finished with value: 0.6893668107332314 and parameters: {'n_estimators': 492}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:47:50,588]\u001b[0m Trial 289 finished with value: 0.6892861665828257 and parameters: {'n_estimators': 471}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:49:40,274]\u001b[0m Trial 290 finished with value: 0.6895727273950552 and parameters: {'n_estimators': 514}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:51:33,528]\u001b[0m Trial 291 finished with value: 0.6897993652899537 and parameters: {'n_estimators': 533}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:53:09,218]\u001b[0m Trial 292 finished with value: 0.6889840569108736 and parameters: {'n_estimators': 452}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:54:52,238]\u001b[0m Trial 293 finished with value: 0.6894053324325069 and parameters: {'n_estimators': 487}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:56:39,674]\u001b[0m Trial 294 finished with value: 0.6895791376833488 and parameters: {'n_estimators': 500}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:58:19,962]\u001b[0m Trial 295 finished with value: 0.689089841387102 and parameters: {'n_estimators': 468}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 01:59:52,883]\u001b[0m Trial 296 finished with value: 0.6892303964845465 and parameters: {'n_estimators': 436}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:01:34,537]\u001b[0m Trial 297 finished with value: 0.6892850151208354 and parameters: {'n_estimators': 477}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:03:24,373]\u001b[0m Trial 298 finished with value: 0.6895812070317424 and parameters: {'n_estimators': 513}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:05:02,277]\u001b[0m Trial 299 finished with value: 0.6890256191498965 and parameters: {'n_estimators': 455}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7131\n",
      "\tBest params:\n",
      "\t\tn_estimators: 464\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686187    0.735183    0.712420    0.725186   \n",
      "1                    TP   34.000000   36.000000   32.000000   35.000000   \n",
      "2                    TN  306.000000  309.000000  303.000000  304.000000   \n",
      "3                    FP    7.000000    5.000000   11.000000   11.000000   \n",
      "4                    FN   35.000000   32.000000   36.000000   32.000000   \n",
      "5              Accuracy    0.890052    0.903141    0.876963    0.887435   \n",
      "6             Precision    0.829268    0.878049    0.744186    0.760870   \n",
      "7           Sensitivity    0.492754    0.529412    0.470588    0.522388   \n",
      "8           Specificity    0.977600    0.984100    0.965000    0.965100   \n",
      "9              F1 score    0.618182    0.660550    0.576577    0.619469   \n",
      "10  F1 score (weighted)    0.878413    0.893141    0.865463    0.878791   \n",
      "11     F1 score (macro)    0.776981    0.802031    0.752301    0.776708   \n",
      "12    Balanced Accuracy    0.735195    0.756744    0.717778    0.743734   \n",
      "13                  MCC    0.584635    0.634572    0.527144    0.569626   \n",
      "14                  NPV    0.897400    0.906200    0.893800    0.904800   \n",
      "15              ROC_AUC    0.735195    0.756744    0.717778    0.743734   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.632286    0.679581  \n",
      "1    31.000000   35.000000  \n",
      "2   306.000000  304.000000  \n",
      "3     9.000000    8.000000  \n",
      "4    36.000000   35.000000  \n",
      "5     0.882199    0.887435  \n",
      "6     0.775000    0.813953  \n",
      "7     0.462687    0.500000  \n",
      "8     0.971400    0.974400  \n",
      "9     0.579439    0.619469  \n",
      "10    0.869757    0.876321  \n",
      "11    0.755473    0.776708  \n",
      "12    0.717058    0.737179  \n",
      "13    0.539206    0.580630  \n",
      "14    0.894700    0.896800  \n",
      "15    0.717058    0.737179  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_rf_5)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_5_cat = np.where(((y_pred_rf_5 >= 2) | (y_pred_rf_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_rf_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 02:06:55,733]\u001b[0m Trial 300 finished with value: 0.6801608265405417 and parameters: {'n_estimators': 493}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:08:35,280]\u001b[0m Trial 301 finished with value: 0.6800627078040973 and parameters: {'n_estimators': 481}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:10:26,101]\u001b[0m Trial 302 finished with value: 0.6805336607886604 and parameters: {'n_estimators': 532}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:11:53,374]\u001b[0m Trial 303 finished with value: 0.6802354114274006 and parameters: {'n_estimators': 419}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:13:26,110]\u001b[0m Trial 304 finished with value: 0.6800036366555606 and parameters: {'n_estimators': 446}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:15:02,938]\u001b[0m Trial 305 finished with value: 0.6798332425748287 and parameters: {'n_estimators': 466}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:16:47,546]\u001b[0m Trial 306 finished with value: 0.6800986860933761 and parameters: {'n_estimators': 504}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:18:27,885]\u001b[0m Trial 307 finished with value: 0.6800842477122216 and parameters: {'n_estimators': 483}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:20:14,497]\u001b[0m Trial 308 finished with value: 0.680201701898874 and parameters: {'n_estimators': 516}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:21:50,286]\u001b[0m Trial 309 finished with value: 0.6799272717253758 and parameters: {'n_estimators': 458}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:23:32,745]\u001b[0m Trial 310 finished with value: 0.6800725159180555 and parameters: {'n_estimators': 490}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:25:05,230]\u001b[0m Trial 311 finished with value: 0.6798026804991759 and parameters: {'n_estimators': 439}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:26:43,895]\u001b[0m Trial 312 finished with value: 0.6798467176947012 and parameters: {'n_estimators': 471}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:28:28,119]\u001b[0m Trial 313 finished with value: 0.6801044240246937 and parameters: {'n_estimators': 501}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:30:19,444]\u001b[0m Trial 314 finished with value: 0.6804665073098557 and parameters: {'n_estimators': 531}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:32:00,297]\u001b[0m Trial 315 finished with value: 0.6800862528201971 and parameters: {'n_estimators': 478}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:33:24,317]\u001b[0m Trial 316 finished with value: 0.6803027568509674 and parameters: {'n_estimators': 397}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:34:57,923]\u001b[0m Trial 317 finished with value: 0.6799197884967274 and parameters: {'n_estimators': 453}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:36:41,383]\u001b[0m Trial 318 finished with value: 0.6801494785997096 and parameters: {'n_estimators': 496}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:38:29,361]\u001b[0m Trial 319 finished with value: 0.6802205844715699 and parameters: {'n_estimators': 515}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:39:59,773]\u001b[0m Trial 320 finished with value: 0.6796080940720203 and parameters: {'n_estimators': 432}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:41:37,958]\u001b[0m Trial 321 finished with value: 0.679802857234276 and parameters: {'n_estimators': 468}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:43:19,046]\u001b[0m Trial 322 finished with value: 0.6801297131136218 and parameters: {'n_estimators': 485}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:44:55,054]\u001b[0m Trial 323 finished with value: 0.6800447775664406 and parameters: {'n_estimators': 462}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:46:40,337]\u001b[0m Trial 324 finished with value: 0.6801464294144186 and parameters: {'n_estimators': 506}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:48:33,228]\u001b[0m Trial 325 finished with value: 0.6806136349169268 and parameters: {'n_estimators': 540}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:50:06,219]\u001b[0m Trial 326 finished with value: 0.6799473640187903 and parameters: {'n_estimators': 445}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:51:46,040]\u001b[0m Trial 327 finished with value: 0.6798916252735805 and parameters: {'n_estimators': 476}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:53:29,612]\u001b[0m Trial 328 finished with value: 0.6801865735848891 and parameters: {'n_estimators': 492}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:54:01,225]\u001b[0m Trial 329 finished with value: 0.6778454742971864 and parameters: {'n_estimators': 146}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:55:49,357]\u001b[0m Trial 330 finished with value: 0.6802472671669586 and parameters: {'n_estimators': 517}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:57:18,590]\u001b[0m Trial 331 finished with value: 0.6800016466038061 and parameters: {'n_estimators': 425}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:58:53,839]\u001b[0m Trial 332 finished with value: 0.6799526777013197 and parameters: {'n_estimators': 457}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:00:37,537]\u001b[0m Trial 333 finished with value: 0.6801511922290644 and parameters: {'n_estimators': 497}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:02:17,207]\u001b[0m Trial 334 finished with value: 0.6800013965495466 and parameters: {'n_estimators': 477}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:04:04,171]\u001b[0m Trial 335 finished with value: 0.680436371616995 and parameters: {'n_estimators': 523}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:05:37,627]\u001b[0m Trial 336 finished with value: 0.6798332425748285 and parameters: {'n_estimators': 466}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:07:07,179]\u001b[0m Trial 337 finished with value: 0.6798034535126913 and parameters: {'n_estimators': 442}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:08:45,659]\u001b[0m Trial 338 finished with value: 0.6800725159180555 and parameters: {'n_estimators': 490}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:10:24,856]\u001b[0m Trial 339 finished with value: 0.6800986860933761 and parameters: {'n_estimators': 504}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:11:57,878]\u001b[0m Trial 340 finished with value: 0.6798916252735804 and parameters: {'n_estimators': 476}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:13:26,915]\u001b[0m Trial 341 finished with value: 0.6799197884967274 and parameters: {'n_estimators': 453}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:14:48,089]\u001b[0m Trial 342 finished with value: 0.680097423514262 and parameters: {'n_estimators': 414}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:16:31,285]\u001b[0m Trial 343 finished with value: 0.6803593924000726 and parameters: {'n_estimators': 522}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:18:09,654]\u001b[0m Trial 344 finished with value: 0.6800916649865714 and parameters: {'n_estimators': 488}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:19:59,730]\u001b[0m Trial 345 finished with value: 0.6805601885193588 and parameters: {'n_estimators': 544}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:21:32,929]\u001b[0m Trial 346 finished with value: 0.6800447775664406 and parameters: {'n_estimators': 462}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:05,716]\u001b[0m Trial 347 finished with value: 0.6800986860933761 and parameters: {'n_estimators': 504}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:26:00,471]\u001b[0m Trial 348 finished with value: 0.6811757085861964 and parameters: {'n_estimators': 968}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:27:14,770]\u001b[0m Trial 349 finished with value: 0.6797524423324142 and parameters: {'n_estimators': 437}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7131\n",
      "\tBest params:\n",
      "\t\tn_estimators: 464\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686187    0.735183    0.712420    0.725186   \n",
      "1                    TP   34.000000   36.000000   32.000000   35.000000   \n",
      "2                    TN  306.000000  309.000000  303.000000  304.000000   \n",
      "3                    FP    7.000000    5.000000   11.000000   11.000000   \n",
      "4                    FN   35.000000   32.000000   36.000000   32.000000   \n",
      "5              Accuracy    0.890052    0.903141    0.876963    0.887435   \n",
      "6             Precision    0.829268    0.878049    0.744186    0.760870   \n",
      "7           Sensitivity    0.492754    0.529412    0.470588    0.522388   \n",
      "8           Specificity    0.977600    0.984100    0.965000    0.965100   \n",
      "9              F1 score    0.618182    0.660550    0.576577    0.619469   \n",
      "10  F1 score (weighted)    0.878413    0.893141    0.865463    0.878791   \n",
      "11     F1 score (macro)    0.776981    0.802031    0.752301    0.776708   \n",
      "12    Balanced Accuracy    0.735195    0.756744    0.717778    0.743734   \n",
      "13                  MCC    0.584635    0.634572    0.527144    0.569626   \n",
      "14                  NPV    0.897400    0.906200    0.893800    0.904800   \n",
      "15              ROC_AUC    0.735195    0.756744    0.717778    0.743734   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.632286    0.679581    0.673585  \n",
      "1    31.000000   35.000000   31.000000  \n",
      "2   306.000000  304.000000  310.000000  \n",
      "3     9.000000    8.000000    5.000000  \n",
      "4    36.000000   35.000000   36.000000  \n",
      "5     0.882199    0.887435    0.892670  \n",
      "6     0.775000    0.813953    0.861111  \n",
      "7     0.462687    0.500000    0.462687  \n",
      "8     0.971400    0.974400    0.984100  \n",
      "9     0.579439    0.619469    0.601942  \n",
      "10    0.869757    0.876321    0.879035  \n",
      "11    0.755473    0.776708    0.769957  \n",
      "12    0.717058    0.737179    0.723407  \n",
      "13    0.539206    0.580630    0.581607  \n",
      "14    0.894700    0.896800    0.896000  \n",
      "15    0.717058    0.737179    0.723407  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_rf_6)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_6_cat = np.where(((y_pred_rf_6 >= 2) | (y_pred_rf_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_rf_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:28:44,433]\u001b[0m Trial 350 finished with value: 0.6957363798287928 and parameters: {'n_estimators': 476}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:30:08,877]\u001b[0m Trial 351 finished with value: 0.696035132405291 and parameters: {'n_estimators': 498}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:31:25,571]\u001b[0m Trial 352 finished with value: 0.6956224075488409 and parameters: {'n_estimators': 455}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:32:45,972]\u001b[0m Trial 353 finished with value: 0.6957157021804184 and parameters: {'n_estimators': 475}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:34:12,371]\u001b[0m Trial 354 finished with value: 0.6961091636840598 and parameters: {'n_estimators': 514}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:35:34,718]\u001b[0m Trial 355 finished with value: 0.6960125350022695 and parameters: {'n_estimators': 489}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:36:52,927]\u001b[0m Trial 356 finished with value: 0.6957954739167168 and parameters: {'n_estimators': 463}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:38:04,403]\u001b[0m Trial 357 finished with value: 0.6956262083993794 and parameters: {'n_estimators': 425}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:39:33,596]\u001b[0m Trial 358 finished with value: 0.6962256798439947 and parameters: {'n_estimators': 530}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:40:55,025]\u001b[0m Trial 359 finished with value: 0.6957768219286333 and parameters: {'n_estimators': 484}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:42:10,499]\u001b[0m Trial 360 finished with value: 0.6955374907236112 and parameters: {'n_estimators': 448}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:43:35,321]\u001b[0m Trial 361 finished with value: 0.6961127231012856 and parameters: {'n_estimators': 505}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:44:54,382]\u001b[0m Trial 362 finished with value: 0.6957341350287833 and parameters: {'n_estimators': 472}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:46:17,663]\u001b[0m Trial 363 finished with value: 0.6960168292348331 and parameters: {'n_estimators': 494}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:47:33,212]\u001b[0m Trial 364 finished with value: 0.6955466057827875 and parameters: {'n_estimators': 449}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:48:58,553]\u001b[0m Trial 365 finished with value: 0.6961382002938207 and parameters: {'n_estimators': 508}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:50:17,389]\u001b[0m Trial 366 finished with value: 0.6959755386166294 and parameters: {'n_estimators': 466}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:51:45,778]\u001b[0m Trial 367 finished with value: 0.6962287919079767 and parameters: {'n_estimators': 526}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:53:06,642]\u001b[0m Trial 368 finished with value: 0.6958867579364518 and parameters: {'n_estimators': 482}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:54:21,375]\u001b[0m Trial 369 finished with value: 0.6954951998268901 and parameters: {'n_estimators': 442}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:55:44,289]\u001b[0m Trial 370 finished with value: 0.6960044580575341 and parameters: {'n_estimators': 493}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:57:02,770]\u001b[0m Trial 371 finished with value: 0.6958867429587852 and parameters: {'n_estimators': 464}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:58:33,946]\u001b[0m Trial 372 finished with value: 0.6963115247671509 and parameters: {'n_estimators': 541}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:59:59,232]\u001b[0m Trial 373 finished with value: 0.6960636185034105 and parameters: {'n_estimators': 507}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:01:08,209]\u001b[0m Trial 374 finished with value: 0.6956724978546552 and parameters: {'n_estimators': 409}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:02:28,631]\u001b[0m Trial 375 finished with value: 0.6957531977750835 and parameters: {'n_estimators': 478}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:03:31,287]\u001b[0m Trial 376 finished with value: 0.6958669393565393 and parameters: {'n_estimators': 371}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:04:43,845]\u001b[0m Trial 377 finished with value: 0.6955419406259237 and parameters: {'n_estimators': 430}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:06:11,835]\u001b[0m Trial 378 finished with value: 0.6962424279518815 and parameters: {'n_estimators': 517}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:07:28,970]\u001b[0m Trial 379 finished with value: 0.6957089403248602 and parameters: {'n_estimators': 458}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:08:51,607]\u001b[0m Trial 380 finished with value: 0.6958374106382392 and parameters: {'n_estimators': 487}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:10:10,896]\u001b[0m Trial 381 finished with value: 0.6958188374582567 and parameters: {'n_estimators': 469}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:11:33,772]\u001b[0m Trial 382 finished with value: 0.6960044580575341 and parameters: {'n_estimators': 493}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:12:49,612]\u001b[0m Trial 383 finished with value: 0.6955466057827875 and parameters: {'n_estimators': 449}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:14:16,957]\u001b[0m Trial 384 finished with value: 0.6962230111174923 and parameters: {'n_estimators': 516}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:15:37,292]\u001b[0m Trial 385 finished with value: 0.6957582266854969 and parameters: {'n_estimators': 477}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:16:50,497]\u001b[0m Trial 386 finished with value: 0.6955882110498648 and parameters: {'n_estimators': 433}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:19:13,802]\u001b[0m Trial 387 finished with value: 0.6971713635183134 and parameters: {'n_estimators': 851}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:20:38,108]\u001b[0m Trial 388 finished with value: 0.6960532349200852 and parameters: {'n_estimators': 501}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:21:55,724]\u001b[0m Trial 389 finished with value: 0.6957305635084262 and parameters: {'n_estimators': 462}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:23:27,068]\u001b[0m Trial 390 finished with value: 0.6965159261317794 and parameters: {'n_estimators': 545}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:24:49,011]\u001b[0m Trial 391 finished with value: 0.6958070545303094 and parameters: {'n_estimators': 485}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:26:03,927]\u001b[0m Trial 392 finished with value: 0.6955657333258431 and parameters: {'n_estimators': 446}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:27:32,070]\u001b[0m Trial 393 finished with value: 0.6962214370263355 and parameters: {'n_estimators': 528}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:28:57,178]\u001b[0m Trial 394 finished with value: 0.6961004186604807 and parameters: {'n_estimators': 504}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:30:16,473]\u001b[0m Trial 395 finished with value: 0.6957407779216398 and parameters: {'n_estimators': 471}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 04:31:38,801]\u001b[0m Trial 396 finished with value: 0.6958374106382392 and parameters: {'n_estimators': 487}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:32:49,227]\u001b[0m Trial 397 finished with value: 0.6955372139026226 and parameters: {'n_estimators': 418}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:34:06,944]\u001b[0m Trial 398 finished with value: 0.6957025742447428 and parameters: {'n_estimators': 459}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:35:33,909]\u001b[0m Trial 399 finished with value: 0.6961091636840598 and parameters: {'n_estimators': 514}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7131\n",
      "\tBest params:\n",
      "\t\tn_estimators: 464\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686187    0.735183    0.712420    0.725186   \n",
      "1                    TP   34.000000   36.000000   32.000000   35.000000   \n",
      "2                    TN  306.000000  309.000000  303.000000  304.000000   \n",
      "3                    FP    7.000000    5.000000   11.000000   11.000000   \n",
      "4                    FN   35.000000   32.000000   36.000000   32.000000   \n",
      "5              Accuracy    0.890052    0.903141    0.876963    0.887435   \n",
      "6             Precision    0.829268    0.878049    0.744186    0.760870   \n",
      "7           Sensitivity    0.492754    0.529412    0.470588    0.522388   \n",
      "8           Specificity    0.977600    0.984100    0.965000    0.965100   \n",
      "9              F1 score    0.618182    0.660550    0.576577    0.619469   \n",
      "10  F1 score (weighted)    0.878413    0.893141    0.865463    0.878791   \n",
      "11     F1 score (macro)    0.776981    0.802031    0.752301    0.776708   \n",
      "12    Balanced Accuracy    0.735195    0.756744    0.717778    0.743734   \n",
      "13                  MCC    0.584635    0.634572    0.527144    0.569626   \n",
      "14                  NPV    0.897400    0.906200    0.893800    0.904800   \n",
      "15              ROC_AUC    0.735195    0.756744    0.717778    0.743734   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.632286    0.679581    0.673585    0.722280  \n",
      "1    31.000000   35.000000   31.000000   35.000000  \n",
      "2   306.000000  304.000000  310.000000  307.000000  \n",
      "3     9.000000    8.000000    5.000000    7.000000  \n",
      "4    36.000000   35.000000   36.000000   33.000000  \n",
      "5     0.882199    0.887435    0.892670    0.895288  \n",
      "6     0.775000    0.813953    0.861111    0.833333  \n",
      "7     0.462687    0.500000    0.462687    0.514706  \n",
      "8     0.971400    0.974400    0.984100    0.977700  \n",
      "9     0.579439    0.619469    0.601942    0.636364  \n",
      "10    0.869757    0.876321    0.879035    0.884994  \n",
      "11    0.755473    0.776708    0.769957    0.787601  \n",
      "12    0.717058    0.737179    0.723407    0.746206  \n",
      "13    0.539206    0.580630    0.581607    0.602122  \n",
      "14    0.894700    0.896800    0.896000    0.902900  \n",
      "15    0.717058    0.737179    0.723407    0.746206  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_rf_7)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_7_cat = np.where(((y_pred_rf_7 >= 2) | (y_pred_rf_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_rf_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 04:37:02,995]\u001b[0m Trial 400 finished with value: 0.6862659145157035 and parameters: {'n_estimators': 479}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:38:26,825]\u001b[0m Trial 401 finished with value: 0.6862101552822224 and parameters: {'n_estimators': 501}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:39:39,316]\u001b[0m Trial 402 finished with value: 0.6862475373646328 and parameters: {'n_estimators': 433}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:40:56,002]\u001b[0m Trial 403 finished with value: 0.6860500702208683 and parameters: {'n_estimators': 459}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:42:17,949]\u001b[0m Trial 404 finished with value: 0.6862346554477299 and parameters: {'n_estimators': 489}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:43:45,958]\u001b[0m Trial 405 finished with value: 0.6864849172225245 and parameters: {'n_estimators': 527}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:45:04,764]\u001b[0m Trial 406 finished with value: 0.6862489372350055 and parameters: {'n_estimators': 473}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:46:18,907]\u001b[0m Trial 407 finished with value: 0.6860620114305227 and parameters: {'n_estimators': 446}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:47:42,390]\u001b[0m Trial 408 finished with value: 0.686182549559003 and parameters: {'n_estimators': 500}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:48:48,178]\u001b[0m Trial 409 finished with value: 0.6858854039558282 and parameters: {'n_estimators': 395}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:50:06,918]\u001b[0m Trial 410 finished with value: 0.6862489372350055 and parameters: {'n_estimators': 473}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:51:32,244]\u001b[0m Trial 411 finished with value: 0.6864486272331936 and parameters: {'n_estimators': 514}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:52:53,429]\u001b[0m Trial 412 finished with value: 0.6862346554477299 and parameters: {'n_estimators': 489}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:54:09,782]\u001b[0m Trial 413 finished with value: 0.6860719882698846 and parameters: {'n_estimators': 454}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:54:45,245]\u001b[0m Trial 414 finished with value: 0.687477395775933 and parameters: {'n_estimators': 208}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:56:15,376]\u001b[0m Trial 415 finished with value: 0.6865523343700894 and parameters: {'n_estimators': 538}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:57:33,552]\u001b[0m Trial 416 finished with value: 0.6861176464882511 and parameters: {'n_estimators': 467}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:58:47,014]\u001b[0m Trial 417 finished with value: 0.6862371113901317 and parameters: {'n_estimators': 440}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:00:10,552]\u001b[0m Trial 418 finished with value: 0.6861835350067175 and parameters: {'n_estimators': 499}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:01:30,121]\u001b[0m Trial 419 finished with value: 0.6862659145157035 and parameters: {'n_estimators': 479}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:02:57,420]\u001b[0m Trial 420 finished with value: 0.6865241129459461 and parameters: {'n_estimators': 519}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:04:08,443]\u001b[0m Trial 421 finished with value: 0.686107480368087 and parameters: {'n_estimators': 423}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:05:24,283]\u001b[0m Trial 422 finished with value: 0.6861032812930539 and parameters: {'n_estimators': 456}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:06:46,760]\u001b[0m Trial 423 finished with value: 0.6862981652229129 and parameters: {'n_estimators': 496}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:08:19,195]\u001b[0m Trial 424 finished with value: 0.6867215509427409 and parameters: {'n_estimators': 552}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:09:39,360]\u001b[0m Trial 425 finished with value: 0.6863406375201668 and parameters: {'n_estimators': 475}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:11:05,427]\u001b[0m Trial 426 finished with value: 0.6863970712115319 and parameters: {'n_estimators': 511}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:12:27,217]\u001b[0m Trial 427 finished with value: 0.6861366409189946 and parameters: {'n_estimators': 488}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:13:41,406]\u001b[0m Trial 428 finished with value: 0.6861210964219024 and parameters: {'n_estimators': 443}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:14:58,533]\u001b[0m Trial 429 finished with value: 0.6862656760771794 and parameters: {'n_estimators': 463}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:16:26,670]\u001b[0m Trial 430 finished with value: 0.6865454158642949 and parameters: {'n_estimators': 529}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:17:46,559]\u001b[0m Trial 431 finished with value: 0.6862558784861109 and parameters: {'n_estimators': 480}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:19:09,694]\u001b[0m Trial 432 finished with value: 0.686182549559003 and parameters: {'n_estimators': 500}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:20:25,577]\u001b[0m Trial 433 finished with value: 0.6860460915051325 and parameters: {'n_estimators': 457}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:21:37,194]\u001b[0m Trial 434 finished with value: 0.6862561967011636 and parameters: {'n_estimators': 431}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:22:57,887]\u001b[0m Trial 435 finished with value: 0.6862558784861109 and parameters: {'n_estimators': 480}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:24:23,775]\u001b[0m Trial 436 finished with value: 0.6863157342925088 and parameters: {'n_estimators': 509}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:25:41,227]\u001b[0m Trial 437 finished with value: 0.6862656760771794 and parameters: {'n_estimators': 463}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:27:03,443]\u001b[0m Trial 438 finished with value: 0.6862632470128588 and parameters: {'n_estimators': 492}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:28:18,207]\u001b[0m Trial 439 finished with value: 0.6860399429868226 and parameters: {'n_estimators': 445}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:29:27,039]\u001b[0m Trial 440 finished with value: 0.6860068732011807 and parameters: {'n_estimators': 409}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:30:55,465]\u001b[0m Trial 441 finished with value: 0.6864569533986328 and parameters: {'n_estimators': 528}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:32:13,999]\u001b[0m Trial 442 finished with value: 0.6862886529528598 and parameters: {'n_estimators': 472}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:33:38,323]\u001b[0m Trial 443 finished with value: 0.6862433304821549 and parameters: {'n_estimators': 503}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:34:59,781]\u001b[0m Trial 444 finished with value: 0.686188193377226 and parameters: {'n_estimators': 487}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:36:16,699]\u001b[0m Trial 445 finished with value: 0.6861380432333801 and parameters: {'n_estimators': 461}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 05:37:42,297]\u001b[0m Trial 446 finished with value: 0.6865389195413905 and parameters: {'n_estimators': 515}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:38:56,517]\u001b[0m Trial 447 finished with value: 0.6860399429868226 and parameters: {'n_estimators': 445}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:40:16,846]\u001b[0m Trial 448 finished with value: 0.6863471325079658 and parameters: {'n_estimators': 477}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:41:39,841]\u001b[0m Trial 449 finished with value: 0.6862422203672298 and parameters: {'n_estimators': 493}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7131\n",
      "\tBest params:\n",
      "\t\tn_estimators: 464\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686187    0.735183    0.712420    0.725186   \n",
      "1                    TP   34.000000   36.000000   32.000000   35.000000   \n",
      "2                    TN  306.000000  309.000000  303.000000  304.000000   \n",
      "3                    FP    7.000000    5.000000   11.000000   11.000000   \n",
      "4                    FN   35.000000   32.000000   36.000000   32.000000   \n",
      "5              Accuracy    0.890052    0.903141    0.876963    0.887435   \n",
      "6             Precision    0.829268    0.878049    0.744186    0.760870   \n",
      "7           Sensitivity    0.492754    0.529412    0.470588    0.522388   \n",
      "8           Specificity    0.977600    0.984100    0.965000    0.965100   \n",
      "9              F1 score    0.618182    0.660550    0.576577    0.619469   \n",
      "10  F1 score (weighted)    0.878413    0.893141    0.865463    0.878791   \n",
      "11     F1 score (macro)    0.776981    0.802031    0.752301    0.776708   \n",
      "12    Balanced Accuracy    0.735195    0.756744    0.717778    0.743734   \n",
      "13                  MCC    0.584635    0.634572    0.527144    0.569626   \n",
      "14                  NPV    0.897400    0.906200    0.893800    0.904800   \n",
      "15              ROC_AUC    0.735195    0.756744    0.717778    0.743734   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.632286    0.679581    0.673585    0.722280    0.676301  \n",
      "1    31.000000   35.000000   31.000000   35.000000   34.000000  \n",
      "2   306.000000  304.000000  310.000000  307.000000  308.000000  \n",
      "3     9.000000    8.000000    5.000000    7.000000    5.000000  \n",
      "4    36.000000   35.000000   36.000000   33.000000   35.000000  \n",
      "5     0.882199    0.887435    0.892670    0.895288    0.895288  \n",
      "6     0.775000    0.813953    0.861111    0.833333    0.871795  \n",
      "7     0.462687    0.500000    0.462687    0.514706    0.492754  \n",
      "8     0.971400    0.974400    0.984100    0.977700    0.984000  \n",
      "9     0.579439    0.619469    0.601942    0.636364    0.629630  \n",
      "10    0.869757    0.876321    0.879035    0.884994    0.883139  \n",
      "11    0.755473    0.776708    0.769957    0.787601    0.784327  \n",
      "12    0.717058    0.737179    0.723407    0.746206    0.738390  \n",
      "13    0.539206    0.580630    0.581607    0.602122    0.605807  \n",
      "14    0.894700    0.896800    0.896000    0.902900    0.898000  \n",
      "15    0.717058    0.737179    0.723407    0.746206    0.738390  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_rf_8)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_8_cat = np.where(((y_pred_rf_8 >= 2) | (y_pred_rf_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_rf_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 05:43:20,505]\u001b[0m Trial 450 finished with value: 0.6840536075498647 and parameters: {'n_estimators': 544}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:44:34,104]\u001b[0m Trial 451 finished with value: 0.683851432736218 and parameters: {'n_estimators': 429}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:45:54,244]\u001b[0m Trial 452 finished with value: 0.6838592847174366 and parameters: {'n_estimators': 469}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:47:21,549]\u001b[0m Trial 453 finished with value: 0.6837276861271807 and parameters: {'n_estimators': 513}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:48:45,239]\u001b[0m Trial 454 finished with value: 0.6836872295164913 and parameters: {'n_estimators': 489}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:50:02,368]\u001b[0m Trial 455 finished with value: 0.6839678265823259 and parameters: {'n_estimators': 452}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:51:27,831]\u001b[0m Trial 456 finished with value: 0.6836400729048244 and parameters: {'n_estimators': 505}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:52:47,510]\u001b[0m Trial 457 finished with value: 0.683829986594004 and parameters: {'n_estimators': 473}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:54:16,189]\u001b[0m Trial 458 finished with value: 0.6840501316004491 and parameters: {'n_estimators': 527}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:55:34,198]\u001b[0m Trial 459 finished with value: 0.6840014868312327 and parameters: {'n_estimators': 460}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:56:56,696]\u001b[0m Trial 460 finished with value: 0.683708710657686 and parameters: {'n_estimators': 487}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:58:10,240]\u001b[0m Trial 461 finished with value: 0.6839139735833955 and parameters: {'n_estimators': 435}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 05:59:34,496]\u001b[0m Trial 462 finished with value: 0.6837099186970209 and parameters: {'n_estimators': 500}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:00:54,573]\u001b[0m Trial 463 finished with value: 0.683829986594004 and parameters: {'n_estimators': 473}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:02:11,923]\u001b[0m Trial 464 finished with value: 0.6839678265823259 and parameters: {'n_estimators': 452}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:03:40,512]\u001b[0m Trial 465 finished with value: 0.6838498437122118 and parameters: {'n_estimators': 519}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:03:58,000]\u001b[0m Trial 466 finished with value: 0.6859686846537726 and parameters: {'n_estimators': 101}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:05:19,966]\u001b[0m Trial 467 finished with value: 0.68395113329348 and parameters: {'n_estimators': 484}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:06:45,348]\u001b[0m Trial 468 finished with value: 0.6837525321398551 and parameters: {'n_estimators': 501}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:07:55,581]\u001b[0m Trial 469 finished with value: 0.6838522431319569 and parameters: {'n_estimators': 412}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:09:30,153]\u001b[0m Trial 470 finished with value: 0.6840344929140058 and parameters: {'n_estimators': 559}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:10:48,861]\u001b[0m Trial 471 finished with value: 0.6839929456259455 and parameters: {'n_estimators': 461}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:12:11,097]\u001b[0m Trial 472 finished with value: 0.6840446724281729 and parameters: {'n_estimators': 481}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:13:26,460]\u001b[0m Trial 473 finished with value: 0.6839088307003308 and parameters: {'n_estimators': 440}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:14:58,184]\u001b[0m Trial 474 finished with value: 0.6840910196872091 and parameters: {'n_estimators': 537}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:16:24,175]\u001b[0m Trial 475 finished with value: 0.6837012078988935 and parameters: {'n_estimators': 510}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:17:44,135]\u001b[0m Trial 476 finished with value: 0.6838423172110283 and parameters: {'n_estimators': 467}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:19:07,896]\u001b[0m Trial 477 finished with value: 0.6836427572569697 and parameters: {'n_estimators': 492}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:20:24,027]\u001b[0m Trial 478 finished with value: 0.6840937429971388 and parameters: {'n_estimators': 448}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:21:35,809]\u001b[0m Trial 479 finished with value: 0.6839560252001814 and parameters: {'n_estimators': 422}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:22:55,982]\u001b[0m Trial 480 finished with value: 0.6839154797374241 and parameters: {'n_estimators': 472}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:24:23,680]\u001b[0m Trial 481 finished with value: 0.6838498437122118 and parameters: {'n_estimators': 519}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:25:47,285]\u001b[0m Trial 482 finished with value: 0.6837208752583406 and parameters: {'n_estimators': 495}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:27:09,053]\u001b[0m Trial 483 finished with value: 0.6840284704235289 and parameters: {'n_estimators': 480}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:28:26,531]\u001b[0m Trial 484 finished with value: 0.6840440172534497 and parameters: {'n_estimators': 458}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:29:51,746]\u001b[0m Trial 485 finished with value: 0.6836881484496335 and parameters: {'n_estimators': 499}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:31:20,811]\u001b[0m Trial 486 finished with value: 0.6839052930595105 and parameters: {'n_estimators': 521}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:32:27,207]\u001b[0m Trial 487 finished with value: 0.6843421301499731 and parameters: {'n_estimators': 389}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:33:11,014]\u001b[0m Trial 488 finished with value: 0.685357976994261 and parameters: {'n_estimators': 258}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:34:25,148]\u001b[0m Trial 489 finished with value: 0.6840225510310922 and parameters: {'n_estimators': 438}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:35:45,355]\u001b[0m Trial 490 finished with value: 0.6839154797374241 and parameters: {'n_estimators': 472}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:37:08,580]\u001b[0m Trial 491 finished with value: 0.6836571985474691 and parameters: {'n_estimators': 488}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:38:25,946]\u001b[0m Trial 492 finished with value: 0.684019882056841 and parameters: {'n_estimators': 456}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:39:52,230]\u001b[0m Trial 493 finished with value: 0.683696096870203 and parameters: {'n_estimators': 508}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:41:14,109]\u001b[0m Trial 494 finished with value: 0.6839514606823235 and parameters: {'n_estimators': 479}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:42:46,326]\u001b[0m Trial 495 finished with value: 0.6840934104245106 and parameters: {'n_estimators': 539}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 06:44:03,078]\u001b[0m Trial 496 finished with value: 0.6840249433971435 and parameters: {'n_estimators': 449}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:45:28,842]\u001b[0m Trial 497 finished with value: 0.6837095184947621 and parameters: {'n_estimators': 502}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:46:47,786]\u001b[0m Trial 498 finished with value: 0.6838101040679397 and parameters: {'n_estimators': 466}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 06:48:01,464]\u001b[0m Trial 499 finished with value: 0.6838317382410096 and parameters: {'n_estimators': 428}. Best is trial 236 with value: 0.7130598708794997.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7131\n",
      "\tBest params:\n",
      "\t\tn_estimators: 464\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686187    0.735183    0.712420    0.725186   \n",
      "1                    TP   34.000000   36.000000   32.000000   35.000000   \n",
      "2                    TN  306.000000  309.000000  303.000000  304.000000   \n",
      "3                    FP    7.000000    5.000000   11.000000   11.000000   \n",
      "4                    FN   35.000000   32.000000   36.000000   32.000000   \n",
      "5              Accuracy    0.890052    0.903141    0.876963    0.887435   \n",
      "6             Precision    0.829268    0.878049    0.744186    0.760870   \n",
      "7           Sensitivity    0.492754    0.529412    0.470588    0.522388   \n",
      "8           Specificity    0.977600    0.984100    0.965000    0.965100   \n",
      "9              F1 score    0.618182    0.660550    0.576577    0.619469   \n",
      "10  F1 score (weighted)    0.878413    0.893141    0.865463    0.878791   \n",
      "11     F1 score (macro)    0.776981    0.802031    0.752301    0.776708   \n",
      "12    Balanced Accuracy    0.735195    0.756744    0.717778    0.743734   \n",
      "13                  MCC    0.584635    0.634572    0.527144    0.569626   \n",
      "14                  NPV    0.897400    0.906200    0.893800    0.904800   \n",
      "15              ROC_AUC    0.735195    0.756744    0.717778    0.743734   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.632286    0.679581    0.673585    0.722280    0.676301    0.717944  \n",
      "1    31.000000   35.000000   31.000000   35.000000   34.000000   28.000000  \n",
      "2   306.000000  304.000000  310.000000  307.000000  308.000000  312.000000  \n",
      "3     9.000000    8.000000    5.000000    7.000000    5.000000    3.000000  \n",
      "4    36.000000   35.000000   36.000000   33.000000   35.000000   39.000000  \n",
      "5     0.882199    0.887435    0.892670    0.895288    0.895288    0.890052  \n",
      "6     0.775000    0.813953    0.861111    0.833333    0.871795    0.903226  \n",
      "7     0.462687    0.500000    0.462687    0.514706    0.492754    0.417910  \n",
      "8     0.971400    0.974400    0.984100    0.977700    0.984000    0.990500  \n",
      "9     0.579439    0.619469    0.601942    0.636364    0.629630    0.571429  \n",
      "10    0.869757    0.876321    0.879035    0.884994    0.883139    0.872829  \n",
      "11    0.755473    0.776708    0.769957    0.787601    0.784327    0.754183  \n",
      "12    0.717058    0.737179    0.723407    0.746206    0.738390    0.704193  \n",
      "13    0.539206    0.580630    0.581607    0.602122    0.605807    0.568761  \n",
      "14    0.894700    0.896800    0.896000    0.902900    0.898000    0.888900  \n",
      "15    0.717058    0.737179    0.723407    0.746206    0.738390    0.704193  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_rf_9)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_9_cat = np.where(((y_pred_rf_9 >= 2) | (y_pred_rf_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_rf_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7131\n",
      "\tBest params:\n",
      "\t\tn_estimators: 464\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEaCAYAAADQVmpMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABFK0lEQVR4nO3deXiU1dn48e8zmSQkBMISWcJSEMENi0tVXjdQQXBFrB7Xam0rUrUubV3ft5VftRXrBq3avohWLS7cbgVX3F4BFypuqLggWyEkLCEYsm/z/P54ngmTyUwyM5nJJDP357pyMXPmWc6ZhLnn7JZt2yillFKx8CQ7A0oppbovDSJKKaVipkFEKaVUzDSIKKWUipkGEaWUUjHTIKKUUipmGkRUl2NZ1juWZc3vKtfpKveJhmVZP7UsqzHZ+Yg3y7IetSzrzWTnQ+2hQURFxbKsgZZl/dWyrI2WZdVblrXDsqxnLcs6OIZr/Y9lWRtDvHQW8OuO5jWO1wE6Jb/t3X+EZVm2ZVnHhHhtlmVZawOSFgJDorj2m5ZlPRqHbMbMsqyJbvn8Pzsty/o/y7KO7eB111qWNStO2VRBNIioiFmWNQz4CDgK+CWwD3Aq0ACssCxrajzuY9t2mW3bu7vKdbrKfaJh23aNbdvbOvu+liOzg5c5FBgMnAjUAK9aljWio3lTCWLbtv7oT0Q/wGJgK9A7xGuvuK/luM9nAWuBC4D1QC3wJjDSff2ngB30M8t97R1gfsC13wEeBm4HtgPfA3/E+RL0e2AbsAP4Y1Cemq8DTAxxPxvY6L5uAQ8B63A+uNYDfwKyY8hvJjAb2ALUA18BFwTlzQauAP4JVACbgRvaef9HuOcdE+K1WcDagOc/BRoDnvcG/uH+jurc+93rvvZoiLJNdF/bF3gZqHR/XgT2Cb4PcDzwqVveqwEfcFRQHie46XuHKZ//dzQ0IG2Im3Z5QF7fDHjdAn7r/r7q3d/ftUF/A8FlG5Hs/0up9KM1ERURy7L64tQ67rdDf+u+AxgITA5IG4zzQXkucCzQC/iXZVkWTnPLnUCRe9xg4O42snA2zofzMThNR7cALwF57rV/C9xiWdbJYc5/P+A+g4EDgWLg//xFxAlGFwD7A9cCl7r3Icr8/gm4zL3GWGABsMCyrBODjrsVWAYcDNwF3GlZ1vFtvAcdcTvON/xpwGic38nX7mvXAMsBYU/Z3rcsKwd4HeiBEwAm4Lzfr1mWlRVwbQ/wZ+A3wH7AU8AbOO9BoF8Ab9m2vT6KfNe4/4ar3VwB3IYTtA/EeR9nW5b1c/f1s4CNwD0BZdscxf1Ve5IdxfSne/wAR+B8i5se5vV+7uvXu89nuc8Dv7WOcdMmuc//B7cmEHStd2hdE/ks6JjVwBdBaauAu8NdJyA9Eyd4LMetaYQp03XAdwHP280vkIvzTf+KoGNeAN4OeG4Dfwk65hvgjjbyM8I9r5o9NQP/Tz1t10QWAY+2ce03g18Hfu7eqyAgbSDOB/vFAfexgWODzj0LqALy3ed93Gud00YeJhJQE8H50vEQTnPpWDftUVrWRDYDfw66zn3A+oDna3FrjfoT/x+tiahIWe28Hmolzx22bTd39tq2vQYoBQ6I4f6rgp5vBT4PkTYggmv9DRiGExDr/ImWZV1mWda/LcvaZllWJU7t6gdR5nMfIAunhhFoKc435UCfBT3fgvMh3Z5LcWovgT9/b+ecB4GzLcv60rKsuZZlnWxZVnv//w8EvrJtu9SfYDv9LN/Suiwrg54vBspxanYAF+EEu0Xt3BPgW/f9Lwem4ASsL4MPsiyrNzCU0O/1CMuyciO4l+ogDSIqUt/htGePDfO6P/3bdq7TXjAKpyHouR0mrc2/acuybsD5lnxq4IejZVnnAA/gNFudAhwC/IHwzSjtCQ6qVoi0+hDnRPJ/cott22sDf4CyNjNj20uA4Th9ST1wmtjetiwro517hfpyEFyWJtu2a4Pu14jTj+Vv0voFTk0nuMyhTAHG4dSAhtu2/VSUeYz1b0zFQIOIioht22XAq8CV7jfAYLfg9Cm8EZC2l2VZo/xPLMsaA/RnT1t8PdDeh1jcWJZ1Jk5gOMu27eBgdxzwqW3b99q2/bFt29/hNB8FiiS/a3GasyaEuP7qWPIdL7Yziuwp27Yvx+nfmsCeWmGosq0GDrQsq8CfYFnWQJxmyUjK8hAwzrKsmThBIdK5NBtt217n/s2FZTt9c0WEfq832LZd7T7v1L+zdKNBREXjSqAJ5xvsVMuyhlmWdbhlWU/ijM75qW3bNQHHVwP/sCzrMMuyfgQ8BnyB0/4OsAEYZFnWf1mWVZDI5gfLsg7E+fY9C/jGsqxB7s9e7iHfAgdZljXNsqxRlmVdg1NjCdRuft0Prr8At1mWdY5lWaMty7oFp0P7TwkqXrssy/qjZVlnWZa1r2VZo4ELcZqXNrmHbAAOc8te4A7TfRJn1NtCy7IOtSzrMOBpnGa3he3d07btTcBrwFzgHbc5M97uAH7lNkWOtizrcpzh54Hv9QbgaMuyhrtl08+9ONI3U0XMtu3/AD8C/g38L85wyleBbOC/bNt+LeiUEmAe8BzwHk6H7HTb7e0E/gU8gzOEdAdwQwKzfzjQE+dDpyTgx9+W/784w23/gTNU9UicgBMo0vz+N8638Dk439gvAi6ybfuteBQkRrU4tbCPceb6/BA42bbtcvf1e3D6q1bhlO1o9wvBSTg1q2U4fQ1VwNQIm6XA+f1nuf8mwt9whnnfgjOU+kbgJtu2Hw445lYgH+eLwg6cZj0VJ9ae/89KxY87Q/gi27b3SXZeVPJYlnUFTvAaEjiIQaUOb7IzoJRKPZZl5eGMVPstztwiDSApSpuzlFKJcD/wIc4gijuTnBeVQNqcpZRSKmZaE1FKKRWzdOwT0aqXUkrFptVEznQMIhQXF8d0XkFBAaWlpe0fmEK0zOlBy5weOlLmwsLCkOnanKWUUipmGkSUUkrFTIOIUkqpmKVln0gw27apra3F5/Ph7JcU2rZt26irS685U92hzLZt4/F46NGjR5u/P6VU/GkQAWpra8nMzMTrbfvt8Hq9ZGSk12Kg3aXMjY2N1NbWkpOTk+ysKJVWNIgAPp+v3QCiujav1xv3GtPWV96k/J9PkL+jGK+vEQ/O+HDL/fGxZ7yjxZ62YR97NvMm4F9PwLH+162gc5OlvP1DUk5aldmyIDOTyoICPIcdStYZZ+AdNar98yKgn5ygTSApIl6/x/+s/Jw1N9zC8J2bW2yT6P/w99/FE5QWHBgCj7WD0oJfDwxIEGIwvlIdYdtQX0/Ttq00vf8Bvh2l9Pj5z+ISSJL9BUipLqVk1Td8fuPvGb5zc4taRuBjgh6H+/CP9VjQGbEqUSyorsb+/nsal78blytqEOkiiouLufTSSzn66KM56qij+P3vf099vbNlw8KFC/nv//7vkOedccYZMd3vtddeY82aPXsE3XXXXSxbFrxVdXQWLlzIFVdc0SKtrKyMgw46KGxTU1tl62wlq75h5+//H/uWbsDDnpqGUinDtqGxEbu+Ht+2bXG5pAaRLsC2bS677DKmTp3Ke++9x/Lly6mqquLOO9tf/HTx4sUx3TM4iFx//fUcd9xxMV3L75RTTmHZsmXU1OzZ3PCll17ipJNOIjs7u0PXTrSSVd+w8Y93M7AyvWYwqzRjWeD1YmVl4Rk4MC6X1CASg+LyOmYt2chVz33HrCUbKS7vWIfuu+++S3Z2Nueeey4AGRkZzJo1i6effrr5A7m4uJgLL7yQY489lnvvvbf53NGjRzc//tvf/sYpp5zCpEmTuPvuu5vTn3nmGSZNmsSkSZP41a9+xcqVK3njjTe4/fbbmTx5Mhs3buTaa6/lpZde4u233+byyy9vPve9997jkksuAWDp0qWcfvrpTJkyhRkzZlBVVdWiHL169WL8+PG8/vrrzWmLFy9m2rRpvP7665x22mmcdNJJnHvuuezYsaPV++DPQzRli5dVzy6hoGInvoC0UE1KdojHodLicaxS8WdDbi5Wnz54jz0mLlfUjvUoFZfXcc0La9mye8/uoKtLqpg7fR8K82P7tr1mzRoOOuigFmm9evViyJAhbNiwAYDPPvuMt956i5ycHE499VROPPFExo0b13z80qVL2bBhAy+//DK2bfPTn/6UFStW0LdvX/7yl7+waNEi+vXrx65du+jbty+TJ09m0qRJnHbaaS3ue9xxx3HjjTdSXV1Nbm4uixYt4owzzqCsrIy5c+eycOFCcnNzeeCBB5g3bx7XXXddi/OnTZvGv/71L6ZNm8bWrVtZv349Rx99NBUVFbz44otYlsWTTz7Jgw8+yK233hrR+xOubOPHj4/l7Q4pa+cOevgaqLe8ZNj1ZND6w9ym5agraN1xHupxNMcGd9orFRfu6KwMHZ2VfPNWlLQIIABbdtczb0UJs6aMiOmatm2HHFkUmH7sscfSr18/AE4++WQ+/PDDVkFk6dKlnHTSSQBUV1ezYcMGvvrqK0499dTmc/v27dtmXrxeL8cffzxvvPEGp556Km+++Sa33HILH3zwAWvWrGHatGkANDQ0cNhhh7U6f9KkSdxyyy3NQePUU08lIyODkpISfvnLX7J9+3bq6+sZPjzyba7DlS2eQaS+/17Ubsgkx/ZRTyYeXxNZ+LCBWk8m9x56Hu8PHdfudYL5g4F/2G97sjMs7p02ikOG9or6Xh2hixGmh0SUWYNIlEorG0KnV4VOj8SYMWN45ZVXWqRVVFRQXFzMiBEj+Pzzz1sFmeDntm1z1VVX8ZOf/KRF+sMPPxz10NfTTz+dxx57jD59+nDwwQeTl5eHbdscd9xxPPjgg22em5OTw8SJE3n11VdZtGgRs2bNAuB3v/sdM2bM4KSTTuL9999v0STn5/V68fl8zeVpaGhos2zxNO7sKaz9+nPGfL+FJiwqs3LJ9jVSl5HFOyeex92/PTch9y0ur2PeihJKqxoo6JnJjPGDY67RKpUM2icSpYK8zNDpPUOnR+LYY4+lpqaGZ555BoCmpib+8Ic/YIxpnoG9fPlydu3aRU1NDUuWLOHwww9vcY2JEyeycOHC5n6KkpISSktLOeaYY3jxxRcpKysDYNeuXQDk5eW16tPwO+qoo/jiiy944oknmmsehx12GCtXrmxuXqupqWHdunUhzz/zzDOZN28epaWlzbWV3bt3M2jQIIDmcgYbOnQoX3zxBQBLlixpDiLhyhZPg8ftR99fX8u63k4es+wmSnr256nDp3PKZWfG9V6BCvOzmTVlBPefNZpZU0ZoAFHdjtZEojRj/GBWl1S1aNIa0juLGeMHx3xNy7KYP38+t9xyC3PmzMG2bU444QRuuumm5mMOP/xwrr76ajZu3Mj06dObm7L8tYwJEybw3XffNQ/5zc3N5a9//Sv77rsvV199NWeffTYej4exY8cyZ84cpk2bxvXXX8/DDz/MvHnzWuQnIyODSZMmISLcf//9APTv35/77ruPK6+8snno8Q033MCoEO2qEyZM4Nprr+X8889vzt9vfvMbLr/8cgYNGsShhx7K5s2bW5134YUXcumll3LqqadyzDHHkJub22bZCgoKYn7PQxl71EFsm3ExH636D6+OPkZrBkpFIB33WLeDN6XydyK3x+v10tjY2GWaIMrKypg6dSoffvhhwu7hL3N3EOnvsS0Ny5aRU1ND45QpccpV96D9A+khDptS6c6G8eBvgkimrVu3cvbZZzNz5syk5iPlhBnkoJQKTYNINzVo0CDefTc+yxaoADbOcEilVES0Y12pADa2BhGloqBBRKlAduCUP6VUezSIKBXItsGjQUSpSHVan4gxZiowF8gA5ovI7KDXrwcuDMjX/sBeIlJmjHkEOA3YLiJjA87pBywERgAbASMiuxJcFJXKbDSIKBWFTqmJGGMygAeAk4EDgPONMQcEHiMid4nIwSJyMHAzsFREytyXHwWmhrj0TcBbIjIaeMt93i0NGzaseT2rKVOmsHLlypiu89BDD7VYRdfvnnvu4Y477miR9uWXXzJhwoSw17rnnnvanaGeenR0llLR6KzmrCOAtSKyXkTqgaeBaW0cfz7wlP+JiCwDykIcNw14zH38GHBmXHLbjsZ166h99DGq7/wztY8+RmOYmdvR6NGjB2+88QZvvvkmN998M7Nnz27/pBDmz58fMohMmzat1bLxixcv5swzz4zpPinLF7zHoFKqLZ3VnDUECJyiXAQcGepAY0wuTq3jqgiuO1BESgBEpMQYMyDUQcaYGcAM97hWM523bdsW8R7r9sb/0PDss1h5vcgYOBC7qoqGZ58l49zzyNynY6ti+vNQXV1Nnz59mp8/8MADLF68mLq6Ok455RRuuOEGqqqqmDFjBsXFxTQ1NfHrX/+aHTt2sG3bNs455xz69evHCy+80Hzt/fbbj/z8fFatWtW8FMmLL77IwoULeeqpp1iwYAH19fWMHDmS+++/n9zcXDweT3O+pk+fzq233srBBx/Mzp07mTJlCh999BFNTU3cfvvtvP/++9TV1fGzn/2Miy++uEPvQ6yys7M7PIu9Ii8Pq6kp7rPhuzqv16tlTgOJKHNnBZFQX+3CTZU/HXgvoCmrw0RkHuBf28MOnrFZV1dHRkYGAA0ffohdFvrWnowM6pYuw66txaqobE63a2upnj+fzGOODnme1a8fmUcc0WYea2trOf7446mrq2P79u2ICI2NjSxdupR169bx0ksvNS+D/u6777Jz504GDBjAY485FbHdu3fTu3dv/v73v/PMM8/Qr1+/VjPNp02bxvPPP8+4ceP4+OOP6du3L8OHDycvL4/zzz8fgDvvvJMFCxbws5/9rHkxxMbGRmzbpqmpicbGRpqamrBtm8bGRhYsWEDPnj15+eWXqaur48wzz+SYY46JapXeeKmrq+vwDOT6igryevTQmcxpQMscHXfGeiud1ZxVBAwLeD4UKA5z7HkENGW1Y5sxZjCA++/2mHMYIXv3bgjepS8720nvAH9z1rJly1iwYAHXXHMNtm23WAZ9ypQprFu3jg0bNrDffvuxfPly/vjHP/Lvf/+b3r17t3uPM844g5dffhmfz8eiRYuaF1f89ttvmT59OieeeCIvvPAC3377bcT5Xrp0Kc8++yyTJ0/mtNNOY9euXc2LNHZLts4TUSoanVUTWQmMNsaMBLbgBIoLgg8yxuQDE4CLIrzuYuASYLb776KOZrStGoPX66VxSzF2RQVWrz37PdgVFVijR5M1NVTff/R+9KMfUVZWxs6dO9tcBv3VV1/l7bff5o477mDChAmtNogKNmTIEIYNG8YHH3zAK6+80txHct111/Hwww9z4IEHsnDhQj744INW52ZkZDTXTGpra1u8dvvttzNx4sQYS9vFaBBRKiqdUhMRkUacPo4lwNdOkqw2xsw0xgQu/jQdeF1EWqxRbox5CvgA2NcYU2SM+bn70mxgsjHmO2Cy+zyhvMceg11ZiV1Rge3zOf9WVsZtq0mAtWvX0tTURN++fcMug75161ZycnL48Y9/zMyZM5uXUM/Ly6OysjLstadNm8asWbMYMWJEc/W0srKSgQMH0tDQ0KIfJdCwYcP4/PPPAXj55Zeb0ydMmMDjjz/evGz7unXrqK6u7vibkCy67IlSUem0eSIi8grwSlDa34OeP4oznDf43PPDXHMncGLcMhkB76hRYM6hcfm7+LZtwzNwIJmnnNzhrSZra2uZPHky4GzCNGfOHDIyMsIug75x40Zuv/12LMsiMzOzefjuhRdeyEUXXcSAAQN49tlnW93n9NNP59Zbb+W2225rTrv++us57bTTGDp0KPvtt1/IIDRz5kxmzpzJc889x9FH7+n7ueCCC9i8eTNTp07Ftm369evHI4880qH3Irl0xrpS0dCl4Il+Kfh00p3KHI+l4OtfeYVeffpSd9R/xSlX3YN2MqeHRCwFr8ueKBXItrUiolQUNIgoFUj7RJSKigYRnD4I1f3F5/dog6X/LZSKlP5vATweT7dp91ehNTY2Ns+w7whbh/gqFRXd2RBnol9tbS11dXVtLr6XnZ1NXV1dJ+Ys+bpDmW3bxuPx0KNHjzhcDO0TUSoKGkQAy7LIyclp9zgdzZEGdI91paKizVlKBbJ92pylVBQ0iCgVSEdnKRUVDSJKBdI91pWKigYRpVrQ0VlKRUODiFKBdMa6UlHRIKJUIO0TUSoqGkSUCqRDfJWKigYRpVrQPhGloqFBRKlAuuyJUlHRIKJUIO0TUSoqGkSUCqQ1EaWiokFEqRY0iCgVjU5bgNEYMxWYC2QA80VkdtDr1wMXBuRrf2AvESkLd64xZhZwGbDDPe8Wdy93pWKjM9aVikqnBBFjTAbwADAZKAJWGmMWi8hX/mNE5C7gLvf404Hr3ADS3rn3icjdnVEOlfpsHeKrVFQ6qznrCGCtiKwXkXrgaWBaG8efDzwV47lKxU5nrCsVlc5qzhoCbA54XgQcGepAY0wuMBW4KsJzrzLGXAx8BPxGRHaFuOYMYAaAiFBQUBBTIbxeb8zndlfpVuZdOblkpFmZIf1+z6Bljts143q18EJ9twu3IfbpwHsiUhbBuX8DbnOf3wbcA/ws+GARmQfM858b6yZLabdBE+lX5tqqKrJ9vrQqM6Tf7xm0zNEqLCwMmd5ZQaQIGBbwfChQHObY89jTlNXmuSKyzZ9ojHkIeCkemVXpTEdnKRWNzgoiK4HRxpiRwBacQHFB8EHGmHxgAnBRJOcaYwaLSIl73HTgy4SVQKUHHZ2lVFQ6pWNdRBpx+jiWAF87SbLaGDPTGDMz4NDpwOsiUtXeue7LfzbGfGGM+Rw4HriuE4qjUpnGEKWiYtl2uK6JlGUXF4drSWubtqGmNtu2qXvscfofdyxVe++d7Ox0qnT6PftpmaPj9om0+oqlM9aVCubR/xZKRUr/tyjl11wr1/YspSKlQUQpP38Q0dFZSkUs4tFZxphMYDxQKCILjTE9AQI7wZVKCRpDlIpYRDURY8xBwBrgIeBhN3kC8EiC8qVU59OaiFJRi7Q562/A70VkP6DBTVsKHJOQXCmVDG4Q0QUYlYpcpEHkQGCB+9iG5masnERkSqmk0JqIUlGLNIhsBA4LTDDGHAGsjXeGlEoaHZ2lVNQi7Vj/HfCyMebvQJYx5mZgJs6GUEqlhuaaSHKzoVR3ElFNREReAk4G9sLpC/kBcJaIvJ7AvCnVubQ5S6moRTzEV0Q+Aa5IYF6U6ho0iCgVsYiCiDHmD+FeE5Hfxy87SiWR1kSUilqkNZFhQc8H4cwTeSG+2VEqiZqH+OpCDkpFKqIgIiKXBqcZY6bi7IWuVGrQjnWlotaRr1yvA2fGKR9KdR3anKVUxCLtEwneXCEXZ3fBzXHPkVLJon0iSkUt0j6RtbTc860a+BS4JBGZUioZbA0iSkUt0j4R7WlUqU9nrCsVNQ0OSgXTGKJUxMLWRIwxm3EXW2yLiAyP5EbuaK65QAYwX0RmB71+PXBhQL72B/YSkbJw5xpj+gELgRE463sZEdkVSX6UasXnA3QVX6Wi0VZN5CLgJxH8tMsYkwE8gLN0ygHA+caYAwKPEZG7RORgETkYuBlY6gaQts69CXhLREYDb7nPlYqN9okoFbWwNRERWRrH+xwBrBWR9QDGmKeBacBXYY4/H3gqgnOnARPd4x4D3gFujGO+U9q3z71G9ZNPMej7rWTajXhoOXrCAsqTl73k8HgofvVVPOOPJOuMM/COGpXsHCnVpUWzPe7BwLFAAQGtxhEuezKElsOBi4Ajw9wnF5gKXBXBuQNFpMTNR4kxZkAEeUk5W195k/J/PkH+jmK8vpbBwP+LsmndNhnqzfKfYwf8609PCz4fTaWlNL3/Ab4dpfT4+c80kCjVhkjnicwA7sOZYHgy8CpwErAowvuE+gwK199yOvCeiJTFcG5Ibv5nAIgIBQUF0ZzezOv1xnxuPHz6z+fY+fAjDNzl1Bz8b0w2LQNCYBDAfRxcywh8HEraBI1QPB6oqSGjspLMjz+h/5Ehv++klGT/bSeDljlO14zwuBuAqSKy3BizS0SmG2NOBs6L8PwiWq6/NRQoDnPseexpymrv3G3GmMFuLWQwsD3UBUVkHjDPfWqXlpZGmO2WCgoKiPXcjihZ9Q3/+fMc9in5juG0bG4KFwysEI9DpUWqvaCTchoaaKiupmrDBuwk/M47W7L+tpNJyxydwsLCkOmRBpEBIrLcfewzxnhE5FVjzBMRnr8SGG2MGQlswQkUFwQfZIzJx1nY8aIIz12MM+FxtvtvpDWjbqNk1Tds+OPdjCndiIeWTVTQuR/saRVIvF6srCw8AwcmOydKdWmRzhMpMsaMcB+vAaYZY44F6iM5WUQacfo4lgBfO0my2hgz0xgzM+DQ6cDr7v7tbZ7rvjwbmGyM+Q6Y7D5PKZvu/gtjSjeSgd0cRBLJDvo3+HF6sCE3F6tPH7zHHpPszCjVpVnNSz20wRjzU2CbW/s4GXgWyAKuFpG/JTaLcWcXF4drSWtbZ1d/v33uNfo9eC8e24cHO6oAEqr/I1yany/MMf7HHpyJOinLsiAzk4yCAjyHHZpWo7O0aSc9xKE5q9XHUJvNWcYYAR4FHhcRH4AbSPoCWSJSGVNuVER2PfM8fbDwYLf4sA/8LYYKBjatR2OFDgwWDRmZbM7bCxl9Au8PHddmfk7aty+zpoyIoSTdSzp+uCgVq/b6RLYADwOWMeZJ4FER+VxE6omwKUvFLr+6nKqMLHo3NmJh0eQ2afntCRbRBQML6N0jgx8O7sl5hwzgrjc3sWX3nl/ngJ5eLI/FtoqG5rQhvbOYMX5wXMunlOr+2gwiInKdMeY3OPM2LgI+cPsfHgeeEJFtnZDHtFWem09ek4+ejXU0AR7LxrZtGiwPdx12QbvBIpQBPb08ePYYCvOzm9PmTt+HeStKKK1qoKBnZnOwmLeihPJ6yM+CGeMHtzhHKaUgwj4RP2NMb+BsnIByFPCmiJyWoLwlSrfqE+GRh+jRUEejJ4NMXxMWNk+NOZF/jTkeAI8FvbKdWsU1xw0FnA//Ld/XsrO6iV7ZFhV1NgU9vRTmZ0cdDNKxaUfLnB60zNGJqU8kmIjsNsa8CvQHRuHMYFcJsu+Pp7Kmpor6p58EG4r7DCTjjDOo7j+WQwNqDcFBIbjfori8zqlpVDYwb0WJ1iqUUnET6Yz1HsBZOHMxJgLLgd/hjNJSCbTPKcdT31hJ5vETGf2DHwBweBTnF5fXcc0La1v0eawuqWLu9H00kCilOqy90VkTgYuBHwMlwD+BX4iIbovbWZqanH89sW39Mm9FSYsAArBldz3zVpSkxUgrpVRitVcTeQF4GmfJkw86IT8qmLvHBZ7YZmiUVjaETq8Kna5UdxfYfFuQF7rJV8VPe0FkkIjUdUpOVGjuwAfLE9tc9YK8zNDpPUOnK9WdafNt52uzjUQDSPLZTf6aSGzNWTPGD2ZI76wWaTrnQ6WqtppvVWJENTpLJYHdsSBSmJ8dch6IfitTqUibbzufBpGuztexIAJOINFOdJUOwjXfFpfXU1xep1+eEiCqTyZjzDBjzPhEZUaF0MHmLKVSSXF5HbOWbOSq575j1pKNFJe3bHEP1XwLsLWinmteWNvqeNVxkc4TGY6zUdTBOMs15RljzsYZtfWLxGVPdbQ5S6muLnA0VW6Wh9qGJtbtrANsRvXvAcC6nXU0+Zqob4Laxj2rbHxWVMG+A3KpqveRm+XBAvr0yGBntYfaRl+L+2zZXc9FT3zDzScO5b2NFZTXbSQ/W5f06ahIm7P+F3gZZ4b6TjftDeCeRGRKBXCbsywNIirFFJfXMWdZER9uqqC+KfTySx8VVYVM99te1cj2Dbsjvmdto49bl2xqkaajtzom0k+mI4DZ7nLwNoCIlAP5icqYcvn7RKy02VNQpYHi8jqufP473t2wO2wA6Sw6eqtjIg0i24B9AhOMMQcAm0IfruLF9geRjJTeDkqlmbnLilpsNZBsOnordpEGkbuBl4wxlwJeY8z5wELgzoTlTDn8qyxrc5ZKIV9ubbuZqrPp5NvYRfTJJCKPADcA5wCbcdbT+p2IPJHAvCnYMzpLm7NUSuk6f885mR6dfNsBkY7OyhCRfwH/ivVGxpipwFycbbrni8jsEMdMBOYAmUCpiExw068BLsP5y3tIROa46bPc9B3uJW4RkVdizWOX5HMXYNTmLJVCDhyUy7tRdIgn0t79srVTvQMibSPZaox50BhzdCw3McZkAA8AJwMHAOe7fSqBx/QBHgTOEJEDcWo9GGPG4gSKI4BxwGnGmNEBp94nIge7P6kVQGBPc5bWRFQKufa4ocS4HFzcDenTI9lZ6NYiDSInAZXAU8aYjcaYO4wxB0VxnyOAtSKy3t2f/WlgWtAxFwDPi8gmABHZ7qbvD6wQkWoRaQSWAtOjuHf31uQDy9IhviqlFOZnc+tJw5OdDW3KioOImrNE5FPgU+AGY8wE4HzgLWPMVhH5YQSXGILTl+JXBBwZdMwYINMY8w7QC5grIo8DXwJ/NMb0B2qAU4CPAs67yhhzsZv2GxHZFUmZug3bR5f5yqZUHE3etz8FPbO47Y1NVNY10sPrYVDvTNbsqG132G+2B3yWRUMbx3ksyM20yMnMoE9OBkXlDdQ07JmAmJuVwV2njdSmrA6KZe2sb4GvcYLC6HaO9Qv1KRj82/cChwEnAjnAB8aYFSLytTHmTpzJjZXAKqDRPedvwG3utW7Dmfz4s+AbGWNmADMARISCgoIIsx2UQa835nNjVZ2XR11eL/p28n39klHmZNMyd57JBQVMPnhki7TNZdXMeXsdm8qqKa2sZ6+8LPr1zMICKuubGNArm2tPGAXAnLfXsb2ijgG9sjn3sEIWflzc/PzaE0YxrF9uq+v6X//t5H0ZnN96iZRUlojfc6Qd631wdje8ABgPvI4zvHdxhPcpAoYFPB8KFIc4plREqoAqY8wynD6QNSLyMPCwm5c/ucciItsC8vgQ8FKom4vIPGCe+9SOdaP6jmxyH6uGXd/jq62hqZPv65eMMiebljm5coCbJ7bTxOSrhlbH2S2f+6opLa0Oe92C/KwuU+bO0pHfc2FhYcj0SGsixcD7wJPAWe5s9WisBEYbY0YCW4DzcAJSoEXA/cYYL5CF09x1H4AxZoCIbHfX8DoL+C83fbCI+KeaTsdp+kottg8s7Q9RSnVNkQaRUQEf1lETkUZjzFXAEpwhvo+IyGpjzEz39b+7zVavAZ8DPpxhwP6g8JzbJ9IAXBnQ7/FnY8zBOM1ZG4HLY81jl+XzQYYGEaVU12TZduiOKWPMcSKyzH18QrgLiMjbCcpbotjFxcEtaZFJSnPW8nfxbdtG9tk/7tT7+nWlZo7OomVOD1rm6LjNWa36t9uqiTwIjHUfPxzmGBvYO6YcpYmtr7xJ2WML6FtaQqbdiIc9IwpsnN+IFZDmIWjctWVBZiYNX35Jj59chHfUqE7Lu1JKtSdsEBGRsQGPR4Y7Lp2VrPqGDQ/MZ+jGb8hprGkOAP4g4AOygcCuP3/A8IdzO0SaL+Axtg319TStWEHN99+T86urNJAopbqMiBrbjTGLwqQ/H9/sdB8lq76heNYfGb32M/Iaa/Cy58202VOzsIJ+oGV9sK00PxvwNTbi27KFxuXvxq0MSinVUZF2rB8fJn1inPLRbfhrH6PWrmJvfK1qEYHiOUXQ5/NBTS2+bdvaP1gppTpJm0HEGPMH92FWwGO/vYH/JCRXXVDJqm/44MFHGPndp+xPy203Ez2f3KnZeNjly6BXn/7ktnuGUkp1jvZqIv4Jgh5aTha0cWasz0pAnrqcklXfsObuBxi1dT1ZbgDpSOBor08k+DFAk+WhKLc/q3LH8KsO3FsppeKpzSAiIpcCGGPeF5GHOidLXc+qZ5fQp6qCTLsxouARHBjsoNfsoLRwwcNyX63xZvPJXmN4er9J9Os5KPaCKKVUnEXaJ1JnjPmhiHzuTzDGjAN+KCL/TEzWuo6snTvI8jWC3TowBAeJpoB0ABuLhoxMNufthYw+gfeHjutQXsboDmxKqS4k0iByG3BwUNpmnLWzUj6I1Pffi/qtRTR4PGQ0OW17gTUGG9jRow8PjT09ZJDomekhN8vDjqrGVq9FY2CvTF22WinVpUQaRHoDwduQlQN94pqbLmrkwfvCx0vp0VSPB2cehz+QNFoeVvUfxb+OOBPP3nvTe0cNtY02OZkefji4J9ccN5TC/GwuW/htm0FkQE8vw/tms25nHWAzqr+zUY7/+dhBe66llFJdRaRB5CucVXwlIG06zpLwKa1k1Tds/dfLDMSiAS+ZNGJhUZfhZeWA/Xl6v0nUDh3B/WftE/YDvri8jvU7a0O+1jcng8OH92bG+MEaIJRS3U6kQeRG4BVjzLnAOmAfnH0/TklUxroKf6d6Y4aXipw8qrw9qMtw+iU29x7IxvxCDs3PajMAzFtRQk2jr1V6TqaHh8y+GjyUUt1WRDPWReRdnHW0VgI9gQ+BsSLyXgLz1iX4O9WrvD2oyMylOrMHDR4vWU0NDKhxFhMuaKezu7SyIWT63v2yNYAopbq1iHc2FJFNxpg/AwM7six8d+PvVAeo9jr9FJlNDdRnZLI9py9Deme129ldkBc6yAzp0yPsOcXldcxbUUJpZQMFeZna3KWU6pKi2dnwQeBsnD09ehpjzgCOEJH/SVz2km/c2VNYs3Et/b4vpcpdNj+vqY7d/QbiO+po5p4Rvi/Eb8b4wawuqWLL7vrmtLaCT3F5Hde8sLbF8atLqpg7vf17KaVUZ4p0t6O/44zG+gHg/2T7ADg3EZnqSgaP248xv72SXfvsT59Mi37ZFgOPPpwjfnc1v/rJsRF9qBfmZzN3+j6ctG9fDh2ax0n79m0zIMxbUdIigABs2V3PvBVpUwFUSnUTkTZnnQgUikiDMcYGEJEdxpgBicta1zF43H4Mvu/3HdvQJT+bWVNGtEgL12QVrg+ltCp0ulJKJUukQaQcKACavwq7+53rV+MYtdVkFa4Ppb0OfKWU6myRBpH5OPuc/zfgMcb8F/AnnGYuFaXi8jquen4tWytCN1lF24fSmdrr8A/3enF5HXOWFbF6azU6eVKp1BFpELkTqAUeADKBR4D/BeYmKF9dTnF5HXe88wVbdlZ2aLSUvwYSHED8SqsamvtQ5q0oobSqgYKeyRmd5S/z+q3lbKtsoKq+kZqgSfdvfLuLwt6Z1DXaIV9//dtdzbP8Ay3fsJvlG74iNxMafRYZ2DT6oNFdiKxnlodDhuRpoFGqi4soiIiIDcxxf2JijJmKE3QygPkiMjvEMRPde2QCpSIywU2/BrgMZ6WRh0RkjpveD1gIjAA2AkZEdsWax3DiOVoqVKd5IH+TVag+lM5UXF7Hlc9/x7aKtvthbGDL7raPaT3Nco/qBv9VWqqs97F8w27WlH7HA2eN1kCiVBcVNogYY44TkWXu4xPauEY9sFFEitq4VgZOLWYyUASsNMYsFpGvAo7pgzOMeKo7J2WAmz4WJ4Ac4d7rNWPMyyLyHXAT8JaIzDbG3OQ+vzGCckelrdFS0X7Qh+s0h67TZAUw++1N7QaQzrCtoiGm97m7irTZ79OiCm57YxOVdY3kZXv53eThHDK0V4vmxNwsDxZQVe+jIC+TaQf2Z9HqnTr3SMVVWzWRB3FmqQM83MZxHqDAGPMXEbk5zDFHAGtFZD2AMeZpYBrOmlx+FwDPi8gmABHZ7qbvD6wQkWr33KU463b92b3GRPe4x4B3SEAQiedoqXCd5oN6ZXWZeSDF5XV8vLky2dloli6j0orL67ji2TVsD1ioc/mG3az4z1eMK+wJOAty1jc2EviWVNbXc9XzazlwUA5rdtRS39S6ZgdO02Kgpd/t4qAhefh8MKR/CZcc0q9L/P2p7iVsEBGRsQGPR7Z1EWPMXsAaIFwQGYKzdLxfEXBk0DFjgExjzDtAL2CuiDwOfAn80RjTH6jBWa/rI/ec5tnzIlISbsixMWYGMMM9joKCgraK0zrz/Uv4ZEvrD9Uh/fKivtaNJ+fyzY5P2FRW05w2vF8O/7j4UIb16xob397xzhchGpiSJ5b3uSO8Xm+n3g9gc1k11yz6ukUA8WvwwUdFVW2ebwNfbq1p85hgdT74yP2y8MmWSlYVlXepv8NES8bvOdkSUeaIlz1xm6TGA4XAFuDfItIEzXNGJrdxeqgNAYM/p7zAYThzUnKAD4wxK0Tka2PMncAbQCWwCohqYw4RmQfM89832rkelxzSj082lrUaLXXJIf2injeSA9x7+shWneY5vmpKS6ujulaibNnZdWohPbxWTO9zR3RkPlAsQvW5JcOmshrufPWrtGk67Ozfc1fQobluhYUh0yNd9uSHwL+AHji1iKFArTHmLBH5DEBEPgp7AeecwD3ahwLFIY4pFZEqoMoYswwYB6wRkYdxm9SMMX9yjwXYZowZ7NZCBgPbSQD/aKnHPi1jS1llh0dLJbvTvD3hmtw6W3aGxT1njEr5Jpb2Blt0pi3fh96yQKlwIl325BGcjvEhInIETvPU/bTdVxJoJTDaGDPSGJMFnIezK2KgRcCxxhivMSYXp7nra4CATvbhwFnAU+45i4FL3MeXuNdIiML8bK49YRQFPTMprXQ6e4vL6xJ1u6SaMX4wQ3pntUr3eiAvy2Kvnl5GF2TTJ9vTqoppAWMH5fCjoT3pne3Ba4Wuhrb1h5eVYXHsyN48cdH+HDK0VwdK0j1s+b7r/B2tL6tL2b9rlRiRNmeNAea4Q30REdsYMxeYFcnJItJojLkKWIIzxPcREVltjJnpvv53t9nqNeBznFGh80XkS/cSz7l9Ig3AlQHDeGcDYoz5ObAJOCfC8kStuLyOX7/4TYu+jFRdFDFwnkp5PeRnEbbm1TwaqI35LOGO8adv+b6WndVNFPT0UpifnXajhnZWd2zb5HiqafCl1Wg41XGWbbffheqOplooIi8EpJ0JnCsi5ycuewlhFxcHt6S1b9aSja1GtwCctG/flP4Pp+3GiXfZ09+went0neKJdOjQPO4/a3Sys5Fw+rcdHbdPpFXDQlvzRP7Jns7vDOBpY8zHOKOshuF0gies+air0UURVaIM6dujSwURXaNNRaOt5qy1Qc+/DHj8FU7TVNrQRRFVoswYP5jPiipCDu/tbF1pwqvqHtqaJ/L/OjMjXd2M8YP5Zkdtiz4R/Q+n4qEwP5sHzx7Taqb6eYcMcGaYVzWQm+mhtqGJdTvraPI1ke3NYGCvLPrkeFul98nJYFd1Ew0+G9v2tTjWAsqqG5r7oPxp9WS02felVDjtdqwbY7zARThLlhQApcCbwAIRSZu2nML8bP5x8aHc+epXSV0UUaWmwvxs/nz6qFbpnTU6LR37B1R8tBlEjDH5OJP8fgC8CnwCDMYZFXWFMWaSiJQnPJddxLB+uSndia6UUtFqryZyB7ADON6dBAiAMaYnIO7rVyQue0op1fna2zdH7dFeEDkTGB8YQABEpMoYcyXOPutpF0T0D0yp1BXPrR/SQXsz1vNx1skKpQjoHd/sdH3+P7DXv93FJ1sqef3bXVzzwlqd5atUimhr6wfVWntBZB0Qbi+RE4H18c1O16d/YEqlNp0TFp32mrPuBR53lyx5QUR8xhgPzvpVfwVuSXQGuxr9A1MqtbU1J0ybsltrM4iIyKPumlWPAk8ZY0pxhvnWAX8QkX8kPotdi046VCp1hAoKoSZ/ZmdYrC+t5idPfkNNw54Nn5evK2fv/j0Y0if8mm+pHnjanSciIvcYY+YBR7FnnsgHIrI70ZnrimaMH8zqkqpWe4vopEOlupfNZdWtOtA/K6pgeN9sdtW0XD2grslm7c7W/Z41jT5Wb6tm9bZqPiuqYN8BuVTV+5q3Ji6ramD9rrpWgefuM/ZOmRWqI1qAMcXEtAAj7JmQFcnKtakiHSehaZlTW3F5HXe89R8+3VKFL0kffxkW/GX6Pp0eSDp1AUYVXlffVEopFVpxeR0zFn5DWa2v/YMTqMmG3764nn9esF/4rRO6SfNXpJtSKaVUtzdvRUnSA4iff++WQN1xCoEGEaVU2gg3ujJZgkd1dscpBBpElFJpI9zoymQJHtXZHacQaJ+IUiptzBg/mI/+U94lmrRyMj2tRnV2ZApBsvpStCailEobhfnZzDt3Pw4c2COp+cjxerj79L1bfcjPGD+YIb2zWqRFMoUgmX0pnVYTMcZMBebibLU7X0RmhzhmIjAHyARKRWSCm34d8Auc7Xq/AC4VkVpjzCzgMpyVhgFuEZFXElsSpVR3VpifzUPn7k+NJ5c7X/2KLd/Xsq2igfK6Jhqa9oz5zcqwOHJ4L645bihAq03DrjluKIX52a2G/E87sD9Pfbo97AZjbU0LKMzPZu70faKeQtBWX0qiR5J2ShAxxmQAD+BsbFUErDTGLBaRrwKO6QM8CEwVkU3GmAFu+hDgauAAEakxxghwHs4seoD7ROTuziiHUip1BO8P1N78r1CbhkHoIf+h5n9EOicklikEyexL6ayayBHAWhFZD2CMeRqYhrNXu98FwPMisglARLYH5TPHGNMA5AKxzRZUSqkwuvP8r2Qux9RZQWQIsDngeRFwZNAxY4BMY8w7QC9grog8LiJbjDF3A5uAGuB1EXk94LyrjDEXAx8BvxGRXcE3N8bMAGYAiAgFBQUxFcLr9cZ8bnelZU4PWubu7eKjLN7dsJvq+qbmtOH9crjx5AMo6JfbnJaIMndWEGk1VR6nfyOQFzgMZ4n5HOADY8wKnP6OacBI4HvgGWPMRSKyAPgbcJt7rduAe4CfBd9IROYB8/z3jXXafzotDeGnZU4PWubuq7i8jhtfWNsigOR4Pdw4cQg5vmpKS6ub0+Ow7EkrnRVEioBhAc+H0rpJqginM70KqDLGLAPGua9tEJEdAMaY53EWg1wgItv8JxtjHgJeSlD+lVKqw0INw91WUc9tb2yisq6RvGwvv5s8vLn/5NOiCm59bSNl1Y3YQM8sD/sNyCEnM4Oqeh+WZfNlSTV1TS2/k9c0+rjhxXXkZGbQ4LPxWDB2UE9mTcslJ85l6qwgshIYbYwZibNT4nk4fSCBFgH3G2O8QBZOc9d9QE9gvDEmF6c560ScpiuMMYNFxD+VczrwZaILopRSsSgur+OKZ9e0WGL+9W9btr5X1tdz5fNrw16jst7HR0VVYV8PVNVgU9Ww517LN+xm0pz3OGxYHjedMDxuc0g6ZZ6IiDQCVwFLgK+dJFltjJlpjJnpHvM18BrwOfAhzjDgL0Xk38CzwCc4w3s97Gma+rMx5gtjzOfA8cB1nVEepZSK1pxlRS0CSDLYwEebK7ny+e/iNodEl4KPQqq0oUZDy5wetMyJd+pDX7TapySZTtq3b1Sj0cItBa8z1pVSqlN0rS/s8ZpDokFEKaU6wdhBPZOdhRbiNYdEg4hSSnWCa44bSlZGqNkOnW9gr8y4bemtQUQppTpBYX42900bRVYSP3U9FvxoWB4PnDU6bqOzdCl41e11t+1EVfo6ZGgvnvzJAcxZVsTnxZXUNtrkZHoYXeCsKrxuZx3hFm0MXthxVP8e5GRmUFbdwM7qJgp6einMz25xXJOviWxvBgN7ZVGYn82NJx9Ajq+6zTxGS4OI6vLCBYni8jrmLCviw00V1AdMtlpdUsXc6ft0+72rVWoqzM8Ou5hjsOBFGyNdxDHccQX9clvMYI8HDSKqywn8sM/N8vBdaQ3bKvaMJFm+rpybThzKvA+2tlr+GkIvge3fbyHw+HDBRikVOQ0iqksJ9WEfrKbRxx9e30RTGyMmV26qoLi8rrnGctXza9lakZz9FpRKZRpEVJcSanOdUNoKIAC7ahq55oW13DJpOH96c1OrAOJXWtXQqpnrxpPjv76QUqlKg4jqUsJtrhOLLbvrueWV9ZS3sZ/2F1sqOX/B1y12tPtmxyfce/pIbeZSKgI6xFd1KblxHv/YVgABaLBpEUAANpXV8Mtn13TK/tRKdXcaRFSX0jWmYsGOqsa4LlKnVKrS5izVpVTVt11z6EzbKhq04z0BdKh1atEgorqUcHtFJ0u8FqlTDh1qnXq0OUt1KTPGDya7i6wvBPFbpE45Qo2+8w+1Vt2T1kRUl1KYn82900bxqxfW4kvyytnxXKQuFcXSLBVu9J3W+LovDSKqyzlkaC/+On0ffvviemoaIusj6eGF+kaIR49K7x4ZjBvck2uOG5oWTSzF5XXc8c4XbNlZGTIYhAoWQJvNUuECTM8wo++0xtd9aRBRXdIhQ3vxzwv2cz6IAhagW7R6J1u+r22x4FzgWlrzVpSwclNFyB3ksjIsengh25tBbqbFtsombNuHZXkY3ieLEf1zmDF+MD8cNSRtdvlrr48i1OvL15Vz4ODckM1Sl8m3HDioZ6ulaj4rqmB432w+29J6f3Ct8XVvuj1uFHQL0e4h1AffkN5ZEXfedscyx+rGF9exfMPukK9ZOEuHt7c6QEcdNrQnfz1rTGJvEkI6/Z79OlLmcNvjdlpNxBgzFZgLZADzRWR2iGMmAnOATKBURCa46dcBv8DZX/IL4FIRqTXG9AMWAiOAjYARkV2JLovq2grzs5k7fZ8WtRgdRtpacXkdK/4TOoCA858t0QEE4IuS6uZ1zlT30ymjs4wxGcADwMnAAcD5xpgDgo7pAzwInCEiBwLnuOlDgKuBH4nIWJwgdJ572k3AWyIyGnjLfa4UhfnZzJoygvvPGs2sKSP0AyqEeStKiLDLKaHqm2zmLCtKdjZUjDpriO8RwFoRWS8i9cDTwLSgYy4AnheRTQAisj3gNS+QY4zxArmAvz1qGvCY+/gx4MzEZF+p1BPPdco66kN31WXV/XRWc9YQYHPA8yLgyKBjxgCZxph3gF7AXBF5XES2GGPuBjYBNcDrIvK6e85AESkBEJESY8yAUDc3xswAZrjHUVBQEFMhvF5vzOd2V1rm1DWkfwmfbKlMdjYApzby2Kdl3HP2QZ12z3T5PQdKRJk7K4iEmj0W3NrqBQ4DTgRygA+MMSuAHTg1jpHA98AzxpiLRGRBpDcXkXnAPP99Y+1Y0o649JAuZb7kkH78e10p26taj2RLhi1llZ36vqfL7zlQHDrWW+ms5qwiYFjA86HsaZIKPOY1EakSkVJgGTAOmARsEJEdItIAPA8c5Z6zzRgzGMD9dztKqYgU5mfz4NljOGZkb7xdYO0KnSvSPXVWTWQlMNoYMxLYgtMxfkHQMYuA+91+jyyc5q77gJ7AeGNMLk5z1onAR+45i4FLgNnuv4sSXA6lUop/v+8aTy4XPPxhi7kd0eiV7cHCJsOyqG30EWKaDgB9e3gY0iebNTtqqQ8Y+jWkd5bOFemmOiWIiEijMeYqYAnO6KpHRGS1MWam+/rfReRrY8xrwOc4E4/ni8iXAMaYZ4FPgEbgU/Y0Tc0GxBjzc5w+k3M6ozxKpZph/XJ54KzRLYZFHz2iF397fyuVdY3kZXv55VGDWu1rn+P1cPcZe3PI0F4trldcXsecZUWs3loN2Iwd1HIFgOYZ7ToEu9vTyYZR0DbU9KBlDi+VPvz19xydpE82VEp1f/75N0r5dYHuNKWUUt2VBhGllFIx0+YspVSH6Za36UuDiFKqQ3TL2/SmzVlKqQ7RLW/TmwYRpVSH6Ja36U2DiFKqQwryQi9XosuYpAcNIkqpDpkxfjBDeme1SNNlTNKHdqwrpTpEd5JMbxpElFIdpjPZ05c2ZymllIqZBhGllFIx0yCilFIqZhpElFJKxUyDiFJKqZil5aZUyc6AUkp1U602pUrHmogV648x5uOOnN8df7TM6fGjZU6PnziUuZV0DCJKKaXiRIOIUkqpmGkQic68ZGcgCbTM6UHLnB7iXuZ07FhXSikVJ1oTUUopFTMNIkoppWKmq/hGyBgzFZgLZADzRWR2krMUF8aYR4DTgO0iMtZN6wcsBEYAGwEjIrvc124Gfg40AVeLyJIkZDtmxphhwOPAIMAHzBORuSle5h7AMiAb5//8syJyayqX2c8YkwF8BGwRkdNSvczGmI1ABU4ZGkXkR4kus9ZEIuD+IT4AnAwcAJxvjDkgubmKm0eBqUFpNwFvicho4C33OW6ZzwMOdM950H1vupNG4Dcisj8wHrjSLVcql7kOOEFExgEHA1ONMeNJ7TL7XQN8HfA8Hcp8vIgcLCI/cp8ntMwaRCJzBLBWRNaLSD3wNDAtyXmKCxFZBpQFJU8DHnMfPwacGZD+tIjUicgGYC3Oe9NtiEiJiHziPq7A+YAZQmqX2RaRSvdppvtjk8JlBjDGDAVOBeYHJKd0mcNIaJk1iERmCLA54HmRm5aqBopICTgfusAANz2l3gdjzAjgEODfpHiZjTEZxpjPgO3AGyKS8mUG5gA34DRb+qV6mW3gdWPMx8aYGW5aQsusQSQyoab7p+PY6JR5H4wxecBzwLUisruNQ1OizCLSJCIHA0OBI4wxY9s4vNuX2Rjj7+f7OMJTun2ZXUeLyKE4Te9XGmOOa+PYuJRZg0hkioBhAc+HAsVJyktn2GaMGQzg/rvdTU+J98EYk4kTQJ4Qkefd5JQus5+IfA+8g9MGnsplPho4w+1ofho4wRizgNQuMyJS7P67HXgBp3kqoWXW0VmRWQmMNsaMBLbgdEZdkNwsJdRi4BJgtvvvooD0J40x9wKFwGjgw6TkMEbGGAt4GPhaRO4NeCmVy7wX0CAi3xtjcoBJwJ2kcJlF5GbgZgBjzETgtyJykTHmLlK0zMaYnoBHRCrcxycBfyDBv2etiURARBqBq4AlOB2xIiKrk5ur+DDGPAV8AOxrjCkyxvwc549tsjHmO2Cy+xy3zAJ8BbwGXCkiTcnJecyOBn6C8830M/fnFFK7zIOB/zPGfI7zhegNEXmJ1C5zOKlc5oHAu8aYVTjB4GUReY0El1mXPVFKKRUzrYkopZSKmQYRpZRSMdMgopRSKmYaRJRSSsVMg4hSSqmYaRBRqgsxxhxrjPk2wmN/aox5N9F5UqotOtlQqTgyxnwIXIiztPazInKoMaYy4JBcnFV1/ePxLxeRJ/wvishyYN/Oyq9SHaVBRKk4cZdT+QHOaqhnA/7VgvMCjtkI/EJE3gxxvted2KpUt6FBRKn4GQt8JSK2MeZHuEEkHHc5jgXAX4HrgDeMMQ8DC0RkqHvMTcBlOCuvbgb+W0ReCHEtC7gXpxaUDfwHuEBEvoxT2ZQKSYOIUh1kjLkUuA/IAjzGmO+BPKDGGPMn4BB3v4ZQBgH9cGowHuDIoNfXAccCW4FzgAXGmH38S3sHOAk4DhgDlAP7Ad93rGRKtU+DiFIdJCL/AP5hjFkO/Apnk6/FOMGjvXWFfMCtIlIHYIwJvvYzAU8XutuZHsGeRfT8GoBeOMHjQxH5GqU6gQYRpTrA3b96Pc7eDHk4y6xnuy/vMsbMEpE5bVxih4jUtnH9i4Ff4+yPjXuPguDjRORtY8z9ONs4DzfGvICzcm1be6Uo1WE6xFepDhCRMhHpA1wOzHcfvwacLiJ92gkg0MYmQMaYHwAP4awg3d+99peE3kwIEfmLiByGs2f2GOD6qAqjVAy0JqJUfBzGno70Q4BId9RrS0+cILMDmvteQu5IaIw5HOdL4SdAFVDLnmHESiWM1kSUio/DgE+MMf2BJhHZ1dELishXwD04+71sAw4C3gtzeG+cWssunJFZO4G7O5oHpdqj+4kopZSKmdZElFJKxUyDiFJKqZhpEFFKKRUzDSJKKaVipkFEKaVUzDSIKKWUipkGEaWUUjHTIKKUUipm/x8HavlooFcjoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.717169</td>\n",
       "      <td>0.048519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.357023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>154.500000</td>\n",
       "      <td>2.173067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1.523884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>1.712698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.009949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.850144</td>\n",
       "      <td>0.055234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.507963</td>\n",
       "      <td>0.058045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.980290</td>\n",
       "      <td>0.009749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.633409</td>\n",
       "      <td>0.045756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.887034</td>\n",
       "      <td>0.011638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.787023</td>\n",
       "      <td>0.025067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.744135</td>\n",
       "      <td>0.027536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.605854</td>\n",
       "      <td>0.044390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.904140</td>\n",
       "      <td>0.008685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.744135</td>\n",
       "      <td>0.027536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.717169     0.048519\n",
       "1                    TP        17.000000     2.357023\n",
       "2                    TN       154.500000     2.173067\n",
       "3                    FP         3.100000     1.523884\n",
       "4                    FN        16.400000     1.712698\n",
       "5              Accuracy         0.897906     0.009949\n",
       "6             Precision         0.850144     0.055234\n",
       "7           Sensitivity         0.507963     0.058045\n",
       "8           Specificity         0.980290     0.009749\n",
       "9              F1 score         0.633409     0.045756\n",
       "10  F1 score (weighted)         0.887034     0.011638\n",
       "11     F1 score (macro)         0.787023     0.025067\n",
       "12    Balanced Accuracy         0.744135     0.027536\n",
       "13                  MCC         0.605854     0.044390\n",
       "14                  NPV         0.904140     0.008685\n",
       "15              ROC_AUC         0.744135     0.027536"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.686187</td>\n",
       "      <td>0.735183</td>\n",
       "      <td>0.712420</td>\n",
       "      <td>0.725186</td>\n",
       "      <td>0.632286</td>\n",
       "      <td>0.679581</td>\n",
       "      <td>0.673585</td>\n",
       "      <td>0.722280</td>\n",
       "      <td>0.676301</td>\n",
       "      <td>0.717944</td>\n",
       "      <td>0.696095</td>\n",
       "      <td>0.031876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>33.100000</td>\n",
       "      <td>2.514403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>306.900000</td>\n",
       "      <td>2.884826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>2.685351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>34.900000</td>\n",
       "      <td>2.131770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.876963</td>\n",
       "      <td>0.887435</td>\n",
       "      <td>0.882199</td>\n",
       "      <td>0.887435</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.007301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.827079</td>\n",
       "      <td>0.053476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.486588</td>\n",
       "      <td>0.033792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.977600</td>\n",
       "      <td>0.984100</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.965100</td>\n",
       "      <td>0.971400</td>\n",
       "      <td>0.974400</td>\n",
       "      <td>0.984100</td>\n",
       "      <td>0.977700</td>\n",
       "      <td>0.984000</td>\n",
       "      <td>0.990500</td>\n",
       "      <td>0.977390</td>\n",
       "      <td>0.008532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.660550</td>\n",
       "      <td>0.576577</td>\n",
       "      <td>0.619469</td>\n",
       "      <td>0.579439</td>\n",
       "      <td>0.619469</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.611305</td>\n",
       "      <td>0.028807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.878413</td>\n",
       "      <td>0.893141</td>\n",
       "      <td>0.865463</td>\n",
       "      <td>0.878791</td>\n",
       "      <td>0.869757</td>\n",
       "      <td>0.876321</td>\n",
       "      <td>0.879035</td>\n",
       "      <td>0.884994</td>\n",
       "      <td>0.883139</td>\n",
       "      <td>0.872829</td>\n",
       "      <td>0.878188</td>\n",
       "      <td>0.007894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.776981</td>\n",
       "      <td>0.802031</td>\n",
       "      <td>0.752301</td>\n",
       "      <td>0.776708</td>\n",
       "      <td>0.755473</td>\n",
       "      <td>0.776708</td>\n",
       "      <td>0.769957</td>\n",
       "      <td>0.787601</td>\n",
       "      <td>0.784327</td>\n",
       "      <td>0.754183</td>\n",
       "      <td>0.773627</td>\n",
       "      <td>0.016065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.735195</td>\n",
       "      <td>0.756744</td>\n",
       "      <td>0.717778</td>\n",
       "      <td>0.743734</td>\n",
       "      <td>0.717058</td>\n",
       "      <td>0.737179</td>\n",
       "      <td>0.723407</td>\n",
       "      <td>0.746206</td>\n",
       "      <td>0.738390</td>\n",
       "      <td>0.704193</td>\n",
       "      <td>0.731988</td>\n",
       "      <td>0.015994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.584635</td>\n",
       "      <td>0.634572</td>\n",
       "      <td>0.527144</td>\n",
       "      <td>0.569626</td>\n",
       "      <td>0.539206</td>\n",
       "      <td>0.580630</td>\n",
       "      <td>0.581607</td>\n",
       "      <td>0.602122</td>\n",
       "      <td>0.605807</td>\n",
       "      <td>0.568761</td>\n",
       "      <td>0.579411</td>\n",
       "      <td>0.031352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.897400</td>\n",
       "      <td>0.906200</td>\n",
       "      <td>0.893800</td>\n",
       "      <td>0.904800</td>\n",
       "      <td>0.894700</td>\n",
       "      <td>0.896800</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.902900</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.888900</td>\n",
       "      <td>0.897950</td>\n",
       "      <td>0.005319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.735195</td>\n",
       "      <td>0.756744</td>\n",
       "      <td>0.717778</td>\n",
       "      <td>0.743734</td>\n",
       "      <td>0.717058</td>\n",
       "      <td>0.737179</td>\n",
       "      <td>0.723407</td>\n",
       "      <td>0.746206</td>\n",
       "      <td>0.738390</td>\n",
       "      <td>0.704193</td>\n",
       "      <td>0.731988</td>\n",
       "      <td>0.015994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.686187    0.735183    0.712420    0.725186   \n",
       "1                    TP   34.000000   36.000000   32.000000   35.000000   \n",
       "2                    TN  306.000000  309.000000  303.000000  304.000000   \n",
       "3                    FP    7.000000    5.000000   11.000000   11.000000   \n",
       "4                    FN   35.000000   32.000000   36.000000   32.000000   \n",
       "5              Accuracy    0.890052    0.903141    0.876963    0.887435   \n",
       "6             Precision    0.829268    0.878049    0.744186    0.760870   \n",
       "7           Sensitivity    0.492754    0.529412    0.470588    0.522388   \n",
       "8           Specificity    0.977600    0.984100    0.965000    0.965100   \n",
       "9              F1 score    0.618182    0.660550    0.576577    0.619469   \n",
       "10  F1 score (weighted)    0.878413    0.893141    0.865463    0.878791   \n",
       "11     F1 score (macro)    0.776981    0.802031    0.752301    0.776708   \n",
       "12    Balanced Accuracy    0.735195    0.756744    0.717778    0.743734   \n",
       "13                  MCC    0.584635    0.634572    0.527144    0.569626   \n",
       "14                  NPV    0.897400    0.906200    0.893800    0.904800   \n",
       "15              ROC_AUC    0.735195    0.756744    0.717778    0.743734   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.632286    0.679581    0.673585    0.722280    0.676301    0.717944   \n",
       "1    31.000000   35.000000   31.000000   35.000000   34.000000   28.000000   \n",
       "2   306.000000  304.000000  310.000000  307.000000  308.000000  312.000000   \n",
       "3     9.000000    8.000000    5.000000    7.000000    5.000000    3.000000   \n",
       "4    36.000000   35.000000   36.000000   33.000000   35.000000   39.000000   \n",
       "5     0.882199    0.887435    0.892670    0.895288    0.895288    0.890052   \n",
       "6     0.775000    0.813953    0.861111    0.833333    0.871795    0.903226   \n",
       "7     0.462687    0.500000    0.462687    0.514706    0.492754    0.417910   \n",
       "8     0.971400    0.974400    0.984100    0.977700    0.984000    0.990500   \n",
       "9     0.579439    0.619469    0.601942    0.636364    0.629630    0.571429   \n",
       "10    0.869757    0.876321    0.879035    0.884994    0.883139    0.872829   \n",
       "11    0.755473    0.776708    0.769957    0.787601    0.784327    0.754183   \n",
       "12    0.717058    0.737179    0.723407    0.746206    0.738390    0.704193   \n",
       "13    0.539206    0.580630    0.581607    0.602122    0.605807    0.568761   \n",
       "14    0.894700    0.896800    0.896000    0.902900    0.898000    0.888900   \n",
       "15    0.717058    0.737179    0.723407    0.746206    0.738390    0.704193   \n",
       "\n",
       "           ave       std  \n",
       "0     0.696095  0.031876  \n",
       "1    33.100000  2.514403  \n",
       "2   306.900000  2.884826  \n",
       "3     7.100000  2.685351  \n",
       "4    34.900000  2.131770  \n",
       "5     0.890052  0.007301  \n",
       "6     0.827079  0.053476  \n",
       "7     0.486588  0.033792  \n",
       "8     0.977390  0.008532  \n",
       "9     0.611305  0.028807  \n",
       "10    0.878188  0.007894  \n",
       "11    0.773627  0.016065  \n",
       "12    0.731988  0.015994  \n",
       "13    0.579411  0.031352  \n",
       "14    0.897950  0.005319  \n",
       "15    0.731988  0.015994  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test['std'] = mat_met_rf_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_rf0</th>\n",
       "      <th>y_pred_rf1</th>\n",
       "      <th>y_pred_rf2</th>\n",
       "      <th>y_pred_rf3</th>\n",
       "      <th>y_pred_rf4</th>\n",
       "      <th>y_pred_rf_ave</th>\n",
       "      <th>y_pred_rf_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL585939</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>-0.466978</td>\n",
       "      <td>-0.516722</td>\n",
       "      <td>-0.510956</td>\n",
       "      <td>-0.499591</td>\n",
       "      <td>-0.444505</td>\n",
       "      <td>-0.604792</td>\n",
       "      <td>0.262926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL96051</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.958527</td>\n",
       "      <td>0.903405</td>\n",
       "      <td>0.959831</td>\n",
       "      <td>0.839009</td>\n",
       "      <td>0.967601</td>\n",
       "      <td>0.901395</td>\n",
       "      <td>0.070350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3356916</td>\n",
       "      <td>2</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.533193</td>\n",
       "      <td>0.523964</td>\n",
       "      <td>0.420861</td>\n",
       "      <td>0.515474</td>\n",
       "      <td>0.533998</td>\n",
       "      <td>0.471248</td>\n",
       "      <td>0.085999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3907413</td>\n",
       "      <td>3</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.838872</td>\n",
       "      <td>0.900065</td>\n",
       "      <td>1.842364</td>\n",
       "      <td>0.922014</td>\n",
       "      <td>0.878542</td>\n",
       "      <td>1.058643</td>\n",
       "      <td>0.352757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2047704</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>-2.751918</td>\n",
       "      <td>-2.828513</td>\n",
       "      <td>-2.863556</td>\n",
       "      <td>-2.844224</td>\n",
       "      <td>-2.870000</td>\n",
       "      <td>-2.734702</td>\n",
       "      <td>0.220203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>CHEMBL1095136</td>\n",
       "      <td>1905</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>-0.882156</td>\n",
       "      <td>-0.995647</td>\n",
       "      <td>-1.012706</td>\n",
       "      <td>-0.924442</td>\n",
       "      <td>-0.922984</td>\n",
       "      <td>-0.989656</td>\n",
       "      <td>0.104136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>CHEMBL2012817</td>\n",
       "      <td>1906</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>0.383658</td>\n",
       "      <td>1.130019</td>\n",
       "      <td>1.149580</td>\n",
       "      <td>1.143315</td>\n",
       "      <td>0.843927</td>\n",
       "      <td>0.285083</td>\n",
       "      <td>1.467482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>CHEMBL496511</td>\n",
       "      <td>1907</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.849727</td>\n",
       "      <td>0.989066</td>\n",
       "      <td>0.927252</td>\n",
       "      <td>1.008317</td>\n",
       "      <td>0.986807</td>\n",
       "      <td>0.926862</td>\n",
       "      <td>0.077612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>CHEMBL3940062</td>\n",
       "      <td>1908</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1.543203</td>\n",
       "      <td>1.531139</td>\n",
       "      <td>1.656392</td>\n",
       "      <td>1.756081</td>\n",
       "      <td>1.682863</td>\n",
       "      <td>1.739946</td>\n",
       "      <td>0.249588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>CHEMBL493749</td>\n",
       "      <td>1909</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>0.379300</td>\n",
       "      <td>0.323649</td>\n",
       "      <td>0.206056</td>\n",
       "      <td>0.484909</td>\n",
       "      <td>0.354848</td>\n",
       "      <td>0.085417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1910 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_rf0  y_pred_rf1  \\\n",
       "0          CHEMBL585939            0    -1.19   -0.466978   -0.516722   \n",
       "1           CHEMBL96051            1     0.78    0.958527    0.903405   \n",
       "2         CHEMBL3356916            2     0.30    0.533193    0.523964   \n",
       "3         CHEMBL3907413            3     0.97    0.838872    0.900065   \n",
       "4         CHEMBL2047704            4    -2.25   -2.751918   -2.828513   \n",
       "...                 ...          ...      ...         ...         ...   \n",
       "1905      CHEMBL1095136         1905    -1.20   -0.882156   -0.995647   \n",
       "1906      CHEMBL2012817         1906    -2.94    0.383658    1.130019   \n",
       "1907       CHEMBL496511         1907     0.80    0.849727    0.989066   \n",
       "1908      CHEMBL3940062         1908     2.27    1.543203    1.531139   \n",
       "1909       CHEMBL493749         1909     0.33    0.405172    0.379300   \n",
       "\n",
       "      y_pred_rf2  y_pred_rf3  y_pred_rf4  y_pred_rf_ave  y_pred_rf_std  \n",
       "0      -0.510956   -0.499591   -0.444505      -0.604792       0.262926  \n",
       "1       0.959831    0.839009    0.967601       0.901395       0.070350  \n",
       "2       0.420861    0.515474    0.533998       0.471248       0.085999  \n",
       "3       1.842364    0.922014    0.878542       1.058643       0.352757  \n",
       "4      -2.863556   -2.844224   -2.870000      -2.734702       0.220203  \n",
       "...          ...         ...         ...            ...            ...  \n",
       "1905   -1.012706   -0.924442   -0.922984      -0.989656       0.104136  \n",
       "1906    1.149580    1.143315    0.843927       0.285083       1.467482  \n",
       "1907    0.927252    1.008317    0.986807       0.926862       0.077612  \n",
       "1908    1.656392    1.756081    1.682863       1.739946       0.249588  \n",
       "1909    0.323649    0.206056    0.484909       0.354848       0.085417  \n",
       "\n",
       "[1910 rows x 10 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_rf_cat = np.where(((y_pred_optimized_rf >= 2) | (y_pred_optimized_rf <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_rf_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_rf))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "rf_run0 = data_rf[['y_test_idx0', 'y_test0', 'y_pred_rf0']]\n",
    "rf_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "rf_run0.reset_index(inplace=True, drop=True)\n",
    "rf_run1 = data_rf[['y_test_idx1', 'y_test1', 'y_pred_rf1']]\n",
    "rf_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "rf_run1.reset_index(inplace=True, drop=True)\n",
    "rf_run2 = data_rf[['y_test_idx2', 'y_test2', 'y_pred_rf2']]\n",
    "rf_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "rf_run2.reset_index(inplace=True, drop=True)\n",
    "rf_run3 = data_rf[['y_test_idx3', 'y_test3', 'y_pred_rf3']]\n",
    "rf_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "rf_run3.reset_index(inplace=True, drop=True)\n",
    "rf_run4 = data_rf[['y_test_idx4', 'y_test4', 'y_pred_rf4']]\n",
    "rf_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "rf_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "rf_5preds = pd.concat([chembl_id, rf_run0, rf_run1, rf_run2, rf_run3, rf_run4], axis=1)\n",
    "rf_5preds = rf_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_rf0', 'y_pred_rf1', 'y_pred_rf2', 'y_pred_rf3', 'y_pred_rf4']]\n",
    "rf_5preds['y_pred_rf_ave'] = rf_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "rf_5preds['y_pred_rf_std'] = rf_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "rf_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47203ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.712916</td>\n",
       "      <td>0.033983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.898743</td>\n",
       "      <td>0.017496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.858208</td>\n",
       "      <td>0.069145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.506166</td>\n",
       "      <td>0.085786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.981968</td>\n",
       "      <td>0.009958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.632754</td>\n",
       "      <td>0.076938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.887277</td>\n",
       "      <td>0.021585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.786991</td>\n",
       "      <td>0.043117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.744072</td>\n",
       "      <td>0.043182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.608291</td>\n",
       "      <td>0.076102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.903892</td>\n",
       "      <td>0.015661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.744072</td>\n",
       "      <td>0.043182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.712916     0.033983\n",
       "1              Accuracy         0.898743     0.017496\n",
       "2             Precision         0.858208     0.069145\n",
       "3           Sensitivity         0.506166     0.085786\n",
       "4           Specificity         0.981968     0.009958\n",
       "5              F1 score         0.632754     0.076938\n",
       "6   F1 score (weighted)         0.887277     0.021585\n",
       "7      F1 score (macro)         0.786991     0.043117\n",
       "8     Balanced Accuracy         0.744072     0.043182\n",
       "9                   MCC         0.608291     0.076102\n",
       "10                  NPV         0.903892     0.015661\n",
       "11              ROC_AUC         0.744072     0.043182"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_optimized_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d030d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_5preds.to_csv(output/'rf_5test_CV_result_withSemiSel.csv')\n",
    "mat_met_optimized_rf.to_csv(output/'mat_met_rf_opt_withSemiSel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfc78124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (RF)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model r2_score 0.7074 with a standard deviation of 0.0471\n",
      "RF optimized model r2_score 0.7098 with a standard deviation of 0.0485\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"RF baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_rf.joblib']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf_reg, \"./rf_reg.joblib\")\n",
    "#joblib.dump(optimized_rf, \"./optimized_rf.joblib\") # fitted to whole training set with last random_state selected\n",
    "joblib.dump(optimizedCV_rf, \"./optimizedCV_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.717002     0.039258\n",
      "1                    TP        18.200000     1.932184\n",
      "2                    TN       153.400000     1.776388\n",
      "3                    FP         4.200000     1.229273\n",
      "4                    FN        15.200000     1.549193\n",
      "5              Accuracy         0.898429     0.009622\n",
      "6             Precision         0.813894     0.047709\n",
      "7           Sensitivity         0.544411     0.050206\n",
      "8           Specificity         0.973340     0.007839\n",
      "9              F1 score         0.651074     0.042146\n",
      "10  F1 score (weighted)         0.889984     0.011173\n",
      "11     F1 score (macro)         0.795800     0.023394\n",
      "12    Balanced Accuracy         0.758876     0.024682\n",
      "13                  MCC         0.611667     0.043945\n",
      "14                  NPV         0.909920     0.008292\n",
      "15              ROC_AUC         0.758876     0.024682\n",
      "CPU times: user 7.68 s, sys: 92.1 ms, total: 7.78 s\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_reg = lgbm.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=4,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_reg.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"rmse\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_reg.predict(X_test) \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=4,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=4,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "         # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:00:07,454]\u001b[0m A new study created in memory with name: lgbmRegressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:11,307]\u001b[0m Trial 0 finished with value: 0.6913377463843091 and parameters: {'n_estimators': 837, 'learning_rate': 0.04431612822281546, 'max_depth': 7, 'max_bin': 182, 'num_leaves': 102}. Best is trial 0 with value: 0.6913377463843091.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:13,305]\u001b[0m Trial 1 finished with value: 0.6945974872639238 and parameters: {'n_estimators': 332, 'learning_rate': 0.09753530040386284, 'max_depth': 7, 'max_bin': 180, 'num_leaves': 311}. Best is trial 1 with value: 0.6945974872639238.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:14,780]\u001b[0m Trial 2 finished with value: 0.6728940315371804 and parameters: {'n_estimators': 852, 'learning_rate': 0.14449908621977858, 'max_depth': 4, 'max_bin': 168, 'num_leaves': 390}. Best is trial 1 with value: 0.6945974872639238.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:15,557]\u001b[0m Trial 3 finished with value: 0.6584736951179161 and parameters: {'n_estimators': 94, 'learning_rate': 0.15087829566437327, 'max_depth': 5, 'max_bin': 225, 'num_leaves': 671}. Best is trial 1 with value: 0.6945974872639238.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:17,322]\u001b[0m Trial 4 finished with value: 0.6886126748941237 and parameters: {'n_estimators': 426, 'learning_rate': 0.1750138179308014, 'max_depth': 6, 'max_bin': 219, 'num_leaves': 50}. Best is trial 1 with value: 0.6945974872639238.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:18,296]\u001b[0m Trial 5 finished with value: 0.503040919263573 and parameters: {'n_estimators': 136, 'learning_rate': 0.014209085138907788, 'max_depth': 4, 'max_bin': 249, 'num_leaves': 439}. Best is trial 1 with value: 0.6945974872639238.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:20,914]\u001b[0m Trial 6 finished with value: 0.7006571109800699 and parameters: {'n_estimators': 736, 'learning_rate': 0.11345554138612111, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 183}. Best is trial 6 with value: 0.7006571109800699.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:23,132]\u001b[0m Trial 7 finished with value: 0.694076624251915 and parameters: {'n_estimators': 594, 'learning_rate': 0.157048590389672, 'max_depth': 10, 'max_bin': 170, 'num_leaves': 97}. Best is trial 6 with value: 0.7006571109800699.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:28,939]\u001b[0m Trial 8 finished with value: 0.672593151679125 and parameters: {'n_estimators': 827, 'learning_rate': 0.01293261875524474, 'max_depth': 7, 'max_bin': 296, 'num_leaves': 170}. Best is trial 6 with value: 0.7006571109800699.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:35,681]\u001b[0m Trial 9 finished with value: 0.6873977414332753 and parameters: {'n_estimators': 860, 'learning_rate': 0.015294655175485278, 'max_depth': 10, 'max_bin': 247, 'num_leaves': 334}. Best is trial 6 with value: 0.7006571109800699.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:39,014]\u001b[0m Trial 10 finished with value: 0.7000901227265336 and parameters: {'n_estimators': 629, 'learning_rate': 0.09602079672976285, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 548}. Best is trial 6 with value: 0.7006571109800699.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:41,918]\u001b[0m Trial 11 finished with value: 0.6953399146755614 and parameters: {'n_estimators': 642, 'learning_rate': 0.09684907891838776, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 584}. Best is trial 6 with value: 0.7006571109800699.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:45,721]\u001b[0m Trial 12 finished with value: 0.705322698287947 and parameters: {'n_estimators': 626, 'learning_rate': 0.06672059532665428, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 529}. Best is trial 12 with value: 0.705322698287947.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:49,283]\u001b[0m Trial 13 finished with value: 0.7049969082303017 and parameters: {'n_estimators': 736, 'learning_rate': 0.06148960782900174, 'max_depth': 10, 'max_bin': 270, 'num_leaves': 194}. Best is trial 12 with value: 0.705322698287947.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:52,982]\u001b[0m Trial 14 finished with value: 0.6995495600266118 and parameters: {'n_estimators': 496, 'learning_rate': 0.047193159330526396, 'max_depth': 10, 'max_bin': 266, 'num_leaves': 480}. Best is trial 12 with value: 0.705322698287947.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:55,864]\u001b[0m Trial 15 finished with value: 0.6968486863953756 and parameters: {'n_estimators': 733, 'learning_rate': 0.06428395053404018, 'max_depth': 9, 'max_bin': 268, 'num_leaves': 742}. Best is trial 12 with value: 0.705322698287947.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:00:58,822]\u001b[0m Trial 16 finished with value: 0.700781992300761 and parameters: {'n_estimators': 508, 'learning_rate': 0.0672698944346043, 'max_depth': 9, 'max_bin': 246, 'num_leaves': 224}. Best is trial 12 with value: 0.705322698287947.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:01:01,749]\u001b[0m Trial 17 finished with value: 0.7029685776922235 and parameters: {'n_estimators': 310, 'learning_rate': 0.07236610942127973, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 279}. Best is trial 12 with value: 0.705322698287947.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:01:06,406]\u001b[0m Trial 18 finished with value: 0.6962312246822726 and parameters: {'n_estimators': 712, 'learning_rate': 0.038056059583675556, 'max_depth': 9, 'max_bin': 271, 'num_leaves': 557}. Best is trial 12 with value: 0.705322698287947.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:01:09,340]\u001b[0m Trial 19 finished with value: 0.7024886114925173 and parameters: {'n_estimators': 555, 'learning_rate': 0.12048188590587472, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 459}. Best is trial 12 with value: 0.705322698287947.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:01:12,061]\u001b[0m Trial 20 finished with value: 0.7003256603148766 and parameters: {'n_estimators': 418, 'learning_rate': 0.08017334214802688, 'max_depth': 8, 'max_bin': 233, 'num_leaves': 379}. Best is trial 12 with value: 0.705322698287947.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:01:14,701]\u001b[0m Trial 21 finished with value: 0.7005412425005738 and parameters: {'n_estimators': 260, 'learning_rate': 0.07115768419135883, 'max_depth': 11, 'max_bin': 199, 'num_leaves': 272}. Best is trial 12 with value: 0.705322698287947.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:01:17,404]\u001b[0m Trial 22 finished with value: 0.6818436407181858 and parameters: {'n_estimators': 233, 'learning_rate': 0.03433734185410483, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 224}. Best is trial 12 with value: 0.705322698287947.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:01:20,163]\u001b[0m Trial 23 finished with value: 0.6994400399097531 and parameters: {'n_estimators': 367, 'learning_rate': 0.08369804788439668, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 259}. Best is trial 12 with value: 0.705322698287947.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:01:24,254]\u001b[0m Trial 24 finished with value: 0.708414549149311 and parameters: {'n_estimators': 679, 'learning_rate': 0.053679183074714976, 'max_depth': 12, 'max_bin': 205, 'num_leaves': 147}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:01:27,602]\u001b[0m Trial 25 finished with value: 0.6977368578379152 and parameters: {'n_estimators': 678, 'learning_rate': 0.05501335018009021, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 144}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:01:29,913]\u001b[0m Trial 26 finished with value: 0.6977708299506798 and parameters: {'n_estimators': 780, 'learning_rate': 0.11829226108222747, 'max_depth': 10, 'max_bin': 284, 'num_leaves': 60}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:01:35,952]\u001b[0m Trial 27 finished with value: 0.6947309487135505 and parameters: {'n_estimators': 565, 'learning_rate': 0.023701926862719463, 'max_depth': 12, 'max_bin': 154, 'num_leaves': 603}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:01:39,010]\u001b[0m Trial 28 finished with value: 0.6939086773108951 and parameters: {'n_estimators': 765, 'learning_rate': 0.05917503044666293, 'max_depth': 8, 'max_bin': 261, 'num_leaves': 507}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:01:47,698]\u001b[0m Trial 29 finished with value: 0.6058872684628197 and parameters: {'n_estimators': 672, 'learning_rate': 0.003005642965382027, 'max_depth': 11, 'max_bin': 198, 'num_leaves': 646}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:01:51,853]\u001b[0m Trial 30 finished with value: 0.6996485881815767 and parameters: {'n_estimators': 809, 'learning_rate': 0.044293029194735614, 'max_depth': 9, 'max_bin': 237, 'num_leaves': 109}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:01:54,831]\u001b[0m Trial 31 finished with value: 0.7008216487330118 and parameters: {'n_estimators': 531, 'learning_rate': 0.08021361446643056, 'max_depth': 11, 'max_bin': 199, 'num_leaves': 213}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:01:58,106]\u001b[0m Trial 32 finished with value: 0.7002422861719854 and parameters: {'n_estimators': 294, 'learning_rate': 0.050680108220974004, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 298}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:00,557]\u001b[0m Trial 33 finished with value: 0.6927223409391993 and parameters: {'n_estimators': 432, 'learning_rate': 0.08753864198820166, 'max_depth': 10, 'max_bin': 187, 'num_leaves': 332}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:03,007]\u001b[0m Trial 34 finished with value: 0.634014496194548 and parameters: {'n_estimators': 697, 'learning_rate': 0.03149378551707962, 'max_depth': 3, 'max_bin': 191, 'num_leaves': 375}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:06,204]\u001b[0m Trial 35 finished with value: 0.7036706482474586 and parameters: {'n_estimators': 899, 'learning_rate': 0.07002756414125583, 'max_depth': 11, 'max_bin': 207, 'num_leaves': 132}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:09,101]\u001b[0m Trial 36 finished with value: 0.7004949232584371 and parameters: {'n_estimators': 899, 'learning_rate': 0.1080276856985928, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 30}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:12,381]\u001b[0m Trial 37 finished with value: 0.7016890415697545 and parameters: {'n_estimators': 891, 'learning_rate': 0.05795250118165461, 'max_depth': 10, 'max_bin': 175, 'num_leaves': 126}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:14,491]\u001b[0m Trial 38 finished with value: 0.6911488310084488 and parameters: {'n_estimators': 596, 'learning_rate': 0.1840145787930269, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 75}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:16,380]\u001b[0m Trial 39 finished with value: 0.6910858891300624 and parameters: {'n_estimators': 785, 'learning_rate': 0.12937442304637958, 'max_depth': 6, 'max_bin': 221, 'num_leaves': 173}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:18,974]\u001b[0m Trial 40 finished with value: 0.6994336664487768 and parameters: {'n_estimators': 854, 'learning_rate': 0.1045379618452757, 'max_depth': 11, 'max_bin': 210, 'num_leaves': 196}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:22,202]\u001b[0m Trial 41 finished with value: 0.7048147170682124 and parameters: {'n_estimators': 632, 'learning_rate': 0.07217300887022093, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 146}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:25,237]\u001b[0m Trial 42 finished with value: 0.69701644225666 and parameters: {'n_estimators': 633, 'learning_rate': 0.0883626591362568, 'max_depth': 11, 'max_bin': 187, 'num_leaves': 144}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:28,819]\u001b[0m Trial 43 finished with value: 0.7046550619164039 and parameters: {'n_estimators': 744, 'learning_rate': 0.07232703736527081, 'max_depth': 12, 'max_bin': 201, 'num_leaves': 73}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:32,040]\u001b[0m Trial 44 finished with value: 0.6970378321161901 and parameters: {'n_estimators': 754, 'learning_rate': 0.07668788316833941, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 419}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:36,524]\u001b[0m Trial 45 finished with value: 0.7080303728714139 and parameters: {'n_estimators': 591, 'learning_rate': 0.04569164869052547, 'max_depth': 12, 'max_bin': 181, 'num_leaves': 93}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:41,160]\u001b[0m Trial 46 finished with value: 0.6897884881636063 and parameters: {'n_estimators': 587, 'learning_rate': 0.02434882591916488, 'max_depth': 10, 'max_bin': 180, 'num_leaves': 96}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:45,210]\u001b[0m Trial 47 finished with value: 0.7027022308729503 and parameters: {'n_estimators': 640, 'learning_rate': 0.04294511321493127, 'max_depth': 12, 'max_bin': 172, 'num_leaves': 159}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:48,523]\u001b[0m Trial 48 finished with value: 0.7031640876476397 and parameters: {'n_estimators': 451, 'learning_rate': 0.06101329916925071, 'max_depth': 10, 'max_bin': 276, 'num_leaves': 248}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:51,446]\u001b[0m Trial 49 finished with value: 0.6857458163826161 and parameters: {'n_estimators': 605, 'learning_rate': 0.05057823401630736, 'max_depth': 6, 'max_bin': 193, 'num_leaves': 30}. Best is trial 24 with value: 0.708414549149311.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.7084\n",
      "\tBest params:\n",
      "\t\tn_estimators: 679\n",
      "\t\tlearning_rate: 0.053679183074714976\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 205\n",
      "\t\tnum_leaves: 147\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.745041\n",
      "1                    TP   45.000000\n",
      "2                    TN  304.000000\n",
      "3                    FP    9.000000\n",
      "4                    FN   24.000000\n",
      "5              Accuracy    0.913613\n",
      "6             Precision    0.833333\n",
      "7           Sensitivity    0.652174\n",
      "8           Specificity    0.971200\n",
      "9              F1 score    0.731707\n",
      "10  F1 score (weighted)    0.909356\n",
      "11     F1 score (macro)    0.840113\n",
      "12    Balanced Accuracy    0.811710\n",
      "13                  MCC    0.688404\n",
      "14                  NPV    0.926800\n",
      "15              ROC_AUC    0.811710\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_lgbm_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_0_cat = np.where(((y_pred_lgbm_0 >= 2) | (y_pred_lgbm_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:02:54,968]\u001b[0m Trial 50 finished with value: 0.710386080909658 and parameters: {'n_estimators': 672, 'learning_rate': 0.09517167292823696, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 97}. Best is trial 50 with value: 0.710386080909658.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:02:58,215]\u001b[0m Trial 51 finished with value: 0.7147007280270257 and parameters: {'n_estimators': 663, 'learning_rate': 0.09155011060127356, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 99}. Best is trial 51 with value: 0.7147007280270257.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:01,239]\u001b[0m Trial 52 finished with value: 0.7090418037062193 and parameters: {'n_estimators': 673, 'learning_rate': 0.09278294348537174, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 79}. Best is trial 51 with value: 0.7147007280270257.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:03,814]\u001b[0m Trial 53 finished with value: 0.7063189555756126 and parameters: {'n_estimators': 679, 'learning_rate': 0.09878789348488534, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 88}. Best is trial 51 with value: 0.7147007280270257.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:05,985]\u001b[0m Trial 54 finished with value: 0.7124596925213835 and parameters: {'n_estimators': 671, 'learning_rate': 0.13744537561955295, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 89}. Best is trial 51 with value: 0.7147007280270257.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:08,179]\u001b[0m Trial 55 finished with value: 0.7101323789215663 and parameters: {'n_estimators': 713, 'learning_rate': 0.14496938892129121, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 108}. Best is trial 51 with value: 0.7147007280270257.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:09,980]\u001b[0m Trial 56 finished with value: 0.6933550193224652 and parameters: {'n_estimators': 711, 'learning_rate': 0.15234557069769777, 'max_depth': 5, 'max_bin': 217, 'num_leaves': 52}. Best is trial 51 with value: 0.7147007280270257.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:12,133]\u001b[0m Trial 57 finished with value: 0.7079613987488523 and parameters: {'n_estimators': 821, 'learning_rate': 0.16541062950653918, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 120}. Best is trial 51 with value: 0.7147007280270257.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:14,723]\u001b[0m Trial 58 finished with value: 0.7146000909142144 and parameters: {'n_estimators': 666, 'learning_rate': 0.13672647470206772, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 183}. Best is trial 51 with value: 0.7147007280270257.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:16,794]\u001b[0m Trial 59 finished with value: 0.7039456347419164 and parameters: {'n_estimators': 659, 'learning_rate': 0.1339410756296468, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 176}. Best is trial 51 with value: 0.7147007280270257.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:19,043]\u001b[0m Trial 60 finished with value: 0.7099161226777319 and parameters: {'n_estimators': 716, 'learning_rate': 0.14852834771717685, 'max_depth': 11, 'max_bin': 241, 'num_leaves': 59}. Best is trial 51 with value: 0.7147007280270257.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:21,469]\u001b[0m Trial 61 finished with value: 0.7164943368263635 and parameters: {'n_estimators': 716, 'learning_rate': 0.142795662481213, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 53}. Best is trial 61 with value: 0.7164943368263635.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:23,692]\u001b[0m Trial 62 finished with value: 0.7086151074815745 and parameters: {'n_estimators': 718, 'learning_rate': 0.14322348081975142, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 111}. Best is trial 61 with value: 0.7164943368263635.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:25,654]\u001b[0m Trial 63 finished with value: 0.7027004608006767 and parameters: {'n_estimators': 793, 'learning_rate': 0.16536349140100887, 'max_depth': 11, 'max_bin': 242, 'num_leaves': 51}. Best is trial 61 with value: 0.7164943368263635.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:28,263]\u001b[0m Trial 64 finished with value: 0.7134412766090075 and parameters: {'n_estimators': 704, 'learning_rate': 0.14125291212259344, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 61}. Best is trial 61 with value: 0.7164943368263635.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:30,699]\u001b[0m Trial 65 finished with value: 0.7107512847959392 and parameters: {'n_estimators': 553, 'learning_rate': 0.13829911419037647, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 30}. Best is trial 61 with value: 0.7164943368263635.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:33,215]\u001b[0m Trial 66 finished with value: 0.7118230694041909 and parameters: {'n_estimators': 556, 'learning_rate': 0.13256544893239502, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 35}. Best is trial 61 with value: 0.7164943368263635.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:35,567]\u001b[0m Trial 67 finished with value: 0.7155571047122157 and parameters: {'n_estimators': 533, 'learning_rate': 0.13205657385321706, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 30}. Best is trial 61 with value: 0.7164943368263635.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:37,978]\u001b[0m Trial 68 finished with value: 0.7171001749707007 and parameters: {'n_estimators': 505, 'learning_rate': 0.12652969998370814, 'max_depth': 11, 'max_bin': 257, 'num_leaves': 68}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:40,167]\u001b[0m Trial 69 finished with value: 0.7070839904769395 and parameters: {'n_estimators': 492, 'learning_rate': 0.12412391487061446, 'max_depth': 11, 'max_bin': 263, 'num_leaves': 70}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:42,456]\u001b[0m Trial 70 finished with value: 0.7059815566520049 and parameters: {'n_estimators': 526, 'learning_rate': 0.11371090553347014, 'max_depth': 7, 'max_bin': 257, 'num_leaves': 59}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:44,764]\u001b[0m Trial 71 finished with value: 0.7159264736252025 and parameters: {'n_estimators': 372, 'learning_rate': 0.12824057177235654, 'max_depth': 11, 'max_bin': 254, 'num_leaves': 43}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:46,910]\u001b[0m Trial 72 finished with value: 0.7120226975596307 and parameters: {'n_estimators': 332, 'learning_rate': 0.16027982582029135, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 79}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:47,819]\u001b[0m Trial 73 finished with value: 0.6863441597661472 and parameters: {'n_estimators': 61, 'learning_rate': 0.12560535540547457, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 129}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:50,063]\u001b[0m Trial 74 finished with value: 0.7091281188198754 and parameters: {'n_estimators': 385, 'learning_rate': 0.1408154604734253, 'max_depth': 11, 'max_bin': 259, 'num_leaves': 52}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:52,378]\u001b[0m Trial 75 finished with value: 0.7067617363185724 and parameters: {'n_estimators': 472, 'learning_rate': 0.11517472818549813, 'max_depth': 9, 'max_bin': 232, 'num_leaves': 110}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:54,737]\u001b[0m Trial 76 finished with value: 0.7076586457006548 and parameters: {'n_estimators': 608, 'learning_rate': 0.15438179396661084, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 41}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:57,248]\u001b[0m Trial 77 finished with value: 0.7052135042415334 and parameters: {'n_estimators': 380, 'learning_rate': 0.1069958962042236, 'max_depth': 10, 'max_bin': 266, 'num_leaves': 86}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:03:59,267]\u001b[0m Trial 78 finished with value: 0.7075750093151296 and parameters: {'n_estimators': 402, 'learning_rate': 0.1370840781415342, 'max_depth': 8, 'max_bin': 236, 'num_leaves': 188}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:01,808]\u001b[0m Trial 79 finished with value: 0.7134237431743334 and parameters: {'n_estimators': 517, 'learning_rate': 0.12365298071348137, 'max_depth': 11, 'max_bin': 250, 'num_leaves': 158}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:04,288]\u001b[0m Trial 80 finished with value: 0.7092089311264544 and parameters: {'n_estimators': 464, 'learning_rate': 0.12211465482878174, 'max_depth': 10, 'max_bin': 254, 'num_leaves': 213}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:04:06,627]\u001b[0m Trial 81 finished with value: 0.7132222200044348 and parameters: {'n_estimators': 522, 'learning_rate': 0.12613912418608036, 'max_depth': 11, 'max_bin': 244, 'num_leaves': 154}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:08,463]\u001b[0m Trial 82 finished with value: 0.7062078546648614 and parameters: {'n_estimators': 182, 'learning_rate': 0.13152573157272415, 'max_depth': 11, 'max_bin': 262, 'num_leaves': 152}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:10,788]\u001b[0m Trial 83 finished with value: 0.7128216729950317 and parameters: {'n_estimators': 510, 'learning_rate': 0.1276482949437351, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 133}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:13,020]\u001b[0m Trial 84 finished with value: 0.7117747298623044 and parameters: {'n_estimators': 441, 'learning_rate': 0.11763756379610954, 'max_depth': 11, 'max_bin': 244, 'num_leaves': 68}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:15,135]\u001b[0m Trial 85 finished with value: 0.7041242065543564 and parameters: {'n_estimators': 531, 'learning_rate': 0.14596346942155114, 'max_depth': 10, 'max_bin': 253, 'num_leaves': 239}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:17,626]\u001b[0m Trial 86 finished with value: 0.7087414980439767 and parameters: {'n_estimators': 570, 'learning_rate': 0.1106055427398794, 'max_depth': 11, 'max_bin': 269, 'num_leaves': 166}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:19,772]\u001b[0m Trial 87 finished with value: 0.7078290507751231 and parameters: {'n_estimators': 497, 'learning_rate': 0.15686300649204787, 'max_depth': 10, 'max_bin': 238, 'num_leaves': 727}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:22,224]\u001b[0m Trial 88 finished with value: 0.7103306648292687 and parameters: {'n_estimators': 341, 'learning_rate': 0.12029631739532039, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 122}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:24,847]\u001b[0m Trial 89 finished with value: 0.71256932391045 and parameters: {'n_estimators': 421, 'learning_rate': 0.10488359209877814, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 202}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:26,304]\u001b[0m Trial 90 finished with value: 0.6713033672366804 and parameters: {'n_estimators': 613, 'learning_rate': 0.1757243429242267, 'max_depth': 3, 'max_bin': 230, 'num_leaves': 104}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:28,703]\u001b[0m Trial 91 finished with value: 0.7136857297086062 and parameters: {'n_estimators': 517, 'learning_rate': 0.12756577828684312, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 133}. Best is trial 68 with value: 0.7171001749707007.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:31,234]\u001b[0m Trial 92 finished with value: 0.7178341678552046 and parameters: {'n_estimators': 483, 'learning_rate': 0.12809550852587728, 'max_depth': 11, 'max_bin': 247, 'num_leaves': 45}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:33,520]\u001b[0m Trial 93 finished with value: 0.7111550316251526 and parameters: {'n_estimators': 454, 'learning_rate': 0.13264819849417603, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 46}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:35,813]\u001b[0m Trial 94 finished with value: 0.7058287929710736 and parameters: {'n_estimators': 574, 'learning_rate': 0.14900363585036416, 'max_depth': 11, 'max_bin': 265, 'num_leaves': 68}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:37,537]\u001b[0m Trial 95 finished with value: 0.7052118199010075 and parameters: {'n_estimators': 541, 'learning_rate': 0.14081027350791098, 'max_depth': 9, 'max_bin': 260, 'num_leaves': 50}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:40,164]\u001b[0m Trial 96 finished with value: 0.711841385143844 and parameters: {'n_estimators': 483, 'learning_rate': 0.12916870928215476, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 83}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:42,845]\u001b[0m Trial 97 finished with value: 0.707619304680926 and parameters: {'n_estimators': 653, 'learning_rate': 0.1007117805529872, 'max_depth': 11, 'max_bin': 211, 'num_leaves': 97}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:45,009]\u001b[0m Trial 98 finished with value: 0.7057384012030046 and parameters: {'n_estimators': 759, 'learning_rate': 0.11235376331920763, 'max_depth': 10, 'max_bin': 254, 'num_leaves': 61}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:47,609]\u001b[0m Trial 99 finished with value: 0.708783412279872 and parameters: {'n_estimators': 410, 'learning_rate': 0.11792723843687353, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 117}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7178\n",
      "\tBest params:\n",
      "\t\tn_estimators: 483\n",
      "\t\tlearning_rate: 0.12809550852587728\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 247\n",
      "\t\tnum_leaves: 45\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.745041    0.725091\n",
      "1                    TP   45.000000   40.000000\n",
      "2                    TN  304.000000  308.000000\n",
      "3                    FP    9.000000    6.000000\n",
      "4                    FN   24.000000   28.000000\n",
      "5              Accuracy    0.913613    0.910995\n",
      "6             Precision    0.833333    0.869565\n",
      "7           Sensitivity    0.652174    0.588235\n",
      "8           Specificity    0.971200    0.980900\n",
      "9              F1 score    0.731707    0.701754\n",
      "10  F1 score (weighted)    0.909356    0.903913\n",
      "11     F1 score (macro)    0.840113    0.824723\n",
      "12    Balanced Accuracy    0.811710    0.784564\n",
      "13                  MCC    0.688404    0.668929\n",
      "14                  NPV    0.926800    0.916700\n",
      "15              ROC_AUC    0.811710    0.784564\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_lgbm_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_1_cat = np.where(((y_pred_lgbm_1 >= 2) | (y_pred_lgbm_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:04:49,742]\u001b[0m Trial 100 finished with value: 0.6826798482955796 and parameters: {'n_estimators': 693, 'learning_rate': 0.1338851721190315, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 135}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:51,877]\u001b[0m Trial 101 finished with value: 0.6832508082236279 and parameters: {'n_estimators': 514, 'learning_rate': 0.12692250117533338, 'max_depth': 11, 'max_bin': 244, 'num_leaves': 159}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:53,844]\u001b[0m Trial 102 finished with value: 0.6886058793907078 and parameters: {'n_estimators': 472, 'learning_rate': 0.13746863802499715, 'max_depth': 11, 'max_bin': 234, 'num_leaves': 41}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:56,026]\u001b[0m Trial 103 finished with value: 0.6860428007027466 and parameters: {'n_estimators': 545, 'learning_rate': 0.12312224179289147, 'max_depth': 11, 'max_bin': 252, 'num_leaves': 75}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:04:57,963]\u001b[0m Trial 104 finished with value: 0.6824732972267535 and parameters: {'n_estimators': 518, 'learning_rate': 0.1437701797209767, 'max_depth': 10, 'max_bin': 257, 'num_leaves': 179}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:00,307]\u001b[0m Trial 105 finished with value: 0.6857095413742028 and parameters: {'n_estimators': 498, 'learning_rate': 0.12820645143574716, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 141}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:02,070]\u001b[0m Trial 106 finished with value: 0.6633454978634199 and parameters: {'n_estimators': 579, 'learning_rate': 0.14990859826531105, 'max_depth': 5, 'max_bin': 249, 'num_leaves': 102}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:03,869]\u001b[0m Trial 107 finished with value: 0.6839533667747639 and parameters: {'n_estimators': 737, 'learning_rate': 0.1407542067110335, 'max_depth': 11, 'max_bin': 227, 'num_leaves': 65}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:06,029]\u001b[0m Trial 108 finished with value: 0.6906486732890769 and parameters: {'n_estimators': 293, 'learning_rate': 0.13562756569117312, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 349}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:08,143]\u001b[0m Trial 109 finished with value: 0.6818242154805765 and parameters: {'n_estimators': 619, 'learning_rate': 0.12186439216465358, 'max_depth': 10, 'max_bin': 256, 'num_leaves': 33}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:10,206]\u001b[0m Trial 110 finished with value: 0.6841281350337977 and parameters: {'n_estimators': 650, 'learning_rate': 0.10945398850214272, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 88}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:12,543]\u001b[0m Trial 111 finished with value: 0.6868968277347982 and parameters: {'n_estimators': 509, 'learning_rate': 0.1256090752703575, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 133}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:15,044]\u001b[0m Trial 112 finished with value: 0.6877560115621225 and parameters: {'n_estimators': 439, 'learning_rate': 0.13123805459217583, 'max_depth': 11, 'max_bin': 263, 'num_leaves': 120}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:17,169]\u001b[0m Trial 113 finished with value: 0.6854906222090618 and parameters: {'n_estimators': 535, 'learning_rate': 0.12664010818615115, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 161}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:19,223]\u001b[0m Trial 114 finished with value: 0.6784196810119388 and parameters: {'n_estimators': 486, 'learning_rate': 0.11848643711442056, 'max_depth': 11, 'max_bin': 244, 'num_leaves': 46}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:21,259]\u001b[0m Trial 115 finished with value: 0.6871792974737447 and parameters: {'n_estimators': 454, 'learning_rate': 0.14543806814557586, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 144}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:23,539]\u001b[0m Trial 116 finished with value: 0.6875136677884037 and parameters: {'n_estimators': 507, 'learning_rate': 0.1019473075816785, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 79}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:25,540]\u001b[0m Trial 117 finished with value: 0.6821857690924619 and parameters: {'n_estimators': 596, 'learning_rate': 0.13999120472033066, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 104}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:27,399]\u001b[0m Trial 118 finished with value: 0.6812796669166075 and parameters: {'n_estimators': 701, 'learning_rate': 0.1994821334578224, 'max_depth': 10, 'max_bin': 236, 'num_leaves': 206}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:29,648]\u001b[0m Trial 119 finished with value: 0.6856571794785025 and parameters: {'n_estimators': 356, 'learning_rate': 0.09253898122324768, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 233}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:31,924]\u001b[0m Trial 120 finished with value: 0.6839634895806712 and parameters: {'n_estimators': 558, 'learning_rate': 0.12984083423886336, 'max_depth': 11, 'max_bin': 211, 'num_leaves': 175}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:34,479]\u001b[0m Trial 121 finished with value: 0.6861923063132688 and parameters: {'n_estimators': 422, 'learning_rate': 0.11538082436089202, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 204}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:37,157]\u001b[0m Trial 122 finished with value: 0.6853999000770843 and parameters: {'n_estimators': 397, 'learning_rate': 0.10442165777797188, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 187}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:39,051]\u001b[0m Trial 123 finished with value: 0.6881898105979884 and parameters: {'n_estimators': 470, 'learning_rate': 0.13564691032627013, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 56}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:41,408]\u001b[0m Trial 124 finished with value: 0.6883739018899735 and parameters: {'n_estimators': 426, 'learning_rate': 0.12339854085770452, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 266}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:43,993]\u001b[0m Trial 125 finished with value: 0.6865469254756813 and parameters: {'n_estimators': 366, 'learning_rate': 0.0850382984210918, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 30}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:45,966]\u001b[0m Trial 126 finished with value: 0.6840484484236032 and parameters: {'n_estimators': 484, 'learning_rate': 0.12932175116221228, 'max_depth': 11, 'max_bin': 242, 'num_leaves': 287}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:48,113]\u001b[0m Trial 127 finished with value: 0.6826830610137236 and parameters: {'n_estimators': 727, 'learning_rate': 0.11406918745419867, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 129}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:50,103]\u001b[0m Trial 128 finished with value: 0.6851099920416022 and parameters: {'n_estimators': 517, 'learning_rate': 0.1531590412759951, 'max_depth': 11, 'max_bin': 252, 'num_leaves': 149}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:52,840]\u001b[0m Trial 129 finished with value: 0.6864878271143805 and parameters: {'n_estimators': 446, 'learning_rate': 0.12055977111094807, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 193}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:54,974]\u001b[0m Trial 130 finished with value: 0.6860588149504904 and parameters: {'n_estimators': 688, 'learning_rate': 0.13356411242331834, 'max_depth': 10, 'max_bin': 273, 'num_leaves': 97}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:05:57,199]\u001b[0m Trial 131 finished with value: 0.6902367268172472 and parameters: {'n_estimators': 775, 'learning_rate': 0.13858055204648945, 'max_depth': 12, 'max_bin': 205, 'num_leaves': 87}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:05:59,486]\u001b[0m Trial 132 finished with value: 0.685868475724274 and parameters: {'n_estimators': 496, 'learning_rate': 0.12643129702382885, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 66}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:01,820]\u001b[0m Trial 133 finished with value: 0.6919150456768509 and parameters: {'n_estimators': 670, 'learning_rate': 0.14611888821014662, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 117}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:03,903]\u001b[0m Trial 134 finished with value: 0.6845858665546222 and parameters: {'n_estimators': 526, 'learning_rate': 0.13601581746112157, 'max_depth': 11, 'max_bin': 250, 'num_leaves': 52}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:05,522]\u001b[0m Trial 135 finished with value: 0.6484695740018932 and parameters: {'n_estimators': 634, 'learning_rate': 0.14048962468206974, 'max_depth': 4, 'max_bin': 213, 'num_leaves': 73}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:07,831]\u001b[0m Trial 136 finished with value: 0.6829829955616666 and parameters: {'n_estimators': 748, 'learning_rate': 0.13209809917018103, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 165}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:11,087]\u001b[0m Trial 137 finished with value: 0.6862028748638437 and parameters: {'n_estimators': 660, 'learning_rate': 0.08049353481401846, 'max_depth': 11, 'max_bin': 232, 'num_leaves': 471}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:13,802]\u001b[0m Trial 138 finished with value: 0.6911202760916952 and parameters: {'n_estimators': 547, 'learning_rate': 0.09144798987425184, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 84}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:15,926]\u001b[0m Trial 139 finished with value: 0.6842437004147885 and parameters: {'n_estimators': 568, 'learning_rate': 0.15906154330721203, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 220}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:17,995]\u001b[0m Trial 140 finished with value: 0.686910174357543 and parameters: {'n_estimators': 460, 'learning_rate': 0.12371893429665665, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 44}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:19,734]\u001b[0m Trial 141 finished with value: 0.6812116081027455 and parameters: {'n_estimators': 316, 'learning_rate': 0.1727395709676334, 'max_depth': 11, 'max_bin': 250, 'num_leaves': 109}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:21,563]\u001b[0m Trial 142 finished with value: 0.6775225998674835 and parameters: {'n_estimators': 327, 'learning_rate': 0.1621558021315147, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 74}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:23,629]\u001b[0m Trial 143 finished with value: 0.6783056265234764 and parameters: {'n_estimators': 353, 'learning_rate': 0.11848202981990075, 'max_depth': 11, 'max_bin': 247, 'num_leaves': 96}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:25,335]\u001b[0m Trial 144 finished with value: 0.6792092558108299 and parameters: {'n_estimators': 293, 'learning_rate': 0.14264040500932096, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 59}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:27,187]\u001b[0m Trial 145 finished with value: 0.6782068438547465 and parameters: {'n_estimators': 381, 'learning_rate': 0.12814754229845127, 'max_depth': 7, 'max_bin': 243, 'num_leaves': 130}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:29,523]\u001b[0m Trial 146 finished with value: 0.6876052759705866 and parameters: {'n_estimators': 693, 'learning_rate': 0.1481364563442063, 'max_depth': 10, 'max_bin': 259, 'num_leaves': 42}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:31,962]\u001b[0m Trial 147 finished with value: 0.6864826087326276 and parameters: {'n_estimators': 534, 'learning_rate': 0.1362817512521704, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 85}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:34,524]\u001b[0m Trial 148 finished with value: 0.6890947585734482 and parameters: {'n_estimators': 477, 'learning_rate': 0.10877661296862098, 'max_depth': 11, 'max_bin': 207, 'num_leaves': 144}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:37,038]\u001b[0m Trial 149 finished with value: 0.686434030486412 and parameters: {'n_estimators': 408, 'learning_rate': 0.13124554666735133, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 114}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7178\n",
      "\tBest params:\n",
      "\t\tn_estimators: 483\n",
      "\t\tlearning_rate: 0.12809550852587728\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 247\n",
      "\t\tnum_leaves: 45\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.745041    0.725091    0.720576\n",
      "1                    TP   45.000000   40.000000   32.000000\n",
      "2                    TN  304.000000  308.000000  301.000000\n",
      "3                    FP    9.000000    6.000000   13.000000\n",
      "4                    FN   24.000000   28.000000   36.000000\n",
      "5              Accuracy    0.913613    0.910995    0.871728\n",
      "6             Precision    0.833333    0.869565    0.711111\n",
      "7           Sensitivity    0.652174    0.588235    0.470588\n",
      "8           Specificity    0.971200    0.980900    0.958600\n",
      "9              F1 score    0.731707    0.701754    0.566372\n",
      "10  F1 score (weighted)    0.909356    0.903913    0.860939\n",
      "11     F1 score (macro)    0.840113    0.824723    0.745551\n",
      "12    Balanced Accuracy    0.811710    0.784564    0.714593\n",
      "13                  MCC    0.688404    0.668929    0.509266\n",
      "14                  NPV    0.926800    0.916700    0.893200\n",
      "15              ROC_AUC    0.811710    0.784564    0.714593\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_lgbm_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_2_cat = np.where(((y_pred_lgbm_2 >= 2) | (y_pred_lgbm_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:06:39,603]\u001b[0m Trial 150 finished with value: 0.6875234552086713 and parameters: {'n_estimators': 247, 'learning_rate': 0.151078380928928, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 64}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:42,051]\u001b[0m Trial 151 finished with value: 0.6929003726778752 and parameters: {'n_estimators': 502, 'learning_rate': 0.12866315050104463, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 87}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:45,312]\u001b[0m Trial 152 finished with value: 0.6955288500529793 and parameters: {'n_estimators': 485, 'learning_rate': 0.123591750798496, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 414}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:47,698]\u001b[0m Trial 153 finished with value: 0.6890550811815153 and parameters: {'n_estimators': 713, 'learning_rate': 0.09749490413203511, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 77}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:50,263]\u001b[0m Trial 154 finished with value: 0.6878820829739024 and parameters: {'n_estimators': 514, 'learning_rate': 0.1341649903553006, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 159}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:52,665]\u001b[0m Trial 155 finished with value: 0.6916764281882711 and parameters: {'n_estimators': 438, 'learning_rate': 0.11645864067130898, 'max_depth': 11, 'max_bin': 245, 'num_leaves': 30}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:55,266]\u001b[0m Trial 156 finished with value: 0.6981700604131433 and parameters: {'n_estimators': 469, 'learning_rate': 0.12701574295775966, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 53}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:57,337]\u001b[0m Trial 157 finished with value: 0.6816230837779682 and parameters: {'n_estimators': 540, 'learning_rate': 0.1382275371615449, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 104}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:06:59,795]\u001b[0m Trial 158 finished with value: 0.6924607526480889 and parameters: {'n_estimators': 338, 'learning_rate': 0.12257121510519961, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 122}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:02,078]\u001b[0m Trial 159 finished with value: 0.6928654834399732 and parameters: {'n_estimators': 275, 'learning_rate': 0.1422714583402656, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 175}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:04,291]\u001b[0m Trial 160 finished with value: 0.6866674123249368 and parameters: {'n_estimators': 492, 'learning_rate': 0.1295669060913622, 'max_depth': 11, 'max_bin': 259, 'num_leaves': 71}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:06,518]\u001b[0m Trial 161 finished with value: 0.692670890019969 and parameters: {'n_estimators': 566, 'learning_rate': 0.13266577445089817, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 42}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:08,599]\u001b[0m Trial 162 finished with value: 0.680458895859368 and parameters: {'n_estimators': 521, 'learning_rate': 0.13471564965265126, 'max_depth': 8, 'max_bin': 252, 'num_leaves': 58}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:11,240]\u001b[0m Trial 163 finished with value: 0.6989532132842864 and parameters: {'n_estimators': 507, 'learning_rate': 0.12571674047178208, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 39}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:13,655]\u001b[0m Trial 164 finished with value: 0.6954379667795784 and parameters: {'n_estimators': 545, 'learning_rate': 0.12085201040182737, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 91}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:16,202]\u001b[0m Trial 165 finished with value: 0.6898628498437333 and parameters: {'n_estimators': 586, 'learning_rate': 0.11281414432853103, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 66}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:18,205]\u001b[0m Trial 166 finished with value: 0.6882662666541105 and parameters: {'n_estimators': 727, 'learning_rate': 0.14434903592191511, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 40}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:20,548]\u001b[0m Trial 167 finished with value: 0.6927884433319138 and parameters: {'n_estimators': 676, 'learning_rate': 0.13113902256109108, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 136}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:07:22,641]\u001b[0m Trial 168 finished with value: 0.6852859602266916 and parameters: {'n_estimators': 557, 'learning_rate': 0.13813151890947356, 'max_depth': 11, 'max_bin': 250, 'num_leaves': 31}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:25,267]\u001b[0m Trial 169 finished with value: 0.6881079246890354 and parameters: {'n_estimators': 648, 'learning_rate': 0.1197072485738754, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 80}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:27,496]\u001b[0m Trial 170 finished with value: 0.6932854793790162 and parameters: {'n_estimators': 622, 'learning_rate': 0.13081964219248635, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 54}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:30,012]\u001b[0m Trial 171 finished with value: 0.6908043551409155 and parameters: {'n_estimators': 427, 'learning_rate': 0.10351202675553092, 'max_depth': 11, 'max_bin': 243, 'num_leaves': 70}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:32,642]\u001b[0m Trial 172 finished with value: 0.6887819859238 and parameters: {'n_estimators': 452, 'learning_rate': 0.1175240525615823, 'max_depth': 11, 'max_bin': 245, 'num_leaves': 96}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:35,199]\u001b[0m Trial 173 finished with value: 0.6884886517798354 and parameters: {'n_estimators': 390, 'learning_rate': 0.12554026206166724, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 56}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:37,532]\u001b[0m Trial 174 finished with value: 0.6907646399677805 and parameters: {'n_estimators': 484, 'learning_rate': 0.13477188280327143, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 110}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:40,299]\u001b[0m Trial 175 finished with value: 0.6859265101648355 and parameters: {'n_estimators': 460, 'learning_rate': 0.1281499248696881, 'max_depth': 11, 'max_bin': 247, 'num_leaves': 502}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:43,206]\u001b[0m Trial 176 finished with value: 0.6883187965054904 and parameters: {'n_estimators': 522, 'learning_rate': 0.12030994040546872, 'max_depth': 10, 'max_bin': 266, 'num_leaves': 589}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:45,232]\u001b[0m Trial 177 finished with value: 0.6942979403283618 and parameters: {'n_estimators': 365, 'learning_rate': 0.17208341546255113, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 80}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:47,679]\u001b[0m Trial 178 finished with value: 0.6900590559199224 and parameters: {'n_estimators': 436, 'learning_rate': 0.11287938427892502, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 47}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:49,891]\u001b[0m Trial 179 finished with value: 0.6818805391183048 and parameters: {'n_estimators': 529, 'learning_rate': 0.12405770979806477, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 317}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:51,963]\u001b[0m Trial 180 finished with value: 0.6911075591609759 and parameters: {'n_estimators': 478, 'learning_rate': 0.13834168923968826, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 62}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:54,434]\u001b[0m Trial 181 finished with value: 0.6909628109796564 and parameters: {'n_estimators': 414, 'learning_rate': 0.13212083337627892, 'max_depth': 12, 'max_bin': 249, 'num_leaves': 45}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:57,107]\u001b[0m Trial 182 finished with value: 0.6954599618868547 and parameters: {'n_estimators': 449, 'learning_rate': 0.13370277766544494, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 30}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:07:59,355]\u001b[0m Trial 183 finished with value: 0.6857921687291674 and parameters: {'n_estimators': 503, 'learning_rate': 0.1274945531171485, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 72}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:01,672]\u001b[0m Trial 184 finished with value: 0.6921926102927347 and parameters: {'n_estimators': 464, 'learning_rate': 0.13658372830869633, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 152}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:04,146]\u001b[0m Trial 185 finished with value: 0.6848795687554141 and parameters: {'n_estimators': 692, 'learning_rate': 0.14267988762029407, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 89}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:06,227]\u001b[0m Trial 186 finished with value: 0.689018462069466 and parameters: {'n_estimators': 493, 'learning_rate': 0.13003770308224433, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 52}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:08,299]\u001b[0m Trial 187 finished with value: 0.6871388989487783 and parameters: {'n_estimators': 705, 'learning_rate': 0.1557079378510178, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 100}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:10,853]\u001b[0m Trial 188 finished with value: 0.6926745579176019 and parameters: {'n_estimators': 509, 'learning_rate': 0.10685562154145141, 'max_depth': 10, 'max_bin': 254, 'num_leaves': 122}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:13,708]\u001b[0m Trial 189 finished with value: 0.6961949107437915 and parameters: {'n_estimators': 473, 'learning_rate': 0.08824644735248559, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 72}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:16,374]\u001b[0m Trial 190 finished with value: 0.6889031075053144 and parameters: {'n_estimators': 317, 'learning_rate': 0.12307996643504943, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 194}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:18,653]\u001b[0m Trial 191 finished with value: 0.6913512654848704 and parameters: {'n_estimators': 556, 'learning_rate': 0.138155787098771, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 42}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:21,424]\u001b[0m Trial 192 finished with value: 0.6956174338158663 and parameters: {'n_estimators': 542, 'learning_rate': 0.14633020622309667, 'max_depth': 12, 'max_bin': 249, 'num_leaves': 33}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:23,986]\u001b[0m Trial 193 finished with value: 0.6917748328523069 and parameters: {'n_estimators': 668, 'learning_rate': 0.13329930136195098, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 30}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:25,984]\u001b[0m Trial 194 finished with value: 0.688898722215847 and parameters: {'n_estimators': 514, 'learning_rate': 0.14086229621057944, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 57}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:28,065]\u001b[0m Trial 195 finished with value: 0.6943489795558712 and parameters: {'n_estimators': 526, 'learning_rate': 0.18279422221279953, 'max_depth': 11, 'max_bin': 263, 'num_leaves': 64}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:30,722]\u001b[0m Trial 196 finished with value: 0.6985795065217799 and parameters: {'n_estimators': 602, 'learning_rate': 0.1260526665531144, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 48}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:33,220]\u001b[0m Trial 197 finished with value: 0.6892816378029646 and parameters: {'n_estimators': 493, 'learning_rate': 0.1292074611676728, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 165}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:35,986]\u001b[0m Trial 198 finished with value: 0.6907266941440021 and parameters: {'n_estimators': 400, 'learning_rate': 0.11785920212645469, 'max_depth': 11, 'max_bin': 242, 'num_leaves': 84}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:08:38,066]\u001b[0m Trial 199 finished with value: 0.6925468649111663 and parameters: {'n_estimators': 572, 'learning_rate': 0.13542089772605023, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 138}. Best is trial 92 with value: 0.7178341678552046.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7178\n",
      "\tBest params:\n",
      "\t\tn_estimators: 483\n",
      "\t\tlearning_rate: 0.12809550852587728\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 247\n",
      "\t\tnum_leaves: 45\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.745041    0.725091    0.720576    0.742078\n",
      "1                    TP   45.000000   40.000000   32.000000   36.000000\n",
      "2                    TN  304.000000  308.000000  301.000000  308.000000\n",
      "3                    FP    9.000000    6.000000   13.000000    7.000000\n",
      "4                    FN   24.000000   28.000000   36.000000   31.000000\n",
      "5              Accuracy    0.913613    0.910995    0.871728    0.900524\n",
      "6             Precision    0.833333    0.869565    0.711111    0.837209\n",
      "7           Sensitivity    0.652174    0.588235    0.470588    0.537313\n",
      "8           Specificity    0.971200    0.980900    0.958600    0.977800\n",
      "9              F1 score    0.731707    0.701754    0.566372    0.654545\n",
      "10  F1 score (weighted)    0.909356    0.903913    0.860939    0.891497\n",
      "11     F1 score (macro)    0.840113    0.824723    0.745551    0.798221\n",
      "12    Balanced Accuracy    0.811710    0.784564    0.714593    0.757546\n",
      "13                  MCC    0.688404    0.668929    0.509266    0.619787\n",
      "14                  NPV    0.926800    0.916700    0.893200    0.908600\n",
      "15              ROC_AUC    0.811710    0.784564    0.714593    0.757546\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_lgbm_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_3_cat = np.where(((y_pred_lgbm_3 >= 2) | (y_pred_lgbm_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:08:40,897]\u001b[0m Trial 200 finished with value: 0.7219741414624783 and parameters: {'n_estimators': 550, 'learning_rate': 0.09569941080342356, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 61}. Best is trial 200 with value: 0.7219741414624783.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:43,573]\u001b[0m Trial 201 finished with value: 0.716352356007096 and parameters: {'n_estimators': 552, 'learning_rate': 0.09694979681243907, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 48}. Best is trial 200 with value: 0.7219741414624783.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:46,074]\u001b[0m Trial 202 finished with value: 0.7172837222235133 and parameters: {'n_estimators': 549, 'learning_rate': 0.09142570718417145, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 62}. Best is trial 200 with value: 0.7219741414624783.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:48,825]\u001b[0m Trial 203 finished with value: 0.7200593445299944 and parameters: {'n_estimators': 581, 'learning_rate': 0.09243654591341967, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 77}. Best is trial 200 with value: 0.7219741414624783.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:51,604]\u001b[0m Trial 204 finished with value: 0.7189449450752068 and parameters: {'n_estimators': 584, 'learning_rate': 0.09314146384934296, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 82}. Best is trial 200 with value: 0.7219741414624783.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:54,309]\u001b[0m Trial 205 finished with value: 0.7234105121134821 and parameters: {'n_estimators': 576, 'learning_rate': 0.0941224415998153, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 101}. Best is trial 205 with value: 0.7234105121134821.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:57,008]\u001b[0m Trial 206 finished with value: 0.7212983797796925 and parameters: {'n_estimators': 592, 'learning_rate': 0.09554298804201942, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 103}. Best is trial 205 with value: 0.7234105121134821.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:08:59,778]\u001b[0m Trial 207 finished with value: 0.7203153963450127 and parameters: {'n_estimators': 585, 'learning_rate': 0.09557386775332254, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 109}. Best is trial 205 with value: 0.7234105121134821.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:02,737]\u001b[0m Trial 208 finished with value: 0.7224298056261071 and parameters: {'n_estimators': 587, 'learning_rate': 0.09357352047164198, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 109}. Best is trial 205 with value: 0.7234105121134821.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:05,093]\u001b[0m Trial 209 finished with value: 0.7188305645090274 and parameters: {'n_estimators': 600, 'learning_rate': 0.09540796235095444, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 122}. Best is trial 205 with value: 0.7234105121134821.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:07,967]\u001b[0m Trial 210 finished with value: 0.7240394012392016 and parameters: {'n_estimators': 594, 'learning_rate': 0.09505525202739586, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 110}. Best is trial 210 with value: 0.7240394012392016.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:10,721]\u001b[0m Trial 211 finished with value: 0.7237314918821774 and parameters: {'n_estimators': 589, 'learning_rate': 0.09500396284782439, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 112}. Best is trial 210 with value: 0.7240394012392016.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:13,204]\u001b[0m Trial 212 finished with value: 0.7179960990922026 and parameters: {'n_estimators': 589, 'learning_rate': 0.09497961679854884, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 104}. Best is trial 210 with value: 0.7240394012392016.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:16,122]\u001b[0m Trial 213 finished with value: 0.7244357231527775 and parameters: {'n_estimators': 588, 'learning_rate': 0.09452960082076829, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 110}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:18,819]\u001b[0m Trial 214 finished with value: 0.7207032680416491 and parameters: {'n_estimators': 602, 'learning_rate': 0.09550591772997552, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 110}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:21,324]\u001b[0m Trial 215 finished with value: 0.7184323553015848 and parameters: {'n_estimators': 592, 'learning_rate': 0.0952326164632855, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 111}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:24,193]\u001b[0m Trial 216 finished with value: 0.7202921137612804 and parameters: {'n_estimators': 592, 'learning_rate': 0.09346152751920826, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 114}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:26,778]\u001b[0m Trial 217 finished with value: 0.719806149116534 and parameters: {'n_estimators': 597, 'learning_rate': 0.09595279480637005, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 114}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:29,423]\u001b[0m Trial 218 finished with value: 0.7219741202001639 and parameters: {'n_estimators': 599, 'learning_rate': 0.09569998199463102, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 110}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:32,217]\u001b[0m Trial 219 finished with value: 0.7218113829004744 and parameters: {'n_estimators': 595, 'learning_rate': 0.09541690730376784, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 111}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:34,885]\u001b[0m Trial 220 finished with value: 0.7230479192725521 and parameters: {'n_estimators': 591, 'learning_rate': 0.09384137123929172, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 110}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:37,718]\u001b[0m Trial 221 finished with value: 0.7198908004040596 and parameters: {'n_estimators': 598, 'learning_rate': 0.09396880601315982, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 117}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:40,350]\u001b[0m Trial 222 finished with value: 0.7222259152067468 and parameters: {'n_estimators': 595, 'learning_rate': 0.09354539445789282, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 112}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:42,997]\u001b[0m Trial 223 finished with value: 0.7172221346600824 and parameters: {'n_estimators': 590, 'learning_rate': 0.09326133654042643, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 114}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:45,894]\u001b[0m Trial 224 finished with value: 0.7202581068488438 and parameters: {'n_estimators': 608, 'learning_rate': 0.0844835316640044, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 114}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:48,639]\u001b[0m Trial 225 finished with value: 0.7202963186228639 and parameters: {'n_estimators': 611, 'learning_rate': 0.09977812150903304, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 114}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:51,198]\u001b[0m Trial 226 finished with value: 0.7199789570745984 and parameters: {'n_estimators': 610, 'learning_rate': 0.09970005035395565, 'max_depth': 12, 'max_bin': 295, 'num_leaves': 117}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:53,860]\u001b[0m Trial 227 finished with value: 0.7228667565874168 and parameters: {'n_estimators': 610, 'learning_rate': 0.09986083130173908, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 121}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:56,924]\u001b[0m Trial 228 finished with value: 0.7211594801673973 and parameters: {'n_estimators': 608, 'learning_rate': 0.0847339707885445, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 124}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:09:59,764]\u001b[0m Trial 229 finished with value: 0.72152581560501 and parameters: {'n_estimators': 619, 'learning_rate': 0.08429713639248888, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 124}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:02,804]\u001b[0m Trial 230 finished with value: 0.7201639901426602 and parameters: {'n_estimators': 616, 'learning_rate': 0.0840941816114515, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 125}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:10:05,376]\u001b[0m Trial 231 finished with value: 0.7215256913831343 and parameters: {'n_estimators': 616, 'learning_rate': 0.09972966729055391, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 123}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:08,015]\u001b[0m Trial 232 finished with value: 0.7202451601557259 and parameters: {'n_estimators': 618, 'learning_rate': 0.08429118986436349, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 128}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:11,064]\u001b[0m Trial 233 finished with value: 0.7199524108629538 and parameters: {'n_estimators': 624, 'learning_rate': 0.0840946292388133, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 129}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:13,900]\u001b[0m Trial 234 finished with value: 0.7185887168869096 and parameters: {'n_estimators': 615, 'learning_rate': 0.07818219155258281, 'max_depth': 12, 'max_bin': 296, 'num_leaves': 130}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:16,742]\u001b[0m Trial 235 finished with value: 0.7212528058372242 and parameters: {'n_estimators': 636, 'learning_rate': 0.08754662549100051, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 104}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:19,446]\u001b[0m Trial 236 finished with value: 0.7184858180746951 and parameters: {'n_estimators': 629, 'learning_rate': 0.08711636076602086, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 99}. Best is trial 213 with value: 0.7244357231527775.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:22,499]\u001b[0m Trial 237 finished with value: 0.7257077408139523 and parameters: {'n_estimators': 633, 'learning_rate': 0.0889173543678935, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 131}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:25,486]\u001b[0m Trial 238 finished with value: 0.7201263203237076 and parameters: {'n_estimators': 638, 'learning_rate': 0.08268241238619867, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 145}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:28,332]\u001b[0m Trial 239 finished with value: 0.717511595651816 and parameters: {'n_estimators': 613, 'learning_rate': 0.08781730619755006, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 130}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:30,861]\u001b[0m Trial 240 finished with value: 0.7186651079416027 and parameters: {'n_estimators': 635, 'learning_rate': 0.10089142689918472, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 138}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:33,922]\u001b[0m Trial 241 finished with value: 0.7205523706774472 and parameters: {'n_estimators': 644, 'learning_rate': 0.08175086379416154, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 102}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:36,911]\u001b[0m Trial 242 finished with value: 0.7229192355333955 and parameters: {'n_estimators': 615, 'learning_rate': 0.08904596950076378, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 105}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:40,254]\u001b[0m Trial 243 finished with value: 0.7225250579605175 and parameters: {'n_estimators': 644, 'learning_rate': 0.07386637376123101, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 105}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:42,943]\u001b[0m Trial 244 finished with value: 0.718194884764623 and parameters: {'n_estimators': 645, 'learning_rate': 0.08947324255132819, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 102}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:45,529]\u001b[0m Trial 245 finished with value: 0.7181177323567971 and parameters: {'n_estimators': 577, 'learning_rate': 0.10034580820788075, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 107}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:48,291]\u001b[0m Trial 246 finished with value: 0.7206490260074043 and parameters: {'n_estimators': 607, 'learning_rate': 0.09010393210626763, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 105}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:51,111]\u001b[0m Trial 247 finished with value: 0.7165024616267164 and parameters: {'n_estimators': 625, 'learning_rate': 0.08974063572684379, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 97}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:54,140]\u001b[0m Trial 248 finished with value: 0.7210737909099701 and parameters: {'n_estimators': 604, 'learning_rate': 0.07540572484706293, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 106}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:10:57,002]\u001b[0m Trial 249 finished with value: 0.720532462030458 and parameters: {'n_estimators': 641, 'learning_rate': 0.07935796302715648, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 101}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7257\n",
      "\tBest params:\n",
      "\t\tn_estimators: 633\n",
      "\t\tlearning_rate: 0.0889173543678935\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 291\n",
      "\t\tnum_leaves: 131\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.745041    0.725091    0.720576    0.742078   \n",
      "1                    TP   45.000000   40.000000   32.000000   36.000000   \n",
      "2                    TN  304.000000  308.000000  301.000000  308.000000   \n",
      "3                    FP    9.000000    6.000000   13.000000    7.000000   \n",
      "4                    FN   24.000000   28.000000   36.000000   31.000000   \n",
      "5              Accuracy    0.913613    0.910995    0.871728    0.900524   \n",
      "6             Precision    0.833333    0.869565    0.711111    0.837209   \n",
      "7           Sensitivity    0.652174    0.588235    0.470588    0.537313   \n",
      "8           Specificity    0.971200    0.980900    0.958600    0.977800   \n",
      "9              F1 score    0.731707    0.701754    0.566372    0.654545   \n",
      "10  F1 score (weighted)    0.909356    0.903913    0.860939    0.891497   \n",
      "11     F1 score (macro)    0.840113    0.824723    0.745551    0.798221   \n",
      "12    Balanced Accuracy    0.811710    0.784564    0.714593    0.757546   \n",
      "13                  MCC    0.688404    0.668929    0.509266    0.619787   \n",
      "14                  NPV    0.926800    0.916700    0.893200    0.908600   \n",
      "15              ROC_AUC    0.811710    0.784564    0.714593    0.757546   \n",
      "\n",
      "          Set4  \n",
      "0     0.653028  \n",
      "1    39.000000  \n",
      "2   305.000000  \n",
      "3    10.000000  \n",
      "4    28.000000  \n",
      "5     0.900524  \n",
      "6     0.795918  \n",
      "7     0.582090  \n",
      "8     0.968300  \n",
      "9     0.672414  \n",
      "10    0.894187  \n",
      "11    0.806886  \n",
      "12    0.775172  \n",
      "13    0.625902  \n",
      "14    0.915900  \n",
      "15    0.775172  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_lgbm_4)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_4_cat = np.where(((y_pred_lgbm_4 >= 2) | (y_pred_lgbm_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:11:00,262]\u001b[0m Trial 250 finished with value: 0.699404863334689 and parameters: {'n_estimators': 647, 'learning_rate': 0.07806691958025944, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 97}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:03,601]\u001b[0m Trial 251 finished with value: 0.6975087886344368 and parameters: {'n_estimators': 633, 'learning_rate': 0.07417411554508313, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 101}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:06,671]\u001b[0m Trial 252 finished with value: 0.6986262285317487 and parameters: {'n_estimators': 574, 'learning_rate': 0.06847743519626366, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 105}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:09,821]\u001b[0m Trial 253 finished with value: 0.6957631241837142 and parameters: {'n_estimators': 602, 'learning_rate': 0.08114609046200844, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 141}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:12,587]\u001b[0m Trial 254 finished with value: 0.6945016475395416 and parameters: {'n_estimators': 652, 'learning_rate': 0.07396804353480362, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 124}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:15,138]\u001b[0m Trial 255 finished with value: 0.6997807834162251 and parameters: {'n_estimators': 629, 'learning_rate': 0.08868895267452648, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 92}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:18,202]\u001b[0m Trial 256 finished with value: 0.7051471872616717 and parameters: {'n_estimators': 571, 'learning_rate': 0.10337808479705238, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 109}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:21,563]\u001b[0m Trial 257 finished with value: 0.7000374713917212 and parameters: {'n_estimators': 604, 'learning_rate': 0.09863478982896182, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 631}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:24,651]\u001b[0m Trial 258 finished with value: 0.6988979777813416 and parameters: {'n_estimators': 583, 'learning_rate': 0.08692860881135547, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 124}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:27,601]\u001b[0m Trial 259 finished with value: 0.7005265789099216 and parameters: {'n_estimators': 627, 'learning_rate': 0.07731525008068764, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 95}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:30,655]\u001b[0m Trial 260 finished with value: 0.6991415671360433 and parameters: {'n_estimators': 603, 'learning_rate': 0.09737187421207942, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 144}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:33,548]\u001b[0m Trial 261 finished with value: 0.7069774663457304 and parameters: {'n_estimators': 646, 'learning_rate': 0.09108792689473841, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 103}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:36,521]\u001b[0m Trial 262 finished with value: 0.7002079094744799 and parameters: {'n_estimators': 584, 'learning_rate': 0.08123687160329703, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 116}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:39,293]\u001b[0m Trial 263 finished with value: 0.7025452341385912 and parameters: {'n_estimators': 619, 'learning_rate': 0.08994531975742123, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 134}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:42,086]\u001b[0m Trial 264 finished with value: 0.7017439574068094 and parameters: {'n_estimators': 567, 'learning_rate': 0.09781103576751823, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 94}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:45,114]\u001b[0m Trial 265 finished with value: 0.696309577729689 and parameters: {'n_estimators': 600, 'learning_rate': 0.0658272633757796, 'max_depth': 12, 'max_bin': 297, 'num_leaves': 116}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:47,752]\u001b[0m Trial 266 finished with value: 0.697120304004901 and parameters: {'n_estimators': 635, 'learning_rate': 0.08729006236621359, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 91}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:50,774]\u001b[0m Trial 267 finished with value: 0.6981055210768995 and parameters: {'n_estimators': 659, 'learning_rate': 0.0715691965066585, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 124}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:52,632]\u001b[0m Trial 268 finished with value: 0.6851753526928557 and parameters: {'n_estimators': 619, 'learning_rate': 0.10238222765810037, 'max_depth': 6, 'max_bin': 290, 'num_leaves': 108}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:55,245]\u001b[0m Trial 269 finished with value: 0.699404656129331 and parameters: {'n_estimators': 589, 'learning_rate': 0.09627818095204013, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 149}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:11:58,324]\u001b[0m Trial 270 finished with value: 0.7025610328689587 and parameters: {'n_estimators': 608, 'learning_rate': 0.09080776850508103, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 134}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:00,873]\u001b[0m Trial 271 finished with value: 0.6984389328802796 and parameters: {'n_estimators': 569, 'learning_rate': 0.08010013253810362, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 88}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:04,120]\u001b[0m Trial 272 finished with value: 0.6991663870982043 and parameters: {'n_estimators': 588, 'learning_rate': 0.08578599800915046, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 105}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:06,458]\u001b[0m Trial 273 finished with value: 0.6931376182816339 and parameters: {'n_estimators': 645, 'learning_rate': 0.09398087834383292, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 120}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:08,695]\u001b[0m Trial 274 finished with value: 0.6933143729685821 and parameters: {'n_estimators': 610, 'learning_rate': 0.10584553814301717, 'max_depth': 12, 'max_bin': 297, 'num_leaves': 107}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:11,495]\u001b[0m Trial 275 finished with value: 0.6991200350441947 and parameters: {'n_estimators': 631, 'learning_rate': 0.09802622646294094, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 90}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:14,374]\u001b[0m Trial 276 finished with value: 0.6968308123613639 and parameters: {'n_estimators': 578, 'learning_rate': 0.07500731014525211, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 131}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:17,602]\u001b[0m Trial 277 finished with value: 0.7038707039589692 and parameters: {'n_estimators': 594, 'learning_rate': 0.08992785911629494, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 147}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:20,359]\u001b[0m Trial 278 finished with value: 0.6965521016749899 and parameters: {'n_estimators': 620, 'learning_rate': 0.10124139462388904, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 120}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:23,317]\u001b[0m Trial 279 finished with value: 0.6947457341395745 and parameters: {'n_estimators': 660, 'learning_rate': 0.08341876917160551, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 557}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:25,706]\u001b[0m Trial 280 finished with value: 0.6991591406829535 and parameters: {'n_estimators': 567, 'learning_rate': 0.09368754450711425, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 103}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:12:28,732]\u001b[0m Trial 281 finished with value: 0.7006784380338615 and parameters: {'n_estimators': 606, 'learning_rate': 0.086046140089931, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 87}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:31,578]\u001b[0m Trial 282 finished with value: 0.7010234645388835 and parameters: {'n_estimators': 638, 'learning_rate': 0.09637274708967326, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 112}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:34,704]\u001b[0m Trial 283 finished with value: 0.6992840118189575 and parameters: {'n_estimators': 591, 'learning_rate': 0.09364741669216121, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 132}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:37,524]\u001b[0m Trial 284 finished with value: 0.7048928713871421 and parameters: {'n_estimators': 623, 'learning_rate': 0.08989867269540922, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 97}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:40,587]\u001b[0m Trial 285 finished with value: 0.6958669022317985 and parameters: {'n_estimators': 605, 'learning_rate': 0.08105508285497919, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 714}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:42,968]\u001b[0m Trial 286 finished with value: 0.6966399013584997 and parameters: {'n_estimators': 560, 'learning_rate': 0.10384698263554325, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 122}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:45,650]\u001b[0m Trial 287 finished with value: 0.6995298259053712 and parameters: {'n_estimators': 579, 'learning_rate': 0.09973530154193842, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 154}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:48,411]\u001b[0m Trial 288 finished with value: 0.6968826530581089 and parameters: {'n_estimators': 639, 'learning_rate': 0.07710408485878699, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 107}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:51,382]\u001b[0m Trial 289 finished with value: 0.7008559258203451 and parameters: {'n_estimators': 617, 'learning_rate': 0.08738979558994979, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 83}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:54,488]\u001b[0m Trial 290 finished with value: 0.7012622715754273 and parameters: {'n_estimators': 595, 'learning_rate': 0.0914161363776284, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 124}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:57,410]\u001b[0m Trial 291 finished with value: 0.6984993401357709 and parameters: {'n_estimators': 580, 'learning_rate': 0.09452367888758946, 'max_depth': 12, 'max_bin': 296, 'num_leaves': 138}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:12:59,987]\u001b[0m Trial 292 finished with value: 0.6997831932930934 and parameters: {'n_estimators': 656, 'learning_rate': 0.09729676734070097, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 97}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:02,978]\u001b[0m Trial 293 finished with value: 0.6981653091729467 and parameters: {'n_estimators': 611, 'learning_rate': 0.08656700040576278, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 110}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:05,832]\u001b[0m Trial 294 finished with value: 0.6998870349083572 and parameters: {'n_estimators': 628, 'learning_rate': 0.08194936256111811, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 84}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:08,538]\u001b[0m Trial 295 finished with value: 0.6969009108864348 and parameters: {'n_estimators': 599, 'learning_rate': 0.10109991889200595, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 117}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:11,313]\u001b[0m Trial 296 finished with value: 0.6989429035332693 and parameters: {'n_estimators': 677, 'learning_rate': 0.09059897911380291, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 135}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:14,457]\u001b[0m Trial 297 finished with value: 0.698936358889662 and parameters: {'n_estimators': 564, 'learning_rate': 0.09786185364237199, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 92}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:17,644]\u001b[0m Trial 298 finished with value: 0.6987891549344181 and parameters: {'n_estimators': 585, 'learning_rate': 0.069785951745683, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 103}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:20,338]\u001b[0m Trial 299 finished with value: 0.6988087343678121 and parameters: {'n_estimators': 647, 'learning_rate': 0.10728230985056815, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 120}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7257\n",
      "\tBest params:\n",
      "\t\tn_estimators: 633\n",
      "\t\tlearning_rate: 0.0889173543678935\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 291\n",
      "\t\tnum_leaves: 131\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.745041    0.725091    0.720576    0.742078   \n",
      "1                    TP   45.000000   40.000000   32.000000   36.000000   \n",
      "2                    TN  304.000000  308.000000  301.000000  308.000000   \n",
      "3                    FP    9.000000    6.000000   13.000000    7.000000   \n",
      "4                    FN   24.000000   28.000000   36.000000   31.000000   \n",
      "5              Accuracy    0.913613    0.910995    0.871728    0.900524   \n",
      "6             Precision    0.833333    0.869565    0.711111    0.837209   \n",
      "7           Sensitivity    0.652174    0.588235    0.470588    0.537313   \n",
      "8           Specificity    0.971200    0.980900    0.958600    0.977800   \n",
      "9              F1 score    0.731707    0.701754    0.566372    0.654545   \n",
      "10  F1 score (weighted)    0.909356    0.903913    0.860939    0.891497   \n",
      "11     F1 score (macro)    0.840113    0.824723    0.745551    0.798221   \n",
      "12    Balanced Accuracy    0.811710    0.784564    0.714593    0.757546   \n",
      "13                  MCC    0.688404    0.668929    0.509266    0.619787   \n",
      "14                  NPV    0.926800    0.916700    0.893200    0.908600   \n",
      "15              ROC_AUC    0.811710    0.784564    0.714593    0.757546   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.653028    0.716987  \n",
      "1    39.000000   39.000000  \n",
      "2   305.000000  304.000000  \n",
      "3    10.000000    8.000000  \n",
      "4    28.000000   31.000000  \n",
      "5     0.900524    0.897906  \n",
      "6     0.795918    0.829787  \n",
      "7     0.582090    0.557143  \n",
      "8     0.968300    0.974400  \n",
      "9     0.672414    0.666667  \n",
      "10    0.894187    0.889686  \n",
      "11    0.806886    0.803194  \n",
      "12    0.775172    0.765751  \n",
      "13    0.625902    0.625979  \n",
      "14    0.915900    0.907500  \n",
      "15    0.775172    0.765751  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_lgbm_5)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_5_cat = np.where(((y_pred_lgbm_5 >= 2) | (y_pred_lgbm_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:13:23,223]\u001b[0m Trial 300 finished with value: 0.6946661049347437 and parameters: {'n_estimators': 615, 'learning_rate': 0.09272464784688675, 'max_depth': 9, 'max_bin': 266, 'num_leaves': 148}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:25,362]\u001b[0m Trial 301 finished with value: 0.6758071247843777 and parameters: {'n_estimators': 600, 'learning_rate': 0.07773339670952986, 'max_depth': 4, 'max_bin': 289, 'num_leaves': 109}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:27,805]\u001b[0m Trial 302 finished with value: 0.6995692606852832 and parameters: {'n_estimators': 631, 'learning_rate': 0.08579258136077306, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 81}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:30,886]\u001b[0m Trial 303 finished with value: 0.691294307945118 and parameters: {'n_estimators': 563, 'learning_rate': 0.06152458321217009, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 127}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:33,761]\u001b[0m Trial 304 finished with value: 0.7003018824805386 and parameters: {'n_estimators': 580, 'learning_rate': 0.0956352786738893, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 96}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:36,174]\u001b[0m Trial 305 finished with value: 0.6960588698951621 and parameters: {'n_estimators': 612, 'learning_rate': 0.08945571646750997, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 138}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:38,133]\u001b[0m Trial 306 finished with value: 0.6921012340502586 and parameters: {'n_estimators': 165, 'learning_rate': 0.10319224423543405, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 109}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:40,764]\u001b[0m Trial 307 finished with value: 0.6963339833324463 and parameters: {'n_estimators': 661, 'learning_rate': 0.08338951769084939, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 80}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:43,430]\u001b[0m Trial 308 finished with value: 0.7009655838056645 and parameters: {'n_estimators': 639, 'learning_rate': 0.10048452717826772, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 434}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:45,148]\u001b[0m Trial 309 finished with value: 0.6816568208046003 and parameters: {'n_estimators': 598, 'learning_rate': 0.09223635481107872, 'max_depth': 5, 'max_bin': 290, 'num_leaves': 118}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:47,889]\u001b[0m Trial 310 finished with value: 0.6962718462850083 and parameters: {'n_estimators': 627, 'learning_rate': 0.07976162256531844, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 96}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:50,263]\u001b[0m Trial 311 finished with value: 0.6956471067486998 and parameters: {'n_estimators': 584, 'learning_rate': 0.0963172031399037, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 156}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:52,115]\u001b[0m Trial 312 finished with value: 0.6573896649167861 and parameters: {'n_estimators': 613, 'learning_rate': 0.07388655145571014, 'max_depth': 3, 'max_bin': 287, 'num_leaves': 131}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:13:54,946]\u001b[0m Trial 313 finished with value: 0.6976051030080452 and parameters: {'n_estimators': 564, 'learning_rate': 0.0885795388164851, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 104}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:02,350]\u001b[0m Trial 314 finished with value: 0.6288235135765505 and parameters: {'n_estimators': 594, 'learning_rate': 0.004172728923808711, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 120}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:04,856]\u001b[0m Trial 315 finished with value: 0.6980848029204114 and parameters: {'n_estimators': 644, 'learning_rate': 0.0843842086338776, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 86}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:07,667]\u001b[0m Trial 316 finished with value: 0.701642732282582 and parameters: {'n_estimators': 623, 'learning_rate': 0.09845519006425199, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 143}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:10,495]\u001b[0m Trial 317 finished with value: 0.6999706670270431 and parameters: {'n_estimators': 576, 'learning_rate': 0.09253584914668275, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 97}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:12,742]\u001b[0m Trial 318 finished with value: 0.6891019241120888 and parameters: {'n_estimators': 606, 'learning_rate': 0.10545218742090202, 'max_depth': 8, 'max_bin': 266, 'num_leaves': 115}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:15,349]\u001b[0m Trial 319 finished with value: 0.694415660752484 and parameters: {'n_estimators': 592, 'learning_rate': 0.08868803145528219, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 77}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:17,805]\u001b[0m Trial 320 finished with value: 0.6991893040313822 and parameters: {'n_estimators': 661, 'learning_rate': 0.0932440877516047, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 127}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:20,114]\u001b[0m Trial 321 finished with value: 0.6993044203147173 and parameters: {'n_estimators': 553, 'learning_rate': 0.09984336043536685, 'max_depth': 12, 'max_bin': 297, 'num_leaves': 105}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:22,648]\u001b[0m Trial 322 finished with value: 0.6951813754328363 and parameters: {'n_estimators': 625, 'learning_rate': 0.081113750512492, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 93}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:25,423]\u001b[0m Trial 323 finished with value: 0.7012765050665623 and parameters: {'n_estimators': 607, 'learning_rate': 0.09549190637480195, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 138}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:28,044]\u001b[0m Trial 324 finished with value: 0.7021370169299972 and parameters: {'n_estimators': 574, 'learning_rate': 0.08661426298673214, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 165}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:30,676]\u001b[0m Trial 325 finished with value: 0.6978703345974718 and parameters: {'n_estimators': 639, 'learning_rate': 0.10320409188066693, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 112}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:34,031]\u001b[0m Trial 326 finished with value: 0.705449381586474 and parameters: {'n_estimators': 595, 'learning_rate': 0.07707167794742613, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 122}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:36,336]\u001b[0m Trial 327 finished with value: 0.6965521419502271 and parameters: {'n_estimators': 621, 'learning_rate': 0.09170396407204603, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 78}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:38,887]\u001b[0m Trial 328 finished with value: 0.700402518375587 and parameters: {'n_estimators': 556, 'learning_rate': 0.09660953838052574, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 98}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:41,727]\u001b[0m Trial 329 finished with value: 0.6994005768096944 and parameters: {'n_estimators': 649, 'learning_rate': 0.08654274346965148, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 130}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:44,603]\u001b[0m Trial 330 finished with value: 0.6997829217142142 and parameters: {'n_estimators': 676, 'learning_rate': 0.10887184743328565, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 343}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:14:47,313]\u001b[0m Trial 331 finished with value: 0.6951279357721277 and parameters: {'n_estimators': 606, 'learning_rate': 0.0829292972729328, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 379}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:50,404]\u001b[0m Trial 332 finished with value: 0.6975073383692555 and parameters: {'n_estimators': 581, 'learning_rate': 0.07167524712645808, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 107}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:52,856]\u001b[0m Trial 333 finished with value: 0.6971736657920194 and parameters: {'n_estimators': 632, 'learning_rate': 0.09915019043412354, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 86}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:55,478]\u001b[0m Trial 334 finished with value: 0.6956133700313194 and parameters: {'n_estimators': 593, 'learning_rate': 0.0903315035236523, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 148}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:14:57,968]\u001b[0m Trial 335 finished with value: 0.6976917308870464 and parameters: {'n_estimators': 616, 'learning_rate': 0.09465588953047074, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 115}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:00,575]\u001b[0m Trial 336 finished with value: 0.6973490652336133 and parameters: {'n_estimators': 571, 'learning_rate': 0.08897010187471519, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 96}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:03,083]\u001b[0m Trial 337 finished with value: 0.6995950230184191 and parameters: {'n_estimators': 599, 'learning_rate': 0.1026277579656509, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 127}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:05,591]\u001b[0m Trial 338 finished with value: 0.6930120548909651 and parameters: {'n_estimators': 649, 'learning_rate': 0.07869843107245168, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 73}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:08,059]\u001b[0m Trial 339 finished with value: 0.7004960159180375 and parameters: {'n_estimators': 626, 'learning_rate': 0.09485095762358266, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 109}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:10,674]\u001b[0m Trial 340 finished with value: 0.6965672971479018 and parameters: {'n_estimators': 555, 'learning_rate': 0.08452835977245587, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 140}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:12,909]\u001b[0m Trial 341 finished with value: 0.6947529871616269 and parameters: {'n_estimators': 580, 'learning_rate': 0.09919267556087272, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 93}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:15,452]\u001b[0m Trial 342 finished with value: 0.6974901478324342 and parameters: {'n_estimators': 616, 'learning_rate': 0.0925086298290274, 'max_depth': 12, 'max_bin': 297, 'num_leaves': 121}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:17,840]\u001b[0m Trial 343 finished with value: 0.6931300014041681 and parameters: {'n_estimators': 593, 'learning_rate': 0.07556251300302674, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 106}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:20,447]\u001b[0m Trial 344 finished with value: 0.6956459775388285 and parameters: {'n_estimators': 667, 'learning_rate': 0.08862451626771269, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 158}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:23,962]\u001b[0m Trial 345 finished with value: 0.6933560204708065 and parameters: {'n_estimators': 633, 'learning_rate': 0.06592220411921601, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 524}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:26,374]\u001b[0m Trial 346 finished with value: 0.7008286455889988 and parameters: {'n_estimators': 607, 'learning_rate': 0.09653880790461979, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 77}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:29,349]\u001b[0m Trial 347 finished with value: 0.6974797718590244 and parameters: {'n_estimators': 575, 'learning_rate': 0.08200918125154098, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 127}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:32,169]\u001b[0m Trial 348 finished with value: 0.7012779503202713 and parameters: {'n_estimators': 543, 'learning_rate': 0.09100842865904533, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 92}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:34,333]\u001b[0m Trial 349 finished with value: 0.6964655720355887 and parameters: {'n_estimators': 641, 'learning_rate': 0.10435991536062596, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 114}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.725708\n",
      "\tBest params:\n",
      "\t\tn_estimators: 633\n",
      "\t\tlearning_rate: 0.0889173543678935\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 291\n",
      "\t\tnum_leaves: 131\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.745041    0.725091    0.720576    0.742078   \n",
      "1                    TP   45.000000   40.000000   32.000000   36.000000   \n",
      "2                    TN  304.000000  308.000000  301.000000  308.000000   \n",
      "3                    FP    9.000000    6.000000   13.000000    7.000000   \n",
      "4                    FN   24.000000   28.000000   36.000000   31.000000   \n",
      "5              Accuracy    0.913613    0.910995    0.871728    0.900524   \n",
      "6             Precision    0.833333    0.869565    0.711111    0.837209   \n",
      "7           Sensitivity    0.652174    0.588235    0.470588    0.537313   \n",
      "8           Specificity    0.971200    0.980900    0.958600    0.977800   \n",
      "9              F1 score    0.731707    0.701754    0.566372    0.654545   \n",
      "10  F1 score (weighted)    0.909356    0.903913    0.860939    0.891497   \n",
      "11     F1 score (macro)    0.840113    0.824723    0.745551    0.798221   \n",
      "12    Balanced Accuracy    0.811710    0.784564    0.714593    0.757546   \n",
      "13                  MCC    0.688404    0.668929    0.509266    0.619787   \n",
      "14                  NPV    0.926800    0.916700    0.893200    0.908600   \n",
      "15              ROC_AUC    0.811710    0.784564    0.714593    0.757546   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.653028    0.716987    0.690088  \n",
      "1    39.000000   39.000000   34.000000  \n",
      "2   305.000000  304.000000  305.000000  \n",
      "3    10.000000    8.000000   10.000000  \n",
      "4    28.000000   31.000000   33.000000  \n",
      "5     0.900524    0.897906    0.887435  \n",
      "6     0.795918    0.829787    0.772727  \n",
      "7     0.582090    0.557143    0.507463  \n",
      "8     0.968300    0.974400    0.968300  \n",
      "9     0.672414    0.666667    0.612613  \n",
      "10    0.894187    0.889686    0.877755  \n",
      "11    0.806886    0.803194    0.773381  \n",
      "12    0.775172    0.765751    0.737858  \n",
      "13    0.625902    0.625979    0.566704  \n",
      "14    0.915900    0.907500    0.902400  \n",
      "15    0.775172    0.765751    0.737858  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_lgbm_6)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_6_cat = np.where(((y_pred_lgbm_6 >= 2) | (y_pred_lgbm_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:15:37,268]\u001b[0m Trial 350 finished with value: 0.702621844724201 and parameters: {'n_estimators': 590, 'learning_rate': 0.08552972311700498, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 141}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:39,743]\u001b[0m Trial 351 finished with value: 0.6980825562348979 and parameters: {'n_estimators': 612, 'learning_rate': 0.10017001089277527, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 99}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:42,448]\u001b[0m Trial 352 finished with value: 0.7014858533707298 and parameters: {'n_estimators': 565, 'learning_rate': 0.0943928138264617, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 114}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:44,554]\u001b[0m Trial 353 finished with value: 0.6890282908895478 and parameters: {'n_estimators': 627, 'learning_rate': 0.08994522991473197, 'max_depth': 7, 'max_bin': 166, 'num_leaves': 131}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:47,235]\u001b[0m Trial 354 finished with value: 0.7050055417633518 and parameters: {'n_estimators': 603, 'learning_rate': 0.0803708558264799, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 85}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:49,769]\u001b[0m Trial 355 finished with value: 0.7020600102785337 and parameters: {'n_estimators': 657, 'learning_rate': 0.09669759125077314, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 104}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:52,807]\u001b[0m Trial 356 finished with value: 0.709003359853529 and parameters: {'n_estimators': 587, 'learning_rate': 0.06964350417476284, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 123}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:55,363]\u001b[0m Trial 357 finished with value: 0.7037211084509294 and parameters: {'n_estimators': 621, 'learning_rate': 0.08736899654614859, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 68}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:15:57,873]\u001b[0m Trial 358 finished with value: 0.7028415377451787 and parameters: {'n_estimators': 605, 'learning_rate': 0.10607556667996093, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 148}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:00,450]\u001b[0m Trial 359 finished with value: 0.7037456689626544 and parameters: {'n_estimators': 572, 'learning_rate': 0.09195430124913614, 'max_depth': 12, 'max_bin': 295, 'num_leaves': 101}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:03,048]\u001b[0m Trial 360 finished with value: 0.698032682610189 and parameters: {'n_estimators': 681, 'learning_rate': 0.10013119010385228, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 87}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:06,190]\u001b[0m Trial 361 finished with value: 0.7051795489217615 and parameters: {'n_estimators': 644, 'learning_rate': 0.08318236209285199, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 116}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:09,272]\u001b[0m Trial 362 finished with value: 0.70876107196544 and parameters: {'n_estimators': 588, 'learning_rate': 0.07494800828142982, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 132}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:10,662]\u001b[0m Trial 363 finished with value: 0.6872066210394967 and parameters: {'n_estimators': 97, 'learning_rate': 0.09574280721848688, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 255}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:13,139]\u001b[0m Trial 364 finished with value: 0.7048834529064051 and parameters: {'n_estimators': 552, 'learning_rate': 0.08634500700882725, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 110}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:15,463]\u001b[0m Trial 365 finished with value: 0.7017539043839573 and parameters: {'n_estimators': 624, 'learning_rate': 0.09212770300616077, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 74}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:18,114]\u001b[0m Trial 366 finished with value: 0.6999742181008286 and parameters: {'n_estimators': 606, 'learning_rate': 0.10206106130708432, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 137}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:20,681]\u001b[0m Trial 367 finished with value: 0.7016170504004595 and parameters: {'n_estimators': 638, 'learning_rate': 0.0980128944385076, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 90}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:23,133]\u001b[0m Trial 368 finished with value: 0.7055198555189408 and parameters: {'n_estimators': 575, 'learning_rate': 0.08962295628907624, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 121}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:25,608]\u001b[0m Trial 369 finished with value: 0.7012261312430192 and parameters: {'n_estimators': 594, 'learning_rate': 0.09390662821436219, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 169}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:28,074]\u001b[0m Trial 370 finished with value: 0.7011840450104879 and parameters: {'n_estimators': 657, 'learning_rate': 0.1091928402058285, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 402}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:31,243]\u001b[0m Trial 371 finished with value: 0.700151671229382 and parameters: {'n_estimators': 617, 'learning_rate': 0.08075898675236598, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 101}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:34,140]\u001b[0m Trial 372 finished with value: 0.7041754900833668 and parameters: {'n_estimators': 562, 'learning_rate': 0.08445025826751817, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 153}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:36,693]\u001b[0m Trial 373 finished with value: 0.7023445632812674 and parameters: {'n_estimators': 585, 'learning_rate': 0.0982921289033128, 'max_depth': 12, 'max_bin': 298, 'num_leaves': 110}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:39,318]\u001b[0m Trial 374 finished with value: 0.7039050896724617 and parameters: {'n_estimators': 872, 'learning_rate': 0.07787637803137823, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 83}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:42,177]\u001b[0m Trial 375 finished with value: 0.7053510657845516 and parameters: {'n_estimators': 605, 'learning_rate': 0.08779414882796277, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 124}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:44,482]\u001b[0m Trial 376 finished with value: 0.7011686970382481 and parameters: {'n_estimators': 634, 'learning_rate': 0.09342383279740372, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 96}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:47,140]\u001b[0m Trial 377 finished with value: 0.7026776172345002 and parameters: {'n_estimators': 613, 'learning_rate': 0.10250708779077264, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 134}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:49,802]\u001b[0m Trial 378 finished with value: 0.7068842396107854 and parameters: {'n_estimators': 580, 'learning_rate': 0.07297425070415844, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 110}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:52,505]\u001b[0m Trial 379 finished with value: 0.706743183089259 and parameters: {'n_estimators': 652, 'learning_rate': 0.08991544703290591, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 74}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:16:54,958]\u001b[0m Trial 380 finished with value: 0.7026880279733161 and parameters: {'n_estimators': 540, 'learning_rate': 0.09764525844124114, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 94}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:16:57,894]\u001b[0m Trial 381 finished with value: 0.707880279229758 and parameters: {'n_estimators': 629, 'learning_rate': 0.08372403573780826, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 145}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:00,361]\u001b[0m Trial 382 finished with value: 0.6996835540660584 and parameters: {'n_estimators': 592, 'learning_rate': 0.09403578973701639, 'max_depth': 12, 'max_bin': 194, 'num_leaves': 119}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:03,254]\u001b[0m Trial 383 finished with value: 0.7087517392490834 and parameters: {'n_estimators': 562, 'learning_rate': 0.08742299520609775, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 105}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:07,393]\u001b[0m Trial 384 finished with value: 0.7023871024784772 and parameters: {'n_estimators': 609, 'learning_rate': 0.030574336528421217, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 132}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:09,943]\u001b[0m Trial 385 finished with value: 0.710831499259539 and parameters: {'n_estimators': 670, 'learning_rate': 0.10462083003189834, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 286}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:12,778]\u001b[0m Trial 386 finished with value: 0.7059954666994993 and parameters: {'n_estimators': 597, 'learning_rate': 0.09161178759706036, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 90}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:15,294]\u001b[0m Trial 387 finished with value: 0.700445804759386 and parameters: {'n_estimators': 621, 'learning_rate': 0.07852938066701384, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 68}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:18,070]\u001b[0m Trial 388 finished with value: 0.7020146274695878 and parameters: {'n_estimators': 641, 'learning_rate': 0.09990627513049134, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 114}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:20,425]\u001b[0m Trial 389 finished with value: 0.6995467495695119 and parameters: {'n_estimators': 574, 'learning_rate': 0.09480357924070862, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 125}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:22,893]\u001b[0m Trial 390 finished with value: 0.703072809912179 and parameters: {'n_estimators': 596, 'learning_rate': 0.08819450103128519, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 104}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:24,909]\u001b[0m Trial 391 finished with value: 0.6899252168865482 and parameters: {'n_estimators': 615, 'learning_rate': 0.08274604410554112, 'max_depth': 8, 'max_bin': 291, 'num_leaves': 81}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:27,330]\u001b[0m Trial 392 finished with value: 0.699977213206152 and parameters: {'n_estimators': 633, 'learning_rate': 0.0965459988586633, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 312}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:29,826]\u001b[0m Trial 393 finished with value: 0.6968626529192166 and parameters: {'n_estimators': 583, 'learning_rate': 0.09074845431318924, 'max_depth': 9, 'max_bin': 276, 'num_leaves': 144}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:32,672]\u001b[0m Trial 394 finished with value: 0.7061731484040384 and parameters: {'n_estimators': 560, 'learning_rate': 0.09979247630551964, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 99}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:35,225]\u001b[0m Trial 395 finished with value: 0.7037350772914523 and parameters: {'n_estimators': 652, 'learning_rate': 0.10615932647397508, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 120}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:38,202]\u001b[0m Trial 396 finished with value: 0.7123737174860103 and parameters: {'n_estimators': 603, 'learning_rate': 0.08577150233541692, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 87}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:40,794]\u001b[0m Trial 397 finished with value: 0.6987991223733433 and parameters: {'n_estimators': 624, 'learning_rate': 0.09360638230382103, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 108}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:43,784]\u001b[0m Trial 398 finished with value: 0.7079116644083076 and parameters: {'n_estimators': 584, 'learning_rate': 0.0750581562670046, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 130}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:46,232]\u001b[0m Trial 399 finished with value: 0.7030653098172654 and parameters: {'n_estimators': 683, 'learning_rate': 0.10244279036132653, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 69}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7257077\n",
      "\tBest params:\n",
      "\t\tn_estimators: 633\n",
      "\t\tlearning_rate: 0.0889173543678935\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 291\n",
      "\t\tnum_leaves: 131\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.745041    0.725091    0.720576    0.742078   \n",
      "1                    TP   45.000000   40.000000   32.000000   36.000000   \n",
      "2                    TN  304.000000  308.000000  301.000000  308.000000   \n",
      "3                    FP    9.000000    6.000000   13.000000    7.000000   \n",
      "4                    FN   24.000000   28.000000   36.000000   31.000000   \n",
      "5              Accuracy    0.913613    0.910995    0.871728    0.900524   \n",
      "6             Precision    0.833333    0.869565    0.711111    0.837209   \n",
      "7           Sensitivity    0.652174    0.588235    0.470588    0.537313   \n",
      "8           Specificity    0.971200    0.980900    0.958600    0.977800   \n",
      "9              F1 score    0.731707    0.701754    0.566372    0.654545   \n",
      "10  F1 score (weighted)    0.909356    0.903913    0.860939    0.891497   \n",
      "11     F1 score (macro)    0.840113    0.824723    0.745551    0.798221   \n",
      "12    Balanced Accuracy    0.811710    0.784564    0.714593    0.757546   \n",
      "13                  MCC    0.688404    0.668929    0.509266    0.619787   \n",
      "14                  NPV    0.926800    0.916700    0.893200    0.908600   \n",
      "15              ROC_AUC    0.811710    0.784564    0.714593    0.757546   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.653028    0.716987    0.690088    0.727135  \n",
      "1    39.000000   39.000000   34.000000   35.000000  \n",
      "2   305.000000  304.000000  305.000000  305.000000  \n",
      "3    10.000000    8.000000   10.000000    9.000000  \n",
      "4    28.000000   31.000000   33.000000   33.000000  \n",
      "5     0.900524    0.897906    0.887435    0.890052  \n",
      "6     0.795918    0.829787    0.772727    0.795455  \n",
      "7     0.582090    0.557143    0.507463    0.514706  \n",
      "8     0.968300    0.974400    0.968300    0.971300  \n",
      "9     0.672414    0.666667    0.612613    0.625000  \n",
      "10    0.894187    0.889686    0.877755    0.880296  \n",
      "11    0.806886    0.803194    0.773381    0.780291  \n",
      "12    0.775172    0.765751    0.737858    0.743022  \n",
      "13    0.625902    0.625979    0.566704    0.582384  \n",
      "14    0.915900    0.907500    0.902400    0.902400  \n",
      "15    0.775172    0.765751    0.737858    0.743022  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_lgbm_7)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_7_cat = np.where(((y_pred_lgbm_7 >= 2) | (y_pred_lgbm_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:17:49,404]\u001b[0m Trial 400 finished with value: 0.692960566954802 and parameters: {'n_estimators': 606, 'learning_rate': 0.06863937314613953, 'max_depth': 12, 'max_bin': 295, 'num_leaves': 156}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:52,684]\u001b[0m Trial 401 finished with value: 0.698632852967705 and parameters: {'n_estimators': 640, 'learning_rate': 0.06282111775565902, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 116}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:55,963]\u001b[0m Trial 402 finished with value: 0.7033454961018804 and parameters: {'n_estimators': 543, 'learning_rate': 0.09708040263526029, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 98}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:17:58,809]\u001b[0m Trial 403 finished with value: 0.6980468155904042 and parameters: {'n_estimators': 572, 'learning_rate': 0.08030526111523144, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 134}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:01,404]\u001b[0m Trial 404 finished with value: 0.6951727738330559 and parameters: {'n_estimators': 618, 'learning_rate': 0.09111742712391914, 'max_depth': 12, 'max_bin': 177, 'num_leaves': 84}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:04,236]\u001b[0m Trial 405 finished with value: 0.6945800822665842 and parameters: {'n_estimators': 665, 'learning_rate': 0.08576128229556278, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 108}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:07,545]\u001b[0m Trial 406 finished with value: 0.7029838060043955 and parameters: {'n_estimators': 596, 'learning_rate': 0.05446694145906188, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 122}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:10,539]\u001b[0m Trial 407 finished with value: 0.6967226047106783 and parameters: {'n_estimators': 619, 'learning_rate': 0.09624326816952396, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 178}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:13,992]\u001b[0m Trial 408 finished with value: 0.6946239109350275 and parameters: {'n_estimators': 595, 'learning_rate': 0.09030176288066562, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 662}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:16,920]\u001b[0m Trial 409 finished with value: 0.6935099555054443 and parameters: {'n_estimators': 560, 'learning_rate': 0.0836215494159466, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 95}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:19,537]\u001b[0m Trial 410 finished with value: 0.6955886010014641 and parameters: {'n_estimators': 645, 'learning_rate': 0.10168257755035222, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 134}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:22,884]\u001b[0m Trial 411 finished with value: 0.6941841138283962 and parameters: {'n_estimators': 579, 'learning_rate': 0.1093910621957134, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 115}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:25,228]\u001b[0m Trial 412 finished with value: 0.6909589810604693 and parameters: {'n_estimators': 627, 'learning_rate': 0.09368005517278094, 'max_depth': 6, 'max_bin': 291, 'num_leaves': 145}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:28,218]\u001b[0m Trial 413 finished with value: 0.697253559150219 and parameters: {'n_estimators': 611, 'learning_rate': 0.07716993632882452, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 78}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:31,141]\u001b[0m Trial 414 finished with value: 0.6974943259317218 and parameters: {'n_estimators': 588, 'learning_rate': 0.08802869381707613, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 453}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:34,091]\u001b[0m Trial 415 finished with value: 0.6965367478427594 and parameters: {'n_estimators': 657, 'learning_rate': 0.09845614667952846, 'max_depth': 12, 'max_bin': 296, 'num_leaves': 103}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:36,835]\u001b[0m Trial 416 finished with value: 0.6979111566256825 and parameters: {'n_estimators': 569, 'learning_rate': 0.07262290113191502, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 91}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:39,837]\u001b[0m Trial 417 finished with value: 0.6945048169080814 and parameters: {'n_estimators': 632, 'learning_rate': 0.09467902800480779, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 124}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:42,471]\u001b[0m Trial 418 finished with value: 0.6909726728232181 and parameters: {'n_estimators': 610, 'learning_rate': 0.08126369230769707, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 67}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:45,591]\u001b[0m Trial 419 finished with value: 0.6982321247542707 and parameters: {'n_estimators': 593, 'learning_rate': 0.09014010617565124, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 162}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:47,954]\u001b[0m Trial 420 finished with value: 0.6881510517107582 and parameters: {'n_estimators': 554, 'learning_rate': 0.10099319266587431, 'max_depth': 7, 'max_bin': 280, 'num_leaves': 105}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:50,860]\u001b[0m Trial 421 finished with value: 0.6909853169644811 and parameters: {'n_estimators': 604, 'learning_rate': 0.08572359138714215, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 140}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:53,327]\u001b[0m Trial 422 finished with value: 0.6924718430802208 and parameters: {'n_estimators': 581, 'learning_rate': 0.09268854517340362, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 116}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:55,925]\u001b[0m Trial 423 finished with value: 0.6958109659106827 and parameters: {'n_estimators': 640, 'learning_rate': 0.09706192406886867, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 95}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:18:58,365]\u001b[0m Trial 424 finished with value: 0.6897338041446459 and parameters: {'n_estimators': 620, 'learning_rate': 0.1059522941965327, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 79}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:00,537]\u001b[0m Trial 425 finished with value: 0.6896995394213252 and parameters: {'n_estimators': 598, 'learning_rate': 0.19697940129881206, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 119}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:02,764]\u001b[0m Trial 426 finished with value: 0.6928547935262415 and parameters: {'n_estimators': 220, 'learning_rate': 0.08826714240117188, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 104}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:05,655]\u001b[0m Trial 427 finished with value: 0.6996910886618192 and parameters: {'n_estimators': 664, 'learning_rate': 0.08294748913438824, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 130}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:08,759]\u001b[0m Trial 428 finished with value: 0.699337242750451 and parameters: {'n_estimators': 576, 'learning_rate': 0.09295237768393237, 'max_depth': 12, 'max_bin': 298, 'num_leaves': 88}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:12,173]\u001b[0m Trial 429 finished with value: 0.7058575938243221 and parameters: {'n_estimators': 538, 'learning_rate': 0.07815045721948218, 'max_depth': 12, 'max_bin': 186, 'num_leaves': 152}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:14,558]\u001b[0m Trial 430 finished with value: 0.6921360371407068 and parameters: {'n_estimators': 633, 'learning_rate': 0.09888635183522196, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 112}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:19:17,283]\u001b[0m Trial 431 finished with value: 0.6945224139709891 and parameters: {'n_estimators': 612, 'learning_rate': 0.10234654470465072, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 63}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:19,860]\u001b[0m Trial 432 finished with value: 0.694767942029551 and parameters: {'n_estimators': 589, 'learning_rate': 0.08789078393757507, 'max_depth': 11, 'max_bin': 288, 'num_leaves': 127}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:23,194]\u001b[0m Trial 433 finished with value: 0.6978107485934023 and parameters: {'n_estimators': 648, 'learning_rate': 0.09556144745414069, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 98}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:25,836]\u001b[0m Trial 434 finished with value: 0.6927934755293332 and parameters: {'n_estimators': 563, 'learning_rate': 0.09125231437548238, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 361}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:28,653]\u001b[0m Trial 435 finished with value: 0.6975443973982897 and parameters: {'n_estimators': 624, 'learning_rate': 0.08498774670501669, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 137}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:30,517]\u001b[0m Trial 436 finished with value: 0.6747414001240924 and parameters: {'n_estimators': 604, 'learning_rate': 0.07989159871086231, 'max_depth': 4, 'max_bin': 265, 'num_leaves': 110}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:33,753]\u001b[0m Trial 437 finished with value: 0.7014861896713309 and parameters: {'n_estimators': 582, 'learning_rate': 0.07127159182391404, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 84}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:36,727]\u001b[0m Trial 438 finished with value: 0.6917770727915248 and parameters: {'n_estimators': 630, 'learning_rate': 0.09710517239181053, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 117}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:39,518]\u001b[0m Trial 439 finished with value: 0.6957469360439824 and parameters: {'n_estimators': 691, 'learning_rate': 0.09139043762306279, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 98}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:42,239]\u001b[0m Trial 440 finished with value: 0.6996599042003798 and parameters: {'n_estimators': 606, 'learning_rate': 0.10399933191992072, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 143}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:45,215]\u001b[0m Trial 441 finished with value: 0.6971711679102548 and parameters: {'n_estimators': 547, 'learning_rate': 0.09930326220295842, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 76}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:47,669]\u001b[0m Trial 442 finished with value: 0.6952490601360103 and parameters: {'n_estimators': 570, 'learning_rate': 0.08785014062480354, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 128}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:50,239]\u001b[0m Trial 443 finished with value: 0.696503344503983 and parameters: {'n_estimators': 652, 'learning_rate': 0.09398065286457323, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 107}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:53,646]\u001b[0m Trial 444 finished with value: 0.7043160158025519 and parameters: {'n_estimators': 596, 'learning_rate': 0.07511601979815768, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 88}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:56,477]\u001b[0m Trial 445 finished with value: 0.6931460622635849 and parameters: {'n_estimators': 613, 'learning_rate': 0.0830552231299218, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 126}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:19:59,500]\u001b[0m Trial 446 finished with value: 0.6938375153315601 and parameters: {'n_estimators': 671, 'learning_rate': 0.09531853836108936, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 106}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:02,081]\u001b[0m Trial 447 finished with value: 0.6907149140156332 and parameters: {'n_estimators': 637, 'learning_rate': 0.08961747585380382, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 157}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:05,040]\u001b[0m Trial 448 finished with value: 0.6957129993128506 and parameters: {'n_estimators': 586, 'learning_rate': 0.06654819005494504, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 117}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:07,433]\u001b[0m Trial 449 finished with value: 0.689227196965859 and parameters: {'n_estimators': 619, 'learning_rate': 0.10666251148315425, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 61}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.72570774\n",
      "\tBest params:\n",
      "\t\tn_estimators: 633\n",
      "\t\tlearning_rate: 0.0889173543678935\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 291\n",
      "\t\tnum_leaves: 131\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.745041    0.725091    0.720576    0.742078   \n",
      "1                    TP   45.000000   40.000000   32.000000   36.000000   \n",
      "2                    TN  304.000000  308.000000  301.000000  308.000000   \n",
      "3                    FP    9.000000    6.000000   13.000000    7.000000   \n",
      "4                    FN   24.000000   28.000000   36.000000   31.000000   \n",
      "5              Accuracy    0.913613    0.910995    0.871728    0.900524   \n",
      "6             Precision    0.833333    0.869565    0.711111    0.837209   \n",
      "7           Sensitivity    0.652174    0.588235    0.470588    0.537313   \n",
      "8           Specificity    0.971200    0.980900    0.958600    0.977800   \n",
      "9              F1 score    0.731707    0.701754    0.566372    0.654545   \n",
      "10  F1 score (weighted)    0.909356    0.903913    0.860939    0.891497   \n",
      "11     F1 score (macro)    0.840113    0.824723    0.745551    0.798221   \n",
      "12    Balanced Accuracy    0.811710    0.784564    0.714593    0.757546   \n",
      "13                  MCC    0.688404    0.668929    0.509266    0.619787   \n",
      "14                  NPV    0.926800    0.916700    0.893200    0.908600   \n",
      "15              ROC_AUC    0.811710    0.784564    0.714593    0.757546   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.653028    0.716987    0.690088    0.727135    0.680677  \n",
      "1    39.000000   39.000000   34.000000   35.000000   38.000000  \n",
      "2   305.000000  304.000000  305.000000  305.000000  306.000000  \n",
      "3    10.000000    8.000000   10.000000    9.000000    7.000000  \n",
      "4    28.000000   31.000000   33.000000   33.000000   31.000000  \n",
      "5     0.900524    0.897906    0.887435    0.890052    0.900524  \n",
      "6     0.795918    0.829787    0.772727    0.795455    0.844444  \n",
      "7     0.582090    0.557143    0.507463    0.514706    0.550725  \n",
      "8     0.968300    0.974400    0.968300    0.971300    0.977600  \n",
      "9     0.672414    0.666667    0.612613    0.625000    0.666667  \n",
      "10    0.894187    0.889686    0.877755    0.880296    0.891889  \n",
      "11    0.806886    0.803194    0.773381    0.780291    0.804103  \n",
      "12    0.775172    0.765751    0.737858    0.743022    0.764180  \n",
      "13    0.625902    0.625979    0.566704    0.582384    0.630530  \n",
      "14    0.915900    0.907500    0.902400    0.902400    0.908000  \n",
      "15    0.775172    0.765751    0.737858    0.743022    0.764180  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_lgbm_8)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_8_cat = np.where(((y_pred_lgbm_8 >= 2) | (y_pred_lgbm_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:20:10,307]\u001b[0m Trial 450 finished with value: 0.7126948443851407 and parameters: {'n_estimators': 562, 'learning_rate': 0.10108046279578074, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 93}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:13,446]\u001b[0m Trial 451 finished with value: 0.7151009305297092 and parameters: {'n_estimators': 600, 'learning_rate': 0.0862056888761353, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 140}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:16,338]\u001b[0m Trial 452 finished with value: 0.7137061483831203 and parameters: {'n_estimators': 632, 'learning_rate': 0.09225171053970618, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 76}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:19,722]\u001b[0m Trial 453 finished with value: 0.7158848145697824 and parameters: {'n_estimators': 576, 'learning_rate': 0.08068270354017754, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 119}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:22,251]\u001b[0m Trial 454 finished with value: 0.7128146385942188 and parameters: {'n_estimators': 617, 'learning_rate': 0.09723964343298125, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 101}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:25,102]\u001b[0m Trial 455 finished with value: 0.7148734252253925 and parameters: {'n_estimators': 831, 'learning_rate': 0.08911079536331577, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 238}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:28,144]\u001b[0m Trial 456 finished with value: 0.7174985099976476 and parameters: {'n_estimators': 591, 'learning_rate': 0.0943873262886723, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 88}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:31,076]\u001b[0m Trial 457 finished with value: 0.7153637466756309 and parameters: {'n_estimators': 654, 'learning_rate': 0.1014215986777475, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 130}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:34,474]\u001b[0m Trial 458 finished with value: 0.7133851845805252 and parameters: {'n_estimators': 603, 'learning_rate': 0.08375867486602022, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 110}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:37,278]\u001b[0m Trial 459 finished with value: 0.715730561922878 and parameters: {'n_estimators': 556, 'learning_rate': 0.11071462632155187, 'max_depth': 12, 'max_bin': 298, 'num_leaves': 169}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:40,364]\u001b[0m Trial 460 finished with value: 0.7113718100955041 and parameters: {'n_estimators': 625, 'learning_rate': 0.0766889802955462, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 99}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:42,920]\u001b[0m Trial 461 finished with value: 0.7130207521369127 and parameters: {'n_estimators': 643, 'learning_rate': 0.09746237197218514, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 149}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:46,231]\u001b[0m Trial 462 finished with value: 0.7190143742998564 and parameters: {'n_estimators': 578, 'learning_rate': 0.08826205108931569, 'max_depth': 11, 'max_bin': 293, 'num_leaves': 626}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:49,005]\u001b[0m Trial 463 finished with value: 0.7194890490710928 and parameters: {'n_estimators': 601, 'learning_rate': 0.09208532200659816, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 121}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:51,892]\u001b[0m Trial 464 finished with value: 0.7131864296094412 and parameters: {'n_estimators': 614, 'learning_rate': 0.08458349270249939, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 71}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:55,071]\u001b[0m Trial 465 finished with value: 0.7225310021757686 and parameters: {'n_estimators': 584, 'learning_rate': 0.09931253821900694, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 137}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:20:58,233]\u001b[0m Trial 466 finished with value: 0.7143072433394846 and parameters: {'n_estimators': 547, 'learning_rate': 0.10411153875378029, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 151}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:00,712]\u001b[0m Trial 467 finished with value: 0.7141070148457086 and parameters: {'n_estimators': 677, 'learning_rate': 0.09944820366944102, 'max_depth': 9, 'max_bin': 278, 'num_leaves': 136}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:03,242]\u001b[0m Trial 468 finished with value: 0.7144036922228661 and parameters: {'n_estimators': 591, 'learning_rate': 0.10573535990350154, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 129}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:06,499]\u001b[0m Trial 469 finished with value: 0.7162581014741456 and parameters: {'n_estimators': 568, 'learning_rate': 0.07919837850356488, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 114}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:09,889]\u001b[0m Trial 470 finished with value: 0.714312883411034 and parameters: {'n_estimators': 630, 'learning_rate': 0.0721747435241992, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 143}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:12,618]\u001b[0m Trial 471 finished with value: 0.7122104261416883 and parameters: {'n_estimators': 647, 'learning_rate': 0.0932956757977983, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 93}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:15,534]\u001b[0m Trial 472 finished with value: 0.7219610869037904 and parameters: {'n_estimators': 613, 'learning_rate': 0.0994278476084947, 'max_depth': 11, 'max_bin': 286, 'num_leaves': 165}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:18,639]\u001b[0m Trial 473 finished with value: 0.7172735335072185 and parameters: {'n_estimators': 602, 'learning_rate': 0.10344800846966072, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 165}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:21,646]\u001b[0m Trial 474 finished with value: 0.716355327869754 and parameters: {'n_estimators': 573, 'learning_rate': 0.10777591785299402, 'max_depth': 11, 'max_bin': 286, 'num_leaves': 154}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:24,495]\u001b[0m Trial 475 finished with value: 0.7182613165858288 and parameters: {'n_estimators': 615, 'learning_rate': 0.09818945080933908, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 143}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:27,188]\u001b[0m Trial 476 finished with value: 0.7205576780858882 and parameters: {'n_estimators': 588, 'learning_rate': 0.10155710437596362, 'max_depth': 11, 'max_bin': 282, 'num_leaves': 123}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:29,595]\u001b[0m Trial 477 finished with value: 0.7158950836611346 and parameters: {'n_estimators': 587, 'learning_rate': 0.10156556776824016, 'max_depth': 11, 'max_bin': 280, 'num_leaves': 136}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:31,932]\u001b[0m Trial 478 finished with value: 0.7082614536465137 and parameters: {'n_estimators': 563, 'learning_rate': 0.11057304973902737, 'max_depth': 10, 'max_bin': 278, 'num_leaves': 160}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:34,603]\u001b[0m Trial 479 finished with value: 0.7098970554313817 and parameters: {'n_estimators': 585, 'learning_rate': 0.1003872078903424, 'max_depth': 10, 'max_bin': 282, 'num_leaves': 132}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:37,211]\u001b[0m Trial 480 finished with value: 0.7107888536929081 and parameters: {'n_estimators': 537, 'learning_rate': 0.10500495766196509, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 175}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:21:39,744]\u001b[0m Trial 481 finished with value: 0.7107711858977954 and parameters: {'n_estimators': 605, 'learning_rate': 0.09615744308880782, 'max_depth': 11, 'max_bin': 284, 'num_leaves': 122}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:42,670]\u001b[0m Trial 482 finished with value: 0.7218187084756644 and parameters: {'n_estimators': 574, 'learning_rate': 0.09939510399705968, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 125}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:45,211]\u001b[0m Trial 483 finished with value: 0.7139229741199575 and parameters: {'n_estimators': 551, 'learning_rate': 0.0962267080158993, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 188}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:48,774]\u001b[0m Trial 484 finished with value: 0.7138564945808562 and parameters: {'n_estimators': 567, 'learning_rate': 0.058876506383277794, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 142}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:50,805]\u001b[0m Trial 485 finished with value: 0.6938496108400104 and parameters: {'n_estimators': 573, 'learning_rate': 0.09248670682335675, 'max_depth': 6, 'max_bin': 274, 'num_leaves': 745}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:53,710]\u001b[0m Trial 486 finished with value: 0.7187434276053211 and parameters: {'n_estimators': 595, 'learning_rate': 0.0988660922060234, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 489}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:56,457]\u001b[0m Trial 487 finished with value: 0.7144076178168095 and parameters: {'n_estimators': 614, 'learning_rate': 0.09553382252363023, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 115}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:21:59,279]\u001b[0m Trial 488 finished with value: 0.7223688222812823 and parameters: {'n_estimators': 527, 'learning_rate': 0.09204201037097992, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 154}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:22:01,884]\u001b[0m Trial 489 finished with value: 0.7089050504105794 and parameters: {'n_estimators': 535, 'learning_rate': 0.10350005766312494, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 160}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:22:04,902]\u001b[0m Trial 490 finished with value: 0.723736726907425 and parameters: {'n_estimators': 533, 'learning_rate': 0.09837069878151514, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 187}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:22:07,153]\u001b[0m Trial 491 finished with value: 0.7061610937128179 and parameters: {'n_estimators': 520, 'learning_rate': 0.10023003424643069, 'max_depth': 8, 'max_bin': 271, 'num_leaves': 184}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:22:09,028]\u001b[0m Trial 492 finished with value: 0.694436490827494 and parameters: {'n_estimators': 559, 'learning_rate': 0.10884102436379425, 'max_depth': 5, 'max_bin': 269, 'num_leaves': 170}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:22:11,731]\u001b[0m Trial 493 finished with value: 0.719126826514578 and parameters: {'n_estimators': 541, 'learning_rate': 0.09837735865704299, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 155}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:22:14,896]\u001b[0m Trial 494 finished with value: 0.718941340144545 and parameters: {'n_estimators': 524, 'learning_rate': 0.09262720131876008, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 176}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:22:17,572]\u001b[0m Trial 495 finished with value: 0.7164072636244625 and parameters: {'n_estimators': 528, 'learning_rate': 0.10215462228273414, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 186}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:22:20,218]\u001b[0m Trial 496 finished with value: 0.7176515657267648 and parameters: {'n_estimators': 546, 'learning_rate': 0.10554781064409707, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 159}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:22:22,744]\u001b[0m Trial 497 finished with value: 0.708144744365202 and parameters: {'n_estimators': 536, 'learning_rate': 0.11364686149204993, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 169}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:22:25,970]\u001b[0m Trial 498 finished with value: 0.7134823022691106 and parameters: {'n_estimators': 558, 'learning_rate': 0.09466697072020534, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 138}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:22:28,780]\u001b[0m Trial 499 finished with value: 0.7110926537756022 and parameters: {'n_estimators': 545, 'learning_rate': 0.09037572117633251, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 207}. Best is trial 237 with value: 0.7257077408139523.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.725707741\n",
      "\tBest params:\n",
      "\t\tn_estimators: 633\n",
      "\t\tlearning_rate: 0.0889173543678935\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 291\n",
      "\t\tnum_leaves: 131\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.745041    0.725091    0.720576    0.742078   \n",
      "1                    TP   45.000000   40.000000   32.000000   36.000000   \n",
      "2                    TN  304.000000  308.000000  301.000000  308.000000   \n",
      "3                    FP    9.000000    6.000000   13.000000    7.000000   \n",
      "4                    FN   24.000000   28.000000   36.000000   31.000000   \n",
      "5              Accuracy    0.913613    0.910995    0.871728    0.900524   \n",
      "6             Precision    0.833333    0.869565    0.711111    0.837209   \n",
      "7           Sensitivity    0.652174    0.588235    0.470588    0.537313   \n",
      "8           Specificity    0.971200    0.980900    0.958600    0.977800   \n",
      "9              F1 score    0.731707    0.701754    0.566372    0.654545   \n",
      "10  F1 score (weighted)    0.909356    0.903913    0.860939    0.891497   \n",
      "11     F1 score (macro)    0.840113    0.824723    0.745551    0.798221   \n",
      "12    Balanced Accuracy    0.811710    0.784564    0.714593    0.757546   \n",
      "13                  MCC    0.688404    0.668929    0.509266    0.619787   \n",
      "14                  NPV    0.926800    0.916700    0.893200    0.908600   \n",
      "15              ROC_AUC    0.811710    0.784564    0.714593    0.757546   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.653028    0.716987    0.690088    0.727135    0.680677    0.719926  \n",
      "1    39.000000   39.000000   34.000000   35.000000   38.000000   33.000000  \n",
      "2   305.000000  304.000000  305.000000  305.000000  306.000000  306.000000  \n",
      "3    10.000000    8.000000   10.000000    9.000000    7.000000    9.000000  \n",
      "4    28.000000   31.000000   33.000000   33.000000   31.000000   34.000000  \n",
      "5     0.900524    0.897906    0.887435    0.890052    0.900524    0.887435  \n",
      "6     0.795918    0.829787    0.772727    0.795455    0.844444    0.785714  \n",
      "7     0.582090    0.557143    0.507463    0.514706    0.550725    0.492537  \n",
      "8     0.968300    0.974400    0.968300    0.971300    0.977600    0.971400  \n",
      "9     0.672414    0.666667    0.612613    0.625000    0.666667    0.605505  \n",
      "10    0.894187    0.889686    0.877755    0.880296    0.891889    0.876674  \n",
      "11    0.806886    0.803194    0.773381    0.780291    0.804103    0.769928  \n",
      "12    0.775172    0.765751    0.737858    0.743022    0.764180    0.731983  \n",
      "13    0.625902    0.625979    0.566704    0.582384    0.630530    0.564046  \n",
      "14    0.915900    0.907500    0.902400    0.902400    0.908000    0.900000  \n",
      "15    0.775172    0.765751    0.737858    0.743022    0.764180    0.731983  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_lgbm_9)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_9_cat = np.where(((y_pred_lgbm_9 >= 2) | (y_pred_lgbm_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.724337     0.039550\n",
      "1                    TP        18.000000     2.000000\n",
      "2                    TN       153.700000     2.263233\n",
      "3                    FP         3.900000     1.728840\n",
      "4                    FN        15.400000     1.646545\n",
      "5              Accuracy         0.898953     0.010486\n",
      "6             Precision         0.826777     0.061355\n",
      "7           Sensitivity         0.538455     0.050479\n",
      "8           Specificity         0.975230     0.011015\n",
      "9              F1 score         0.650019     0.038619\n",
      "10  F1 score (weighted)         0.890088     0.011386\n",
      "11     F1 score (macro)         0.795464     0.021827\n",
      "12    Balanced Accuracy         0.756845     0.023844\n",
      "13                  MCC         0.613991     0.042704\n",
      "14                  NPV         0.909020     0.008706\n",
      "15              ROC_AUC         0.756845     0.023844\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.745041</td>\n",
       "      <td>0.725091</td>\n",
       "      <td>0.720576</td>\n",
       "      <td>0.742078</td>\n",
       "      <td>0.653028</td>\n",
       "      <td>0.716987</td>\n",
       "      <td>0.690088</td>\n",
       "      <td>0.727135</td>\n",
       "      <td>0.680677</td>\n",
       "      <td>0.719926</td>\n",
       "      <td>0.712063</td>\n",
       "      <td>0.028862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>37.100000</td>\n",
       "      <td>3.900142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>305.200000</td>\n",
       "      <td>2.043961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>1.988858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>30.900000</td>\n",
       "      <td>3.478505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.913613</td>\n",
       "      <td>0.910995</td>\n",
       "      <td>0.871728</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.887435</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.887435</td>\n",
       "      <td>0.896073</td>\n",
       "      <td>0.012344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.807527</td>\n",
       "      <td>0.045360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.537313</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.557143</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.492537</td>\n",
       "      <td>0.545297</td>\n",
       "      <td>0.053319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.971200</td>\n",
       "      <td>0.980900</td>\n",
       "      <td>0.958600</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.968300</td>\n",
       "      <td>0.974400</td>\n",
       "      <td>0.968300</td>\n",
       "      <td>0.971300</td>\n",
       "      <td>0.977600</td>\n",
       "      <td>0.971400</td>\n",
       "      <td>0.971980</td>\n",
       "      <td>0.006310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.566372</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.605505</td>\n",
       "      <td>0.650324</td>\n",
       "      <td>0.048795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.909356</td>\n",
       "      <td>0.903913</td>\n",
       "      <td>0.860939</td>\n",
       "      <td>0.891497</td>\n",
       "      <td>0.894187</td>\n",
       "      <td>0.889686</td>\n",
       "      <td>0.877755</td>\n",
       "      <td>0.880296</td>\n",
       "      <td>0.891889</td>\n",
       "      <td>0.876674</td>\n",
       "      <td>0.887619</td>\n",
       "      <td>0.014147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.840113</td>\n",
       "      <td>0.824723</td>\n",
       "      <td>0.745551</td>\n",
       "      <td>0.798221</td>\n",
       "      <td>0.806886</td>\n",
       "      <td>0.803194</td>\n",
       "      <td>0.773381</td>\n",
       "      <td>0.780291</td>\n",
       "      <td>0.804103</td>\n",
       "      <td>0.769928</td>\n",
       "      <td>0.794639</td>\n",
       "      <td>0.027842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.811710</td>\n",
       "      <td>0.784564</td>\n",
       "      <td>0.714593</td>\n",
       "      <td>0.757546</td>\n",
       "      <td>0.775172</td>\n",
       "      <td>0.765751</td>\n",
       "      <td>0.737858</td>\n",
       "      <td>0.743022</td>\n",
       "      <td>0.764180</td>\n",
       "      <td>0.731983</td>\n",
       "      <td>0.758638</td>\n",
       "      <td>0.028227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.688404</td>\n",
       "      <td>0.668929</td>\n",
       "      <td>0.509266</td>\n",
       "      <td>0.619787</td>\n",
       "      <td>0.625902</td>\n",
       "      <td>0.625979</td>\n",
       "      <td>0.566704</td>\n",
       "      <td>0.582384</td>\n",
       "      <td>0.630530</td>\n",
       "      <td>0.564046</td>\n",
       "      <td>0.608193</td>\n",
       "      <td>0.053238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.926800</td>\n",
       "      <td>0.916700</td>\n",
       "      <td>0.893200</td>\n",
       "      <td>0.908600</td>\n",
       "      <td>0.915900</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>0.902400</td>\n",
       "      <td>0.902400</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.908150</td>\n",
       "      <td>0.009648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.811710</td>\n",
       "      <td>0.784564</td>\n",
       "      <td>0.714593</td>\n",
       "      <td>0.757546</td>\n",
       "      <td>0.775172</td>\n",
       "      <td>0.765751</td>\n",
       "      <td>0.737858</td>\n",
       "      <td>0.743022</td>\n",
       "      <td>0.764180</td>\n",
       "      <td>0.731983</td>\n",
       "      <td>0.758638</td>\n",
       "      <td>0.028227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.745041    0.725091    0.720576    0.742078   \n",
       "1                    TP   45.000000   40.000000   32.000000   36.000000   \n",
       "2                    TN  304.000000  308.000000  301.000000  308.000000   \n",
       "3                    FP    9.000000    6.000000   13.000000    7.000000   \n",
       "4                    FN   24.000000   28.000000   36.000000   31.000000   \n",
       "5              Accuracy    0.913613    0.910995    0.871728    0.900524   \n",
       "6             Precision    0.833333    0.869565    0.711111    0.837209   \n",
       "7           Sensitivity    0.652174    0.588235    0.470588    0.537313   \n",
       "8           Specificity    0.971200    0.980900    0.958600    0.977800   \n",
       "9              F1 score    0.731707    0.701754    0.566372    0.654545   \n",
       "10  F1 score (weighted)    0.909356    0.903913    0.860939    0.891497   \n",
       "11     F1 score (macro)    0.840113    0.824723    0.745551    0.798221   \n",
       "12    Balanced Accuracy    0.811710    0.784564    0.714593    0.757546   \n",
       "13                  MCC    0.688404    0.668929    0.509266    0.619787   \n",
       "14                  NPV    0.926800    0.916700    0.893200    0.908600   \n",
       "15              ROC_AUC    0.811710    0.784564    0.714593    0.757546   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.653028    0.716987    0.690088    0.727135    0.680677    0.719926   \n",
       "1    39.000000   39.000000   34.000000   35.000000   38.000000   33.000000   \n",
       "2   305.000000  304.000000  305.000000  305.000000  306.000000  306.000000   \n",
       "3    10.000000    8.000000   10.000000    9.000000    7.000000    9.000000   \n",
       "4    28.000000   31.000000   33.000000   33.000000   31.000000   34.000000   \n",
       "5     0.900524    0.897906    0.887435    0.890052    0.900524    0.887435   \n",
       "6     0.795918    0.829787    0.772727    0.795455    0.844444    0.785714   \n",
       "7     0.582090    0.557143    0.507463    0.514706    0.550725    0.492537   \n",
       "8     0.968300    0.974400    0.968300    0.971300    0.977600    0.971400   \n",
       "9     0.672414    0.666667    0.612613    0.625000    0.666667    0.605505   \n",
       "10    0.894187    0.889686    0.877755    0.880296    0.891889    0.876674   \n",
       "11    0.806886    0.803194    0.773381    0.780291    0.804103    0.769928   \n",
       "12    0.775172    0.765751    0.737858    0.743022    0.764180    0.731983   \n",
       "13    0.625902    0.625979    0.566704    0.582384    0.630530    0.564046   \n",
       "14    0.915900    0.907500    0.902400    0.902400    0.908000    0.900000   \n",
       "15    0.775172    0.765751    0.737858    0.743022    0.764180    0.731983   \n",
       "\n",
       "           ave       std  \n",
       "0     0.712063  0.028862  \n",
       "1    37.100000  3.900142  \n",
       "2   305.200000  2.043961  \n",
       "3     8.800000  1.988858  \n",
       "4    30.900000  3.478505  \n",
       "5     0.896073  0.012344  \n",
       "6     0.807527  0.045360  \n",
       "7     0.545297  0.053319  \n",
       "8     0.971980  0.006310  \n",
       "9     0.650324  0.048795  \n",
       "10    0.887619  0.014147  \n",
       "11    0.794639  0.027842  \n",
       "12    0.758638  0.028227  \n",
       "13    0.608193  0.053238  \n",
       "14    0.908150  0.009648  \n",
       "15    0.758638  0.028227  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test['std'] = mat_met_lgbm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_lgbm0</th>\n",
       "      <th>y_pred_lgbm1</th>\n",
       "      <th>y_pred_lgbm2</th>\n",
       "      <th>y_pred_lgbm3</th>\n",
       "      <th>y_pred_lgbm4</th>\n",
       "      <th>y_pred_lgbm_ave</th>\n",
       "      <th>y_pred_lgbm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL585939</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>-0.714486</td>\n",
       "      <td>-0.255430</td>\n",
       "      <td>-0.788424</td>\n",
       "      <td>-0.185674</td>\n",
       "      <td>-0.414071</td>\n",
       "      <td>-0.591347</td>\n",
       "      <td>0.346705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL96051</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.276888</td>\n",
       "      <td>1.434402</td>\n",
       "      <td>1.308201</td>\n",
       "      <td>1.412354</td>\n",
       "      <td>1.540924</td>\n",
       "      <td>1.292128</td>\n",
       "      <td>0.244732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3356916</td>\n",
       "      <td>2</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.273121</td>\n",
       "      <td>0.348923</td>\n",
       "      <td>0.594428</td>\n",
       "      <td>0.519786</td>\n",
       "      <td>0.734478</td>\n",
       "      <td>0.461789</td>\n",
       "      <td>0.168239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3907413</td>\n",
       "      <td>3</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.585416</td>\n",
       "      <td>0.942361</td>\n",
       "      <td>1.901628</td>\n",
       "      <td>1.103528</td>\n",
       "      <td>0.677827</td>\n",
       "      <td>1.030127</td>\n",
       "      <td>0.427805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2047704</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>-2.973014</td>\n",
       "      <td>-3.031431</td>\n",
       "      <td>-2.883394</td>\n",
       "      <td>-2.948150</td>\n",
       "      <td>-2.845229</td>\n",
       "      <td>-2.821870</td>\n",
       "      <td>0.262702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>CHEMBL1095136</td>\n",
       "      <td>1905</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>-0.820868</td>\n",
       "      <td>-0.564535</td>\n",
       "      <td>-0.936651</td>\n",
       "      <td>-0.863018</td>\n",
       "      <td>-0.926041</td>\n",
       "      <td>-0.885186</td>\n",
       "      <td>0.187470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>CHEMBL2012817</td>\n",
       "      <td>1906</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-0.542199</td>\n",
       "      <td>-0.954378</td>\n",
       "      <td>0.700284</td>\n",
       "      <td>0.403273</td>\n",
       "      <td>-0.520349</td>\n",
       "      <td>-0.642228</td>\n",
       "      <td>1.175972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>CHEMBL496511</td>\n",
       "      <td>1907</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.980597</td>\n",
       "      <td>1.127518</td>\n",
       "      <td>1.313734</td>\n",
       "      <td>0.978516</td>\n",
       "      <td>0.957668</td>\n",
       "      <td>1.026339</td>\n",
       "      <td>0.159769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>CHEMBL3940062</td>\n",
       "      <td>1908</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1.526553</td>\n",
       "      <td>1.155296</td>\n",
       "      <td>1.409799</td>\n",
       "      <td>1.476165</td>\n",
       "      <td>1.091139</td>\n",
       "      <td>1.488159</td>\n",
       "      <td>0.384590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>CHEMBL493749</td>\n",
       "      <td>1909</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.379101</td>\n",
       "      <td>0.601379</td>\n",
       "      <td>0.149508</td>\n",
       "      <td>0.114854</td>\n",
       "      <td>0.419116</td>\n",
       "      <td>0.332326</td>\n",
       "      <td>0.164738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1910 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_lgbm0  y_pred_lgbm1  \\\n",
       "0          CHEMBL585939            0    -1.19     -0.714486     -0.255430   \n",
       "1           CHEMBL96051            1     0.78      1.276888      1.434402   \n",
       "2         CHEMBL3356916            2     0.30      0.273121      0.348923   \n",
       "3         CHEMBL3907413            3     0.97      0.585416      0.942361   \n",
       "4         CHEMBL2047704            4    -2.25     -2.973014     -3.031431   \n",
       "...                 ...          ...      ...           ...           ...   \n",
       "1905      CHEMBL1095136         1905    -1.20     -0.820868     -0.564535   \n",
       "1906      CHEMBL2012817         1906    -2.94     -0.542199     -0.954378   \n",
       "1907       CHEMBL496511         1907     0.80      0.980597      1.127518   \n",
       "1908      CHEMBL3940062         1908     2.27      1.526553      1.155296   \n",
       "1909       CHEMBL493749         1909     0.33      0.379101      0.601379   \n",
       "\n",
       "      y_pred_lgbm2  y_pred_lgbm3  y_pred_lgbm4  y_pred_lgbm_ave  \\\n",
       "0        -0.788424     -0.185674     -0.414071        -0.591347   \n",
       "1         1.308201      1.412354      1.540924         1.292128   \n",
       "2         0.594428      0.519786      0.734478         0.461789   \n",
       "3         1.901628      1.103528      0.677827         1.030127   \n",
       "4        -2.883394     -2.948150     -2.845229        -2.821870   \n",
       "...            ...           ...           ...              ...   \n",
       "1905     -0.936651     -0.863018     -0.926041        -0.885186   \n",
       "1906      0.700284      0.403273     -0.520349        -0.642228   \n",
       "1907      1.313734      0.978516      0.957668         1.026339   \n",
       "1908      1.409799      1.476165      1.091139         1.488159   \n",
       "1909      0.149508      0.114854      0.419116         0.332326   \n",
       "\n",
       "      y_pred_lgbm_std  \n",
       "0            0.346705  \n",
       "1            0.244732  \n",
       "2            0.168239  \n",
       "3            0.427805  \n",
       "4            0.262702  \n",
       "...               ...  \n",
       "1905         0.187470  \n",
       "1906         1.175972  \n",
       "1907         0.159769  \n",
       "1908         0.384590  \n",
       "1909         0.164738  \n",
       "\n",
       "[1910 rows x 10 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_lgbm_cat = np.where(((y_pred_optimized_lgbm >= 2) | (y_pred_optimized_lgbm <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_lgbm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_lgbm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "lgbm_run0 = data_lgbm[['y_test_idx0', 'y_test0', 'y_pred_lgbm0']]\n",
    "lgbm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "lgbm_run0.reset_index(inplace=True, drop=True)\n",
    "lgbm_run1 = data_lgbm[['y_test_idx1', 'y_test1', 'y_pred_lgbm1']]\n",
    "lgbm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "lgbm_run1.reset_index(inplace=True, drop=True)\n",
    "lgbm_run2 = data_lgbm[['y_test_idx2', 'y_test2', 'y_pred_lgbm2']]\n",
    "lgbm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "lgbm_run2.reset_index(inplace=True, drop=True)\n",
    "lgbm_run3 = data_lgbm[['y_test_idx3', 'y_test3', 'y_pred_lgbm3']]\n",
    "lgbm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "lgbm_run3.reset_index(inplace=True, drop=True)\n",
    "lgbm_run4 = data_lgbm[['y_test_idx4', 'y_test4', 'y_pred_lgbm4']]\n",
    "lgbm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "lgbm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "lgbm_5preds = pd.concat([chembl_id, lgbm_run0, lgbm_run1, lgbm_run2, lgbm_run3, lgbm_run4], axis=1)\n",
    "lgbm_5preds = lgbm_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_lgbm0', 'y_pred_lgbm1', 'y_pred_lgbm2', 'y_pred_lgbm3', 'y_pred_lgbm4']]\n",
    "lgbm_5preds['y_pred_lgbm_ave'] = lgbm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "lgbm_5preds['y_pred_lgbm_std'] = lgbm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "lgbm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c16510fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_lgbm.to_csv(output/'mat_met_lgbm_opt_withSemiSel.csv')\n",
    "lgbm_5preds.to_csv(output/'lgbm_5test_CV_result_withSemiSel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db4ac315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model r2_score 0.7087 with a standard deviation of 0.0478\n",
      "LightGBM optimized model r2_score 0.7215 with a standard deviation of 0.0436\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#r2_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"r2\")\n",
    "r2_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"r2\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)r2_score %0.4f with a standard deviation of %0.4f\" % (r2_cv_lgbm_opt_testSet.mean(), r2_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(r2_cv_lgbm_opt), np.std(r2_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_lgbm.joblib']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm_reg, \"./lgbm_reg.joblib\")\n",
    "#joblib.dump(optimized_lgbm, \"./optimized_lgbm.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"./optimizedCV_lgbm.joblib\") \n",
    "#loaded_rf = joblib.load(\"./optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.722420     0.042071\n",
      "1                    TP        17.300000     2.945807\n",
      "2                    TN       153.500000     2.368778\n",
      "3                    FP         4.100000     2.233582\n",
      "4                    FN        16.100000     2.424413\n",
      "5              Accuracy         0.894241     0.019402\n",
      "6             Precision         0.812706     0.097307\n",
      "7           Sensitivity         0.516854     0.078971\n",
      "8           Specificity         0.974000     0.014109\n",
      "9              F1 score         0.628782     0.076185\n",
      "10  F1 score (weighted)         0.884332     0.021516\n",
      "11     F1 score (macro)         0.783532     0.043284\n",
      "12    Balanced Accuracy         0.745424     0.041844\n",
      "13                  MCC         0.592344     0.084341\n",
      "14                  NPV         0.905200     0.013191\n",
      "15              ROC_AUC         0.745424     0.041844\n",
      "CPU times: user 59min 39s, sys: 1.04 s, total: 59min 40s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_reg.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores),np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=4, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=4, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:25:13,632]\u001b[0m A new study created in memory with name: XGBRegressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:25:27,509]\u001b[0m Trial 0 finished with value: 0.7207669382186664 and parameters: {'n_estimators': 713, 'eta': 0.0546014861815863, 'max_depth': 8, 'alpha': 0.6425000000000001, 'lambda': 35.06832039283391, 'max_bin': 407}. Best is trial 0 with value: 0.7207669382186664.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:25:33,948]\u001b[0m Trial 1 finished with value: 0.7021512305208821 and parameters: {'n_estimators': 472, 'eta': 0.06314044113455024, 'max_depth': 5, 'alpha': 0.6693, 'lambda': 33.91299816568814, 'max_bin': 358}. Best is trial 0 with value: 0.7207669382186664.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:25:38,094]\u001b[0m Trial 2 finished with value: 0.2744184776338769 and parameters: {'n_estimators': 115, 'eta': 0.0030980732607482197, 'max_depth': 11, 'alpha': 0.2305, 'lambda': 3.377614226586842, 'max_bin': 290}. Best is trial 0 with value: 0.7207669382186664.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:25:49,946]\u001b[0m Trial 3 finished with value: 0.6409708833854174 and parameters: {'n_estimators': 843, 'eta': 0.00793231112413534, 'max_depth': 5, 'alpha': 0.1715, 'lambda': 20.150186832147746, 'max_bin': 322}. Best is trial 0 with value: 0.7207669382186664.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:25:58,876]\u001b[0m Trial 4 finished with value: 0.7197247908950062 and parameters: {'n_estimators': 900, 'eta': 0.07005931288941976, 'max_depth': 6, 'alpha': 0.32730000000000004, 'lambda': 18.598845344797613, 'max_bin': 483}. Best is trial 0 with value: 0.7207669382186664.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:26:08,616]\u001b[0m Trial 5 finished with value: 0.7145636888841044 and parameters: {'n_estimators': 613, 'eta': 0.057152416531762805, 'max_depth': 6, 'alpha': 0.9913000000000001, 'lambda': 35.15990983696944, 'max_bin': 498}. Best is trial 0 with value: 0.7207669382186664.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:26:17,661]\u001b[0m Trial 6 finished with value: 0.537529839202102 and parameters: {'n_estimators': 333, 'eta': 0.005731716461956046, 'max_depth': 8, 'alpha': 0.3022, 'lambda': 30.02659638534609, 'max_bin': 414}. Best is trial 0 with value: 0.7207669382186664.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:26:24,388]\u001b[0m Trial 7 finished with value: 0.7221398165397979 and parameters: {'n_estimators': 380, 'eta': 0.060378949879145404, 'max_depth': 7, 'alpha': 0.8822000000000001, 'lambda': 1.4184299888191068, 'max_bin': 263}. Best is trial 7 with value: 0.7221398165397979.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:26:33,910]\u001b[0m Trial 8 finished with value: 0.7185445261885421 and parameters: {'n_estimators': 664, 'eta': 0.09827103507269336, 'max_depth': 9, 'alpha': 0.7213, 'lambda': 23.464976625649136, 'max_bin': 336}. Best is trial 7 with value: 0.7221398165397979.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:26:49,949]\u001b[0m Trial 9 finished with value: 0.7212367615984113 and parameters: {'n_estimators': 881, 'eta': 0.05682773672828001, 'max_depth': 10, 'alpha': 0.7043, 'lambda': 34.52691890847587, 'max_bin': 446}. Best is trial 7 with value: 0.7221398165397979.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:26:58,570]\u001b[0m Trial 10 finished with value: 0.7135054889846116 and parameters: {'n_estimators': 252, 'eta': 0.028689268466489337, 'max_depth': 12, 'alpha': 0.9922000000000001, 'lambda': 1.6811448791653134, 'max_bin': 265}. Best is trial 7 with value: 0.7221398165397979.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:27:07,097]\u001b[0m Trial 11 finished with value: 0.7210833082133693 and parameters: {'n_estimators': 443, 'eta': 0.08371258028470323, 'max_depth': 10, 'alpha': 0.8098000000000001, 'lambda': 10.14370856816514, 'max_bin': 444}. Best is trial 7 with value: 0.7221398165397979.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:27:13,192]\u001b[0m Trial 12 finished with value: 0.7044973454278158 and parameters: {'n_estimators': 304, 'eta': 0.036114234906869117, 'max_depth': 7, 'alpha': 0.5284, 'lambda': 11.91585204630767, 'max_bin': 392}. Best is trial 7 with value: 0.7221398165397979.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:27:28,139]\u001b[0m Trial 13 finished with value: 0.7221259078012401 and parameters: {'n_estimators': 555, 'eta': 0.04098122789748426, 'max_depth': 10, 'alpha': 0.8510000000000001, 'lambda': 28.690064199635923, 'max_bin': 450}. Best is trial 7 with value: 0.7221398165397979.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:27:42,392]\u001b[0m Trial 14 finished with value: 0.7212666061409165 and parameters: {'n_estimators': 544, 'eta': 0.036007466511902744, 'max_depth': 9, 'alpha': 0.0268, 'lambda': 26.918321951484305, 'max_bin': 288}. Best is trial 7 with value: 0.7221398165397979.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:27:50,208]\u001b[0m Trial 15 finished with value: 0.6907948730841118 and parameters: {'n_estimators': 386, 'eta': 0.02246984358598304, 'max_depth': 7, 'alpha': 0.8438, 'lambda': 15.275311050502681, 'max_bin': 462}. Best is trial 7 with value: 0.7221398165397979.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:27:52,692]\u001b[0m Trial 16 finished with value: 0.6658372578154611 and parameters: {'n_estimators': 63, 'eta': 0.07836111139364886, 'max_depth': 12, 'alpha': 0.8494, 'lambda': 39.404549502170966, 'max_bin': 367}. Best is trial 7 with value: 0.7221398165397979.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:27:58,415]\u001b[0m Trial 17 finished with value: 0.6996954684143815 and parameters: {'n_estimators': 186, 'eta': 0.04280617757531172, 'max_depth': 10, 'alpha': 0.5108, 'lambda': 25.36361642634941, 'max_bin': 318}. Best is trial 7 with value: 0.7221398165397979.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:28:09,250]\u001b[0m Trial 18 finished with value: 0.7059749841463985 and parameters: {'n_estimators': 552, 'eta': 0.018941101924528494, 'max_depth': 7, 'alpha': 0.9004000000000001, 'lambda': 6.488071912471744, 'max_bin': 252}. Best is trial 7 with value: 0.7221398165397979.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:28:25,186]\u001b[0m Trial 19 finished with value: 0.725557715808469 and parameters: {'n_estimators': 750, 'eta': 0.04613081543560374, 'max_depth': 11, 'alpha': 0.5853, 'lambda': 16.152883230591165, 'max_bin': 434}. Best is trial 19 with value: 0.725557715808469.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:28:34,109]\u001b[0m Trial 20 finished with value: 0.7207069202131484 and parameters: {'n_estimators': 761, 'eta': 0.07509884576555721, 'max_depth': 11, 'alpha': 0.5583, 'lambda': 7.516251248004945, 'max_bin': 415}. Best is trial 19 with value: 0.725557715808469.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:28:49,818]\u001b[0m Trial 21 finished with value: 0.7212807915194143 and parameters: {'n_estimators': 546, 'eta': 0.045041817891224564, 'max_depth': 11, 'alpha': 0.7705000000000001, 'lambda': 29.360696151106744, 'max_bin': 441}. Best is trial 19 with value: 0.725557715808469.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:29:05,076]\u001b[0m Trial 22 finished with value: 0.7245941010026344 and parameters: {'n_estimators': 793, 'eta': 0.044778260868809164, 'max_depth': 9, 'alpha': 0.4015, 'lambda': 14.776828760295784, 'max_bin': 467}. Best is trial 19 with value: 0.725557715808469.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:29:15,020]\u001b[0m Trial 23 finished with value: 0.7236412829476313 and parameters: {'n_estimators': 774, 'eta': 0.06764168842838525, 'max_depth': 8, 'alpha': 0.4027, 'lambda': 15.68483064812623, 'max_bin': 472}. Best is trial 19 with value: 0.725557715808469.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:29:28,526]\u001b[0m Trial 24 finished with value: 0.7250051544044191 and parameters: {'n_estimators': 788, 'eta': 0.04992718345324693, 'max_depth': 9, 'alpha': 0.3886, 'lambda': 15.311632246689959, 'max_bin': 481}. Best is trial 19 with value: 0.725557715808469.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:29:42,553]\u001b[0m Trial 25 finished with value: 0.7260302042508978 and parameters: {'n_estimators': 803, 'eta': 0.050461254137374845, 'max_depth': 9, 'alpha': 0.4323, 'lambda': 15.637006674990083, 'max_bin': 492}. Best is trial 25 with value: 0.7260302042508978.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:29:56,566]\u001b[0m Trial 26 finished with value: 0.7255641195387115 and parameters: {'n_estimators': 697, 'eta': 0.0507281269210574, 'max_depth': 9, 'alpha': 0.4388, 'lambda': 22.376116381075907, 'max_bin': 497}. Best is trial 25 with value: 0.7260302042508978.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:30:16,072]\u001b[0m Trial 27 finished with value: 0.7212204282678637 and parameters: {'n_estimators': 635, 'eta': 0.026603747348785473, 'max_depth': 11, 'alpha': 0.5892000000000001, 'lambda': 22.95683492464628, 'max_bin': 426}. Best is trial 25 with value: 0.7260302042508978.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:30:31,860]\u001b[0m Trial 28 finished with value: 0.7179703507734401 and parameters: {'n_estimators': 694, 'eta': 0.04956599330824598, 'max_depth': 12, 'alpha': 0.4677, 'lambda': 18.630335271179902, 'max_bin': 491}. Best is trial 25 with value: 0.7260302042508978.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:30:46,170]\u001b[0m Trial 29 finished with value: 0.7213582133735051 and parameters: {'n_estimators': 722, 'eta': 0.03521643690117439, 'max_depth': 8, 'alpha': 0.6204000000000001, 'lambda': 12.661688941655644, 'max_bin': 391}. Best is trial 25 with value: 0.7260302042508978.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:31:01,314]\u001b[0m Trial 30 finished with value: 0.7248822452139067 and parameters: {'n_estimators': 844, 'eta': 0.053015977879483756, 'max_depth': 10, 'alpha': 0.4728, 'lambda': 22.741245452892223, 'max_bin': 460}. Best is trial 25 with value: 0.7260302042508978.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:31:14,723]\u001b[0m Trial 31 finished with value: 0.7214175736968808 and parameters: {'n_estimators': 723, 'eta': 0.052101013331770446, 'max_depth': 9, 'alpha': 0.3677, 'lambda': 17.123851424198758, 'max_bin': 479}. Best is trial 25 with value: 0.7260302042508978.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:31:27,713]\u001b[0m Trial 32 finished with value: 0.7261961297184086 and parameters: {'n_estimators': 815, 'eta': 0.06416837263913072, 'max_depth': 9, 'alpha': 0.24860000000000002, 'lambda': 20.130962976439847, 'max_bin': 500}. Best is trial 32 with value: 0.7261961297184086.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:31:38,979]\u001b[0m Trial 33 finished with value: 0.7236236599809049 and parameters: {'n_estimators': 841, 'eta': 0.06438880977131399, 'max_depth': 9, 'alpha': 0.2248, 'lambda': 21.15101798183261, 'max_bin': 499}. Best is trial 32 with value: 0.7261961297184086.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:31:50,605]\u001b[0m Trial 34 finished with value: 0.7243297152494587 and parameters: {'n_estimators': 819, 'eta': 0.062144540056783874, 'max_depth': 8, 'alpha': 0.0442, 'lambda': 19.62517613278568, 'max_bin': 500}. Best is trial 32 with value: 0.7261961297184086.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:32:01,409]\u001b[0m Trial 35 finished with value: 0.719397848020598 and parameters: {'n_estimators': 746, 'eta': 0.06919600111689526, 'max_depth': 10, 'alpha': 0.12340000000000001, 'lambda': 12.124472388737233, 'max_bin': 433}. Best is trial 32 with value: 0.7261961297184086.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:32:13,051]\u001b[0m Trial 36 finished with value: 0.7219609024897944 and parameters: {'n_estimators': 607, 'eta': 0.08828216758297121, 'max_depth': 11, 'alpha': 0.2854, 'lambda': 25.294019572932463, 'max_bin': 484}. Best is trial 32 with value: 0.7261961297184086.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:32:22,091]\u001b[0m Trial 37 finished with value: 0.7213030961097912 and parameters: {'n_estimators': 676, 'eta': 0.07403982648586369, 'max_depth': 6, 'alpha': 0.2268, 'lambda': 21.44782379627724, 'max_bin': 460}. Best is trial 32 with value: 0.7261961297184086.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:32:32,759]\u001b[0m Trial 38 finished with value: 0.7226438085750078 and parameters: {'n_estimators': 883, 'eta': 0.05686617483295438, 'max_depth': 8, 'alpha': 0.4641, 'lambda': 9.118383702759147, 'max_bin': 394}. Best is trial 32 with value: 0.7261961297184086.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:32:45,804]\u001b[0m Trial 39 finished with value: 0.7243273601704538 and parameters: {'n_estimators': 643, 'eta': 0.04891668277844967, 'max_depth': 9, 'alpha': 0.6447, 'lambda': 16.775033282187163, 'max_bin': 475}. Best is trial 32 with value: 0.7261961297184086.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:33:00,052]\u001b[0m Trial 40 finished with value: 0.722569482193865 and parameters: {'n_estimators': 827, 'eta': 0.06044639957969709, 'max_depth': 11, 'alpha': 0.1584, 'lambda': 18.191657577948135, 'max_bin': 350}. Best is trial 32 with value: 0.7261961297184086.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:33:13,220]\u001b[0m Trial 41 finished with value: 0.726440942041735 and parameters: {'n_estimators': 806, 'eta': 0.048440545216615144, 'max_depth': 9, 'alpha': 0.3558, 'lambda': 14.406132910358032, 'max_bin': 487}. Best is trial 41 with value: 0.726440942041735.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:33:26,505]\u001b[0m Trial 42 finished with value: 0.7252170086891672 and parameters: {'n_estimators': 897, 'eta': 0.05551686285168871, 'max_depth': 10, 'alpha': 0.3336, 'lambda': 13.306968429437305, 'max_bin': 490}. Best is trial 41 with value: 0.726440942041735.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:33:41,702]\u001b[0m Trial 43 finished with value: 0.7230565862868789 and parameters: {'n_estimators': 735, 'eta': 0.03859013443246858, 'max_depth': 9, 'alpha': 0.2685, 'lambda': 19.62615599895167, 'max_bin': 490}. Best is trial 41 with value: 0.726440942041735.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:33:53,188]\u001b[0m Trial 44 finished with value: 0.7201204328453268 and parameters: {'n_estimators': 794, 'eta': 0.04654400238223862, 'max_depth': 8, 'alpha': 0.44110000000000005, 'lambda': 10.024363436148791, 'max_bin': 455}. Best is trial 41 with value: 0.726440942041735.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:34:07,954]\u001b[0m Trial 45 finished with value: 0.7233449778468856 and parameters: {'n_estimators': 844, 'eta': 0.062330300007229326, 'max_depth': 10, 'alpha': 0.3259, 'lambda': 31.291787830673794, 'max_bin': 468}. Best is trial 41 with value: 0.726440942041735.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:34:16,001]\u001b[0m Trial 46 finished with value: 0.6951255876699804 and parameters: {'n_estimators': 594, 'eta': 0.03221791936874633, 'max_depth': 5, 'alpha': 0.5511, 'lambda': 24.748761187952013, 'max_bin': 500}. Best is trial 41 with value: 0.726440942041735.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:34:28,036]\u001b[0m Trial 47 finished with value: 0.7203483792616693 and parameters: {'n_estimators': 692, 'eta': 0.041391355624559055, 'max_depth': 9, 'alpha': 0.3567, 'lambda': 4.818250811277771, 'max_bin': 429}. Best is trial 41 with value: 0.726440942041735.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:34:39,748]\u001b[0m Trial 48 finished with value: 0.7251059742292785 and parameters: {'n_estimators': 859, 'eta': 0.057998035443555813, 'max_depth': 8, 'alpha': 0.6861, 'lambda': 14.01020577024395, 'max_bin': 480}. Best is trial 41 with value: 0.726440942041735.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:34:52,971]\u001b[0m Trial 49 finished with value: 0.7252404003714474 and parameters: {'n_estimators': 508, 'eta': 0.06642369301956802, 'max_depth': 10, 'alpha': 0.2647, 'lambda': 27.079197906795773, 'max_bin': 442}. Best is trial 41 with value: 0.726440942041735.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.7264\n",
      "\tBest params:\n",
      "\t\tn_estimators: 806\n",
      "\t\teta: 0.048440545216615144\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.3558\n",
      "\t\tlambda: 14.406132910358032\n",
      "\t\tmax_bin: 487\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBRegressor\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.743638\n",
      "1                    TP   43.000000\n",
      "2                    TN  305.000000\n",
      "3                    FP    8.000000\n",
      "4                    FN   26.000000\n",
      "5              Accuracy    0.910995\n",
      "6             Precision    0.843137\n",
      "7           Sensitivity    0.623188\n",
      "8           Specificity    0.974400\n",
      "9              F1 score    0.716667\n",
      "10  F1 score (weighted)    0.905563\n",
      "11     F1 score (macro)    0.831936\n",
      "12    Balanced Accuracy    0.798815\n",
      "13                  MCC    0.675973\n",
      "14                  NPV    0.921500\n",
      "15              ROC_AUC    0.798815\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_xgb_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_xgb_0_cat = np.where(((y_pred_xgb_0 >= 2) | (y_pred_xgb_0 <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:35:10,850]\u001b[0m Trial 50 finished with value: 0.7053372741752071 and parameters: {'n_estimators': 813, 'eta': 0.012113843628789811, 'max_depth': 7, 'alpha': 0.43570000000000003, 'lambda': 17.174733790554484, 'max_bin': 415}. Best is trial 41 with value: 0.726440942041735.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:35:21,024]\u001b[0m Trial 51 finished with value: 0.7305840345666594 and parameters: {'n_estimators': 394, 'eta': 0.06446547002246494, 'max_depth': 10, 'alpha': 0.2592, 'lambda': 27.453509530451154, 'max_bin': 449}. Best is trial 51 with value: 0.7305840345666594.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:35:30,276]\u001b[0m Trial 52 finished with value: 0.7277488161285154 and parameters: {'n_estimators': 376, 'eta': 0.053953105809956714, 'max_depth': 9, 'alpha': 0.15990000000000001, 'lambda': 20.659618017057753, 'max_bin': 485}. Best is trial 51 with value: 0.7305840345666594.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:35:39,697]\u001b[0m Trial 53 finished with value: 0.7315314053262874 and parameters: {'n_estimators': 437, 'eta': 0.0712784556567793, 'max_depth': 9, 'alpha': 0.16770000000000002, 'lambda': 21.799705271036828, 'max_bin': 485}. Best is trial 53 with value: 0.7315314053262874.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:35:49,398]\u001b[0m Trial 54 finished with value: 0.7329618875829114 and parameters: {'n_estimators': 421, 'eta': 0.07188107533684089, 'max_depth': 9, 'alpha': 0.1043, 'lambda': 20.414725163296584, 'max_bin': 487}. Best is trial 54 with value: 0.7329618875829114.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:35:59,547]\u001b[0m Trial 55 finished with value: 0.7332934578391407 and parameters: {'n_estimators': 437, 'eta': 0.08278941166214664, 'max_depth': 10, 'alpha': 0.0844, 'lambda': 31.36917300308874, 'max_bin': 471}. Best is trial 55 with value: 0.7332934578391407.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:36:10,799]\u001b[0m Trial 56 finished with value: 0.7349951920656611 and parameters: {'n_estimators': 435, 'eta': 0.08490707307287161, 'max_depth': 10, 'alpha': 0.0878, 'lambda': 32.698766004868084, 'max_bin': 452}. Best is trial 56 with value: 0.7349951920656611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:36:20,608]\u001b[0m Trial 57 finished with value: 0.7311377917423282 and parameters: {'n_estimators': 415, 'eta': 0.08610335100380215, 'max_depth': 10, 'alpha': 0.08, 'lambda': 32.29423999661087, 'max_bin': 451}. Best is trial 56 with value: 0.7349951920656611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:36:31,538]\u001b[0m Trial 58 finished with value: 0.7341277450059375 and parameters: {'n_estimators': 445, 'eta': 0.08724187394123734, 'max_depth': 10, 'alpha': 0.0753, 'lambda': 32.46165455977198, 'max_bin': 454}. Best is trial 56 with value: 0.7349951920656611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:36:41,630]\u001b[0m Trial 59 finished with value: 0.7331539742291943 and parameters: {'n_estimators': 458, 'eta': 0.09444044683119535, 'max_depth': 10, 'alpha': 0.0716, 'lambda': 36.5952356176516, 'max_bin': 454}. Best is trial 56 with value: 0.7349951920656611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:36:51,739]\u001b[0m Trial 60 finished with value: 0.7330577942599856 and parameters: {'n_estimators': 473, 'eta': 0.09607722636762767, 'max_depth': 10, 'alpha': 0.0001, 'lambda': 36.272224969379366, 'max_bin': 469}. Best is trial 56 with value: 0.7349951920656611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:37:01,775]\u001b[0m Trial 61 finished with value: 0.7344239097091521 and parameters: {'n_estimators': 475, 'eta': 0.09608349021920608, 'max_depth': 10, 'alpha': 0.0067, 'lambda': 36.04175037711492, 'max_bin': 468}. Best is trial 56 with value: 0.7349951920656611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:37:12,078]\u001b[0m Trial 62 finished with value: 0.7335300079828864 and parameters: {'n_estimators': 495, 'eta': 0.09889077851700098, 'max_depth': 10, 'alpha': 0.0089, 'lambda': 37.694685791564964, 'max_bin': 467}. Best is trial 56 with value: 0.7349951920656611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:37:22,449]\u001b[0m Trial 63 finished with value: 0.7341865316762383 and parameters: {'n_estimators': 478, 'eta': 0.09979830229020609, 'max_depth': 10, 'alpha': 0.009300000000000001, 'lambda': 37.1425584571644, 'max_bin': 469}. Best is trial 56 with value: 0.7349951920656611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:37:34,054]\u001b[0m Trial 64 finished with value: 0.7322289101845045 and parameters: {'n_estimators': 507, 'eta': 0.09366699985411457, 'max_depth': 11, 'alpha': 0.0546, 'lambda': 37.30960793564105, 'max_bin': 458}. Best is trial 56 with value: 0.7349951920656611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:37:45,117]\u001b[0m Trial 65 finished with value: 0.7345145472825244 and parameters: {'n_estimators': 333, 'eta': 0.09117464603188395, 'max_depth': 11, 'alpha': 0.0086, 'lambda': 38.859981790859074, 'max_bin': 423}. Best is trial 56 with value: 0.7349951920656611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:37:54,864]\u001b[0m Trial 66 finished with value: 0.7324548104396997 and parameters: {'n_estimators': 327, 'eta': 0.08177066453636089, 'max_depth': 11, 'alpha': 0.0086, 'lambda': 32.65859725454675, 'max_bin': 422}. Best is trial 56 with value: 0.7349951920656611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:38:04,636]\u001b[0m Trial 67 finished with value: 0.7372846414475894 and parameters: {'n_estimators': 281, 'eta': 0.09991575107965912, 'max_depth': 12, 'alpha': 0.11860000000000001, 'lambda': 39.52636832968151, 'max_bin': 438}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:38:12,493]\u001b[0m Trial 68 finished with value: 0.731117142570654 and parameters: {'n_estimators': 213, 'eta': 0.09920548441805896, 'max_depth': 12, 'alpha': 0.1189, 'lambda': 39.14493935654992, 'max_bin': 404}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:38:21,386]\u001b[0m Trial 69 finished with value: 0.731322990062572 and parameters: {'n_estimators': 257, 'eta': 0.09134768458931035, 'max_depth': 12, 'alpha': 0.0332, 'lambda': 38.38069174769694, 'max_bin': 439}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:38:31,187]\u001b[0m Trial 70 finished with value: 0.7351611215428889 and parameters: {'n_estimators': 357, 'eta': 0.08955418344930001, 'max_depth': 11, 'alpha': 0.202, 'lambda': 34.17025652564726, 'max_bin': 423}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:38:41,971]\u001b[0m Trial 71 finished with value: 0.7339388650829857 and parameters: {'n_estimators': 359, 'eta': 0.09010924243140264, 'max_depth': 11, 'alpha': 0.1247, 'lambda': 34.57632542961661, 'max_bin': 406}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:38:51,904]\u001b[0m Trial 72 finished with value: 0.733058062701306 and parameters: {'n_estimators': 348, 'eta': 0.09028960781397352, 'max_depth': 11, 'alpha': 0.1378, 'lambda': 34.998788970662304, 'max_bin': 407}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:39:01,837]\u001b[0m Trial 73 finished with value: 0.7346986398356024 and parameters: {'n_estimators': 285, 'eta': 0.07880176597347108, 'max_depth': 12, 'alpha': 0.20670000000000002, 'lambda': 33.81489984759795, 'max_bin': 422}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:39:11,913]\u001b[0m Trial 74 finished with value: 0.7333994349516554 and parameters: {'n_estimators': 305, 'eta': 0.08004285477939548, 'max_depth': 12, 'alpha': 0.2134, 'lambda': 33.80099654546515, 'max_bin': 419}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:39:22,268]\u001b[0m Trial 75 finished with value: 0.7341509872024767 and parameters: {'n_estimators': 271, 'eta': 0.08633228315549571, 'max_depth': 12, 'alpha': 0.049600000000000005, 'lambda': 39.87453945868218, 'max_bin': 437}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:39:30,743]\u001b[0m Trial 76 finished with value: 0.7285621621463129 and parameters: {'n_estimators': 235, 'eta': 0.0774435560381435, 'max_depth': 12, 'alpha': 0.1943, 'lambda': 39.343630507611124, 'max_bin': 378}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:39:40,266]\u001b[0m Trial 77 finished with value: 0.734527474942136 and parameters: {'n_estimators': 277, 'eta': 0.09657237861813113, 'max_depth': 12, 'alpha': 0.0425, 'lambda': 36.12505695483504, 'max_bin': 433}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:39:46,970]\u001b[0m Trial 78 finished with value: 0.7292690436457143 and parameters: {'n_estimators': 180, 'eta': 0.09360989878293816, 'max_depth': 12, 'alpha': 0.09620000000000001, 'lambda': 35.849385056278116, 'max_bin': 431}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:39:56,656]\u001b[0m Trial 79 finished with value: 0.7326214781123309 and parameters: {'n_estimators': 296, 'eta': 0.09666323819707767, 'max_depth': 12, 'alpha': 0.1829, 'lambda': 33.82879908933694, 'max_bin': 398}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:40:02,639]\u001b[0m Trial 80 finished with value: 0.7272967168763469 and parameters: {'n_estimators': 171, 'eta': 0.09964676754434804, 'max_depth': 11, 'alpha': 0.039900000000000005, 'lambda': 38.16679741618924, 'max_bin': 443}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:40:12,185]\u001b[0m Trial 81 finished with value: 0.734557352559462 and parameters: {'n_estimators': 273, 'eta': 0.0919275984044568, 'max_depth': 12, 'alpha': 0.0548, 'lambda': 39.58622002503617, 'max_bin': 437}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:40:21,954]\u001b[0m Trial 82 finished with value: 0.7319285788815129 and parameters: {'n_estimators': 288, 'eta': 0.09265536403492988, 'max_depth': 12, 'alpha': 0.14250000000000002, 'lambda': 36.93621499902919, 'max_bin': 426}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:40:27,515]\u001b[0m Trial 83 finished with value: 0.7300175230021273 and parameters: {'n_estimators': 145, 'eta': 0.09609462381198555, 'max_depth': 12, 'alpha': 0.0238, 'lambda': 35.541735339283925, 'max_bin': 446}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:40:35,831]\u001b[0m Trial 84 finished with value: 0.7327916930615194 and parameters: {'n_estimators': 224, 'eta': 0.08500337661754664, 'max_depth': 12, 'alpha': 0.0678, 'lambda': 38.25345003455849, 'max_bin': 383}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:40:45,935]\u001b[0m Trial 85 finished with value: 0.7346582566022415 and parameters: {'n_estimators': 328, 'eta': 0.08899050739046681, 'max_depth': 11, 'alpha': 0.1056, 'lambda': 29.8648613524733, 'max_bin': 411}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:40:55,505]\u001b[0m Trial 86 finished with value: 0.7318202797266171 and parameters: {'n_estimators': 336, 'eta': 0.08824722726918309, 'max_depth': 11, 'alpha': 0.10350000000000001, 'lambda': 29.976659961729215, 'max_bin': 421}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:41:04,836]\u001b[0m Trial 87 finished with value: 0.7332108845427298 and parameters: {'n_estimators': 316, 'eta': 0.0899612995794707, 'max_depth': 11, 'alpha': 0.1905, 'lambda': 31.21160446940996, 'max_bin': 412}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:41:13,977]\u001b[0m Trial 88 finished with value: 0.7324372838117935 and parameters: {'n_estimators': 279, 'eta': 0.08112299777031602, 'max_depth': 11, 'alpha': 0.1429, 'lambda': 34.1259297527253, 'max_bin': 435}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:41:25,289]\u001b[0m Trial 89 finished with value: 0.7352055052137161 and parameters: {'n_estimators': 365, 'eta': 0.08405087469788906, 'max_depth': 12, 'alpha': 0.0558, 'lambda': 33.038496534157446, 'max_bin': 426}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:41:37,088]\u001b[0m Trial 90 finished with value: 0.7325960909176001 and parameters: {'n_estimators': 366, 'eta': 0.07657842030548175, 'max_depth': 12, 'alpha': 0.0577, 'lambda': 29.039122273402604, 'max_bin': 401}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:41:48,091]\u001b[0m Trial 91 finished with value: 0.7334753304777449 and parameters: {'n_estimators': 407, 'eta': 0.08414700161195404, 'max_depth': 12, 'alpha': 0.1095, 'lambda': 33.328175706058346, 'max_bin': 426}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:41:56,907]\u001b[0m Trial 92 finished with value: 0.7340515646417841 and parameters: {'n_estimators': 254, 'eta': 0.09633875152267249, 'max_depth': 12, 'alpha': 0.0391, 'lambda': 35.50243797798147, 'max_bin': 411}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:42:06,895]\u001b[0m Trial 93 finished with value: 0.7345598189440173 and parameters: {'n_estimators': 342, 'eta': 0.08870803130722771, 'max_depth': 12, 'alpha': 0.9568000000000001, 'lambda': 38.65005933641425, 'max_bin': 421}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:42:17,424]\u001b[0m Trial 94 finished with value: 0.7316931667092822 and parameters: {'n_estimators': 345, 'eta': 0.07912577275956226, 'max_depth': 12, 'alpha': 0.9102, 'lambda': 38.83267270620781, 'max_bin': 419}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:42:26,137]\u001b[0m Trial 95 finished with value: 0.7352814192922192 and parameters: {'n_estimators': 306, 'eta': 0.09158631429990051, 'max_depth': 11, 'alpha': 0.7709, 'lambda': 34.872511889153756, 'max_bin': 432}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:42:36,869]\u001b[0m Trial 96 finished with value: 0.7326692246482653 and parameters: {'n_estimators': 386, 'eta': 0.08827164091985475, 'max_depth': 12, 'alpha': 0.7552, 'lambda': 30.56208461828166, 'max_bin': 434}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:42:46,127]\u001b[0m Trial 97 finished with value: 0.7336898960925496 and parameters: {'n_estimators': 308, 'eta': 0.083645706684089, 'max_depth': 12, 'alpha': 0.9169, 'lambda': 33.20614156673173, 'max_bin': 430}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:42:54,005]\u001b[0m Trial 98 finished with value: 0.733646188033477 and parameters: {'n_estimators': 240, 'eta': 0.0925096018603039, 'max_depth': 12, 'alpha': 0.8182, 'lambda': 34.62992616996349, 'max_bin': 447}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:43:00,664]\u001b[0m Trial 99 finished with value: 0.7251671333087248 and parameters: {'n_estimators': 206, 'eta': 0.07471431475115106, 'max_depth': 11, 'alpha': 0.9876, 'lambda': 39.95450713904383, 'max_bin': 440}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7373\n",
      "\tBest params:\n",
      "\t\tn_estimators: 281\n",
      "\t\teta: 0.09991575107965912\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.11860000000000001\n",
      "\t\tlambda: 39.52636832968151\n",
      "\t\tmax_bin: 438\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.743638    0.730201\n",
      "1                    TP   43.000000   43.000000\n",
      "2                    TN  305.000000  305.000000\n",
      "3                    FP    8.000000    9.000000\n",
      "4                    FN   26.000000   25.000000\n",
      "5              Accuracy    0.910995    0.910995\n",
      "6             Precision    0.843137    0.826923\n",
      "7           Sensitivity    0.623188    0.632353\n",
      "8           Specificity    0.974400    0.971300\n",
      "9              F1 score    0.716667    0.716667\n",
      "10  F1 score (weighted)    0.905563    0.906167\n",
      "11     F1 score (macro)    0.831936    0.831936\n",
      "12    Balanced Accuracy    0.798815    0.801845\n",
      "13                  MCC    0.675973    0.673403\n",
      "14                  NPV    0.921500    0.924200\n",
      "15              ROC_AUC    0.798815    0.801845\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_xgb_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_1_cat = np.where(((y_pred_xgb_1 >= 2) | (y_pred_xgb_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:43:10,849]\u001b[0m Trial 100 finished with value: 0.7176671196253248 and parameters: {'n_estimators': 270, 'eta': 0.08870352333317158, 'max_depth': 12, 'alpha': 0.9503, 'lambda': 37.87255021354108, 'max_bin': 416}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:43:20,915]\u001b[0m Trial 101 finished with value: 0.7212905296454132 and parameters: {'n_estimators': 324, 'eta': 0.08595139096906346, 'max_depth': 11, 'alpha': 0.09620000000000001, 'lambda': 36.536350002680784, 'max_bin': 425}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:43:31,815]\u001b[0m Trial 102 finished with value: 0.7208719439997612 and parameters: {'n_estimators': 359, 'eta': 0.09419527942775346, 'max_depth': 11, 'alpha': 0.0819, 'lambda': 31.922331596523268, 'max_bin': 387}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:43:40,622]\u001b[0m Trial 103 finished with value: 0.7185152606100569 and parameters: {'n_estimators': 289, 'eta': 0.09161432962759354, 'max_depth': 11, 'alpha': 0.8815000000000001, 'lambda': 37.47712451444121, 'max_bin': 409}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:43:52,918]\u001b[0m Trial 104 finished with value: 0.1656063400136773 and parameters: {'n_estimators': 338, 'eta': 0.0008665385207304449, 'max_depth': 12, 'alpha': 0.7292000000000001, 'lambda': 38.72277031887963, 'max_bin': 364}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:44:05,675]\u001b[0m Trial 105 finished with value: 0.7235913592223414 and parameters: {'n_estimators': 371, 'eta': 0.08263054432811011, 'max_depth': 12, 'alpha': 0.12300000000000001, 'lambda': 28.398113032379356, 'max_bin': 430}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:44:17,800]\u001b[0m Trial 106 finished with value: 0.7204894169820493 and parameters: {'n_estimators': 398, 'eta': 0.09773979940020341, 'max_depth': 11, 'alpha': 0.0253, 'lambda': 35.32034400181278, 'max_bin': 398}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:44:28,194]\u001b[0m Trial 107 finished with value: 0.7229068199863751 and parameters: {'n_estimators': 323, 'eta': 0.09032274474113731, 'max_depth': 11, 'alpha': 0.1458, 'lambda': 33.401805773968995, 'max_bin': 415}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:44:37,417]\u001b[0m Trial 108 finished with value: 0.7132023505642974 and parameters: {'n_estimators': 266, 'eta': 0.0950318061511401, 'max_depth': 12, 'alpha': 0.2061, 'lambda': 36.831320747575255, 'max_bin': 299}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:44:47,339]\u001b[0m Trial 109 finished with value: 0.7215506999641921 and parameters: {'n_estimators': 308, 'eta': 0.08666624755917635, 'max_depth': 11, 'alpha': 0.0637, 'lambda': 34.90365954204125, 'max_bin': 439}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:44:56,046]\u001b[0m Trial 110 finished with value: 0.7154449700174019 and parameters: {'n_estimators': 242, 'eta': 0.09176789495375795, 'max_depth': 12, 'alpha': 0.1617, 'lambda': 32.87396757218159, 'max_bin': 421}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:45:07,162]\u001b[0m Trial 111 finished with value: 0.7188162336154317 and parameters: {'n_estimators': 349, 'eta': 0.09710287020325155, 'max_depth': 12, 'alpha': 0.0199, 'lambda': 36.10679508234345, 'max_bin': 461}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:45:19,642]\u001b[0m Trial 112 finished with value: 0.7193202665881941 and parameters: {'n_estimators': 418, 'eta': 0.09489195487688623, 'max_depth': 11, 'alpha': 0.0509, 'lambda': 39.219079410814544, 'max_bin': 451}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:45:30,232]\u001b[0m Trial 113 finished with value: 0.7210317115338334 and parameters: {'n_estimators': 293, 'eta': 0.08882089857186586, 'max_depth': 12, 'alpha': 0.003, 'lambda': 34.33663264806108, 'max_bin': 427}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:45:42,250]\u001b[0m Trial 114 finished with value: 0.7223230669021546 and parameters: {'n_estimators': 528, 'eta': 0.08413455304560044, 'max_depth': 11, 'alpha': 0.095, 'lambda': 36.17659522079951, 'max_bin': 435}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:45:54,193]\u001b[0m Trial 115 finished with value: 0.7192554811828642 and parameters: {'n_estimators': 383, 'eta': 0.09795019910856169, 'max_depth': 12, 'alpha': 0.23820000000000002, 'lambda': 37.693089456817184, 'max_bin': 444}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:46:00,719]\u001b[0m Trial 116 finished with value: 0.7147264109755389 and parameters: {'n_estimators': 199, 'eta': 0.09282977159053868, 'max_depth': 11, 'alpha': 0.3104, 'lambda': 39.93447516223231, 'max_bin': 463}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:46:10,926]\u001b[0m Trial 117 finished with value: 0.7222648773834879 and parameters: {'n_estimators': 282, 'eta': 0.0810907444965258, 'max_depth': 12, 'alpha': 0.0369, 'lambda': 30.489861113941863, 'max_bin': 417}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:46:20,880]\u001b[0m Trial 118 finished with value: 0.7185094971559506 and parameters: {'n_estimators': 319, 'eta': 0.09031238713889535, 'max_depth': 11, 'alpha': 0.061500000000000006, 'lambda': 32.099421558365165, 'max_bin': 476}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:46:35,885]\u001b[0m Trial 119 finished with value: 0.7213345627403249 and parameters: {'n_estimators': 447, 'eta': 0.08702300900037015, 'max_depth': 12, 'alpha': 0.07970000000000001, 'lambda': 38.5246081867842, 'max_bin': 423}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:46:47,142]\u001b[0m Trial 120 finished with value: 0.7181758043250291 and parameters: {'n_estimators': 357, 'eta': 0.09461847644956875, 'max_depth': 12, 'alpha': 0.12480000000000001, 'lambda': 35.719437218871946, 'max_bin': 456}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:46:58,070]\u001b[0m Trial 121 finished with value: 0.7177839230394695 and parameters: {'n_estimators': 465, 'eta': 0.09977038771253122, 'max_depth': 10, 'alpha': 0.0151, 'lambda': 36.77403320243827, 'max_bin': 464}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:47:08,657]\u001b[0m Trial 122 finished with value: 0.7164980440209254 and parameters: {'n_estimators': 478, 'eta': 0.09992634829649505, 'max_depth': 10, 'alpha': 0.6398, 'lambda': 37.338732193924706, 'max_bin': 448}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:47:17,917]\u001b[0m Trial 123 finished with value: 0.7190971761777766 and parameters: {'n_estimators': 427, 'eta': 0.09736565065250521, 'max_depth': 10, 'alpha': 0.0322, 'lambda': 34.24013294232094, 'max_bin': 433}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:47:28,956]\u001b[0m Trial 124 finished with value: 0.7202627277528425 and parameters: {'n_estimators': 499, 'eta': 0.0929496566976248, 'max_depth': 10, 'alpha': 0.8, 'lambda': 35.01934882340651, 'max_bin': 441}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:47:43,145]\u001b[0m Trial 125 finished with value: 0.7239081885551207 and parameters: {'n_estimators': 568, 'eta': 0.09545065440735583, 'max_depth': 11, 'alpha': 0.00030000000000000003, 'lambda': 38.14340521380097, 'max_bin': 437}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:47:57,944]\u001b[0m Trial 126 finished with value: 0.7196530178733056 and parameters: {'n_estimators': 530, 'eta': 0.08538207067915782, 'max_depth': 12, 'alpha': 0.0458, 'lambda': 39.14683195607306, 'max_bin': 429}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:48:09,310]\u001b[0m Trial 127 finished with value: 0.7192194789857596 and parameters: {'n_estimators': 404, 'eta': 0.08902206563696838, 'max_depth': 11, 'alpha': 0.496, 'lambda': 37.26111741999514, 'max_bin': 452}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:48:16,943]\u001b[0m Trial 128 finished with value: 0.714226017280973 and parameters: {'n_estimators': 485, 'eta': 0.09110227516128867, 'max_depth': 6, 'alpha': 0.08850000000000001, 'lambda': 31.52436876326609, 'max_bin': 411}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:48:26,521]\u001b[0m Trial 129 finished with value: 0.7190043891959154 and parameters: {'n_estimators': 335, 'eta': 0.07865824031722175, 'max_depth': 10, 'alpha': 0.0666, 'lambda': 33.768237426806614, 'max_bin': 402}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:48:35,637]\u001b[0m Trial 130 finished with value: 0.718313189738183 and parameters: {'n_estimators': 254, 'eta': 0.09316497417916889, 'max_depth': 12, 'alpha': 0.17900000000000002, 'lambda': 36.02311094561626, 'max_bin': 423}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:48:45,581]\u001b[0m Trial 131 finished with value: 0.7141632564717437 and parameters: {'n_estimators': 279, 'eta': 0.0869509630979066, 'max_depth': 12, 'alpha': 0.026500000000000003, 'lambda': 39.80638233809472, 'max_bin': 439}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:48:53,855]\u001b[0m Trial 132 finished with value: 0.7161197596332352 and parameters: {'n_estimators': 228, 'eta': 0.0850231553281797, 'max_depth': 12, 'alpha': 0.051300000000000005, 'lambda': 38.513770587077104, 'max_bin': 433}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:49:03,841]\u001b[0m Trial 133 finished with value: 0.7153575239967765 and parameters: {'n_estimators': 304, 'eta': 0.09751607665857755, 'max_depth': 12, 'alpha': 0.1134, 'lambda': 39.93761661193524, 'max_bin': 445}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:49:13,838]\u001b[0m Trial 134 finished with value: 0.7171227126252713 and parameters: {'n_estimators': 270, 'eta': 0.08728226485217112, 'max_depth': 12, 'alpha': 0.0487, 'lambda': 39.033863395613466, 'max_bin': 427}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:49:26,386]\u001b[0m Trial 135 finished with value: 0.7193844610873408 and parameters: {'n_estimators': 370, 'eta': 0.08273258908814958, 'max_depth': 12, 'alpha': 0.0013000000000000002, 'lambda': 36.960728474947516, 'max_bin': 419}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:49:36,605]\u001b[0m Trial 136 finished with value: 0.718724486230666 and parameters: {'n_estimators': 301, 'eta': 0.09133081577006955, 'max_depth': 11, 'alpha': 0.0724, 'lambda': 37.86447753181224, 'max_bin': 457}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:49:47,663]\u001b[0m Trial 137 finished with value: 0.7186395330060027 and parameters: {'n_estimators': 330, 'eta': 0.08992265220536037, 'max_depth': 12, 'alpha': 0.101, 'lambda': 32.72193656767913, 'max_bin': 436}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:49:56,144]\u001b[0m Trial 138 finished with value: 0.7169301714857659 and parameters: {'n_estimators': 266, 'eta': 0.09603658074165783, 'max_depth': 11, 'alpha': 0.5919, 'lambda': 35.081911031527014, 'max_bin': 472}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:50:08,524]\u001b[0m Trial 139 finished with value: 0.7185565371333898 and parameters: {'n_estimators': 454, 'eta': 0.093255707295324, 'max_depth': 12, 'alpha': 0.039, 'lambda': 36.62264253429119, 'max_bin': 349}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:50:15,975]\u001b[0m Trial 140 finished with value: 0.7153741163092184 and parameters: {'n_estimators': 243, 'eta': 0.09824926087479069, 'max_depth': 10, 'alpha': 0.13440000000000002, 'lambda': 38.98788967982163, 'max_bin': 413}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:50:28,308]\u001b[0m Trial 141 finished with value: 0.7212438767532738 and parameters: {'n_estimators': 521, 'eta': 0.08675303475593107, 'max_depth': 10, 'alpha': 0.0731, 'lambda': 32.999860363411635, 'max_bin': 449}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:50:39,927]\u001b[0m Trial 142 finished with value: 0.7228301491935808 and parameters: {'n_estimators': 389, 'eta': 0.08041401110545436, 'max_depth': 10, 'alpha': 0.015600000000000001, 'lambda': 29.801270964153723, 'max_bin': 455}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:50:49,775]\u001b[0m Trial 143 finished with value: 0.7232660855351206 and parameters: {'n_estimators': 347, 'eta': 0.08876275766427404, 'max_depth': 10, 'alpha': 0.0926, 'lambda': 33.858987806060725, 'max_bin': 442}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:50:52,030]\u001b[0m Trial 144 finished with value: 0.6751912229402318 and parameters: {'n_estimators': 60, 'eta': 0.08478251563379352, 'max_depth': 11, 'alpha': 0.0591, 'lambda': 32.22621820941343, 'max_bin': 466}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:51:03,415]\u001b[0m Trial 145 finished with value: 0.7194782117299912 and parameters: {'n_estimators': 440, 'eta': 0.09451774183691955, 'max_depth': 10, 'alpha': 0.0263, 'lambda': 35.283241370299095, 'max_bin': 430}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:51:14,674]\u001b[0m Trial 146 finished with value: 0.7164725682541594 and parameters: {'n_estimators': 316, 'eta': 0.09144789206522866, 'max_depth': 12, 'alpha': 0.1553, 'lambda': 37.862880539779475, 'max_bin': 425}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:51:24,018]\u001b[0m Trial 147 finished with value: 0.7180027836631905 and parameters: {'n_estimators': 293, 'eta': 0.08320149427895898, 'max_depth': 12, 'alpha': 0.9444, 'lambda': 30.799825489825274, 'max_bin': 445}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:51:36,446]\u001b[0m Trial 148 finished with value: 0.7228862788904934 and parameters: {'n_estimators': 427, 'eta': 0.08864032491291879, 'max_depth': 11, 'alpha': 0.1116, 'lambda': 35.93328300774059, 'max_bin': 407}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:51:46,892]\u001b[0m Trial 149 finished with value: 0.7174165223309523 and parameters: {'n_estimators': 376, 'eta': 0.09566569891579668, 'max_depth': 11, 'alpha': 0.8607, 'lambda': 34.356925809322185, 'max_bin': 459}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7373\n",
      "\tBest params:\n",
      "\t\tn_estimators: 281\n",
      "\t\teta: 0.09991575107965912\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.11860000000000001\n",
      "\t\tlambda: 39.52636832968151\n",
      "\t\tmax_bin: 438\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.743638    0.730201    0.745800\n",
      "1                    TP   43.000000   43.000000   35.000000\n",
      "2                    TN  305.000000  305.000000  299.000000\n",
      "3                    FP    8.000000    9.000000   15.000000\n",
      "4                    FN   26.000000   25.000000   33.000000\n",
      "5              Accuracy    0.910995    0.910995    0.874346\n",
      "6             Precision    0.843137    0.826923    0.700000\n",
      "7           Sensitivity    0.623188    0.632353    0.514706\n",
      "8           Specificity    0.974400    0.971300    0.952200\n",
      "9              F1 score    0.716667    0.716667    0.593220\n",
      "10  F1 score (weighted)    0.905563    0.906167    0.866512\n",
      "11     F1 score (macro)    0.831936    0.831936    0.759458\n",
      "12    Balanced Accuracy    0.798815    0.801845    0.733468\n",
      "13                  MCC    0.675973    0.673403    0.529568\n",
      "14                  NPV    0.921500    0.924200    0.900600\n",
      "15              ROC_AUC    0.798815    0.801845    0.733468\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_xgb_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_2_cat = np.where(((y_pred_xgb_2 >= 2) | (y_pred_xgb_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:51:59,411]\u001b[0m Trial 150 finished with value: 0.7271042386428457 and parameters: {'n_estimators': 476, 'eta': 0.08598441953304929, 'max_depth': 12, 'alpha': 0.07970000000000001, 'lambda': 23.770551484940906, 'max_bin': 437}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:52:08,358]\u001b[0m Trial 151 finished with value: 0.7253810023280864 and parameters: {'n_estimators': 252, 'eta': 0.09690383075080686, 'max_depth': 12, 'alpha': 0.0403, 'lambda': 35.35817675668797, 'max_bin': 411}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:52:17,395]\u001b[0m Trial 152 finished with value: 0.7210084191789192 and parameters: {'n_estimators': 256, 'eta': 0.09960476149080222, 'max_depth': 12, 'alpha': 0.0195, 'lambda': 37.331532401076686, 'max_bin': 418}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:52:25,085]\u001b[0m Trial 153 finished with value: 0.7232502814891393 and parameters: {'n_estimators': 214, 'eta': 0.09401309707876716, 'max_depth': 12, 'alpha': 0.051800000000000006, 'lambda': 33.20246083640926, 'max_bin': 476}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:52:34,346]\u001b[0m Trial 154 finished with value: 0.721970210542507 and parameters: {'n_estimators': 282, 'eta': 0.09142784109450387, 'max_depth': 12, 'alpha': 0.0313, 'lambda': 38.421387604722845, 'max_bin': 423}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:52:44,687]\u001b[0m Trial 155 finished with value: 0.7214900215590594 and parameters: {'n_estimators': 314, 'eta': 0.09558315793608281, 'max_depth': 12, 'alpha': 0.0711, 'lambda': 36.43442836516733, 'max_bin': 432}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:52:52,743]\u001b[0m Trial 156 finished with value: 0.7241358590517881 and parameters: {'n_estimators': 273, 'eta': 0.08787874652104784, 'max_depth': 10, 'alpha': 0.0021000000000000003, 'lambda': 34.58647069523595, 'max_bin': 415}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:53:03,361]\u001b[0m Trial 157 finished with value: 0.7259229167163775 and parameters: {'n_estimators': 338, 'eta': 0.09000468315317024, 'max_depth': 12, 'alpha': 0.0935, 'lambda': 31.990309057342515, 'max_bin': 452}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:53:14,035]\u001b[0m Trial 158 finished with value: 0.6576578715310433 and parameters: {'n_estimators': 300, 'eta': 0.013985529443182473, 'max_depth': 11, 'alpha': 0.0509, 'lambda': 39.16741732041997, 'max_bin': 393}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:53:24,238]\u001b[0m Trial 159 finished with value: 0.7206449308560592 and parameters: {'n_estimators': 320, 'eta': 0.09259940485403019, 'max_depth': 12, 'alpha': 0.1184, 'lambda': 35.74065413311699, 'max_bin': 428}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:53:31,775]\u001b[0m Trial 160 finished with value: 0.7235933082806456 and parameters: {'n_estimators': 225, 'eta': 0.09779320733986005, 'max_depth': 11, 'alpha': 0.025500000000000002, 'lambda': 37.26572491443466, 'max_bin': 421}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:53:41,628]\u001b[0m Trial 161 finished with value: 0.7222931121410182 and parameters: {'n_estimators': 352, 'eta': 0.08996443910598845, 'max_depth': 11, 'alpha': 0.2906, 'lambda': 34.840915445988884, 'max_bin': 404}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:53:51,930]\u001b[0m Trial 162 finished with value: 0.7236571998823604 and parameters: {'n_estimators': 360, 'eta': 0.08664963715414657, 'max_depth': 11, 'alpha': 0.07680000000000001, 'lambda': 33.5685277605637, 'max_bin': 409}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:54:02,209]\u001b[0m Trial 163 finished with value: 0.7244298370733198 and parameters: {'n_estimators': 414, 'eta': 0.08275906239278813, 'max_depth': 10, 'alpha': 0.14450000000000002, 'lambda': 34.23964897450682, 'max_bin': 406}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:54:11,803]\u001b[0m Trial 164 finished with value: 0.7240848029066971 and parameters: {'n_estimators': 290, 'eta': 0.09252613554717173, 'max_depth': 11, 'alpha': 0.1124, 'lambda': 39.7492749092229, 'max_bin': 436}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:54:21,956]\u001b[0m Trial 165 finished with value: 0.7220853292841045 and parameters: {'n_estimators': 396, 'eta': 0.09583970098303353, 'max_depth': 12, 'alpha': 0.058600000000000006, 'lambda': 32.69449974862318, 'max_bin': 415}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:54:34,372]\u001b[0m Trial 166 finished with value: 0.7249931955075588 and parameters: {'n_estimators': 490, 'eta': 0.076438956713361, 'max_depth': 11, 'alpha': 0.17320000000000002, 'lambda': 38.3214264140589, 'max_bin': 398}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:54:45,533]\u001b[0m Trial 167 finished with value: 0.7234414748677356 and parameters: {'n_estimators': 332, 'eta': 0.08492196673618575, 'max_depth': 12, 'alpha': 0.12510000000000002, 'lambda': 36.46342618776531, 'max_bin': 431}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:54:53,327]\u001b[0m Trial 168 finished with value: 0.7210291071755665 and parameters: {'n_estimators': 261, 'eta': 0.08889357889170697, 'max_depth': 10, 'alpha': 0.7201000000000001, 'lambda': 35.622583072439674, 'max_bin': 424}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:55:05,096]\u001b[0m Trial 169 finished with value: 0.7244998310139097 and parameters: {'n_estimators': 460, 'eta': 0.09404031473703532, 'max_depth': 12, 'alpha': 0.08950000000000001, 'lambda': 31.101334902439753, 'max_bin': 443}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:55:10,360]\u001b[0m Trial 170 finished with value: 0.7072032013854777 and parameters: {'n_estimators': 360, 'eta': 0.09964054823386875, 'max_depth': 11, 'alpha': 0.20620000000000002, 'lambda': 1.0313102954987379, 'max_bin': 417}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:55:19,495]\u001b[0m Trial 171 finished with value: 0.7222813781324859 and parameters: {'n_estimators': 309, 'eta': 0.08164023085881232, 'max_depth': 12, 'alpha': 0.8904000000000001, 'lambda': 33.37249453398731, 'max_bin': 427}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:55:28,391]\u001b[0m Trial 172 finished with value: 0.7245298745638831 and parameters: {'n_estimators': 283, 'eta': 0.08441283800756201, 'max_depth': 12, 'alpha': 0.9204, 'lambda': 32.57384686873858, 'max_bin': 434}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:55:34,845]\u001b[0m Trial 173 finished with value: 0.7199694222306496 and parameters: {'n_estimators': 328, 'eta': 0.07914203224620933, 'max_depth': 12, 'alpha': 0.9599000000000001, 'lambda': 2.7972119538548235, 'max_bin': 440}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:55:44,239]\u001b[0m Trial 174 finished with value: 0.7238097793205223 and parameters: {'n_estimators': 303, 'eta': 0.08764334184382959, 'max_depth': 12, 'alpha': 0.9861000000000001, 'lambda': 34.65719528208147, 'max_bin': 421}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:55:52,473]\u001b[0m Trial 175 finished with value: 0.7212694004074869 and parameters: {'n_estimators': 242, 'eta': 0.09050106585853855, 'max_depth': 12, 'alpha': 0.9680000000000001, 'lambda': 31.80300610631202, 'max_bin': 429}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:56:00,434]\u001b[0m Trial 176 finished with value: 0.7269270427898469 and parameters: {'n_estimators': 345, 'eta': 0.09729945069218349, 'max_depth': 9, 'alpha': 0.0374, 'lambda': 28.366556881252315, 'max_bin': 463}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:56:10,648]\u001b[0m Trial 177 finished with value: 0.7226995929231916 and parameters: {'n_estimators': 380, 'eta': 0.0833960043995671, 'max_depth': 12, 'alpha': 0.925, 'lambda': 33.981507159826805, 'max_bin': 411}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:56:22,794]\u001b[0m Trial 178 finished with value: 0.7251952623705149 and parameters: {'n_estimators': 512, 'eta': 0.09224031046840178, 'max_depth': 11, 'alpha': 0.06620000000000001, 'lambda': 37.70175663665791, 'max_bin': 446}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:56:30,501]\u001b[0m Trial 179 finished with value: 0.7206945094750705 and parameters: {'n_estimators': 271, 'eta': 0.08675051250294777, 'max_depth': 10, 'alpha': 0.0179, 'lambda': 39.98684010463155, 'max_bin': 437}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:56:39,755]\u001b[0m Trial 180 finished with value: 0.7213011698640743 and parameters: {'n_estimators': 307, 'eta': 0.09011935244033596, 'max_depth': 12, 'alpha': 0.7634000000000001, 'lambda': 33.25588682282923, 'max_bin': 431}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:56:47,779]\u001b[0m Trial 181 finished with value: 0.7204859709675048 and parameters: {'n_estimators': 237, 'eta': 0.09290058861557889, 'max_depth': 12, 'alpha': 0.8290000000000001, 'lambda': 34.72437364504739, 'max_bin': 448}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:56:54,652]\u001b[0m Trial 182 finished with value: 0.7178474629756548 and parameters: {'n_estimators': 199, 'eta': 0.09549910030138425, 'max_depth': 12, 'alpha': 0.8611000000000001, 'lambda': 35.57966224410485, 'max_bin': 454}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:57:03,033]\u001b[0m Trial 183 finished with value: 0.7203700947070452 and parameters: {'n_estimators': 251, 'eta': 0.09145908119170257, 'max_depth': 12, 'alpha': 0.7924, 'lambda': 38.923824399183744, 'max_bin': 472}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:57:11,813]\u001b[0m Trial 184 finished with value: 0.719203794580924 and parameters: {'n_estimators': 287, 'eta': 0.08926382313759508, 'max_depth': 12, 'alpha': 0.9293, 'lambda': 36.71968298570354, 'max_bin': 441}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:57:20,469]\u001b[0m Trial 185 finished with value: 0.7247669995573285 and parameters: {'n_estimators': 266, 'eta': 0.09468345198778078, 'max_depth': 12, 'alpha': 0.8196, 'lambda': 33.727606456477545, 'max_bin': 426}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:57:29,457]\u001b[0m Trial 186 finished with value: 0.7227777500507389 and parameters: {'n_estimators': 327, 'eta': 0.09991663017120385, 'max_depth': 12, 'alpha': 0.996, 'lambda': 34.84000244174133, 'max_bin': 449}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:57:41,563]\u001b[0m Trial 187 finished with value: 0.7275253520682977 and parameters: {'n_estimators': 448, 'eta': 0.08549819092643988, 'max_depth': 12, 'alpha': 0.0458, 'lambda': 32.6137469493129, 'max_bin': 435}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:57:51,864]\u001b[0m Trial 188 finished with value: 0.7220982530314429 and parameters: {'n_estimators': 368, 'eta': 0.09754609760133105, 'max_depth': 11, 'alpha': 0.0014, 'lambda': 36.227434992197104, 'max_bin': 419}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:57:58,545]\u001b[0m Trial 189 finished with value: 0.7182702838051022 and parameters: {'n_estimators': 218, 'eta': 0.08828310390353879, 'max_depth': 10, 'alpha': 0.0947, 'lambda': 38.34635523636007, 'max_bin': 272}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:58:08,637]\u001b[0m Trial 190 finished with value: 0.7239229370443176 and parameters: {'n_estimators': 293, 'eta': 0.08053952566046901, 'max_depth': 12, 'alpha': 0.0308, 'lambda': 34.18314112000348, 'max_bin': 459}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:58:19,484]\u001b[0m Trial 191 finished with value: 0.7235846493464562 and parameters: {'n_estimators': 504, 'eta': 0.09778348625677796, 'max_depth': 10, 'alpha': 0.0002, 'lambda': 37.664748074997185, 'max_bin': 472}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:58:29,218]\u001b[0m Trial 192 finished with value: 0.7256131784053881 and parameters: {'n_estimators': 481, 'eta': 0.09340997666665436, 'max_depth': 10, 'alpha': 0.0475, 'lambda': 26.003677437381768, 'max_bin': 479}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:58:40,209]\u001b[0m Trial 193 finished with value: 0.7238196532685881 and parameters: {'n_estimators': 551, 'eta': 0.09614501366727835, 'max_depth': 10, 'alpha': 0.021400000000000002, 'lambda': 39.1526194467168, 'max_bin': 465}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:58:51,056]\u001b[0m Trial 194 finished with value: 0.7228454338095796 and parameters: {'n_estimators': 496, 'eta': 0.0986337586584467, 'max_depth': 10, 'alpha': 0.0683, 'lambda': 37.16337529264445, 'max_bin': 468}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:59:02,026]\u001b[0m Trial 195 finished with value: 0.7233563005738335 and parameters: {'n_estimators': 462, 'eta': 0.09229209078124133, 'max_depth': 11, 'alpha': 0.0201, 'lambda': 35.55102018640488, 'max_bin': 423}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:59:11,622]\u001b[0m Trial 196 finished with value: 0.7235993052208988 and parameters: {'n_estimators': 432, 'eta': 0.09423861522604525, 'max_depth': 10, 'alpha': 0.8995000000000001, 'lambda': 38.107960495896386, 'max_bin': 431}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:59:21,647]\u001b[0m Trial 197 finished with value: 0.7235059769034667 and parameters: {'n_estimators': 315, 'eta': 0.09066285875627146, 'max_depth': 12, 'alpha': 0.6654, 'lambda': 36.475242792230475, 'max_bin': 441}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:59:30,500]\u001b[0m Trial 198 finished with value: 0.7198883029686209 and parameters: {'n_estimators': 239, 'eta': 0.08678245267390333, 'max_depth': 12, 'alpha': 0.1353, 'lambda': 35.011109764737604, 'max_bin': 456}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:59:40,447]\u001b[0m Trial 199 finished with value: 0.726306009407266 and parameters: {'n_estimators': 344, 'eta': 0.09999473403252018, 'max_depth': 11, 'alpha': 0.084, 'lambda': 39.27202662383221, 'max_bin': 323}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7373\n",
      "\tBest params:\n",
      "\t\tn_estimators: 281\n",
      "\t\teta: 0.09991575107965912\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.11860000000000001\n",
      "\t\tlambda: 39.52636832968151\n",
      "\t\tmax_bin: 438\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.743638    0.730201    0.745800    0.748070\n",
      "1                    TP   43.000000   43.000000   35.000000   36.000000\n",
      "2                    TN  305.000000  305.000000  299.000000  300.000000\n",
      "3                    FP    8.000000    9.000000   15.000000   15.000000\n",
      "4                    FN   26.000000   25.000000   33.000000   31.000000\n",
      "5              Accuracy    0.910995    0.910995    0.874346    0.879581\n",
      "6             Precision    0.843137    0.826923    0.700000    0.705882\n",
      "7           Sensitivity    0.623188    0.632353    0.514706    0.537313\n",
      "8           Specificity    0.974400    0.971300    0.952200    0.952400\n",
      "9              F1 score    0.716667    0.716667    0.593220    0.610169\n",
      "10  F1 score (weighted)    0.905563    0.906167    0.866512    0.872908\n",
      "11     F1 score (macro)    0.831936    0.831936    0.759458    0.769481\n",
      "12    Balanced Accuracy    0.798815    0.801845    0.733468    0.744847\n",
      "13                  MCC    0.675973    0.673403    0.529568    0.547544\n",
      "14                  NPV    0.921500    0.924200    0.900600    0.906300\n",
      "15              ROC_AUC    0.798815    0.801845    0.733468    0.744847\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_xgb_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_3_cat = np.where(((y_pred_xgb_3 >= 2) | (y_pred_xgb_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 07:59:45,262]\u001b[0m Trial 200 finished with value: 0.7116560970043829 and parameters: {'n_estimators': 260, 'eta': 0.08365857037946006, 'max_depth': 5, 'alpha': 0.046900000000000004, 'lambda': 33.25842399546457, 'max_bin': 413}. Best is trial 67 with value: 0.7372846414475894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 07:59:56,287]\u001b[0m Trial 201 finished with value: 0.7398814228623598 and parameters: {'n_estimators': 404, 'eta': 0.08480651831012566, 'max_depth': 12, 'alpha': 0.095, 'lambda': 31.690019609250456, 'max_bin': 426}. Best is trial 201 with value: 0.7398814228623598.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:00:07,654]\u001b[0m Trial 202 finished with value: 0.7455406770366659 and parameters: {'n_estimators': 408, 'eta': 0.08777846500101312, 'max_depth': 12, 'alpha': 0.11, 'lambda': 31.855553059350896, 'max_bin': 422}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:00:19,433]\u001b[0m Trial 203 finished with value: 0.7409558561657154 and parameters: {'n_estimators': 404, 'eta': 0.08540856010032605, 'max_depth': 12, 'alpha': 0.1208, 'lambda': 31.527670790496003, 'max_bin': 419}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:00:30,201]\u001b[0m Trial 204 finished with value: 0.7403743827973933 and parameters: {'n_estimators': 404, 'eta': 0.08468273585785413, 'max_depth': 12, 'alpha': 0.1087, 'lambda': 31.10410685111486, 'max_bin': 418}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:00:41,347]\u001b[0m Trial 205 finished with value: 0.740079358357592 and parameters: {'n_estimators': 416, 'eta': 0.08543164062468105, 'max_depth': 12, 'alpha': 0.1263, 'lambda': 30.14032383394203, 'max_bin': 416}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:00:52,356]\u001b[0m Trial 206 finished with value: 0.742139500576591 and parameters: {'n_estimators': 412, 'eta': 0.08182146471493461, 'max_depth': 12, 'alpha': 0.16390000000000002, 'lambda': 30.1191321090599, 'max_bin': 420}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:01:03,467]\u001b[0m Trial 207 finished with value: 0.7400926693382989 and parameters: {'n_estimators': 412, 'eta': 0.08181975697004296, 'max_depth': 12, 'alpha': 0.1545, 'lambda': 30.131775616504424, 'max_bin': 418}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:01:14,466]\u001b[0m Trial 208 finished with value: 0.7381191499304232 and parameters: {'n_estimators': 408, 'eta': 0.08013585078116962, 'max_depth': 12, 'alpha': 0.2257, 'lambda': 29.685330737190885, 'max_bin': 419}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:01:25,931]\u001b[0m Trial 209 finished with value: 0.7410955744557923 and parameters: {'n_estimators': 404, 'eta': 0.07887582576628487, 'max_depth': 12, 'alpha': 0.23290000000000002, 'lambda': 29.887135306870842, 'max_bin': 418}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:01:38,236]\u001b[0m Trial 210 finished with value: 0.7383965517719799 and parameters: {'n_estimators': 401, 'eta': 0.07618862811901173, 'max_depth': 12, 'alpha': 0.2333, 'lambda': 29.692540983276213, 'max_bin': 418}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:01:49,674]\u001b[0m Trial 211 finished with value: 0.7433129758680102 and parameters: {'n_estimators': 410, 'eta': 0.07277172738420838, 'max_depth': 12, 'alpha': 0.23500000000000001, 'lambda': 29.714765117330714, 'max_bin': 417}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:02:01,731]\u001b[0m Trial 212 finished with value: 0.7424248244664808 and parameters: {'n_estimators': 407, 'eta': 0.07465269747777273, 'max_depth': 12, 'alpha': 0.1912, 'lambda': 29.632452007173715, 'max_bin': 417}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:02:13,753]\u001b[0m Trial 213 finished with value: 0.7416641961427746 and parameters: {'n_estimators': 409, 'eta': 0.07200456613089833, 'max_depth': 12, 'alpha': 0.24080000000000001, 'lambda': 29.293701170834044, 'max_bin': 417}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:02:25,362]\u001b[0m Trial 214 finished with value: 0.739334347482547 and parameters: {'n_estimators': 409, 'eta': 0.07351529513801137, 'max_depth': 12, 'alpha': 0.23570000000000002, 'lambda': 29.50544428181484, 'max_bin': 417}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:02:36,819]\u001b[0m Trial 215 finished with value: 0.7407677230075663 and parameters: {'n_estimators': 403, 'eta': 0.07198881753281053, 'max_depth': 12, 'alpha': 0.2576, 'lambda': 29.239910894678612, 'max_bin': 417}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:02:48,490]\u001b[0m Trial 216 finished with value: 0.7421239774576204 and parameters: {'n_estimators': 408, 'eta': 0.07317413937123801, 'max_depth': 12, 'alpha': 0.2349, 'lambda': 29.325625757933338, 'max_bin': 417}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:03:00,357]\u001b[0m Trial 217 finished with value: 0.7416787739550541 and parameters: {'n_estimators': 410, 'eta': 0.07269164119490473, 'max_depth': 12, 'alpha': 0.2379, 'lambda': 29.36418598064519, 'max_bin': 418}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:03:12,592]\u001b[0m Trial 218 finished with value: 0.7424945114783186 and parameters: {'n_estimators': 410, 'eta': 0.07336329494151836, 'max_depth': 12, 'alpha': 0.2482, 'lambda': 28.978035396459408, 'max_bin': 418}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:03:24,233]\u001b[0m Trial 219 finished with value: 0.7432207462386076 and parameters: {'n_estimators': 401, 'eta': 0.07227096644234422, 'max_depth': 12, 'alpha': 0.2467, 'lambda': 28.915113578411507, 'max_bin': 416}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:03:36,249]\u001b[0m Trial 220 finished with value: 0.7429925398675392 and parameters: {'n_estimators': 405, 'eta': 0.07169880632966631, 'max_depth': 12, 'alpha': 0.248, 'lambda': 29.251790564904585, 'max_bin': 415}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:03:47,809]\u001b[0m Trial 221 finished with value: 0.7414015643359978 and parameters: {'n_estimators': 407, 'eta': 0.07254965756471132, 'max_depth': 12, 'alpha': 0.2538, 'lambda': 29.141189744324343, 'max_bin': 416}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:03:58,367]\u001b[0m Trial 222 finished with value: 0.742232925360866 and parameters: {'n_estimators': 409, 'eta': 0.07245615536428303, 'max_depth': 12, 'alpha': 0.25630000000000003, 'lambda': 29.069719205796243, 'max_bin': 414}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:04:09,816]\u001b[0m Trial 223 finished with value: 0.7447409199002698 and parameters: {'n_estimators': 406, 'eta': 0.072534692386772, 'max_depth': 12, 'alpha': 0.2416, 'lambda': 29.056608143915735, 'max_bin': 417}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:04:20,972]\u001b[0m Trial 224 finished with value: 0.7427607774761431 and parameters: {'n_estimators': 408, 'eta': 0.07238350144828049, 'max_depth': 12, 'alpha': 0.2553, 'lambda': 29.017721808619605, 'max_bin': 415}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:04:32,872]\u001b[0m Trial 225 finished with value: 0.7415244990275964 and parameters: {'n_estimators': 404, 'eta': 0.07268056617239545, 'max_depth': 12, 'alpha': 0.2558, 'lambda': 27.747870115262558, 'max_bin': 414}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:04:44,739]\u001b[0m Trial 226 finished with value: 0.7404838889529749 and parameters: {'n_estimators': 420, 'eta': 0.07096669751258836, 'max_depth': 12, 'alpha': 0.2596, 'lambda': 27.79519708067812, 'max_bin': 407}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:04:56,674]\u001b[0m Trial 227 finished with value: 0.7442461150008478 and parameters: {'n_estimators': 421, 'eta': 0.06851386681057153, 'max_depth': 12, 'alpha': 0.2679, 'lambda': 27.80070641737075, 'max_bin': 407}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:05:08,975]\u001b[0m Trial 228 finished with value: 0.7427136262623745 and parameters: {'n_estimators': 422, 'eta': 0.0692017007730436, 'max_depth': 12, 'alpha': 0.2657, 'lambda': 27.93551727407493, 'max_bin': 405}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 08:05:20,148]\u001b[0m Trial 229 finished with value: 0.7402759623458556 and parameters: {'n_estimators': 425, 'eta': 0.07013892719836781, 'max_depth': 12, 'alpha': 0.262, 'lambda': 27.842489772412293, 'max_bin': 403}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:05:31,301]\u001b[0m Trial 230 finished with value: 0.7418320728791638 and parameters: {'n_estimators': 427, 'eta': 0.06894401053859586, 'max_depth': 12, 'alpha': 0.2635, 'lambda': 27.858941270858303, 'max_bin': 408}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:05:42,992]\u001b[0m Trial 231 finished with value: 0.741366215622677 and parameters: {'n_estimators': 425, 'eta': 0.06936932747199634, 'max_depth': 12, 'alpha': 0.2669, 'lambda': 27.782243276023873, 'max_bin': 404}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:05:54,280]\u001b[0m Trial 232 finished with value: 0.7431769365838206 and parameters: {'n_estimators': 387, 'eta': 0.06676478601345803, 'max_depth': 12, 'alpha': 0.2788, 'lambda': 26.390231457396556, 'max_bin': 407}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:06:06,169]\u001b[0m Trial 233 finished with value: 0.7411504084172889 and parameters: {'n_estimators': 389, 'eta': 0.06747518482568404, 'max_depth': 12, 'alpha': 0.2772, 'lambda': 26.817067239999602, 'max_bin': 409}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:06:17,191]\u001b[0m Trial 234 finished with value: 0.7437919988855763 and parameters: {'n_estimators': 385, 'eta': 0.06764891559722715, 'max_depth': 12, 'alpha': 0.2783, 'lambda': 26.30557661807164, 'max_bin': 401}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:06:28,461]\u001b[0m Trial 235 finished with value: 0.7418032012263102 and parameters: {'n_estimators': 386, 'eta': 0.06665642431298843, 'max_depth': 12, 'alpha': 0.2952, 'lambda': 26.563193068091504, 'max_bin': 397}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:06:40,276]\u001b[0m Trial 236 finished with value: 0.7430200560552226 and parameters: {'n_estimators': 387, 'eta': 0.06758875034995267, 'max_depth': 12, 'alpha': 0.2933, 'lambda': 26.445746088442807, 'max_bin': 398}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:06:51,521]\u001b[0m Trial 237 finished with value: 0.7423699601979891 and parameters: {'n_estimators': 386, 'eta': 0.06744325893897668, 'max_depth': 12, 'alpha': 0.2869, 'lambda': 27.014737512179686, 'max_bin': 395}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:07:03,347]\u001b[0m Trial 238 finished with value: 0.742467368000112 and parameters: {'n_estimators': 387, 'eta': 0.06552375091072528, 'max_depth': 12, 'alpha': 0.30460000000000004, 'lambda': 26.677593410587537, 'max_bin': 388}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:07:15,051]\u001b[0m Trial 239 finished with value: 0.7425081007716764 and parameters: {'n_estimators': 384, 'eta': 0.06621370035239567, 'max_depth': 12, 'alpha': 0.3085, 'lambda': 26.163121154985628, 'max_bin': 390}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:07:26,096]\u001b[0m Trial 240 finished with value: 0.7446038304536864 and parameters: {'n_estimators': 386, 'eta': 0.06602405790365919, 'max_depth': 12, 'alpha': 0.3131, 'lambda': 26.25445244297126, 'max_bin': 388}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:07:37,484]\u001b[0m Trial 241 finished with value: 0.7411148384818962 and parameters: {'n_estimators': 382, 'eta': 0.06504590190579632, 'max_depth': 12, 'alpha': 0.3088, 'lambda': 26.501237992183267, 'max_bin': 387}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:07:48,934]\u001b[0m Trial 242 finished with value: 0.7447933231991772 and parameters: {'n_estimators': 389, 'eta': 0.06789347770967234, 'max_depth': 12, 'alpha': 0.3256, 'lambda': 25.135133366816294, 'max_bin': 394}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:08:00,028]\u001b[0m Trial 243 finished with value: 0.7414136810704621 and parameters: {'n_estimators': 382, 'eta': 0.06651259439650832, 'max_depth': 12, 'alpha': 0.33640000000000003, 'lambda': 25.0187000939017, 'max_bin': 395}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:08:11,552]\u001b[0m Trial 244 finished with value: 0.7451433464072409 and parameters: {'n_estimators': 387, 'eta': 0.06251669397233824, 'max_depth': 12, 'alpha': 0.298, 'lambda': 25.768346012757046, 'max_bin': 388}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:08:23,743]\u001b[0m Trial 245 finished with value: 0.7421776261913167 and parameters: {'n_estimators': 389, 'eta': 0.0611640344614668, 'max_depth': 12, 'alpha': 0.2911, 'lambda': 25.486204179169846, 'max_bin': 381}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:08:35,423]\u001b[0m Trial 246 finished with value: 0.7437843009725806 and parameters: {'n_estimators': 383, 'eta': 0.06207661906718042, 'max_depth': 12, 'alpha': 0.2959, 'lambda': 25.908700538686013, 'max_bin': 376}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:08:47,688]\u001b[0m Trial 247 finished with value: 0.7408431615559635 and parameters: {'n_estimators': 385, 'eta': 0.06385895950864556, 'max_depth': 12, 'alpha': 0.32480000000000003, 'lambda': 25.551470749960384, 'max_bin': 378}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:09:00,204]\u001b[0m Trial 248 finished with value: 0.7405683671457005 and parameters: {'n_estimators': 442, 'eta': 0.06080767121670692, 'max_depth': 12, 'alpha': 0.2949, 'lambda': 23.771615684343132, 'max_bin': 389}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:09:11,460]\u001b[0m Trial 249 finished with value: 0.7401109325970964 and parameters: {'n_estimators': 373, 'eta': 0.06841348552336012, 'max_depth': 12, 'alpha': 0.281, 'lambda': 28.487906811284773, 'max_bin': 377}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7455\n",
      "\tBest params:\n",
      "\t\tn_estimators: 408\n",
      "\t\teta: 0.08777846500101312\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.11\n",
      "\t\tlambda: 31.855553059350896\n",
      "\t\tmax_bin: 422\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.743638    0.730201    0.745800    0.748070   \n",
      "1                    TP   43.000000   43.000000   35.000000   36.000000   \n",
      "2                    TN  305.000000  305.000000  299.000000  300.000000   \n",
      "3                    FP    8.000000    9.000000   15.000000   15.000000   \n",
      "4                    FN   26.000000   25.000000   33.000000   31.000000   \n",
      "5              Accuracy    0.910995    0.910995    0.874346    0.879581   \n",
      "6             Precision    0.843137    0.826923    0.700000    0.705882   \n",
      "7           Sensitivity    0.623188    0.632353    0.514706    0.537313   \n",
      "8           Specificity    0.974400    0.971300    0.952200    0.952400   \n",
      "9              F1 score    0.716667    0.716667    0.593220    0.610169   \n",
      "10  F1 score (weighted)    0.905563    0.906167    0.866512    0.872908   \n",
      "11     F1 score (macro)    0.831936    0.831936    0.759458    0.769481   \n",
      "12    Balanced Accuracy    0.798815    0.801845    0.733468    0.744847   \n",
      "13                  MCC    0.675973    0.673403    0.529568    0.547544   \n",
      "14                  NPV    0.921500    0.924200    0.900600    0.906300   \n",
      "15              ROC_AUC    0.798815    0.801845    0.733468    0.744847   \n",
      "\n",
      "          Set4  \n",
      "0     0.677579  \n",
      "1    37.000000  \n",
      "2   301.000000  \n",
      "3    14.000000  \n",
      "4    30.000000  \n",
      "5     0.884817  \n",
      "6     0.725490  \n",
      "7     0.552239  \n",
      "8     0.955600  \n",
      "9     0.627119  \n",
      "10    0.878434  \n",
      "11    0.779504  \n",
      "12    0.753897  \n",
      "13    0.567782  \n",
      "14    0.909400  \n",
      "15    0.753897  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_xgb_4)\n",
    "# now convert the resuls to binary with cutoff 6.4\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_4_cat = np.where(((y_pred_xgb_4 >= 2) | (y_pred_xgb_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 08:09:23,412]\u001b[0m Trial 250 finished with value: 0.7205864851125388 and parameters: {'n_estimators': 432, 'eta': 0.06235147521659232, 'max_depth': 12, 'alpha': 0.32020000000000004, 'lambda': 24.339937346644035, 'max_bin': 371}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:09:33,899]\u001b[0m Trial 251 finished with value: 0.7223381701068096 and parameters: {'n_estimators': 388, 'eta': 0.06973620893869859, 'max_depth': 12, 'alpha': 0.3493, 'lambda': 25.8346431505001, 'max_bin': 383}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:09:44,828]\u001b[0m Trial 252 finished with value: 0.7233818212649784 and parameters: {'n_estimators': 374, 'eta': 0.06548837282799881, 'max_depth': 12, 'alpha': 0.29000000000000004, 'lambda': 27.245017042574787, 'max_bin': 385}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:09:52,215]\u001b[0m Trial 253 finished with value: 0.7112876645056272 and parameters: {'n_estimators': 390, 'eta': 0.05788085054508, 'max_depth': 7, 'alpha': 0.304, 'lambda': 26.377819422198634, 'max_bin': 393}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:10:04,022]\u001b[0m Trial 254 finished with value: 0.7224243333610448 and parameters: {'n_estimators': 434, 'eta': 0.06839150033577703, 'max_depth': 12, 'alpha': 0.2792, 'lambda': 27.009869654108154, 'max_bin': 382}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:10:14,054]\u001b[0m Trial 255 finished with value: 0.7200698042456225 and parameters: {'n_estimators': 391, 'eta': 0.07443075546118388, 'max_depth': 12, 'alpha': 0.2762, 'lambda': 28.337178611758652, 'max_bin': 399}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:10:24,962]\u001b[0m Trial 256 finished with value: 0.7238601762919779 and parameters: {'n_estimators': 373, 'eta': 0.06346581248444702, 'max_depth': 12, 'alpha': 0.3678, 'lambda': 25.18897770383954, 'max_bin': 391}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:10:36,479]\u001b[0m Trial 257 finished with value: 0.723433813554727 and parameters: {'n_estimators': 426, 'eta': 0.06825656717405566, 'max_depth': 12, 'alpha': 0.3003, 'lambda': 28.380314798446282, 'max_bin': 401}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:10:48,014]\u001b[0m Trial 258 finished with value: 0.7210451523793562 and parameters: {'n_estimators': 390, 'eta': 0.06040230993684725, 'max_depth': 12, 'alpha': 0.21860000000000002, 'lambda': 27.269985108913154, 'max_bin': 389}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:10:59,053]\u001b[0m Trial 259 finished with value: 0.7209827978626323 and parameters: {'n_estimators': 428, 'eta': 0.06612655304368473, 'max_depth': 12, 'alpha': 0.33680000000000004, 'lambda': 25.995488391923715, 'max_bin': 395}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:11:10,179]\u001b[0m Trial 260 finished with value: 0.7226204385049173 and parameters: {'n_estimators': 366, 'eta': 0.06983542561666262, 'max_depth': 12, 'alpha': 0.2758, 'lambda': 24.58741557527052, 'max_bin': 371}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:11:21,900]\u001b[0m Trial 261 finished with value: 0.7221634250192985 and parameters: {'n_estimators': 448, 'eta': 0.0745705854604374, 'max_depth': 12, 'alpha': 0.3133, 'lambda': 28.742568766581716, 'max_bin': 401}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:11:32,757]\u001b[0m Trial 262 finished with value: 0.7225513691721239 and parameters: {'n_estimators': 390, 'eta': 0.07010179191150916, 'max_depth': 12, 'alpha': 0.251, 'lambda': 27.06824767285776, 'max_bin': 407}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:11:43,944]\u001b[0m Trial 263 finished with value: 0.7205868569872161 and parameters: {'n_estimators': 419, 'eta': 0.06364496906180178, 'max_depth': 12, 'alpha': 0.2144, 'lambda': 25.719864960918855, 'max_bin': 383}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:11:55,273]\u001b[0m Trial 264 finished with value: 0.7223083313338101 and parameters: {'n_estimators': 368, 'eta': 0.0670233180299776, 'max_depth': 12, 'alpha': 0.19140000000000001, 'lambda': 28.030229791221974, 'max_bin': 390}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:12:05,590]\u001b[0m Trial 265 finished with value: 0.7233272496602112 and parameters: {'n_estimators': 395, 'eta': 0.0711144803384455, 'max_depth': 12, 'alpha': 0.2856, 'lambda': 26.23604076737188, 'max_bin': 400}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:12:16,792]\u001b[0m Trial 266 finished with value: 0.7198458528622994 and parameters: {'n_estimators': 438, 'eta': 0.06841013967279212, 'max_depth': 12, 'alpha': 0.2682, 'lambda': 28.615100225304, 'max_bin': 380}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:12:27,387]\u001b[0m Trial 267 finished with value: 0.7221218385305882 and parameters: {'n_estimators': 415, 'eta': 0.07525832459757745, 'max_depth': 12, 'alpha': 0.3114, 'lambda': 27.331622230514018, 'max_bin': 410}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:12:38,118]\u001b[0m Trial 268 finished with value: 0.720204676150723 and parameters: {'n_estimators': 379, 'eta': 0.06425894621523694, 'max_depth': 12, 'alpha': 0.24860000000000002, 'lambda': 26.731425886836448, 'max_bin': 373}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:12:49,716]\u001b[0m Trial 269 finished with value: 0.7227216763991755 and parameters: {'n_estimators': 421, 'eta': 0.061552519619803694, 'max_depth': 12, 'alpha': 0.29350000000000004, 'lambda': 24.90017356231442, 'max_bin': 396}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:13:01,372]\u001b[0m Trial 270 finished with value: 0.7229413677332343 and parameters: {'n_estimators': 396, 'eta': 0.06569754331599845, 'max_depth': 12, 'alpha': 0.225, 'lambda': 30.502048430625738, 'max_bin': 405}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:13:12,334]\u001b[0m Trial 271 finished with value: 0.7240327983729546 and parameters: {'n_estimators': 364, 'eta': 0.07086168213814184, 'max_depth': 12, 'alpha': 0.2718, 'lambda': 27.6125088416573, 'max_bin': 363}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:13:21,796]\u001b[0m Trial 272 finished with value: 0.7221996405487365 and parameters: {'n_estimators': 395, 'eta': 0.07320037523366016, 'max_depth': 12, 'alpha': 0.3271, 'lambda': 22.864603952713153, 'max_bin': 391}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:13:33,653]\u001b[0m Trial 273 finished with value: 0.7215347949810521 and parameters: {'n_estimators': 447, 'eta': 0.06803319087712284, 'max_depth': 12, 'alpha': 0.24780000000000002, 'lambda': 28.89634512599765, 'max_bin': 408}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:13:44,230]\u001b[0m Trial 274 finished with value: 0.7202508595371409 and parameters: {'n_estimators': 420, 'eta': 0.07660544222574496, 'max_depth': 12, 'alpha': 0.21130000000000002, 'lambda': 25.645031312979658, 'max_bin': 387}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:13:54,838]\u001b[0m Trial 275 finished with value: 0.7195234884124024 and parameters: {'n_estimators': 373, 'eta': 0.07101694910086893, 'max_depth': 12, 'alpha': 0.2667, 'lambda': 26.47637615491452, 'max_bin': 402}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:14:05,428]\u001b[0m Trial 276 finished with value: 0.7167205052694149 and parameters: {'n_estimators': 388, 'eta': 0.06696363913525366, 'max_depth': 12, 'alpha': 0.3012, 'lambda': 28.88345383563716, 'max_bin': 395}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:14:17,752]\u001b[0m Trial 277 finished with value: 0.7208355338110198 and parameters: {'n_estimators': 403, 'eta': 0.05909596029147199, 'max_depth': 12, 'alpha': 0.24580000000000002, 'lambda': 28.012297473686743, 'max_bin': 410}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:14:28,400]\u001b[0m Trial 278 finished with value: 0.7236595308454941 and parameters: {'n_estimators': 434, 'eta': 0.07389565304717788, 'max_depth': 12, 'alpha': 0.3472, 'lambda': 24.069732540540976, 'max_bin': 412}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 08:14:38,900]\u001b[0m Trial 279 finished with value: 0.7210801652727612 and parameters: {'n_estimators': 359, 'eta': 0.06933766463067936, 'max_depth': 12, 'alpha': 0.2843, 'lambda': 27.3650484488378, 'max_bin': 405}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:14:49,546]\u001b[0m Trial 280 finished with value: 0.7201861454795335 and parameters: {'n_estimators': 423, 'eta': 0.06538037986008156, 'max_depth': 12, 'alpha': 0.19210000000000002, 'lambda': 24.98421580549584, 'max_bin': 397}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:15:01,257]\u001b[0m Trial 281 finished with value: 0.7208889825370497 and parameters: {'n_estimators': 379, 'eta': 0.06325420017886095, 'max_depth': 12, 'alpha': 0.32080000000000003, 'lambda': 30.75710166086431, 'max_bin': 375}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:15:13,923]\u001b[0m Trial 282 finished with value: 0.7216592653015349 and parameters: {'n_estimators': 399, 'eta': 0.05583080057259436, 'max_depth': 12, 'alpha': 0.2622, 'lambda': 28.68041719362478, 'max_bin': 384}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:15:24,985]\u001b[0m Trial 283 finished with value: 0.721526362099044 and parameters: {'n_estimators': 412, 'eta': 0.07152585936945827, 'max_depth': 12, 'alpha': 0.22990000000000002, 'lambda': 26.397860758392092, 'max_bin': 392}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:15:36,885]\u001b[0m Trial 284 finished with value: 0.722089550374312 and parameters: {'n_estimators': 445, 'eta': 0.06883258425395453, 'max_depth': 12, 'alpha': 0.29350000000000004, 'lambda': 27.746610564981534, 'max_bin': 412}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:15:47,866]\u001b[0m Trial 285 finished with value: 0.7228559116451239 and parameters: {'n_estimators': 382, 'eta': 0.07494167951234668, 'max_depth': 12, 'alpha': 0.3791, 'lambda': 25.471153866111372, 'max_bin': 400}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:15:59,966]\u001b[0m Trial 286 finished with value: 0.719655385830531 and parameters: {'n_estimators': 415, 'eta': 0.06590060120637005, 'max_depth': 12, 'alpha': 0.24680000000000002, 'lambda': 29.980048074336892, 'max_bin': 407}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:16:10,901]\u001b[0m Trial 287 finished with value: 0.7218585232854212 and parameters: {'n_estimators': 392, 'eta': 0.07314179514432192, 'max_depth': 12, 'alpha': 0.3111, 'lambda': 27.057580346464604, 'max_bin': 388}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:16:21,745]\u001b[0m Trial 288 finished with value: 0.7199414436678178 and parameters: {'n_estimators': 354, 'eta': 0.06777113926293135, 'max_depth': 12, 'alpha': 0.2697, 'lambda': 29.226757657919993, 'max_bin': 379}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:16:32,618]\u001b[0m Trial 289 finished with value: 0.7194188429999142 and parameters: {'n_estimators': 372, 'eta': 0.07028859386121108, 'max_depth': 12, 'alpha': 0.22360000000000002, 'lambda': 28.294094119875446, 'max_bin': 412}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:16:36,228]\u001b[0m Trial 290 finished with value: 0.6966530300579767 and parameters: {'n_estimators': 93, 'eta': 0.0617710354928825, 'max_depth': 12, 'alpha': 0.2852, 'lambda': 26.404728311699827, 'max_bin': 403}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:16:47,137]\u001b[0m Trial 291 finished with value: 0.720209650162257 and parameters: {'n_estimators': 400, 'eta': 0.06501705750349751, 'max_depth': 12, 'alpha': 0.2519, 'lambda': 29.0169782466177, 'max_bin': 395}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:16:58,216]\u001b[0m Trial 292 finished with value: 0.7216227840414265 and parameters: {'n_estimators': 432, 'eta': 0.07644481784550958, 'max_depth': 12, 'alpha': 0.20450000000000002, 'lambda': 30.068152800102002, 'max_bin': 407}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:17:08,813]\u001b[0m Trial 293 finished with value: 0.7233624185188903 and parameters: {'n_estimators': 458, 'eta': 0.07211206614234564, 'max_depth': 12, 'alpha': 0.3321, 'lambda': 25.750594008220705, 'max_bin': 385}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:17:20,081]\u001b[0m Trial 294 finished with value: 0.7246527969139385 and parameters: {'n_estimators': 411, 'eta': 0.06887299291681835, 'max_depth': 12, 'alpha': 0.3047, 'lambda': 26.96737511809668, 'max_bin': 412}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:17:30,728]\u001b[0m Trial 295 finished with value: 0.7209854498242934 and parameters: {'n_estimators': 379, 'eta': 0.06704370285655813, 'max_depth': 12, 'alpha': 0.17830000000000001, 'lambda': 28.06370152689703, 'max_bin': 401}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:17:41,647]\u001b[0m Trial 296 finished with value: 0.7183226362602355 and parameters: {'n_estimators': 395, 'eta': 0.0704426766666988, 'max_depth': 12, 'alpha': 0.2687, 'lambda': 30.623146786030482, 'max_bin': 389}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:17:52,376]\u001b[0m Trial 297 finished with value: 0.7231127562640944 and parameters: {'n_estimators': 418, 'eta': 0.07345921916652791, 'max_depth': 12, 'alpha': 0.2369, 'lambda': 27.450838296801265, 'max_bin': 413}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:18:02,513]\u001b[0m Trial 298 finished with value: 0.7197327865301636 and parameters: {'n_estimators': 358, 'eta': 0.06331765323042868, 'max_depth': 12, 'alpha': 0.27990000000000004, 'lambda': 24.579715626880198, 'max_bin': 394}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:18:15,103]\u001b[0m Trial 299 finished with value: 0.7219657664310757 and parameters: {'n_estimators': 430, 'eta': 0.05978480381945839, 'max_depth': 12, 'alpha': 0.29610000000000003, 'lambda': 28.69278824767345, 'max_bin': 406}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7455\n",
      "\tBest params:\n",
      "\t\tn_estimators: 408\n",
      "\t\teta: 0.08777846500101312\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.11\n",
      "\t\tlambda: 31.855553059350896\n",
      "\t\tmax_bin: 422\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.743638    0.730201    0.745800    0.748070   \n",
      "1                    TP   43.000000   43.000000   35.000000   36.000000   \n",
      "2                    TN  305.000000  305.000000  299.000000  300.000000   \n",
      "3                    FP    8.000000    9.000000   15.000000   15.000000   \n",
      "4                    FN   26.000000   25.000000   33.000000   31.000000   \n",
      "5              Accuracy    0.910995    0.910995    0.874346    0.879581   \n",
      "6             Precision    0.843137    0.826923    0.700000    0.705882   \n",
      "7           Sensitivity    0.623188    0.632353    0.514706    0.537313   \n",
      "8           Specificity    0.974400    0.971300    0.952200    0.952400   \n",
      "9              F1 score    0.716667    0.716667    0.593220    0.610169   \n",
      "10  F1 score (weighted)    0.905563    0.906167    0.866512    0.872908   \n",
      "11     F1 score (macro)    0.831936    0.831936    0.759458    0.769481   \n",
      "12    Balanced Accuracy    0.798815    0.801845    0.733468    0.744847   \n",
      "13                  MCC    0.675973    0.673403    0.529568    0.547544   \n",
      "14                  NPV    0.921500    0.924200    0.900600    0.906300   \n",
      "15              ROC_AUC    0.798815    0.801845    0.733468    0.744847   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.677579    0.714666  \n",
      "1    37.000000   40.000000  \n",
      "2   301.000000  301.000000  \n",
      "3    14.000000   11.000000  \n",
      "4    30.000000   30.000000  \n",
      "5     0.884817    0.892670  \n",
      "6     0.725490    0.784314  \n",
      "7     0.552239    0.571429  \n",
      "8     0.955600    0.964700  \n",
      "9     0.627119    0.661157  \n",
      "10    0.878434    0.885829  \n",
      "11    0.779504    0.798697  \n",
      "12    0.753897    0.768086  \n",
      "13    0.567782    0.609862  \n",
      "14    0.909400    0.909400  \n",
      "15    0.753897    0.768086  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_xgb_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_5_cat = np.where(((y_pred_xgb_5 >= 2) | (y_pred_xgb_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 08:18:28,594]\u001b[0m Trial 300 finished with value: 0.7151087338799724 and parameters: {'n_estimators': 397, 'eta': 0.06698171064069713, 'max_depth': 12, 'alpha': 0.2558, 'lambda': 29.48726107807855, 'max_bin': 400}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:18:40,556]\u001b[0m Trial 301 finished with value: 0.7199529950537316 and parameters: {'n_estimators': 375, 'eta': 0.07501881073584357, 'max_depth': 12, 'alpha': 0.31970000000000004, 'lambda': 25.988131557216487, 'max_bin': 413}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:18:51,910]\u001b[0m Trial 302 finished with value: 0.7193977184525979 and parameters: {'n_estimators': 414, 'eta': 0.0712932965457246, 'max_depth': 12, 'alpha': 0.3553, 'lambda': 26.918978446853675, 'max_bin': 382}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:19:01,443]\u001b[0m Trial 303 finished with value: 0.7182892549105955 and parameters: {'n_estimators': 444, 'eta': 0.0645886108068791, 'max_depth': 8, 'alpha': 0.2099, 'lambda': 25.239334028072367, 'max_bin': 424}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:19:07,871]\u001b[0m Trial 304 finished with value: 0.7065886300353911 and parameters: {'n_estimators': 394, 'eta': 0.06826602166448675, 'max_depth': 6, 'alpha': 0.2356, 'lambda': 27.963332533817127, 'max_bin': 398}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:19:19,337]\u001b[0m Trial 305 finished with value: 0.7166690494987783 and parameters: {'n_estimators': 357, 'eta': 0.06974722525609382, 'max_depth': 12, 'alpha': 0.2822, 'lambda': 30.44953265767033, 'max_bin': 368}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:19:31,088]\u001b[0m Trial 306 finished with value: 0.7176120471850829 and parameters: {'n_estimators': 380, 'eta': 0.07264069966695236, 'max_depth': 12, 'alpha': 0.2629, 'lambda': 29.364116134710986, 'max_bin': 408}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:19:42,137]\u001b[0m Trial 307 finished with value: 0.7184833801236734 and parameters: {'n_estimators': 425, 'eta': 0.07723842086785021, 'max_depth': 12, 'alpha': 0.3074, 'lambda': 26.19292793022421, 'max_bin': 391}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:19:54,509]\u001b[0m Trial 308 finished with value: 0.7184203954914914 and parameters: {'n_estimators': 406, 'eta': 0.061798061735428636, 'max_depth': 12, 'alpha': 0.2199, 'lambda': 28.41022687303572, 'max_bin': 404}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:20:07,094]\u001b[0m Trial 309 finished with value: 0.7196689853357068 and parameters: {'n_estimators': 466, 'eta': 0.0662602876081321, 'max_depth': 12, 'alpha': 0.2464, 'lambda': 27.39891423901308, 'max_bin': 411}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:20:18,719]\u001b[0m Trial 310 finished with value: 0.7162683482629043 and parameters: {'n_estimators': 390, 'eta': 0.06979396359933888, 'max_depth': 12, 'alpha': 0.3972, 'lambda': 26.611722464040707, 'max_bin': 422}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:20:29,891]\u001b[0m Trial 311 finished with value: 0.7177169335887575 and parameters: {'n_estimators': 367, 'eta': 0.07393068576294282, 'max_depth': 12, 'alpha': 0.2869, 'lambda': 29.79499503664838, 'max_bin': 380}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:20:41,768]\u001b[0m Trial 312 finished with value: 0.7180956267113172 and parameters: {'n_estimators': 431, 'eta': 0.06422568185143467, 'max_depth': 12, 'alpha': 0.33130000000000004, 'lambda': 25.422682269265287, 'max_bin': 387}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:20:54,200]\u001b[0m Trial 313 finished with value: 0.7204778121071873 and parameters: {'n_estimators': 410, 'eta': 0.06786186185858425, 'max_depth': 12, 'alpha': 0.19190000000000002, 'lambda': 23.59514962438746, 'max_bin': 395}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:21:04,621]\u001b[0m Trial 314 finished with value: 0.7170956960212624 and parameters: {'n_estimators': 383, 'eta': 0.07217359227104266, 'max_depth': 12, 'alpha': 0.2707, 'lambda': 21.780681708944403, 'max_bin': 414}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:21:16,379]\u001b[0m Trial 315 finished with value: 0.7143445275125744 and parameters: {'n_estimators': 400, 'eta': 0.0657530392185258, 'max_depth': 12, 'alpha': 0.2979, 'lambda': 28.026139997812173, 'max_bin': 422}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:21:28,947]\u001b[0m Trial 316 finished with value: 0.7162183497853378 and parameters: {'n_estimators': 452, 'eta': 0.07539523697704667, 'max_depth': 12, 'alpha': 0.24130000000000001, 'lambda': 29.021443952913852, 'max_bin': 403}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:21:41,153]\u001b[0m Trial 317 finished with value: 0.7172949928826854 and parameters: {'n_estimators': 414, 'eta': 0.06974730009979642, 'max_depth': 12, 'alpha': 0.25980000000000003, 'lambda': 30.7863891985695, 'max_bin': 408}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:21:51,852]\u001b[0m Trial 318 finished with value: 0.7186343299228586 and parameters: {'n_estimators': 364, 'eta': 0.07118173250069473, 'max_depth': 12, 'alpha': 0.22010000000000002, 'lambda': 24.35832438780443, 'max_bin': 392}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:22:05,520]\u001b[0m Trial 319 finished with value: 0.7188002313893135 and parameters: {'n_estimators': 435, 'eta': 0.05381991950349857, 'max_depth': 12, 'alpha': 0.4224, 'lambda': 27.383625018324338, 'max_bin': 377}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:22:17,101]\u001b[0m Trial 320 finished with value: 0.7151889327112965 and parameters: {'n_estimators': 393, 'eta': 0.06765192069791441, 'max_depth': 12, 'alpha': 0.1694, 'lambda': 26.507164379170657, 'max_bin': 415}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:22:28,171]\u001b[0m Trial 321 finished with value: 0.7153039705944039 and parameters: {'n_estimators': 375, 'eta': 0.07750592384281377, 'max_depth': 12, 'alpha': 0.31070000000000003, 'lambda': 28.786626256733122, 'max_bin': 400}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:22:39,697]\u001b[0m Trial 322 finished with value: 0.7201021547916722 and parameters: {'n_estimators': 351, 'eta': 0.0635641370203956, 'max_depth': 12, 'alpha': 0.2751, 'lambda': 25.777761970685837, 'max_bin': 421}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:22:51,804]\u001b[0m Trial 323 finished with value: 0.7168284911823111 and parameters: {'n_estimators': 419, 'eta': 0.07351190983521548, 'max_depth': 12, 'alpha': 0.2909, 'lambda': 29.797317956472845, 'max_bin': 386}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:23:04,585]\u001b[0m Trial 324 finished with value: 0.716363410176811 and parameters: {'n_estimators': 401, 'eta': 0.05847918371019474, 'max_depth': 12, 'alpha': 0.3377, 'lambda': 28.026364989666522, 'max_bin': 414}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:23:16,501]\u001b[0m Trial 325 finished with value: 0.7195428215708906 and parameters: {'n_estimators': 382, 'eta': 0.06867095324254642, 'max_depth': 12, 'alpha': 0.25070000000000003, 'lambda': 27.029780597248564, 'max_bin': 409}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:23:29,490]\u001b[0m Trial 326 finished with value: 0.7193651398064268 and parameters: {'n_estimators': 444, 'eta': 0.0712492380508121, 'max_depth': 12, 'alpha': 0.2325, 'lambda': 24.9952699873407, 'max_bin': 396}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:23:41,936]\u001b[0m Trial 327 finished with value: 0.7198768370764504 and parameters: {'n_estimators': 419, 'eta': 0.065713099733178, 'max_depth': 12, 'alpha': 0.2721, 'lambda': 28.616031372012834, 'max_bin': 373}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:23:54,354]\u001b[0m Trial 328 finished with value: 0.7134451309372274 and parameters: {'n_estimators': 399, 'eta': 0.06116630254779124, 'max_depth': 12, 'alpha': 0.1988, 'lambda': 30.753244876762835, 'max_bin': 403}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 08:24:06,255]\u001b[0m Trial 329 finished with value: 0.7172794961430121 and parameters: {'n_estimators': 368, 'eta': 0.06704907139082043, 'max_depth': 12, 'alpha': 0.29650000000000004, 'lambda': 29.33430643593273, 'max_bin': 423}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:24:16,803]\u001b[0m Trial 330 finished with value: 0.7173995075314864 and parameters: {'n_estimators': 429, 'eta': 0.07197082233383048, 'max_depth': 12, 'alpha': 0.31570000000000004, 'lambda': 18.000658288639244, 'max_bin': 416}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:24:27,371]\u001b[0m Trial 331 finished with value: 0.7161075796509108 and parameters: {'n_estimators': 386, 'eta': 0.07470991911902217, 'max_depth': 12, 'alpha': 0.2619, 'lambda': 27.53436822049579, 'max_bin': 407}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:24:42,218]\u001b[0m Trial 332 finished with value: 0.7099039870997798 and parameters: {'n_estimators': 409, 'eta': 0.024012968715142444, 'max_depth': 12, 'alpha': 0.23170000000000002, 'lambda': 26.001469444600257, 'max_bin': 392}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:24:52,929]\u001b[0m Trial 333 finished with value: 0.7161764600044866 and parameters: {'n_estimators': 356, 'eta': 0.06910613642820815, 'max_depth': 12, 'alpha': 0.2876, 'lambda': 30.214784868849527, 'max_bin': 381}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:25:05,114]\u001b[0m Trial 334 finished with value: 0.7187554387691415 and parameters: {'n_estimators': 399, 'eta': 0.06276816759254973, 'max_depth': 12, 'alpha': 0.24930000000000002, 'lambda': 28.065848666359692, 'max_bin': 399}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:25:18,013]\u001b[0m Trial 335 finished with value: 0.7157081666152522 and parameters: {'n_estimators': 456, 'eta': 0.0646263152992506, 'max_depth': 12, 'alpha': 0.2162, 'lambda': 26.909110541392078, 'max_bin': 411}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:25:30,072]\u001b[0m Trial 336 finished with value: 0.7155109472989908 and parameters: {'n_estimators': 381, 'eta': 0.06969283474589393, 'max_depth': 12, 'alpha': 0.3612, 'lambda': 31.24420753288198, 'max_bin': 387}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:25:42,300]\u001b[0m Trial 337 finished with value: 0.716853223211353 and parameters: {'n_estimators': 419, 'eta': 0.0777163460945136, 'max_depth': 12, 'alpha': 0.3199, 'lambda': 29.32041939965042, 'max_bin': 417}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:25:54,034]\u001b[0m Trial 338 finished with value: 0.7179107338601417 and parameters: {'n_estimators': 406, 'eta': 0.06677309732670726, 'max_depth': 12, 'alpha': 0.27490000000000003, 'lambda': 26.252328895155756, 'max_bin': 405}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:26:06,219]\u001b[0m Trial 339 finished with value: 0.7152000400676604 and parameters: {'n_estimators': 443, 'eta': 0.07332722347349409, 'max_depth': 12, 'alpha': 0.2554, 'lambda': 25.248110222239553, 'max_bin': 421}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:26:17,418]\u001b[0m Trial 340 finished with value: 0.7166038149713929 and parameters: {'n_estimators': 347, 'eta': 0.0710788210289425, 'max_depth': 12, 'alpha': 0.2978, 'lambda': 28.65732894507035, 'max_bin': 397}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:26:28,628]\u001b[0m Trial 341 finished with value: 0.7169239459706199 and parameters: {'n_estimators': 371, 'eta': 0.07566685523952037, 'max_depth': 12, 'alpha': 0.1809, 'lambda': 27.445125069582613, 'max_bin': 425}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:26:42,828]\u001b[0m Trial 342 finished with value: 0.7169221450587331 and parameters: {'n_estimators': 429, 'eta': 0.051183111477786924, 'max_depth': 12, 'alpha': 0.33580000000000004, 'lambda': 30.11243031875136, 'max_bin': 413}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:26:54,232]\u001b[0m Trial 343 finished with value: 0.7182492422133111 and parameters: {'n_estimators': 387, 'eta': 0.06496346611145087, 'max_depth': 12, 'alpha': 0.27540000000000003, 'lambda': 26.765368096464098, 'max_bin': 391}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:27:06,537]\u001b[0m Trial 344 finished with value: 0.7207905232443953 and parameters: {'n_estimators': 411, 'eta': 0.06812936219474515, 'max_depth': 12, 'alpha': 0.2386, 'lambda': 28.47890899070035, 'max_bin': 384}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:27:18,479]\u001b[0m Trial 345 finished with value: 0.7203052082418266 and parameters: {'n_estimators': 392, 'eta': 0.06230654032035896, 'max_depth': 12, 'alpha': 0.2043, 'lambda': 24.109124143397054, 'max_bin': 403}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:27:29,814]\u001b[0m Trial 346 finished with value: 0.7195647458198046 and parameters: {'n_estimators': 370, 'eta': 0.07044881611928873, 'max_depth': 12, 'alpha': 0.308, 'lambda': 25.695129891216958, 'max_bin': 410}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:27:42,421]\u001b[0m Trial 347 finished with value: 0.714741081845305 and parameters: {'n_estimators': 432, 'eta': 0.07323214951817175, 'max_depth': 12, 'alpha': 0.2832, 'lambda': 27.855883966925987, 'max_bin': 365}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:27:53,454]\u001b[0m Trial 348 finished with value: 0.7147190913317087 and parameters: {'n_estimators': 401, 'eta': 0.06686999386055398, 'max_depth': 12, 'alpha': 0.25720000000000004, 'lambda': 23.16928787758259, 'max_bin': 418}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:28:06,258]\u001b[0m Trial 349 finished with value: 0.7152978029988291 and parameters: {'n_estimators': 417, 'eta': 0.06011896117897779, 'max_depth': 12, 'alpha': 0.21880000000000002, 'lambda': 29.40565929093753, 'max_bin': 377}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7455\n",
      "\tBest params:\n",
      "\t\tn_estimators: 408\n",
      "\t\teta: 0.08777846500101312\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.11\n",
      "\t\tlambda: 31.855553059350896\n",
      "\t\tmax_bin: 422\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.743638    0.730201    0.745800    0.748070   \n",
      "1                    TP   43.000000   43.000000   35.000000   36.000000   \n",
      "2                    TN  305.000000  305.000000  299.000000  300.000000   \n",
      "3                    FP    8.000000    9.000000   15.000000   15.000000   \n",
      "4                    FN   26.000000   25.000000   33.000000   31.000000   \n",
      "5              Accuracy    0.910995    0.910995    0.874346    0.879581   \n",
      "6             Precision    0.843137    0.826923    0.700000    0.705882   \n",
      "7           Sensitivity    0.623188    0.632353    0.514706    0.537313   \n",
      "8           Specificity    0.974400    0.971300    0.952200    0.952400   \n",
      "9              F1 score    0.716667    0.716667    0.593220    0.610169   \n",
      "10  F1 score (weighted)    0.905563    0.906167    0.866512    0.872908   \n",
      "11     F1 score (macro)    0.831936    0.831936    0.759458    0.769481   \n",
      "12    Balanced Accuracy    0.798815    0.801845    0.733468    0.744847   \n",
      "13                  MCC    0.675973    0.673403    0.529568    0.547544   \n",
      "14                  NPV    0.921500    0.924200    0.900600    0.906300   \n",
      "15              ROC_AUC    0.798815    0.801845    0.733468    0.744847   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.677579    0.714666    0.729740  \n",
      "1    37.000000   40.000000   37.000000  \n",
      "2   301.000000  301.000000  303.000000  \n",
      "3    14.000000   11.000000   12.000000  \n",
      "4    30.000000   30.000000   30.000000  \n",
      "5     0.884817    0.892670    0.890052  \n",
      "6     0.725490    0.784314    0.755102  \n",
      "7     0.552239    0.571429    0.552239  \n",
      "8     0.955600    0.964700    0.961900  \n",
      "9     0.627119    0.661157    0.637931  \n",
      "10    0.878434    0.885829    0.883049  \n",
      "11    0.779504    0.798697    0.786558  \n",
      "12    0.753897    0.768086    0.757072  \n",
      "13    0.567782    0.609862    0.584732  \n",
      "14    0.909400    0.909400    0.909900  \n",
      "15    0.753897    0.768086    0.757072  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_xgb_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_6_cat = np.where(((y_pred_xgb_6 >= 2) | (y_pred_xgb_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 08:28:19,107]\u001b[0m Trial 350 finished with value: 0.726485770011792 and parameters: {'n_estimators': 385, 'eta': 0.06904974743083539, 'max_depth': 12, 'alpha': 0.23750000000000002, 'lambda': 24.63939595479541, 'max_bin': 358}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:28:30,109]\u001b[0m Trial 351 finished with value: 0.7252667518259528 and parameters: {'n_estimators': 356, 'eta': 0.07212546928945315, 'max_depth': 12, 'alpha': 0.3229, 'lambda': 26.20936461727509, 'max_bin': 426}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:28:42,017]\u001b[0m Trial 352 finished with value: 0.7231845844724702 and parameters: {'n_estimators': 398, 'eta': 0.07492071073292766, 'max_depth': 12, 'alpha': 0.2713, 'lambda': 27.1930406626295, 'max_bin': 255}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:28:55,134]\u001b[0m Trial 353 finished with value: 0.7252597709529376 and parameters: {'n_estimators': 469, 'eta': 0.06529038136092528, 'max_depth': 12, 'alpha': 0.3009, 'lambda': 28.975607125427704, 'max_bin': 399}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:29:07,595]\u001b[0m Trial 354 finished with value: 0.7230677423868632 and parameters: {'n_estimators': 439, 'eta': 0.06842127150339668, 'max_depth': 12, 'alpha': 0.34850000000000003, 'lambda': 29.898787399162806, 'max_bin': 408}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:29:18,584]\u001b[0m Trial 355 finished with value: 0.7222402865190902 and parameters: {'n_estimators': 373, 'eta': 0.07056611857444475, 'max_depth': 12, 'alpha': 0.532, 'lambda': 31.3533841829258, 'max_bin': 414}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:29:31,788]\u001b[0m Trial 356 finished with value: 0.7248533728505604 and parameters: {'n_estimators': 419, 'eta': 0.06301090784251497, 'max_depth': 12, 'alpha': 0.2554, 'lambda': 28.460822128735238, 'max_bin': 300}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:29:43,978]\u001b[0m Trial 357 finished with value: 0.7236321032334054 and parameters: {'n_estimators': 402, 'eta': 0.06692218041625675, 'max_depth': 12, 'alpha': 0.2897, 'lambda': 27.714381574111112, 'max_bin': 393}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:29:54,154]\u001b[0m Trial 358 finished with value: 0.7217146983422896 and parameters: {'n_estimators': 382, 'eta': 0.0768339612203337, 'max_depth': 12, 'alpha': 0.2245, 'lambda': 26.61619948573171, 'max_bin': 420}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:30:04,853]\u001b[0m Trial 359 finished with value: 0.725399800161566 and parameters: {'n_estimators': 347, 'eta': 0.07239278838905784, 'max_depth': 12, 'alpha': 0.2679, 'lambda': 25.59820591718509, 'max_bin': 404}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:30:17,554]\u001b[0m Trial 360 finished with value: 0.7248822292342705 and parameters: {'n_estimators': 425, 'eta': 0.06468035766417662, 'max_depth': 12, 'alpha': 0.1976, 'lambda': 29.234377677698603, 'max_bin': 415}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:30:25,378]\u001b[0m Trial 361 finished with value: 0.7199976673778764 and parameters: {'n_estimators': 407, 'eta': 0.06919476365531181, 'max_depth': 7, 'alpha': 0.3144, 'lambda': 30.256800771853406, 'max_bin': 386}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:30:36,802]\u001b[0m Trial 362 finished with value: 0.7244044258083513 and parameters: {'n_estimators': 390, 'eta': 0.07383132077451898, 'max_depth': 12, 'alpha': 0.2434, 'lambda': 27.851427839786467, 'max_bin': 397}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:30:49,197]\u001b[0m Trial 363 finished with value: 0.7248454366111317 and parameters: {'n_estimators': 451, 'eta': 0.07100171609335258, 'max_depth': 12, 'alpha': 0.1598, 'lambda': 26.885231904419495, 'max_bin': 409}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:31:00,783]\u001b[0m Trial 364 finished with value: 0.7258990691033227 and parameters: {'n_estimators': 373, 'eta': 0.06656880235113116, 'max_depth': 12, 'alpha': 0.2843, 'lambda': 28.481053332684752, 'max_bin': 420}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:31:12,870]\u001b[0m Trial 365 finished with value: 0.7250552418531775 and parameters: {'n_estimators': 418, 'eta': 0.06830126061856351, 'max_depth': 12, 'alpha': 0.2612, 'lambda': 24.965960569296485, 'max_bin': 381}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:31:22,307]\u001b[0m Trial 366 finished with value: 0.720324032477876 and parameters: {'n_estimators': 362, 'eta': 0.0570122904023813, 'max_depth': 12, 'alpha': 0.2285, 'lambda': 4.932534694446485, 'max_bin': 390}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:31:31,292]\u001b[0m Trial 367 finished with value: 0.7223021214565464 and parameters: {'n_estimators': 395, 'eta': 0.07565307207284161, 'max_depth': 12, 'alpha': 0.3335, 'lambda': 11.308032688829215, 'max_bin': 427}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:31:45,826]\u001b[0m Trial 368 finished with value: 0.7232994273128852 and parameters: {'n_estimators': 435, 'eta': 0.0429435465253971, 'max_depth': 12, 'alpha': 0.3023, 'lambda': 30.543515815000863, 'max_bin': 404}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:31:57,684]\u001b[0m Trial 369 finished with value: 0.7259186472023885 and parameters: {'n_estimators': 410, 'eta': 0.06249181446920546, 'max_depth': 12, 'alpha': 0.4867, 'lambda': 26.291409988507045, 'max_bin': 411}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:32:09,085]\u001b[0m Trial 370 finished with value: 0.7229879436121276 and parameters: {'n_estimators': 391, 'eta': 0.07209711969665442, 'max_depth': 12, 'alpha': 0.2818, 'lambda': 27.25768550344293, 'max_bin': 399}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:32:24,550]\u001b[0m Trial 371 finished with value: 0.7281393466052698 and parameters: {'n_estimators': 649, 'eta': 0.06578078706399758, 'max_depth': 12, 'alpha': 0.2472, 'lambda': 29.650796221398604, 'max_bin': 416}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:32:36,000]\u001b[0m Trial 372 finished with value: 0.7231670796629172 and parameters: {'n_estimators': 371, 'eta': 0.06980945521266418, 'max_depth': 12, 'alpha': 0.2079, 'lambda': 28.093571291128494, 'max_bin': 376}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:32:48,264]\u001b[0m Trial 373 finished with value: 0.7237516715466457 and parameters: {'n_estimators': 422, 'eta': 0.06445917673094137, 'max_depth': 12, 'alpha': 0.3703, 'lambda': 28.94761286985215, 'max_bin': 421}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:33:02,562]\u001b[0m Trial 374 finished with value: 0.7218923195906106 and parameters: {'n_estimators': 439, 'eta': 0.04667282227835662, 'max_depth': 12, 'alpha': 0.26890000000000003, 'lambda': 25.466481148784684, 'max_bin': 392}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:33:11,459]\u001b[0m Trial 375 finished with value: 0.721609540286362 and parameters: {'n_estimators': 386, 'eta': 0.06002716032885183, 'max_depth': 12, 'alpha': 0.1825, 'lambda': 8.295476947455985, 'max_bin': 406}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:33:22,430]\u001b[0m Trial 376 finished with value: 0.7203256230892545 and parameters: {'n_estimators': 404, 'eta': 0.07424482525943431, 'max_depth': 12, 'alpha': 0.3125, 'lambda': 26.33458711762358, 'max_bin': 386}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:33:35,883]\u001b[0m Trial 377 finished with value: 0.587273936346522 and parameters: {'n_estimators': 341, 'eta': 0.005529915551846891, 'max_depth': 12, 'alpha': 0.29250000000000004, 'lambda': 19.667366768126843, 'max_bin': 413}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:33:47,632]\u001b[0m Trial 378 finished with value: 0.7216249387562247 and parameters: {'n_estimators': 359, 'eta': 0.06744387600915545, 'max_depth': 12, 'alpha': 0.2441, 'lambda': 31.863297971360463, 'max_bin': 402}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 08:33:59,433]\u001b[0m Trial 379 finished with value: 0.7210820049893865 and parameters: {'n_estimators': 462, 'eta': 0.0794489253997296, 'max_depth': 12, 'alpha': 0.2647, 'lambda': 31.126287223730227, 'max_bin': 424}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:34:11,673]\u001b[0m Trial 380 finished with value: 0.7243267659662564 and parameters: {'n_estimators': 409, 'eta': 0.07065166185132313, 'max_depth': 12, 'alpha': 0.2286, 'lambda': 27.393795245215113, 'max_bin': 370}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:34:16,878]\u001b[0m Trial 381 finished with value: 0.7024268997538027 and parameters: {'n_estimators': 382, 'eta': 0.07306342516826443, 'max_depth': 5, 'alpha': 0.3252, 'lambda': 29.562983369908945, 'max_bin': 395}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:34:28,226]\u001b[0m Trial 382 finished with value: 0.7243232385687222 and parameters: {'n_estimators': 425, 'eta': 0.07758411104451071, 'max_depth': 12, 'alpha': 0.281, 'lambda': 24.579922294779227, 'max_bin': 410}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:34:41,909]\u001b[0m Trial 383 finished with value: 0.7229934169809797 and parameters: {'n_estimators': 396, 'eta': 0.03551458871889882, 'max_depth': 12, 'alpha': 0.3014, 'lambda': 28.264971036086727, 'max_bin': 417}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:34:53,291]\u001b[0m Trial 384 finished with value: 0.7258867969965076 and parameters: {'n_estimators': 377, 'eta': 0.06904277629077697, 'max_depth': 12, 'alpha': 0.2526, 'lambda': 25.832740571662754, 'max_bin': 383}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:35:06,282]\u001b[0m Trial 385 finished with value: 0.7243192281615316 and parameters: {'n_estimators': 439, 'eta': 0.06610015089674817, 'max_depth': 12, 'alpha': 0.3456, 'lambda': 26.880353339550165, 'max_bin': 389}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:35:18,702]\u001b[0m Trial 386 finished with value: 0.7233079592184727 and parameters: {'n_estimators': 413, 'eta': 0.06346858804120988, 'max_depth': 12, 'alpha': 0.218, 'lambda': 28.877029479776688, 'max_bin': 407}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:35:29,980]\u001b[0m Trial 387 finished with value: 0.7218971430110623 and parameters: {'n_estimators': 363, 'eta': 0.07142001983553287, 'max_depth': 12, 'alpha': 0.2768, 'lambda': 30.18017200910718, 'max_bin': 420}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:35:42,166]\u001b[0m Trial 388 finished with value: 0.724242399316029 and parameters: {'n_estimators': 401, 'eta': 0.06147816556596562, 'max_depth': 12, 'alpha': 0.2371, 'lambda': 27.699582582963266, 'max_bin': 400}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:35:53,009]\u001b[0m Trial 389 finished with value: 0.7221441910061122 and parameters: {'n_estimators': 384, 'eta': 0.0678248093772706, 'max_depth': 12, 'alpha': 0.2937, 'lambda': 25.28328774550158, 'max_bin': 413}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:36:03,859]\u001b[0m Trial 390 finished with value: 0.7220173261951159 and parameters: {'n_estimators': 424, 'eta': 0.07543966336629607, 'max_depth': 12, 'alpha': 0.2626, 'lambda': 26.523024969220344, 'max_bin': 429}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:36:16,660]\u001b[0m Trial 391 finished with value: 0.7241252217461073 and parameters: {'n_estimators': 452, 'eta': 0.06958667219986639, 'max_depth': 12, 'alpha': 0.1592, 'lambda': 28.8193245357533, 'max_bin': 393}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:36:28,631]\u001b[0m Trial 392 finished with value: 0.7219329810672196 and parameters: {'n_estimators': 402, 'eta': 0.07259654990791649, 'max_depth': 12, 'alpha': 0.2096, 'lambda': 30.62861464677202, 'max_bin': 405}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:36:39,525]\u001b[0m Trial 393 finished with value: 0.7261863625437892 and parameters: {'n_estimators': 344, 'eta': 0.06518023366225181, 'max_depth': 12, 'alpha': 0.3139, 'lambda': 27.30224526201744, 'max_bin': 424}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:36:50,607]\u001b[0m Trial 394 finished with value: 0.7262541075550556 and parameters: {'n_estimators': 369, 'eta': 0.06736267453036185, 'max_depth': 12, 'alpha': 0.2526, 'lambda': 23.629622509444594, 'max_bin': 397}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:37:02,476]\u001b[0m Trial 395 finished with value: 0.7228017550779214 and parameters: {'n_estimators': 417, 'eta': 0.07095274222668534, 'max_depth': 12, 'alpha': 0.2821, 'lambda': 29.496442769996396, 'max_bin': 416}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:37:13,001]\u001b[0m Trial 396 finished with value: 0.7221167890794724 and parameters: {'n_estimators': 386, 'eta': 0.07364712519671303, 'max_depth': 12, 'alpha': 0.1961, 'lambda': 28.16173494355803, 'max_bin': 375}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:37:25,819]\u001b[0m Trial 397 finished with value: 0.7250679008862633 and parameters: {'n_estimators': 436, 'eta': 0.06373629126798655, 'max_depth': 12, 'alpha': 0.2349, 'lambda': 26.20211359270402, 'max_bin': 410}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:37:36,698]\u001b[0m Trial 398 finished with value: 0.7265831365962436 and parameters: {'n_estimators': 396, 'eta': 0.06901902600434606, 'max_depth': 12, 'alpha': 0.30720000000000003, 'lambda': 24.720980042015874, 'max_bin': 382}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:37:49,529]\u001b[0m Trial 399 finished with value: 0.7266878935469463 and parameters: {'n_estimators': 411, 'eta': 0.05929935936138512, 'max_depth': 12, 'alpha': 0.2627, 'lambda': 27.000642434493166, 'max_bin': 402}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7455\n",
      "\tBest params:\n",
      "\t\tn_estimators: 408\n",
      "\t\teta: 0.08777846500101312\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.11\n",
      "\t\tlambda: 31.855553059350896\n",
      "\t\tmax_bin: 422\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.743638    0.730201    0.745800    0.748070   \n",
      "1                    TP   43.000000   43.000000   35.000000   36.000000   \n",
      "2                    TN  305.000000  305.000000  299.000000  300.000000   \n",
      "3                    FP    8.000000    9.000000   15.000000   15.000000   \n",
      "4                    FN   26.000000   25.000000   33.000000   31.000000   \n",
      "5              Accuracy    0.910995    0.910995    0.874346    0.879581   \n",
      "6             Precision    0.843137    0.826923    0.700000    0.705882   \n",
      "7           Sensitivity    0.623188    0.632353    0.514706    0.537313   \n",
      "8           Specificity    0.974400    0.971300    0.952200    0.952400   \n",
      "9              F1 score    0.716667    0.716667    0.593220    0.610169   \n",
      "10  F1 score (weighted)    0.905563    0.906167    0.866512    0.872908   \n",
      "11     F1 score (macro)    0.831936    0.831936    0.759458    0.769481   \n",
      "12    Balanced Accuracy    0.798815    0.801845    0.733468    0.744847   \n",
      "13                  MCC    0.675973    0.673403    0.529568    0.547544   \n",
      "14                  NPV    0.921500    0.924200    0.900600    0.906300   \n",
      "15              ROC_AUC    0.798815    0.801845    0.733468    0.744847   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.677579    0.714666    0.729740    0.750225  \n",
      "1    37.000000   40.000000   37.000000   36.000000  \n",
      "2   301.000000  301.000000  303.000000  307.000000  \n",
      "3    14.000000   11.000000   12.000000    7.000000  \n",
      "4    30.000000   30.000000   30.000000   32.000000  \n",
      "5     0.884817    0.892670    0.890052    0.897906  \n",
      "6     0.725490    0.784314    0.755102    0.837209  \n",
      "7     0.552239    0.571429    0.552239    0.529412  \n",
      "8     0.955600    0.964700    0.961900    0.977700  \n",
      "9     0.627119    0.661157    0.637931    0.648649  \n",
      "10    0.878434    0.885829    0.883049    0.888363  \n",
      "11    0.779504    0.798697    0.786558    0.794462  \n",
      "12    0.753897    0.768086    0.757072    0.753559  \n",
      "13    0.567782    0.609862    0.584732    0.613755  \n",
      "14    0.909400    0.909400    0.909900    0.905600  \n",
      "15    0.753897    0.768086    0.757072    0.753559  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_xgb_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_7_cat = np.where(((y_pred_xgb_7 >= 2) | (y_pred_xgb_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 08:38:00,387]\u001b[0m Trial 400 finished with value: 0.7167709763516326 and parameters: {'n_estimators': 381, 'eta': 0.07677247604215687, 'max_depth': 12, 'alpha': 0.3254, 'lambda': 28.048126230625616, 'max_bin': 388}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:38:11,254]\u001b[0m Trial 401 finished with value: 0.7182557360105039 and parameters: {'n_estimators': 359, 'eta': 0.06633009870560744, 'max_depth': 12, 'alpha': 0.29050000000000004, 'lambda': 28.868106670296182, 'max_bin': 421}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:38:23,114]\u001b[0m Trial 402 finished with value: 0.71801511957984 and parameters: {'n_estimators': 765, 'eta': 0.07112227545713067, 'max_depth': 12, 'alpha': 0.1779, 'lambda': 29.95617851058406, 'max_bin': 413}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:38:33,990]\u001b[0m Trial 403 finished with value: 0.7200887908355151 and parameters: {'n_estimators': 423, 'eta': 0.06808814624304294, 'max_depth': 12, 'alpha': 0.45990000000000003, 'lambda': 25.73537300831617, 'max_bin': 395}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:38:44,395]\u001b[0m Trial 404 finished with value: 0.719486808581049 and parameters: {'n_estimators': 395, 'eta': 0.0786997546721052, 'max_depth': 12, 'alpha': 0.2715, 'lambda': 30.98418260517813, 'max_bin': 409}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:38:55,124]\u001b[0m Trial 405 finished with value: 0.716722081315407 and parameters: {'n_estimators': 444, 'eta': 0.07370731401965853, 'max_depth': 12, 'alpha': 0.22440000000000002, 'lambda': 27.591318119210385, 'max_bin': 418}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:39:05,917]\u001b[0m Trial 406 finished with value: 0.7191966516095337 and parameters: {'n_estimators': 371, 'eta': 0.06274666812496803, 'max_depth': 12, 'alpha': 0.24430000000000002, 'lambda': 22.438056841293978, 'max_bin': 426}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:39:16,955]\u001b[0m Trial 407 finished with value: 0.7191154107143537 and parameters: {'n_estimators': 411, 'eta': 0.06996106781909756, 'max_depth': 12, 'alpha': 0.3448, 'lambda': 29.24508387581775, 'max_bin': 402}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:39:30,330]\u001b[0m Trial 408 finished with value: 0.7167609409044855 and parameters: {'n_estimators': 392, 'eta': 0.029934112524203346, 'max_depth': 12, 'alpha': 0.30760000000000004, 'lambda': 26.684439081757816, 'max_bin': 407}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:39:41,814]\u001b[0m Trial 409 finished with value: 0.7187541446427884 and parameters: {'n_estimators': 426, 'eta': 0.06540529120203542, 'max_depth': 12, 'alpha': 0.28400000000000003, 'lambda': 25.21548406365933, 'max_bin': 390}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:39:51,327]\u001b[0m Trial 410 finished with value: 0.716518840105117 and parameters: {'n_estimators': 352, 'eta': 0.0753030265997932, 'max_depth': 12, 'alpha': 0.2545, 'lambda': 28.531720053595734, 'max_bin': 416}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:40:02,458]\u001b[0m Trial 411 finished with value: 0.7180274279070417 and parameters: {'n_estimators': 407, 'eta': 0.07186377694369996, 'max_depth': 12, 'alpha': 0.21810000000000002, 'lambda': 30.071992810767725, 'max_bin': 380}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:40:10,857]\u001b[0m Trial 412 finished with value: 0.7176382822554568 and parameters: {'n_estimators': 462, 'eta': 0.06743754650667433, 'max_depth': 8, 'alpha': 0.2626, 'lambda': 26.17180436372424, 'max_bin': 397}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:40:23,520]\u001b[0m Trial 413 finished with value: 0.7204054774727885 and parameters: {'n_estimators': 390, 'eta': 0.03865527654724687, 'max_depth': 12, 'alpha': 0.2969, 'lambda': 24.01321546026982, 'max_bin': 337}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:40:34,283]\u001b[0m Trial 414 finished with value: 0.7189442551097878 and parameters: {'n_estimators': 375, 'eta': 0.06939474749421295, 'max_depth': 12, 'alpha': 0.3285, 'lambda': 27.512317989879946, 'max_bin': 422}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:40:45,625]\u001b[0m Trial 415 finished with value: 0.7201142433382508 and parameters: {'n_estimators': 433, 'eta': 0.06493786414180329, 'max_depth': 12, 'alpha': 0.23720000000000002, 'lambda': 28.169595608305546, 'max_bin': 413}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:40:57,108]\u001b[0m Trial 416 finished with value: 0.7176380616983173 and parameters: {'n_estimators': 407, 'eta': 0.06145216017598855, 'max_depth': 12, 'alpha': 0.2, 'lambda': 31.99417947689262, 'max_bin': 405}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:41:07,326]\u001b[0m Trial 417 finished with value: 0.7165390016866279 and parameters: {'n_estimators': 374, 'eta': 0.0723475020231371, 'max_depth': 12, 'alpha': 0.14400000000000002, 'lambda': 29.496056182073985, 'max_bin': 384}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:41:17,227]\u001b[0m Trial 418 finished with value: 0.7172077614989879 and parameters: {'n_estimators': 336, 'eta': 0.07470255515765872, 'max_depth': 12, 'alpha': 0.2787, 'lambda': 26.89228657873559, 'max_bin': 410}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:41:28,307]\u001b[0m Trial 419 finished with value: 0.7167278706928057 and parameters: {'n_estimators': 419, 'eta': 0.06836446280287048, 'max_depth': 12, 'alpha': 0.24910000000000002, 'lambda': 25.74486818714572, 'max_bin': 393}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:41:39,242]\u001b[0m Trial 420 finished with value: 0.7177502093566056 and parameters: {'n_estimators': 397, 'eta': 0.07064806816924038, 'max_depth': 12, 'alpha': 0.3057, 'lambda': 30.791729781920047, 'max_bin': 417}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:41:46,703]\u001b[0m Trial 421 finished with value: 0.7159298829136167 and parameters: {'n_estimators': 453, 'eta': 0.06670240399513724, 'max_depth': 6, 'alpha': 0.2732, 'lambda': 28.72723942661215, 'max_bin': 426}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:41:57,342]\u001b[0m Trial 422 finished with value: 0.7200119350150469 and parameters: {'n_estimators': 362, 'eta': 0.06381899249429568, 'max_depth': 12, 'alpha': 0.3658, 'lambda': 27.29588076902781, 'max_bin': 399}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:42:08,503]\u001b[0m Trial 423 finished with value: 0.717026314347395 and parameters: {'n_estimators': 384, 'eta': 0.0727525842735418, 'max_depth': 12, 'alpha': 0.21980000000000002, 'lambda': 25.070404439402385, 'max_bin': 405}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:42:20,510]\u001b[0m Trial 424 finished with value: 0.716280153572916 and parameters: {'n_estimators': 434, 'eta': 0.05748287621642498, 'max_depth': 12, 'alpha': 0.2901, 'lambda': 29.49285916757267, 'max_bin': 388}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:42:31,873]\u001b[0m Trial 425 finished with value: 0.717411151411868 and parameters: {'n_estimators': 411, 'eta': 0.06989582177151676, 'max_depth': 12, 'alpha': 0.3225, 'lambda': 28.01875094866942, 'max_bin': 373}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:42:41,733]\u001b[0m Trial 426 finished with value: 0.7207612532388006 and parameters: {'n_estimators': 396, 'eta': 0.07752691813435855, 'max_depth': 12, 'alpha': 0.2594, 'lambda': 26.015958368351175, 'max_bin': 421}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:42:51,980]\u001b[0m Trial 427 finished with value: 0.7172384277492517 and parameters: {'n_estimators': 378, 'eta': 0.06651182280156562, 'max_depth': 12, 'alpha': 0.1768, 'lambda': 20.91773641850486, 'max_bin': 414}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:43:03,232]\u001b[0m Trial 428 finished with value: 0.715593184031617 and parameters: {'n_estimators': 423, 'eta': 0.07403310357105188, 'max_depth': 12, 'alpha': 0.2768, 'lambda': 31.46371836262144, 'max_bin': 378}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 08:43:14,473]\u001b[0m Trial 429 finished with value: 0.7169064284112711 and parameters: {'n_estimators': 350, 'eta': 0.055038748679824036, 'max_depth': 12, 'alpha': 0.23820000000000002, 'lambda': 30.300394594600938, 'max_bin': 401}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:43:25,337]\u001b[0m Trial 430 finished with value: 0.7170048949129388 and parameters: {'n_estimators': 407, 'eta': 0.061215480607902696, 'max_depth': 12, 'alpha': 0.30310000000000004, 'lambda': 26.740134703271604, 'max_bin': 409}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:43:36,216]\u001b[0m Trial 431 finished with value: 0.7167547749050407 and parameters: {'n_estimators': 479, 'eta': 0.06889185973415352, 'max_depth': 12, 'alpha': 0.5932000000000001, 'lambda': 28.98345887097794, 'max_bin': 429}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:43:46,567]\u001b[0m Trial 432 finished with value: 0.7198872151196272 and parameters: {'n_estimators': 386, 'eta': 0.07178619555490882, 'max_depth': 12, 'alpha': 0.19940000000000002, 'lambda': 24.35310399532524, 'max_bin': 394}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:43:57,179]\u001b[0m Trial 433 finished with value: 0.7188694326076561 and parameters: {'n_estimators': 365, 'eta': 0.06478018833660426, 'max_depth': 12, 'alpha': 0.35050000000000003, 'lambda': 27.70133211508027, 'max_bin': 419}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:44:07,397]\u001b[0m Trial 434 finished with value: 0.7200555264976687 and parameters: {'n_estimators': 441, 'eta': 0.07592356041502268, 'max_depth': 12, 'alpha': 0.2594, 'lambda': 26.35197502512118, 'max_bin': 387}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:44:18,445]\u001b[0m Trial 435 finished with value: 0.719095592672406 and parameters: {'n_estimators': 401, 'eta': 0.06712199373946241, 'max_depth': 12, 'alpha': 0.22560000000000002, 'lambda': 28.483134298812228, 'max_bin': 360}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:44:28,579]\u001b[0m Trial 436 finished with value: 0.7178584614385591 and parameters: {'n_estimators': 417, 'eta': 0.08082040301261266, 'max_depth': 12, 'alpha': 0.2891, 'lambda': 29.889196160274015, 'max_bin': 413}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:44:39,238]\u001b[0m Trial 437 finished with value: 0.7183307687536505 and parameters: {'n_estimators': 387, 'eta': 0.07100572451728751, 'max_depth': 12, 'alpha': 0.3179, 'lambda': 25.380025805184292, 'max_bin': 399}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:44:51,206]\u001b[0m Trial 438 finished with value: 0.7195193386826526 and parameters: {'n_estimators': 431, 'eta': 0.06355430233889726, 'max_depth': 12, 'alpha': 0.2467, 'lambda': 27.37529301650277, 'max_bin': 408}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:45:01,796]\u001b[0m Trial 439 finished with value: 0.7205058390831157 and parameters: {'n_estimators': 371, 'eta': 0.06836638674180856, 'max_depth': 12, 'alpha': 0.269, 'lambda': 29.105221077888807, 'max_bin': 417}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:45:12,639]\u001b[0m Trial 440 finished with value: 0.7186852217138762 and parameters: {'n_estimators': 450, 'eta': 0.07324660847346623, 'max_depth': 12, 'alpha': 0.29510000000000003, 'lambda': 26.74089953285661, 'max_bin': 423}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:45:23,341]\u001b[0m Trial 441 finished with value: 0.7183748570159579 and parameters: {'n_estimators': 396, 'eta': 0.06577525247489041, 'max_depth': 12, 'alpha': 0.3336, 'lambda': 28.15597875295769, 'max_bin': 381}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:45:36,695]\u001b[0m Trial 442 finished with value: 0.7191983478533663 and parameters: {'n_estimators': 593, 'eta': 0.07029419054384868, 'max_depth': 12, 'alpha': 0.2117, 'lambda': 31.038457094438215, 'max_bin': 392}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:45:46,276]\u001b[0m Trial 443 finished with value: 0.7182156495330105 and parameters: {'n_estimators': 414, 'eta': 0.07467131070288346, 'max_depth': 12, 'alpha': 0.24250000000000002, 'lambda': 25.948536566432686, 'max_bin': 402}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:45:57,218]\u001b[0m Trial 444 finished with value: 0.7206491739331102 and parameters: {'n_estimators': 359, 'eta': 0.062405367643457194, 'max_depth': 12, 'alpha': 0.2736, 'lambda': 30.075581334684635, 'max_bin': 411}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:46:06,977]\u001b[0m Trial 445 finished with value: 0.7172811020510446 and parameters: {'n_estimators': 378, 'eta': 0.0677941495209254, 'max_depth': 12, 'alpha': 0.3116, 'lambda': 23.135509134502154, 'max_bin': 429}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:46:17,219]\u001b[0m Trial 446 finished with value: 0.7197182315060763 and parameters: {'n_estimators': 401, 'eta': 0.07195554264494403, 'max_depth': 12, 'alpha': 0.2586, 'lambda': 24.727796875903657, 'max_bin': 405}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:46:28,681]\u001b[0m Trial 447 finished with value: 0.7196955556376299 and parameters: {'n_estimators': 427, 'eta': 0.059621885233026736, 'max_depth': 12, 'alpha': 0.1681, 'lambda': 27.395007511991352, 'max_bin': 397}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:46:38,326]\u001b[0m Trial 448 finished with value: 0.7180466804423102 and parameters: {'n_estimators': 345, 'eta': 0.06921632066243731, 'max_depth': 12, 'alpha': 0.3916, 'lambda': 28.520846022485713, 'max_bin': 386}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:46:50,237]\u001b[0m Trial 449 finished with value: 0.7210934184256336 and parameters: {'n_estimators': 410, 'eta': 0.06519175803438021, 'max_depth': 12, 'alpha': 0.2296, 'lambda': 29.31538215691968, 'max_bin': 368}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7455\n",
      "\tBest params:\n",
      "\t\tn_estimators: 408\n",
      "\t\teta: 0.08777846500101312\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.11\n",
      "\t\tlambda: 31.855553059350896\n",
      "\t\tmax_bin: 422\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.743638    0.730201    0.745800    0.748070   \n",
      "1                    TP   43.000000   43.000000   35.000000   36.000000   \n",
      "2                    TN  305.000000  305.000000  299.000000  300.000000   \n",
      "3                    FP    8.000000    9.000000   15.000000   15.000000   \n",
      "4                    FN   26.000000   25.000000   33.000000   31.000000   \n",
      "5              Accuracy    0.910995    0.910995    0.874346    0.879581   \n",
      "6             Precision    0.843137    0.826923    0.700000    0.705882   \n",
      "7           Sensitivity    0.623188    0.632353    0.514706    0.537313   \n",
      "8           Specificity    0.974400    0.971300    0.952200    0.952400   \n",
      "9              F1 score    0.716667    0.716667    0.593220    0.610169   \n",
      "10  F1 score (weighted)    0.905563    0.906167    0.866512    0.872908   \n",
      "11     F1 score (macro)    0.831936    0.831936    0.759458    0.769481   \n",
      "12    Balanced Accuracy    0.798815    0.801845    0.733468    0.744847   \n",
      "13                  MCC    0.675973    0.673403    0.529568    0.547544   \n",
      "14                  NPV    0.921500    0.924200    0.900600    0.906300   \n",
      "15              ROC_AUC    0.798815    0.801845    0.733468    0.744847   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.677579    0.714666    0.729740    0.750225    0.702400  \n",
      "1    37.000000   40.000000   37.000000   36.000000   41.000000  \n",
      "2   301.000000  301.000000  303.000000  307.000000  304.000000  \n",
      "3    14.000000   11.000000   12.000000    7.000000    9.000000  \n",
      "4    30.000000   30.000000   30.000000   32.000000   28.000000  \n",
      "5     0.884817    0.892670    0.890052    0.897906    0.903141  \n",
      "6     0.725490    0.784314    0.755102    0.837209    0.820000  \n",
      "7     0.552239    0.571429    0.552239    0.529412    0.594203  \n",
      "8     0.955600    0.964700    0.961900    0.977700    0.971200  \n",
      "9     0.627119    0.661157    0.637931    0.648649    0.689076  \n",
      "10    0.878434    0.885829    0.883049    0.888363    0.896836  \n",
      "11    0.779504    0.798697    0.786558    0.794462    0.815856  \n",
      "12    0.753897    0.768086    0.757072    0.753559    0.782724  \n",
      "13    0.567782    0.609862    0.584732    0.613755    0.644965  \n",
      "14    0.909400    0.909400    0.909900    0.905600    0.915700  \n",
      "15    0.753897    0.768086    0.757072    0.753559    0.782724  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_xgb_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_8_cat = np.where(((y_pred_xgb_8 >= 2) | (y_pred_xgb_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 08:47:02,309]\u001b[0m Trial 450 finished with value: 0.7180636649003491 and parameters: {'n_estimators': 388, 'eta': 0.0767724427392718, 'max_depth': 12, 'alpha': 0.29100000000000004, 'lambda': 26.78445302675143, 'max_bin': 414}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:47:13,660]\u001b[0m Trial 451 finished with value: 0.7201852450719708 and parameters: {'n_estimators': 439, 'eta': 0.07063861751268183, 'max_depth': 12, 'alpha': 0.2741, 'lambda': 28.182947185005773, 'max_bin': 421}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:47:24,256]\u001b[0m Trial 452 finished with value: 0.7203741940992747 and parameters: {'n_estimators': 470, 'eta': 0.06759145721582852, 'max_depth': 9, 'alpha': 0.24780000000000002, 'lambda': 25.69716454008576, 'max_bin': 276}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:47:35,368]\u001b[0m Trial 453 finished with value: 0.7166106859841608 and parameters: {'n_estimators': 367, 'eta': 0.07380189557936284, 'max_depth': 12, 'alpha': 0.335, 'lambda': 29.65408548314078, 'max_bin': 407}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:47:47,547]\u001b[0m Trial 454 finished with value: 0.7224174924019138 and parameters: {'n_estimators': 421, 'eta': 0.06360289743679852, 'max_depth': 12, 'alpha': 0.303, 'lambda': 27.593909344565876, 'max_bin': 417}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:47:59,354]\u001b[0m Trial 455 finished with value: 0.7213896449072373 and parameters: {'n_estimators': 395, 'eta': 0.06612564312502547, 'max_depth': 12, 'alpha': 0.19390000000000002, 'lambda': 30.379447629857623, 'max_bin': 394}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:48:09,985]\u001b[0m Trial 456 finished with value: 0.7180432680911917 and parameters: {'n_estimators': 384, 'eta': 0.07231151374552415, 'max_depth': 12, 'alpha': 0.2706, 'lambda': 26.293299349824384, 'max_bin': 425}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:48:21,255]\u001b[0m Trial 457 finished with value: 0.7184203240974332 and parameters: {'n_estimators': 411, 'eta': 0.06956034702546249, 'max_depth': 12, 'alpha': 0.4164, 'lambda': 28.844808082609738, 'max_bin': 390}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:48:30,442]\u001b[0m Trial 458 finished with value: 0.7203306103556362 and parameters: {'n_estimators': 374, 'eta': 0.07572059879635165, 'max_depth': 12, 'alpha': 0.2262, 'lambda': 13.862312547019622, 'max_bin': 377}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:48:45,557]\u001b[0m Trial 459 finished with value: 0.6772561265298286 and parameters: {'n_estimators': 401, 'eta': 0.011775293429294141, 'max_depth': 12, 'alpha': 0.2858, 'lambda': 31.99310566273242, 'max_bin': 412}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:48:56,392]\u001b[0m Trial 460 finished with value: 0.722309046509668 and parameters: {'n_estimators': 455, 'eta': 0.07829730813894681, 'max_depth': 12, 'alpha': 0.24680000000000002, 'lambda': 25.048962267762672, 'max_bin': 399}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:49:08,778]\u001b[0m Trial 461 finished with value: 0.7208524811027719 and parameters: {'n_estimators': 428, 'eta': 0.06759453345694456, 'max_depth': 12, 'alpha': 0.1506, 'lambda': 27.21957166845468, 'max_bin': 404}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:49:20,004]\u001b[0m Trial 462 finished with value: 0.7239355141961186 and parameters: {'n_estimators': 352, 'eta': 0.07102378613536461, 'max_depth': 12, 'alpha': 0.3116, 'lambda': 23.93860488146391, 'max_bin': 384}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:49:32,688]\u001b[0m Trial 463 finished with value: 0.7232871561434044 and parameters: {'n_estimators': 402, 'eta': 0.06254615447543319, 'max_depth': 12, 'alpha': 0.2152, 'lambda': 28.085655811869813, 'max_bin': 418}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:49:44,119]\u001b[0m Trial 464 finished with value: 0.7200547477680672 and parameters: {'n_estimators': 386, 'eta': 0.0651761148630948, 'max_depth': 12, 'alpha': 0.2614, 'lambda': 29.093554118120853, 'max_bin': 410}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:49:55,092]\u001b[0m Trial 465 finished with value: 0.72303761535783 and parameters: {'n_estimators': 422, 'eta': 0.0733602227827891, 'max_depth': 11, 'alpha': 0.2911, 'lambda': 30.597084100518707, 'max_bin': 421}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:50:07,021]\u001b[0m Trial 466 finished with value: 0.7181915644580498 and parameters: {'n_estimators': 441, 'eta': 0.06902837957550884, 'max_depth': 12, 'alpha': 0.32580000000000003, 'lambda': 26.244596897732027, 'max_bin': 402}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:50:18,929]\u001b[0m Trial 467 finished with value: 0.7212090764477391 and parameters: {'n_estimators': 363, 'eta': 0.058695878704153506, 'max_depth': 12, 'alpha': 0.2406, 'lambda': 26.9799227650052, 'max_bin': 395}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:50:30,317]\u001b[0m Trial 468 finished with value: 0.7234252231131637 and parameters: {'n_estimators': 334, 'eta': 0.06072076144919952, 'max_depth': 12, 'alpha': 0.3513, 'lambda': 31.26868951765001, 'max_bin': 415}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:50:42,056]\u001b[0m Trial 469 finished with value: 0.7204375777670513 and parameters: {'n_estimators': 408, 'eta': 0.06608088642999303, 'max_depth': 12, 'alpha': 0.1922, 'lambda': 25.407504635281796, 'max_bin': 407}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:50:52,651]\u001b[0m Trial 470 finished with value: 0.7172614004122949 and parameters: {'n_estimators': 378, 'eta': 0.0723091301552153, 'max_depth': 12, 'alpha': 0.2737, 'lambda': 29.6251193559378, 'max_bin': 429}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:51:04,140]\u001b[0m Trial 471 finished with value: 0.7182205188317416 and parameters: {'n_estimators': 392, 'eta': 0.06880203340928577, 'max_depth': 12, 'alpha': 0.25880000000000003, 'lambda': 27.763856147563722, 'max_bin': 387}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:51:17,345]\u001b[0m Trial 472 finished with value: 0.7194307770506857 and parameters: {'n_estimators': 868, 'eta': 0.07524779755517241, 'max_depth': 12, 'alpha': 0.2984, 'lambda': 28.46832693521896, 'max_bin': 374}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:51:29,593]\u001b[0m Trial 473 finished with value: 0.7225416725452589 and parameters: {'n_estimators': 418, 'eta': 0.06447851596377861, 'max_depth': 12, 'alpha': 0.2351, 'lambda': 26.539841324681166, 'max_bin': 411}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:51:40,165]\u001b[0m Trial 474 finished with value: 0.7229470051640077 and parameters: {'n_estimators': 398, 'eta': 0.07061193700193506, 'max_depth': 12, 'alpha': 0.2856, 'lambda': 24.56847655964296, 'max_bin': 422}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:51:48,202]\u001b[0m Trial 475 finished with value: 0.7178271986215131 and parameters: {'n_estimators': 437, 'eta': 0.06992893374175942, 'max_depth': 7, 'alpha': 0.2074, 'lambda': 28.8894184494774, 'max_bin': 381}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:52:03,019]\u001b[0m Trial 476 finished with value: 0.7209748799356576 and parameters: {'n_estimators': 715, 'eta': 0.06747774285748871, 'max_depth': 12, 'alpha': 0.3199, 'lambda': 25.76832414651198, 'max_bin': 351}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:52:13,301]\u001b[0m Trial 477 finished with value: 0.7188943958481406 and parameters: {'n_estimators': 364, 'eta': 0.07423597503298238, 'max_depth': 12, 'alpha': 0.2616, 'lambda': 27.350841774650966, 'max_bin': 390}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:52:18,712]\u001b[0m Trial 478 finished with value: 0.7134309535285224 and parameters: {'n_estimators': 148, 'eta': 0.08061183055219458, 'max_depth': 12, 'alpha': 0.2238, 'lambda': 30.37229774624014, 'max_bin': 416}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 08:52:29,965]\u001b[0m Trial 479 finished with value: 0.7193008090343234 and parameters: {'n_estimators': 380, 'eta': 0.07187699176048624, 'max_depth': 12, 'alpha': 0.30460000000000004, 'lambda': 28.125369181899522, 'max_bin': 397}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:52:41,979]\u001b[0m Trial 480 finished with value: 0.7257108014848161 and parameters: {'n_estimators': 412, 'eta': 0.062084054444616996, 'max_depth': 12, 'alpha': 0.2456, 'lambda': 29.61119318870621, 'max_bin': 403}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:52:54,697]\u001b[0m Trial 481 finished with value: 0.7195300929095365 and parameters: {'n_estimators': 454, 'eta': 0.06691237265139087, 'max_depth': 12, 'alpha': 0.17650000000000002, 'lambda': 27.031194244229994, 'max_bin': 425}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:53:06,453]\u001b[0m Trial 482 finished with value: 0.7209053729406641 and parameters: {'n_estimators': 426, 'eta': 0.07864235425288961, 'max_depth': 12, 'alpha': 0.2758, 'lambda': 28.708018397011323, 'max_bin': 411}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:53:17,202]\u001b[0m Trial 483 finished with value: 0.7205496765317997 and parameters: {'n_estimators': 400, 'eta': 0.06449914686115751, 'max_depth': 11, 'alpha': 0.2916, 'lambda': 25.989793570866002, 'max_bin': 407}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:53:27,873]\u001b[0m Trial 484 finished with value: 0.7215027491641715 and parameters: {'n_estimators': 381, 'eta': 0.0688083426011605, 'max_depth': 12, 'alpha': 0.13820000000000002, 'lambda': 25.125158045638674, 'max_bin': 418}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:53:38,337]\u001b[0m Trial 485 finished with value: 0.7181534052072074 and parameters: {'n_estimators': 349, 'eta': 0.07222899982623254, 'max_depth': 12, 'alpha': 0.2727, 'lambda': 29.789406334609918, 'max_bin': 432}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:53:50,090]\u001b[0m Trial 486 finished with value: 0.7207269598005793 and parameters: {'n_estimators': 412, 'eta': 0.07609669006949453, 'max_depth': 12, 'alpha': 0.25320000000000004, 'lambda': 31.08555953482914, 'max_bin': 393}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:54:01,797]\u001b[0m Trial 487 finished with value: 0.7217569221627507 and parameters: {'n_estimators': 390, 'eta': 0.06642608190278144, 'max_depth': 12, 'alpha': 0.3289, 'lambda': 27.651819955369742, 'max_bin': 384}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:54:13,286]\u001b[0m Trial 488 finished with value: 0.7184485212752098 and parameters: {'n_estimators': 369, 'eta': 0.07089144125608156, 'max_depth': 12, 'alpha': 0.23070000000000002, 'lambda': 26.526027203138344, 'max_bin': 398}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:54:26,208]\u001b[0m Trial 489 finished with value: 0.7273440549450461 and parameters: {'n_estimators': 435, 'eta': 0.049300614237402555, 'max_depth': 12, 'alpha': 0.20270000000000002, 'lambda': 16.509757194229703, 'max_bin': 414}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:54:37,766]\u001b[0m Trial 490 finished with value: 0.7200703829317114 and parameters: {'n_estimators': 395, 'eta': 0.06859385242212802, 'max_depth': 12, 'alpha': 0.3093, 'lambda': 28.2227072228228, 'max_bin': 402}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:54:49,517]\u001b[0m Trial 491 finished with value: 0.7224521113995932 and parameters: {'n_estimators': 418, 'eta': 0.06366348125149765, 'max_depth': 12, 'alpha': 0.37220000000000003, 'lambda': 29.15684522126006, 'max_bin': 420}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:55:00,095]\u001b[0m Trial 492 finished with value: 0.7196727089577919 and parameters: {'n_estimators': 358, 'eta': 0.07491461176423148, 'max_depth': 12, 'alpha': 0.34600000000000003, 'lambda': 26.759162779033943, 'max_bin': 408}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:55:11,804]\u001b[0m Trial 493 finished with value: 0.7194809964938426 and parameters: {'n_estimators': 378, 'eta': 0.07291362032509913, 'max_depth': 12, 'alpha': 0.5597, 'lambda': 30.40149929930245, 'max_bin': 390}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:55:24,241]\u001b[0m Trial 494 finished with value: 0.7188260736594236 and parameters: {'n_estimators': 405, 'eta': 0.07020278940855376, 'max_depth': 12, 'alpha': 0.27690000000000003, 'lambda': 32.32029189262149, 'max_bin': 379}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:55:35,520]\u001b[0m Trial 495 finished with value: 0.7232627147122384 and parameters: {'n_estimators': 445, 'eta': 0.06624758598568997, 'max_depth': 12, 'alpha': 0.2508, 'lambda': 24.42133290168052, 'max_bin': 413}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:55:47,958]\u001b[0m Trial 496 finished with value: 0.7201947794645867 and parameters: {'n_estimators': 425, 'eta': 0.06795925295026546, 'max_depth': 12, 'alpha': 0.2877, 'lambda': 27.618000804802566, 'max_bin': 423}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:56:00,586]\u001b[0m Trial 497 finished with value: 0.7222773162238838 and parameters: {'n_estimators': 392, 'eta': 0.052628992973417346, 'max_depth': 12, 'alpha': 0.2146, 'lambda': 25.350777943607195, 'max_bin': 404}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:56:10,963]\u001b[0m Trial 498 finished with value: 0.718131921799418 and parameters: {'n_estimators': 373, 'eta': 0.07362642752403648, 'max_depth': 11, 'alpha': 0.2627, 'lambda': 28.67222186531526, 'max_bin': 396}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 08:56:22,731]\u001b[0m Trial 499 finished with value: 0.7204967012967508 and parameters: {'n_estimators': 408, 'eta': 0.06414232509652917, 'max_depth': 12, 'alpha': 0.3135, 'lambda': 26.014436852670357, 'max_bin': 426}. Best is trial 202 with value: 0.7455406770366659.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7455\n",
      "\tBest params:\n",
      "\t\tn_estimators: 408\n",
      "\t\teta: 0.08777846500101312\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.11\n",
      "\t\tlambda: 31.855553059350896\n",
      "\t\tmax_bin: 422\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.743638    0.730201    0.745800    0.748070   \n",
      "1                    TP   43.000000   43.000000   35.000000   36.000000   \n",
      "2                    TN  305.000000  305.000000  299.000000  300.000000   \n",
      "3                    FP    8.000000    9.000000   15.000000   15.000000   \n",
      "4                    FN   26.000000   25.000000   33.000000   31.000000   \n",
      "5              Accuracy    0.910995    0.910995    0.874346    0.879581   \n",
      "6             Precision    0.843137    0.826923    0.700000    0.705882   \n",
      "7           Sensitivity    0.623188    0.632353    0.514706    0.537313   \n",
      "8           Specificity    0.974400    0.971300    0.952200    0.952400   \n",
      "9              F1 score    0.716667    0.716667    0.593220    0.610169   \n",
      "10  F1 score (weighted)    0.905563    0.906167    0.866512    0.872908   \n",
      "11     F1 score (macro)    0.831936    0.831936    0.759458    0.769481   \n",
      "12    Balanced Accuracy    0.798815    0.801845    0.733468    0.744847   \n",
      "13                  MCC    0.675973    0.673403    0.529568    0.547544   \n",
      "14                  NPV    0.921500    0.924200    0.900600    0.906300   \n",
      "15              ROC_AUC    0.798815    0.801845    0.733468    0.744847   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.677579    0.714666    0.729740    0.750225    0.702400    0.745376  \n",
      "1    37.000000   40.000000   37.000000   36.000000   41.000000   31.000000  \n",
      "2   301.000000  301.000000  303.000000  307.000000  304.000000  310.000000  \n",
      "3    14.000000   11.000000   12.000000    7.000000    9.000000    5.000000  \n",
      "4    30.000000   30.000000   30.000000   32.000000   28.000000   36.000000  \n",
      "5     0.884817    0.892670    0.890052    0.897906    0.903141    0.892670  \n",
      "6     0.725490    0.784314    0.755102    0.837209    0.820000    0.861111  \n",
      "7     0.552239    0.571429    0.552239    0.529412    0.594203    0.462687  \n",
      "8     0.955600    0.964700    0.961900    0.977700    0.971200    0.984100  \n",
      "9     0.627119    0.661157    0.637931    0.648649    0.689076    0.601942  \n",
      "10    0.878434    0.885829    0.883049    0.888363    0.896836    0.879035  \n",
      "11    0.779504    0.798697    0.786558    0.794462    0.815856    0.769957  \n",
      "12    0.753897    0.768086    0.757072    0.753559    0.782724    0.723407  \n",
      "13    0.567782    0.609862    0.584732    0.613755    0.644965    0.581607  \n",
      "14    0.909400    0.909400    0.909900    0.905600    0.915700    0.896000  \n",
      "15    0.753897    0.768086    0.757072    0.753559    0.782724    0.723407  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_xgb_9)\n",
    "# now convert the resuls to binary with cutoff 6.9\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_9_cat = np.where(((y_pred_xgb_9 >= 2) | (y_pred_xgb_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA/wklEQVR4nO3dd1hTZ/sH8G8WhC1JFGRYEXfdxVEcqETcSN/Xaoe+VdtqW1dt9Vdxd9iqdaF11Mqr1tq+dmitVStSFQdSUYtarAooDgwgoGwSkjy/PzCnxCQQRsLI/bkuL8nJGfcTwrnPM85zeIwxBkIIIQQAv64DIIQQUn9QUiCEEMKhpEAIIYRDSYEQQgiHkgIhhBAOJQVCCCEcSgrEogYOHIg33nij3uynvhynKnbu3AmhUFjXYdS6SZMmQS6X13UY5CmUFGxYRkYGZs6ciZYtW8LOzg5NmzbF2LFjkZCQUOV9ffLJJ2jZsqXB8n379mHt2rU1jrW29qNj6Xgrk5qaCh6PhzNnzhi8t2zZMrRu3Zp7PX78eKSlpZm9b7lcjkmTJtVGmNV28uRJ8Hg87p9UKsWgQYNw+vTpGu23devWWLZsWe0ESYyipGCj7t27h4CAAMTGxmLLli1ITk7GoUOHIBKJ0KdPH/z222+1chyJRAJXV9d6s5/6cpyqcHBwgIeHh9WPyxhDaWlpjfZx6dIlKBQK/P7773BwcMDw4cORmppaOwESy2DEJo0ePZp5eHiw3Nxcg/eGDx/OPDw8WFFREWOMsaVLlzJ/f3+2Z88e5ufnx+zt7VlwcDC7desWY4yxHTt2MAB6/5YuXcoYYywoKIi9/vrr3L6DgoLYlClT2MKFC1nTpk2Zm5sbW7BgAdNoNOzDDz9kzZo1YzKZjC1YsEAvpvL7OXHihMHxALBnnnmGMcaYVqtlb7zxBmvVqhUTi8XMz8+PhYeHs5KSkirHq1Kp2AcffMC8vLyYSCRiHTp0YHv27NGLDQDbtGkTmzBhAnN2dmY+Pj5s5cqVFX7+t2/fZgDY6dOnDd7Tfd46O3bsYAKBgHudm5vLJk2axDw8PJidnR3z8fFhc+bMYYwx9tprrxmU7cSJE4wxxq5fv85GjBjBnJycmJOTExs1ahRLSkoyOM7x48dZt27dmEgkYhEREYzH47GzZ8/qxXjy5EnG4/FYSkqK0fLpfkf37t3jlt2/f58BYFu3buViDQ4O5t7XarXs888/Z35+fkwkErFWrVqxdevWce8HBQUZlO327dsVfs6k6igp2KCcnBzG5/PZxx9/bPT9U6dOMQDswIEDjLGyk5SjoyPr27cvO3/+PDt//jzr1asX69KlC9NqtayoqIh98MEHzMfHhykUCqZQKFh+fj5jzHhScHV1Zf/3f//Hbty4wSIjIxkANnz4cDZv3jx248YNtnPnTgaAHT58WG873X6USiV3HIVCwRITE5mXlxebNGkSY4wxjUbDFi5cyOLi4tjt27fZgQMHmKenJ1uyZAljjFUp3rlz5zKJRMK+//57duPGDbZ8+XLG4/FYdHQ0tw4A1qxZM7Zt2zaWnJzMIiIiGAB2/Phxk7+DmiSFmTNnsi5durC4uDh2584ddvbsWbZt2zbGGGOPHz9m/fv3Z+PGjePKplQqWVFREWvRogUbPHgwu3DhArtw4QIbOHAg8/f3Z0qlkjsOj8djAQEB7Pfff2cpKSksMzOThYSEcJ+tzoQJE5hcLjdZPmNJITs7mwFgGzduZIwZJoUvvviCicVi9uWXX7KbN2+yLVu2MHt7e7Z9+3Zu+5YtW7L333+fK5tarTYZA6keSgo26I8//mAA2L59+4y+r/vjXbVqFWOs7CQFQO+q8saNGwwAO3bsGGOMsY8//pi7Ui/PWFLo2rWr3jodO3ZknTp10lvWpUsX9v7775vcj45KpWIDBw5k/fr142oCxqxdu5a1bt2ae21OvIWFhczOzo5t2rRJb52wsDA2aNAg7jUANnPmTL112rVrx+bPn28yHl1ScHBw4K7cdf9EIlGFSSE0NJS99tprJvcdHBxs8P727duZg4MDe/jwIbcsPT2dicVitmvXLu44ANipU6f0tv3pp5+Yo6Mje/z4MWOMsUePHjEHBwf2/fffm4zh6aSQl5fH3njjDSYUCtnVq1cZY4ZJwcfHh82bN09vP++++y7z8/PjXvv7+3O1OmIZ1Kdgg1glcyDyeDyDZU2bNtXr/Gzbti1kMhmuXbtW5eN37dpV77Wnpye6dOlisCwzM7PSfb399tu4d+8e9u/fD3t7e275V199hd69e8PDwwPOzs4IDw/HnTt3qhRncnIyVCoVBgwYoLc8KCgIiYmJesu6deum99rb2xsZGRmVHmPHjh1ISEjQ+/fWW29VuM0777yDH3/8EZ06dcLs2bNx5MgRaLXaCrdJTExEx44dIZPJuGUeHh5o166dQVl69uyp9zo0NBRubm749ttvAQDffPMNnJ2dMWbMmErL165dOzg7O8PNzQ1Hjx7F119/jU6dOhmsl5eXh/v37xv9rFNTU1FUVFTpsUjtoKRgg9q0aQM+n4+//vrL6Pu65e3atatwP5UlF1NEIpHeax6PZ3RZZSe6VatWYd++fTh06JDeye6HH37A9OnTMX78eBw+fBh//vknlixZUu1O06eTJGPMYJmdnV2V4wfKkkfr1q31/kkkkgq3GTp0KO7evYuFCxeipKQEEyZMwODBg6HRaKpUDmNlEQgEEIvFeusIhUK8/vrr+OqrrwAA27dvx6RJkwzKbMzRo0dx+fJlZGVl4e7du3j55ZerFGN1v2Ok+igp2CCJRILhw4dj06ZNyMvLM3j/008/hYeHB4YMGcIte/jwIVJSUrjXN2/eRHZ2Njp06ACg7KRY2UmpNv38889YsmQJ9u3bZ5C8Tp06he7du+O9997Dc889hzZt2hiMeDEn3tatW8Pe3h4xMTEG+3/22WdrpRzVJZFI8PLLL+PLL7/EoUOHEBMTw9XajJXt2WefRWJiIrKysrhlGRkZuHnzplllefPNN3H58mVs3boVly9fNvtejpYtW8Lf37/SROfq6gofHx+jn7Wfnx8cHR1Nlo3ULkoKNmrTpk0QCAQYPHgwfvvtN9y7dw/x8fF45ZVXcOLECezcuRMODg7c+o6Ojpg8eTIuXryICxcu4LXXXkPnzp25m4/8/PyQnp6Oc+fOISsry6LV/cTEREyYMAHLli1D+/btkZ6ejvT0dDx8+BBAWQ3n6tWrOHDgAFJSUhAREYF9+/bp7cOceB0dHTFr1iwsXrwYP/zwA5KSkvDpp5/iwIEDWLBggcXKV5mFCxdi3759uHHjBpKSkrBnzx44OzujRYsWAMrKdvHiRaSkpCArKwulpaV45ZVX0LRpU4wfPx6XLl3CxYsX8dJLL8Hb2xvjx4+v9JgtWrTAsGHDMHv2bAwcOBBt27at9XKFh4dj48aN+Oqrr5CUlIQvv/wSW7Zs0fus/fz8cPbsWdy9exdZWVlm1cZI1VBSsFHPPPMMLly4gN69e2PatGnw9/fH8OHDoVQqce7cOQwbNkxv/ebNm2Pq1Kn497//jb59+8LBwQH79+/nqvthYWF48cUXMXLkSDRt2hSrVq2yWOzx8fEoLCxEeHg4mjdvzv3TtYVPmzYNEydOxOTJk9G9e3f88ccfBjc8mRvv8uXL8eabb+Ldd9/Fs88+i2+++QbffPMNgoODLVa+yojFYixZsgTPPfccAgICcOXKFRw5cgRubm4AgPfffx8ymQxdu3ZF06ZNcfbsWTg4OCAqKgr29vYYMGAAgoKC4OTkhN9++82sZiAAmDp1KlQqFaZOnWqRcr399tv46KOP8Omnn6Jjx45YuXIlVqxYgddff51b58MPP0Rubi7atWuHpk2b4u7duxaJxZbxGDXakUosW7YM33zzDZKTk+s6FFKHNm/ejCVLliAtLU2vU580Lo1vQhVCSK0qKChAcnIyVq9ejRkzZlBCaOSo+YgQUqEZM2agV69e6NChAz744IO6DodYGDUfEUII4VBNgRBCCIeSAiGEEE6D72h+8OBBtbaTyWR6N/LYAiqzbaAy24aalNnLy8vke1RTIIQQwqGkQAghhENJgRBCCIeSAiGEEA4lBUIIIZwGP/qIkIrc3vcrMjdsgFvOQ/CgBR+1eyWkxT8PDAYA3pN/dXm1lVuHx64rNldmgQD5zs7gde0K8cQJEPr719quKSmQRiv9cDSUGyLgpiriTtQ8/HMCx5OfedX4Wfe/bjnfxPuEWIRGA21+PvDHHyh+/BgOM2fUWmKg5iPSaD36+ls4qkq42oGxkzSvmj+bs275GgQhte7JDEXatDSoT5+ptd1STaEeUly+jqSd38Mn6Qpci3IheNLsUVO2VMXWAvBE/bhap1oDsQjdtHUlJdCa8Txwc1FSqCOKy9dxe9N2tLiVCLFWpdfkIAbQ+an1tTDenFFeRc0c5dnCCcoWykhsnO551mIx+B4etbZbSgpWln44GgWbt8K96BE6PPWeuScyU+tV1syhQ1euxpNmdfoUqrouIbXmSVLge3tD2L9fre2WkoIVpR+ORklEBNxLi0y2cRPzGGurN/V5Pt2xrHuqb1WTwdPHqcq61HlnSDdyS1cLru2RYQ1dhZ+PQAA+jT6qG4rL13Fv83/hc+sq7DQqo1d+5YcjGqMbpWL/5N/TI2AaM2uW09Sxyv6weMi3c8TCvtOQ6mZ6MrDaJuABG15oje4+LhbZ/4NcJbbFKZBVUAqZswhT+zRHF39vq08O9yBXifWn7uPKgwKUqBns+IBIwEOplqGklEHD9EdsOdvzUaxmKNX881vzdrXDAnkLfPdnJhLTi6BSq6HW8iDg8+AqFmLxkBbwcLHD+lP3ufdV6rKTpoDPQxcvJ8wf3AJebg3ryXDGfocAMHt/MtLyVNx63q52iHihNVc+S00C2OAfslMbs6SmH45G7u49cHv4AEKtGnwYDjksr6bNCjVVG80cpvZX0bLyTO2nspit3VyjGwGk5Iuwu30Ifm47yGh8Qj4gFvLgIBLAw8UO9kIe7j5SokCpRqkaAB/QaAEhDxDb8SHk8+AvFcNBJEBOUSmyizRwsefhYX4pCksZSrVln4aAB2hZ2QmSBwaNpuwkpqut6GqMuqtCHgABH2jfzAHuDiLkFJYio6AUhSo1itX/vN/N2xl9W7rgizMKlDuvgs8DXMVCdPZ0xOwBPlY5QT7IVeKdH28is1Bt8WNVRuYkxNaxba2aGHQJMTG9CACDv1QMALj5sBhFKi14PB7sBYCDnQDNnO3g7ihESakGKdlKaLQaqDRAifqfX6I9v+wiRvcdKs9JxIOLWASpoxCtPFzxWndJtcpa0SypNpcUdCN7fJOvwrnwMQTQGu2Irct2YGO/kPInDVTys05tr/v0dvxyP1vqGDUth4YngMJJit3th4Lfvz8c7QRIe1yC7CINZE5CeLnZY2qf5rV6Ejl2IxtLj96ttf1VFw9Aa5k9sgpKka/SgmkBN0chmrsYnpjshQI4CHlIzy+FhgF8Hg9dvZ0wuacnDiRm613FernZ40GuEp/9fgdXFUUo1bB6V/MVAODzy07GuhqLSgM4CPno4uWEl7o3w4HEbKQ9KkF2sQZSRyG8m9hjzLNS/Pe8ApfTCrnajZAH7ufyzb58Pg98xqCsw8I7CPlYHdqqyjVRSgpPKC5fx83Vm+CdeRdNSgsgeLK8Ltv2y59Uy48wUvMEeCR2xQWPdjjsF2jVJo/GqoePM774VxuLHuNBrhLjv76md/VOiCU5iPjY/Ur7Kl3YVJQUbKpP4fKPR9GkMB9OGiXXYVMXCaH8+SJX5Ijv+76Eot59MeZZadnVy+MSZOSXokRdVrV0EPERIBPjdk7ZFW5FhHzAyU6ANrKyKmxCWiHUdIICAMicRBY/xrY4BSUEYlXFpVpsi1Ng2dCWtbI/m0oKdtkPYadVQ1DutFyTZqLqtn1rwEMpX4gMz2fgMW8O/q9re26fFVUDDTrzBODawE01g3xwMAWnb+dVs4SNi64Dz5KyCkotfgxCnpZVWHvfO5tKCippU6jS76MUfAhQcQesqTbrp5t5qtr2zXh8FAnFuNuiHZ6f/3aVhpJ5udlj1eiqDT0rVGkrX8kG2AusUyeUOVu+NkLI02qzFmxTSaHr2KG4mZqM4sLHsNeq9N5jRv431lkZ69O1xnHYCXjo3cIF/jIfWLqngE5SZZQaVqtVbFOm9mmOREWh3lBCW6Xrs9MNkqgOPspGcJUyw+W6v1Fb5yDi12ot2KY6moGyzuZr//0ffJKuoElJHoRPTv3FQntcatoW/2svN9mpywPQRMwHeHw8Kv5n+J2HiwhLhjzDja8GGDp5lhvh8KSPIFepMRiXXX7csSU8yFUajHeuKyI+0OcZV+5zySoshczpnxEtppq6RHweWrqLkJKtwtP1HnsBD8qnPlNHOz6SskoM9mONjmag7DN/8/sbeFRs2P/DByBxFKKlVIxilQYZ+aUo1TLweUAnTyf08HHChtMKg5Od8MkwVbGQrzfUUchnUGrKhr3WJ54udtg3+VkA5cbhF5bCUcQHD0BhqVbvd69bR/e3Uv4zMTW0tj4NhTXFUQQU1bBlx14AaLWGiRFo4KOPEhISsGPHDmi1WgQHByMsLEzv/V9++QWnT58GAGi1Wty/fx+RkZFwdnaucL9VTQoVnSTNuakspJ07pvZpzn3Jy3+xK7LsaCqibjwyuj9LX73q/uByVYCbHTDmWSk+jb5baaLgoWzcOxggfOrkWxl7PiAS8riO8i7NTf9x68z4KQmX0goMlutO5uVPLrrPHYDBsm1xijr7rHVq8vs2Vs6KPjddX9P5u/lQVfA7Ehm54raEmtywZ+zvs6KLJ13ZryqKUKhUl90TICy7kNDdOKdbxgdDoQqoeKhG2XfezZ4P7yb2SMst5YbsNnEQ4FGRBqVaBsa0EPB40IIHxrTc+/lKZjDU2VjC021fUKp/A18zJyHaNXM0SJrly6v7bnhLnBvufQparRazZ8/GokWLIJVKER4ejtmzZ8PHx8fo+hcuXMChQ4ewdOnSSvdd1aRg6o/V08UOLvbGrzDL6+TpiG3j2pl9PN0vMfZ2LgqMtO9b6+oV0L9hr/wXVTdmv4mD0OhVnLH1Xex5en8AupFTVUmUT6utxFnVE4slWDIGY3fAlj/5nEvNQ77S8NRnLLnywHD3kRLFpVo42wvxdqAnzqbmI+1xCW7llC3XMXbCAsDdr6DRMkgchVg2tGW17+Cu7nfAUnf3WlpVLwDKq0mZ63xIanJyMjw9PeHxZCa/wMBAxMfHm0wKZ8+eRd++fS0Si6nRIV5udpA5iSpNCllVqKqa03RjjWGSxni52VfpRGvO+jWdysFYe7y3q12V20u93OwR8UJrvdpRbd+gVpUYapIon2bsO5WoKOSSzbKhLfHZSQUOXkk32Fb3XavsdzmknZQ7ljnxb/xX2xqVqTxTf5+1ObqmPqnq36E1WCUp5OTkQCqVcq+lUimSkpKMrqtUKpGQkIDXX3/d6PvR0dGIjo4GAKxYsQIymaxKsXhLFUabKLwlznh3sD+uP7yEuznFJrf3cBObfczPTl6tMCG0kDjgg+EdIZM4mrW/mhIKhVX+vKxJJgO+nuKO9cdTkJmvRDMXe7w72B++1fh8ZDLgC39vCIVCqNV10+asi6E2GftOpeWpsOvPHKwZWzbh+twhrrh8P1fve1yd75ol4q9MRX+fFX136/t32xIsVWarJAVjLVQ8nvEhghcvXkS7du1M9iXI5XLI5XLudVWrT691l+BSao7B1ehr3SVw0BZh7Wg/rD91H7GpeUY77zycBGYfMy3b8MsNAM52fAT6uWFqn+Zw0BYhK6uoSmWoroZQxXYAED6wXM2ghp9PQyhzVZj6TqXlFHDlbC6TYe1oP4OrfGt+16qror/Pin6Pje33bI4G3XwklUqRnZ3Nvc7Ozoa7u7vRdc+ePYt+/WpvbvCn6ar1u/7MQVpOgUG12MvNHo52AqMJoapDv0wNBw30c6t3VUbSMJj6Tj3dDFkfmyXMYalmN2I+qyQFf39/KBQKZGZmQiKRIDY2FrNmzTJYr6ioCNeuXcPMmTMtGo+Xmz3WjO1sMsuaatdsJbGv0pezttrICdGxhe9UQ01ojYVVkoJAIMCUKVOwfPlyaLVaDBo0CL6+voiKigIAhISEAADOnz+Prl27QiwWWyMsk0xdjXk3qVpcdNVDaht9p4il2dzNazoVtcfVhyGNlmCNdldTwyXrCrU124b6UmZrfv8bdJ9CQ0NXY9VT2XBJQhqzxvL9p6RgArVrVt22OIXR4ZLWmHOIkLrWWL7/lBRMqG/NIA2Brd14REh5jeX7T0nBiMZSDbQ2c4dLEtIYNZbvP7/yVWxPRdVAYtrUPs3h7Wqnt6yxDZckxJTG8v2nmoIRjaUaaG3mdtBT0xxpjBrLABVKCkY0lmpgXaisg56a5khj1hgGqFDzkRGNpRpYH1HTHCH1G9UUjGgs1cD6iJrmCKnfKCmY0BiqgfURNc0RUr9R8xGxKmqaI6R+s9mawr2cIqw8mkojYKyMmuYIqd9sMik8yFXivYPX9Z5MRSNgrIea5gipv2yy+WhbnMLgkZs0AoYQQmw0KdAIGEIIMc4mkwKNgCGEEONsMilM7dMcLSQOestoBAwhhNhoR7OXmz12/KcHVh65RiNgCCGkHJtMCgDgK3GkETCEEPIUm2w+IoQQYhwlBUIIIRxKCoQQQjiUFAghhHAoKRBCCOFQUiCEEMKx2pDUhIQE7NixA1qtFsHBwQgLCzNYJzExETt37oRGo4GLiws+/PBDa4VHCCEEVkoKWq0WkZGRWLRoEaRSKcLDwxEQEAAfHx9uncLCQmzfvh0LFy6ETCZDbm6uNUIjhBBSjlWaj5KTk+Hp6QkPDw8IhUIEBgYiPj5eb50zZ86gd+/ekMlkAAA3NzdrhEYIIaQcq9QUcnJyIJVKuddSqRRJSUl66ygUCqjVaixbtgzFxcUYMWIEgoKCDPYVHR2N6OhoAMCKFSu4JFJVQqGw2ts2VFRm20Bltg2WKrNVkgJjzGAZj8fTe63RaHD79m0sXrwYKpUKixYtQps2beDl5aW3nlwuh1wu515nZWVVKyaZTFbtbRsqKrNtaIhlfpCrLHsaXzWfhNgQy1xTNSnz0+fV8qySFKRSKbKzs7nX2dnZcHd3N1jHxcUFYrEYYrEYHTp0wJ07dyoMnhDS8D3IVWL2/mSk5am4ZfQkxLpjlT4Ff39/KBQKZGZmQq1WIzY2FgEBAXrrBAQE4Pr169BoNFAqlUhOToa3t7c1wiOE1KFtcQq9hADQkxDrktk1BbVajaSkJDx69AiBgYEoKSkBAIjF4kq3FQgEmDJlCpYvXw6tVotBgwbB19cXUVFRAICQkBD4+PigW7dumDt3Lvh8PgYPHowWLVpUs1iEkIaCnoRYv5iVFO7evYuVK1dCJBIhOzsbgYGBuHbtGmJiYjBnzhyzDtSjRw/06NFDb1lISIje69DQUISGhpoZOiGkMaAnIdYvZjUfffXVVxg/fjzWr18PobAsj3Ts2BHXr1+3aHCEkMZvap/m8Ha101tGT0KsO2bVFO7fv4/+/fvrLROLxVCpVCa2IIQQ83i52SPihdZlo4/oSYh1zqyk0LRpU9y6dQv+/v7cMt0NaYQQUlNebvb0JMR6wqykMH78eKxYsQJDhgyBWq3G/v37cezYMUybNs3S8RFCCLEis/oUnnvuOYSHhyMvLw8dO3bEw4cPMXfuXHTt2tXS8RFCCLEis4ektmrVCq1atbJkLIQQQuqYWUlh7969Jt8bP358rQVDCCGkbpmVFMpPUQEAjx8/xrVr19CrVy+LBEUIIaRumJUU3nnnHYNlCQkJOHPmTK0HRAghpO5Ue+6jLl26GDwTgRBCSMNmVk0hIyND77VSqcSZM2dsbv5yQghp7MxKCrNmzdJ7bWdnBz8/P0yfPt0iQRFCCKkbNR591NDV9OEehBDSmFjlITv1FT3cgxBC9JlMCm+//bZZO9iyZUutBWNtFT3cg+ZhIYTYIpNJYebMmdaMo07Qwz0IIUSfyaTQsWNHa8ZRJ+jhHoQQos/sPoXU1FT8/fffyM/PB2OMW96Qp7mY2qc5EhWFek1I9HAPQogtMyspREdHY9euXejSpQsSEhLQrVs3XLlyBQEBAZaOz6Lo4R6EEKLPrKRw4MABLFiwAB06dMDkyZMxb948/Pnnnzh79qyl47M4ergHIYT8w6xpLvLy8tChQwcAAI/Hg1arRffu3XHx4kWLBkcIIcS6zKopSCQSZGZmolmzZmjevDkuXLgAFxcXCIU2fZsDIYQ0Omad1ceMGYO0tDQ0a9YMY8eOxdq1a6FWqzF58mRLx0cIIcSKKkwKa9euxcCBAzFgwADw+WUtTd27d8eOHTugVqshFoutEiQhhBDrqDApSCQSbN26FYwx9OvXDwMHDsQzzzwDoVBITUeEENIIVXhmnzRpEv7zn/8gISEBp0+fxqJFi+Dp6YmgoCD069cPTZo0MftACQkJ2LFjB7RaLYKDgxEWFqb3fmJiIlatWoVmzZoBAHr37o2xY8dWuUCEEEKqr9LLfT6fjx49eqBHjx4oKipCXFwcTp8+je+++w6dO3fG/PnzKz2IVqtFZGQkFi1aBKlUivDwcAQEBMDHx0dvvQ4dOpi1P0IIIZZRpTYgR0dHdO/eHQUFBcjIyMDff/9t1nbJycnw9PSEh4cHACAwMBDx8fEGSYEQQkjdMispqFQqnD9/HjExMUhMTESHDh0wfvx49OnTx6yD5OTkQCqVcq+lUimSkpIM1rt58ybmzZsHd3d3TJw4Eb6+vgbrREdHIzo6GgCwYsWKaj/9TSgU2tyT46jMtoHKbBssVeYKk0JiYiJiYmLwxx9/wN3dHQMGDMC0adOqHEj5uZJ0eDye3ms/Pz9s3rwZYrEYly5dwueff44NGzYYbCeXyyGXy7nXWVlZVYpFRyaTVXvbhorKbBuozLahJmX28vIy+V6FSWH16tUIDAzEwoUL0bZt22odHCirGWRnZ3Ovs7Oz4e7urreOo6Mj93OPHj0QGRmJvLw8uLq6Vvu4hBBCqqbCpLBt2zaIRDWfRtrf3x8KhQKZmZmQSCSIjY01eO7z48eP4ebmBh6Ph+TkZGi1Wri4uNT42IQQQsxXYVKojYQAAAKBAFOmTMHy5cuh1WoxaNAg+Pr6IioqCgAQEhKCuLg4REVFQSAQwM7ODu+++65BExMhhBDL4jFjDf4NyIMHD6q1HbVB2gYqs22gMldNRX0KZs2SSgghxDZUKSlkZWXh5s2bloqFEEJIHTPrPoWsrCxEREQgNTUVALB7927ExcUhISEBb731liXjI4QQYkVm1RS2bduG7t27Y9euXdxEeF26dMGVK1csGhwhhBDrMispJCcnIywsjJs+Gyi7r6CoqMhigRFCCLE+s5KCm5sb0tPT9Zbdv3/f5m4rJ4SQxs6sPoXRo0dj5cqVCAsLg1arxZkzZ7B//36D6a8JIYQ0bGYlhcGDB8PZ2Rm///47pFIpTp06hfHjx6NXr16Wjo8QQogVmZUUtFotevXqRUmAEEIaObP6FN58801s374d169ft3Q8hBBC6pBZNYVFixbh7NmziIiIAJ/PR9++fdGvXz+0aNHC0vERQgixIrOSgp+fH/z8/DBhwgRcu3YNZ86cwUcffYQmTZpg9erVlo6REEKIlVR57iMvLy/4+PhAKpXi4cOHloiJEEJIHTGrplBYWIg//vgDZ86cQVJSErp06YIxY8YgICDA0vERQgixIrOSwrRp09CuXTv069cPc+fO1XtKGiGEkMbDrKSwceNGg8dnEkIIaXxMJoVr166hY8eOAIC0tDSkpaUZXa9Tp06WiYwQQojVmUwKkZGRWLNmDQBgy5YtRtfh8Xj44osvLBMZIYQQqzOZFHQJAQA2bdpklWAIIYTULbOGpK5atcrocrpHgRBCGhezkkJiYmKVlhNCCGmYKhx9tHfvXgCAWq3mftbJyMhA06ZNLRcZIYQQq6swKWRnZwMomyVV97OOTCbDuHHjLBcZIYQQq6swKbzzzjsAgLZt20Iul1slIEIIIXXHrD4FkUiEO3fu6C1LTU3FqVOnLBIUIYSQumFWUti7dy+kUqneMplMhv/9739mHyghIQGzZ8/GzJkz8fPPP5tcLzk5GePHj0dcXJzZ+yaEEFI7zEoKxcXFBvMdOTo6orCw0KyDaLVaREZGYsGCBVi3bh3Onj2L+/fvG11vz5496Natm1n7JYQQUrvMSgo+Pj4GV+7nz5+Hj4+PWQdJTk6Gp6cnPDw8IBQKERgYiPj4eIP1jhw5gt69e8PV1dWs/RJCCKldZk2I9+qrr+Kzzz5DbGwsPD09kZ6ejqtXryI8PNysg+Tk5Og1P0mlUiQlJRmsc/78eSxdutTktBoAEB0djejoaADAihUrIJPJzIrhaUKhsNrbNlRUZttAZbYNliqzWUmhffv2WLNmDc6cOYOsrCy0bt0akyZNMjsgxpjBMh6Pp/d6586dePXVV8HnV1x5kcvleiOhsrKyzIrhaTKZrNrbNlRUZttAZbYNNSmzl5eXyffMSgq6AEJDQ5Gbm1vlabSlUqnefQ7Z2dkG+0hJSUFERAQAIC8vD3/++Sf4fD569epVpWMRQgipPrOfvLZ9+3bExcVBKBRi9+7duHDhApKTk/HSSy9Vur2/vz8UCgUyMzMhkUgQGxuLWbNm6a1TftK9TZs24bnnnqOEQAghVmZWR/NXX30FR0dHbN68GUJhWR5p27YtYmNjzTqIQCDAlClTsHz5csyZMwfPP/88fH19ERUVhaioqOpHTwghpFaZVVO4evUqvvzySy4hAICrqytyc3PNPlCPHj3Qo0cPvWUhISFG150+fbrZ+yWEEFJ7zKopODo6Ij8/X29ZVlYWPaKTEEIaGbOSQnBwMNasWYO//voLjDHcvHkTmzZtwpAhQywdHyGEECsyq/lozJgxEIlEiIyMhEajwZYtWyCXyzFixAhLx0cIIcSKzEoKPB4PI0eOxMiRIy0dDyGEkDpkMilcu3YNHTt2BAD89ddfpncgFKJp06YGE+YRQghpeEwmhcjISKxZswYAKpx2gjGG/Px8DB8+HK+88krtR0gIIcRqTCYFXUIA9G8sMyYvLw+zZ8+mpEAIIQ2c2dNcaLVa3Lx5E48ePYJEIkGbNm24eYpcXV2xaNEiiwVJCCHEOsxKCnfu3MHnn3+O0tJSSCQS5OTkQCQSYe7cuWjZsiWAsqksCCGENGxmJYUtW7Zg6NChGDVqFHg8HhhjOHToELZs2YKVK1daOkZCCCFWYtbNawqFAiNHjuSmu+bxeBgxYgTS09MtGhwhhBDrMispdO/eHRcuXNBbduHCBXTv3t0iQRFCCKkbJpuPNm7cyNUMtFot1q9fj1atWnHPRrh16xYCAgKsFighhBDLM5kUPD099V77+vpyP/v4+KBr166Wi4oQQkidMJkUXnzxRWvGQQghpB6odPSRRqPB6dOnceXKFeTn58PFxQWdO3dG//799Z6vQAghpOGrsKO5qKgIixYtwp49eyAQCODn5weBQIBvv/0WixcvRlFRkbXiJIQQYgUVXup/++23cHV1xdKlSyEWi7nlJSUlWLduHb799lu88cYbFg+SEEKIdVRYU4iPj8ebb76plxAAQCwW4/XXX8f58+ctGhwhhBDrqrT5SCKRGH1PKpWiuLjYIkERQgipGxUmBQ8PD5PPUrh69SqaNWtmkaAIIYTUjQqTwqhRo/DFF18gLi4OWq0WQNmNbHFxcdi8eTNGjRpllSAJIYRYR4UdzQMHDkR+fj42b96MiIgIuLq6Ii8vDyKRCGPHjsWgQYOsFSchhBArqPRGg9GjR0Mul+PGjRvcfQpt27aFo6OjNeIjhBBiRWbdfebg4IBu3bpZOBRCCCF1zWq3JCckJGDHjh3QarUIDg5GWFiY3vvx8fHYu3cveDweBAIBJk2ahPbt21srPEIIIbBSUtBqtYiMjMSiRYsglUoRHh6OgIAA+Pj4cOt07twZAQEB4PF4uHPnDtatW4f169dbIzxCCCFPmPU8hZpKTk6Gp6cnPDw8IBQKERgYiPj4eL11xGIxN1W3UqnkfiaEEGI9Vqkp5OTkQCqVcq+lUimSkpIM1jt//jy+/fZb5ObmIjw83Oi+oqOjER0dDQBYsWIFZDJZtWISCoXV3rahojLbBiqzbbBUma2SFBhjBsuM1QR69eqFXr164dq1a9i7dy8WL15ssI5cLodcLudeZ2VlVSsmmUxW7W0bKiqzbaAy24aalNnLy8vke1ZpPtI9rU0nOzsb7u7uJtfv2LEj0tPTkZeXZ43wCCGEPGGVpODv7w+FQoHMzEyo1WrExsYaPMozPT2dq1HcunULarUaLi4u1giPEELIE1ZpPhIIBJgyZQqWL18OrVaLQYMGwdfXF1FRUQCAkJAQxMXF4dSpUxAIBLCzs8OcOXOos5kQQqyMx4w1+DcgDx48qNZ21AZpG6jMtoHKXDUV9SnY7PM07+UUYeXRVGQVlELmLMLUPs3h5WZf12ERQkidssmk8CBXifcOXsfdnH+eB5GoKETEC60pMRBCbJpVOprrm21xCr2EAABpeSpsi1PUUUSEEFI/2GRSyCooNb680PhyQgixFTaZFGTOIuPLnYwvJ4QQW2GTSWFqn+ZoIXHQW+btaoepfZrXUUSEEFI/2GRHs5ebPXb8pwdWHrmGrMJSyJxo9BEhhAA2mhQAwFfiiGVDW9Z1GIQQUq/YZPMRIYQQ4ygpEEII4VBSIIQQwqGkQAghhGOzHc2EkIoxxlBSUgKtVlvvZyzOyMiAUqms6zCsqrIyM8bA5/P1HnVsDkoKhBCjSkpKIBKJIBTW/9OEUCiEQCCo6zCsypwyq9VqlJSUwMHBocL1yqPmI0KIUVqttkEkBGKaUCiEVqut0jaUFAghRtX3JiNinqr+HikpEEII4VBSIITUWw8ePMDkyZPRt29fBAYGYsmSJVCpVACAvXv3YuHChUa3Cw0NrdbxfvvtN9y8eZN7/fnnn+PUqVPV2pfO3r178c477+gty8nJQefOnU12FFdUNkujpEAIqRUPcpVYdjQVM35KwrKjqXiQW7PRQIwxvPnmmxg2bBjOnj2L06dPo7CwECtXrqx0219++aVax3w6KcybNw8DBgyo1r50RowYgVOnTqG4+J9nuPz6668ICQmBvX39m2+NkgIhpMYe5Coxe38yom48wqW0AkTdeITZ+5NrlBjOnDkDe3t7jB8/HgAgEAiwbNky/O9//+NOsA8ePMCrr76KwMBArF27ltu2TZs23M9btmzBiBEjIJfLsXr1am75Dz/8ALlcDrlcjpkzZyI+Ph7Hjh3DJ598giFDhiA1NRXvvvsufv31Vxw/fhzTpk3jto2NjcVrr70GAIiJicHo0aMxdOhQTJ06FYWFhXrlcHFxQZ8+fRAVFcUt++WXXzBmzBhERUVh1KhRCAkJwfjx4/Hw4UODz0EXQ1XKVhOUFAghNbYtToG0PJXespo+zfDmzZvo3Lmz3jIXFxd4e3vj9u3bAICEhARs3LgRv//+O3799VdcvnxZb/2YmBjcvn0bhw4dQlRUFK5cuYK4uDjcuHEDGzZswPfff4/o6Gh89NFH6NmzJ4YMGYJFixbh2LFjaNmyJbefAQMG4NKlSygqKgJQdlIPDQ1FTk4OIiIisHfvXhw9ehRdu3bFtm3bDMoyZswYrvaSnp6OW7duoW/fvujVqxcOHjyIqKgojBkzBps3bzb78zl58qTRstUUjTcjhNSYJZ5myBgzOnKm/PL+/ftDIpFAKBRi+PDhOH/+PLp27cqtGxMTg5iYGISEhAAAioqKcPv2bVy7dg0jR46ERCIBALi7u1cYi1AoxKBBg3Ds2DGMHDkSv//+OxYtWoRz587h5s2bGDNmDACgtLQUzz33nMH2crkcCxYsQH5+Pg4ePIiRI0dCIBBAoVDg7bffRmZmJlQqFVq0aGH253Py5EmjZevTp4/Z+zBa1hptTQghsMzTDNu2bYvDhw/rLcvPz8eDBw/QsmVLXLlyxSBpPP2aMYYZM2Zg4sSJessjIyOrPFRz9OjR2LVrF5o0aYJu3brB2dkZjDEMGDCg0it8BwcHDBw4EEeOHMGBAwewbNkyAMDixYsxdepUhISEIDY2Vq8JTKf8vQaMMZSWllZYtpqi5iNCSI1N7dMc3q52estq+jTD/v37o7i4GD/88AMAQKPR4KOPPsK4ceO4O3RPnz6NR48eobi4GEePHkXPnj319jFw4EDs3buXa+dXKBTIyspCv379cPDgQeTk5AAAHj16BABwdnY26BPQCQwMxNWrV7Fnzx6MHj0aAPDcc88hPj6ea84qLi5GSkqK0e3DwsKwbds2ZGVlcbWJvLw8eHp6AgBXzqf5+Pjg6tWrAICjR49ySWHQoEFGy1ZTlBQIITXm5WaPiBdaI6SdO3r4OCOknTsiXmhdo6cZ8ng8bN++Hb/++iv69u2L/v37w97eHvPnz+fW6dmzJ2bNmoXg4GCMGDGCazrS1QKCgoIQFhaG0NBQBAcHY+rUqSgoKEC7du0wa9YsjB07FnK5HB9++CGAsrb/LVu2ICQkBKmpqXrxCAQCyOVynDhxAkOGDAEASKVSrFu3DtOnT4dcLsfo0aNNJoWgoCBkZGQgNDSUi+/999/HtGnT8MILL3BNWU979dVXce7cOYwcORJ//vknHB0dAZQlPGNlqykeY4zVeC9mSEhIwI4dO6DVahEcHIywsDC990+fPo0DBw4AAMRiMd544w29jh5THjx4UK14ZDJZrWTVhoTKbBtqq8xFRUXcCai+EwqFUKvVAMruARg2bBjOnz9fx1FZVvkyV8TY79HLy8vk+lapKWi1WkRGRmLBggVYt24dzp49i/v37+ut06xZMyxbtgyrV6/Gv//9b6M9+IQQUpH09HSEhobirbfequtQGiyrdDQnJyfD09MTHh4eAMra5uLj4+Hj48Ot065dO+7nNm3aIDs72xqhEUIaEU9PT5w5c6auw2jQrJIUcnJyIJVKuddSqRRJSUkm1z9+/Di6d+9u9L3o6GhER0cDAFasWAGZTFatmIRCYbW3baiozLahtsqckZHRoGZJbUix1hZzymxvb1+l74NVPkVj3RamhoP99ddfOHHiBD766COj7+vuQNSpbtsptTXbBipz9SmVygbzjAJz29cbE3PLrFQqDb4Pdd6nIJVK9ZqDsrOzjd4scufOHXz55ZeYN28eXFxcrBEaIYSQcqySFPz9/aFQKJCZmQm1Wo3Y2FgEBATorZOVlYXVq1djxowZFWYxQgghlmOV5iOBQIApU6Zg+fLl0Gq1GDRoEHx9fbkJokJCQvDjjz+ioKAA27dv57ZZsWKFNcIjhNQCdUoK1KfPQJuRAb6HB4T9+0Ho71+jffr6+qJ9+/ZgjEEgEOCTTz4xuEHNHF999RUmTJhg8FjKNWvWQKVSITw8nFv2119/Yfr06YiJiTG6rzVr1sDJyanRjnCy2n0KlkL3KZiPymwb6uI+BXVKClTf/wCeszPg5AQUFoIVFMBu3Is1Sgxt2rThBqWcPHkSGzduxE8//WSwXmXt671798aRI0cMbhBLTk7GxIkTce7cOW7Zp59+CgcHB8yZM8fovupLUrDUfQq2111PCKmy0vPnwZ5MCWH0/TNnwUpKwCs3RQQrKYFyx05o+/U1ug1PIoGoVy+zY8jPz4ebmxv3esuWLTh48CBUKhVGjBiB9957D0VFRZg2bRoUCgW0Wi1mz56NrKwsZGRk4MUXX4S7uzt+/PFHbh+tW7eGq6srLl26hB49egAADh48iD179nD/VCoV/Pz8sGHDBoOaxtixY7F48WJ07doVOTk5GD58OP744w9oNBp8+umnOHfuHFQqFV577bVan6PIUigpEEJqjOXlAU8PDrG3L1teAyUlJRgyZAiUSiUyMzPx/fffA9CfEpsxhsmTJyMuLg7Z2dnw9PTE7t27AZTNLeTq6opt27bhhx9+MDqVRFhYGA4cOIAePXrg4sWLcHd3R6tWrdCkSRO8+uqrAICVK1fiu+++w5QpU8yK+7vvvoOLiwsOHz4MpVKJsLAwBAUFVWkW1LpCSYEQUqnKrui16Rlg+fnglUsMLD8fvDZtYDdsWLWPKxaLcezYMQDAhQsXMHv2bBw/ftzklNi9evXCxx9/jOXLl0Mul6N3796VHiM0NBRjxozB0qVLceDAAW4a7Bs3bmDVqlXIy8tDYWEhgoKCzI47JiYGf//9Nw4dOgSgrJZz+/ZtSgqk/nmQq8S2OAWyCkohcxZhap/mNZq0jBAAEPbvB9X3T2b5LNenIBoxvNaOERAQgJycHGRnZxtMG12+ff3IkSM4fvw4PvvsMwQFBZnsG9Dx9vaGr68vzp07h8OHD3MPw5kzZw4iIyPx7LPPYu/evXr9DjoCgYCb1rqkpETvvU8++QQDBw6sabGtjmZJtSH3copq/ZGJhACA0N8fduNeBM/FBezhQ/BcXGrcyfy05ORkaDQauLu7m5wSOz09HQ4ODvj3v/+Nt956i5ty2tnZucIZRMeMGYNly5ahZcuWXCdsQUEBPDw8UFpaiv379xvdztfXF1euXAEArlYAlM2I+vXXX3PTXKekpHBPbavvqKZgQ9YfTzH5yMRlQ1vWTVCk0RD6+9dqEgD+6VMAymZGWL9+PQQCAYKCgpCUlITQ0FAAgJOTEzZs2IDU1FR88skn4PF4EIlE+OyzzwCUTT89YcIENGvWTK+jWWf06NFYunQpPv74Y27ZvHnzMGrUKPj4+KB9+/ZGk8pbb72Ft956Cz/99BP69v2nQ/2VV17BvXv3MGzYMDDGIJFI8N///rdWPxtLoSGpNmTOL6n4I/WRwfIePs744l9tjGzR8Nni79nWp862FQ166mxSPzRzNd53UJNHJhJCGhdKCjbk3cH+tf7IREJI40J9CjbEV+KIiBdal40+KiyFzIlGHxHTGnjLMnmiqr9HSgo2xsvNnjqViVn4fD7UarVNPqegsVCr1eDzq9YgRL9tQohRYrEYJSUlUCqVJp9/Ul/Y29tDqbStodWVlZkxBj6fD7FYXKX9UlIghBjF4/EM5vqpr2iUWe2hjmZCCCEcSgqEEEI4lBQIIYRwGvwdzYQQQmqPzdYU5s+fX9chWB2V2TZQmW2Dpcpss0mBEEKIIUoKhBBCODabFORyeV2HYHVUZttAZbYNliozdTQTQgjh2GxNgRBCiCFKCoQQQjg2OfdRQkICduzYAa1Wi+DgYISFhdV1SLVi8+bNuHTpEtzc3LBmzRoAZc+ZXbduHR4+fIimTZtizpw5cHZ2BgDs378fx48fB5/Px+TJk9GtW7c6jL56srKysGnTJjx+/Bg8Hg9yuRwjRoxo1OVWqVRYunQp1Go1NBoN+vTpg3HjxjXqMgOAVqvF/PnzIZFIMH/+/EZfXgCYPn06xGIx+Hw+BAIBVqxYYflyMxuj0WjYjBkzWHp6OistLWVz585l9+7dq+uwakViYiJLSUlh7733Hrds9+7dbP/+/Ywxxvbv3892797NGGPs3r17bO7cuUylUrGMjAw2Y8YMptFo6iLsGsnJyWEpKSmMMcaKiorYrFmz2L179xp1ubVaLSsuLmaMMVZaWsrCw8PZjRs3GnWZGWPs4MGDbP369eyzzz5jjDX+7zZjjL3zzjssNzdXb5mly21zzUfJycnw9PSEh4cHhEIhAgMDER8fX9dh1YqOHTtyVww68fHxCAoKAgAEBQVxZY2Pj0dgYCBEIhGaNWsGT09PJCcnWz3mmnJ3d0erVq0AAA4ODvD29kZOTk6jLjePx+OmQ9ZoNNBoNODxeI26zNnZ2bh06RKCg4O5ZY25vBWxdLltLink5ORAKpVyr6VSKXJycuowIsvKzc2Fu7s7gLITaF5eHgDDz0EikTT4zyEzMxO3b99G69atG325tVot5s2bhzfeeAOdO3dGmzZtGnWZd+7ciQkTJug916Exl7e85cuX44MPPkB0dDQAy5fb5voUmJERuPX9ASKWYOxzaMhKSkqwZs0aTJo0CY6OjibXayzl5vP5+Pzzz1FYWIjVq1fj7t27Jtdt6GW+ePEi3Nzc0KpVKyQmJla6fkMvb3kff/wxJBIJcnNz8cknn8DLy8vkurVVbptLClKpFNnZ2dzr7OxsLus2Rm5ubnj06BHc3d3x6NEjuLq6AjD8HHJyciCRSOoqzBpRq9VYs2YN+vfvj969ewOwjXIDgJOTEzp27IiEhIRGW+YbN27gwoUL+PPPP6FSqVBcXIwNGzY02vKWp4vbzc0NPXv2RHJyssXLbXPNR/7+/lAoFMjMzIRarUZsbCwCAgLqOiyLCQgIQExMDAAgJiYGPXv25JbHxsaitLQUmZmZUCgUaN26dV2GWi2MMWzduhXe3t4YNWoUt7wxlzsvLw+FhYUAykYiXb16Fd7e3o22zK+88gq2bt2KTZs24d1330WnTp0wa9asRltenZKSEhQXF3M/X7lyBS1atLB4uW3yjuZLly5h165d0Gq1GDRoEP71r3/VdUi1Yv369bh27Rry8/Ph5uaGcePGoWfPnli3bh2ysrIgk8nw3nvvcZ3R+/btw4kTJ8Dn8zFp0iR07969jktQddevX8eSJUvQokULrhnw5ZdfRps2bRptue/cuYNNmzZBq9WCMYbnn38eY8eORX5+fqMts05iYiIOHjyI+fPnN/ryZmRkYPXq1QDKBhT069cP//rXvyxebptMCoQQQoyzueYjQgghplFSIIQQwqGkQAghhENJgRBCCIeSAiGEEA4lBUIs5O+//8bs2bPNWvfkyZNYvHixhSMipHI2d0czIeYKDw/HrFmzwOfzsXbtWqxcuRITJ07k3lepVBAKheDzy66tpk6div79+3Pvd+jQAREREVaPm5CaoKRAiBFqtRpZWVnw9PREXFwc/Pz8AAC7d+/m1pk+fTqmTZuGLl26GGyv0WggEAisFi8htYWSAiFG3Lt3Dz4+PuDxeEhJSeGSgimJiYnYuHEjhg0bhkOHDqFLly4YPHgwNm7ciK1btwIAfv75Z/z+++/Izc2FVCrFyy+/jF69ehnsizGGXbt24cyZMygtLUXTpk0xa9YstGjRwiJlJaQ8SgqElHPixAns2rULarUajDFMmjQJJSUlsLOzw3fffYdVq1ahWbNmRrd9/PgxCgoKsHnzZjDGkJSUpPe+h4cHPvzwQzRp0gRxcXHYuHEjNmzYYDAh4+XLl/H3338jIiICjo6OSEtLg5OTk8XKTEh51NFMSDmDBg3Czp070apVKyxfvhyrV6+Gr68vdu3ahZ07d5pMCEDZFOzjxo2DSCSCnZ2dwfvPP/88JBIJ+Hw+AgMDTT4ERSgUoqSkBGlpaWCMwcfHp1HP5EvqF6opEPJEQUEBZsyYAcYYSkpKsGzZMpSWlgIAJk+ejBdffBEjR440ub2rq6vRZKATExODX3/9FQ8fPgRQNvNlfn6+wXqdOnXC0KFDERkZiaysLPTq1QsTJ06s8DkRhNQWSgqEPOHs7IydO3fi7NmzSExMxNSpU/H5559j6NChRjuTn1bRw5oePnyIL7/8EkuWLEHbtm3B5/Mxb948kw9GGTFiBEaMGIHc3FysW7cOv/zyC1566aVql40Qc1FSIOQpt27d4jqWU1NTuWdA14RSqQSPx+MeiHLixAncu3fP6LrJyclgjMHPzw/29vYQiUTcsFdCLI2SAiFPuXXrFp5//nnk5+eDz+dzc9XXhI+PD0aNGoWFCxeCz+djwIABaNeundF1i4uLsWvXLmRkZMDOzg5du3ZFaGhojWMgxBz0PAVCCCEcqpMSQgjhUFIghBDCoaRACCGEQ0mBEEIIh5ICIYQQDiUFQgghHEoKhBBCOJQUCCGEcP4fZnd08HRlDZ4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.744585</td>\n",
       "      <td>0.042165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>1.946507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>153.800000</td>\n",
       "      <td>2.250926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.549193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>1.370320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.901047</td>\n",
       "      <td>0.007978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.832574</td>\n",
       "      <td>0.050782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.547195</td>\n",
       "      <td>0.046431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.975850</td>\n",
       "      <td>0.009928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.658341</td>\n",
       "      <td>0.032081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.892547</td>\n",
       "      <td>0.008594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.800218</td>\n",
       "      <td>0.017589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.761527</td>\n",
       "      <td>0.020930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.622732</td>\n",
       "      <td>0.032435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.910690</td>\n",
       "      <td>0.006938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.761527</td>\n",
       "      <td>0.020930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.744585     0.042165\n",
       "1                    TP        18.300000     1.946507\n",
       "2                    TN       153.800000     2.250926\n",
       "3                    FP         3.800000     1.549193\n",
       "4                    FN        15.100000     1.370320\n",
       "5              Accuracy         0.901047     0.007978\n",
       "6             Precision         0.832574     0.050782\n",
       "7           Sensitivity         0.547195     0.046431\n",
       "8           Specificity         0.975850     0.009928\n",
       "9              F1 score         0.658341     0.032081\n",
       "10  F1 score (weighted)         0.892547     0.008594\n",
       "11     F1 score (macro)         0.800218     0.017589\n",
       "12    Balanced Accuracy         0.761527     0.020930\n",
       "13                  MCC         0.622732     0.032435\n",
       "14                  NPV         0.910690     0.006938\n",
       "15              ROC_AUC         0.761527     0.020930"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.743638</td>\n",
       "      <td>0.730201</td>\n",
       "      <td>0.745800</td>\n",
       "      <td>0.748070</td>\n",
       "      <td>0.677579</td>\n",
       "      <td>0.714666</td>\n",
       "      <td>0.729740</td>\n",
       "      <td>0.750225</td>\n",
       "      <td>0.702400</td>\n",
       "      <td>0.745376</td>\n",
       "      <td>0.728770</td>\n",
       "      <td>0.023892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>37.900000</td>\n",
       "      <td>3.813718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>303.500000</td>\n",
       "      <td>3.407508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>3.472111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>30.100000</td>\n",
       "      <td>3.247221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.910995</td>\n",
       "      <td>0.910995</td>\n",
       "      <td>0.874346</td>\n",
       "      <td>0.879581</td>\n",
       "      <td>0.884817</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.893717</td>\n",
       "      <td>0.012353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.785917</td>\n",
       "      <td>0.060363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.632353</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.537313</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.556977</td>\n",
       "      <td>0.051131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.974400</td>\n",
       "      <td>0.971300</td>\n",
       "      <td>0.952200</td>\n",
       "      <td>0.952400</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.964700</td>\n",
       "      <td>0.961900</td>\n",
       "      <td>0.977700</td>\n",
       "      <td>0.971200</td>\n",
       "      <td>0.984100</td>\n",
       "      <td>0.966550</td>\n",
       "      <td>0.011001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.593220</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.650260</td>\n",
       "      <td>0.045130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.905563</td>\n",
       "      <td>0.906167</td>\n",
       "      <td>0.866512</td>\n",
       "      <td>0.872908</td>\n",
       "      <td>0.878434</td>\n",
       "      <td>0.885829</td>\n",
       "      <td>0.883049</td>\n",
       "      <td>0.888363</td>\n",
       "      <td>0.896836</td>\n",
       "      <td>0.879035</td>\n",
       "      <td>0.886270</td>\n",
       "      <td>0.013239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.831936</td>\n",
       "      <td>0.831936</td>\n",
       "      <td>0.759458</td>\n",
       "      <td>0.769481</td>\n",
       "      <td>0.779504</td>\n",
       "      <td>0.798697</td>\n",
       "      <td>0.786558</td>\n",
       "      <td>0.794462</td>\n",
       "      <td>0.815856</td>\n",
       "      <td>0.769957</td>\n",
       "      <td>0.793784</td>\n",
       "      <td>0.025853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.798815</td>\n",
       "      <td>0.801845</td>\n",
       "      <td>0.733468</td>\n",
       "      <td>0.744847</td>\n",
       "      <td>0.753897</td>\n",
       "      <td>0.768086</td>\n",
       "      <td>0.757072</td>\n",
       "      <td>0.753559</td>\n",
       "      <td>0.782724</td>\n",
       "      <td>0.723407</td>\n",
       "      <td>0.761772</td>\n",
       "      <td>0.026179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.675973</td>\n",
       "      <td>0.673403</td>\n",
       "      <td>0.529568</td>\n",
       "      <td>0.547544</td>\n",
       "      <td>0.567782</td>\n",
       "      <td>0.609862</td>\n",
       "      <td>0.584732</td>\n",
       "      <td>0.613755</td>\n",
       "      <td>0.644965</td>\n",
       "      <td>0.581607</td>\n",
       "      <td>0.602919</td>\n",
       "      <td>0.050227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.921500</td>\n",
       "      <td>0.924200</td>\n",
       "      <td>0.900600</td>\n",
       "      <td>0.906300</td>\n",
       "      <td>0.909400</td>\n",
       "      <td>0.909400</td>\n",
       "      <td>0.909900</td>\n",
       "      <td>0.905600</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.909860</td>\n",
       "      <td>0.008720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.798815</td>\n",
       "      <td>0.801845</td>\n",
       "      <td>0.733468</td>\n",
       "      <td>0.744847</td>\n",
       "      <td>0.753897</td>\n",
       "      <td>0.768086</td>\n",
       "      <td>0.757072</td>\n",
       "      <td>0.753559</td>\n",
       "      <td>0.782724</td>\n",
       "      <td>0.723407</td>\n",
       "      <td>0.761772</td>\n",
       "      <td>0.026179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.743638    0.730201    0.745800    0.748070   \n",
       "1                    TP   43.000000   43.000000   35.000000   36.000000   \n",
       "2                    TN  305.000000  305.000000  299.000000  300.000000   \n",
       "3                    FP    8.000000    9.000000   15.000000   15.000000   \n",
       "4                    FN   26.000000   25.000000   33.000000   31.000000   \n",
       "5              Accuracy    0.910995    0.910995    0.874346    0.879581   \n",
       "6             Precision    0.843137    0.826923    0.700000    0.705882   \n",
       "7           Sensitivity    0.623188    0.632353    0.514706    0.537313   \n",
       "8           Specificity    0.974400    0.971300    0.952200    0.952400   \n",
       "9              F1 score    0.716667    0.716667    0.593220    0.610169   \n",
       "10  F1 score (weighted)    0.905563    0.906167    0.866512    0.872908   \n",
       "11     F1 score (macro)    0.831936    0.831936    0.759458    0.769481   \n",
       "12    Balanced Accuracy    0.798815    0.801845    0.733468    0.744847   \n",
       "13                  MCC    0.675973    0.673403    0.529568    0.547544   \n",
       "14                  NPV    0.921500    0.924200    0.900600    0.906300   \n",
       "15              ROC_AUC    0.798815    0.801845    0.733468    0.744847   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.677579    0.714666    0.729740    0.750225    0.702400    0.745376   \n",
       "1    37.000000   40.000000   37.000000   36.000000   41.000000   31.000000   \n",
       "2   301.000000  301.000000  303.000000  307.000000  304.000000  310.000000   \n",
       "3    14.000000   11.000000   12.000000    7.000000    9.000000    5.000000   \n",
       "4    30.000000   30.000000   30.000000   32.000000   28.000000   36.000000   \n",
       "5     0.884817    0.892670    0.890052    0.897906    0.903141    0.892670   \n",
       "6     0.725490    0.784314    0.755102    0.837209    0.820000    0.861111   \n",
       "7     0.552239    0.571429    0.552239    0.529412    0.594203    0.462687   \n",
       "8     0.955600    0.964700    0.961900    0.977700    0.971200    0.984100   \n",
       "9     0.627119    0.661157    0.637931    0.648649    0.689076    0.601942   \n",
       "10    0.878434    0.885829    0.883049    0.888363    0.896836    0.879035   \n",
       "11    0.779504    0.798697    0.786558    0.794462    0.815856    0.769957   \n",
       "12    0.753897    0.768086    0.757072    0.753559    0.782724    0.723407   \n",
       "13    0.567782    0.609862    0.584732    0.613755    0.644965    0.581607   \n",
       "14    0.909400    0.909400    0.909900    0.905600    0.915700    0.896000   \n",
       "15    0.753897    0.768086    0.757072    0.753559    0.782724    0.723407   \n",
       "\n",
       "           ave       std  \n",
       "0     0.728770  0.023892  \n",
       "1    37.900000  3.813718  \n",
       "2   303.500000  3.407508  \n",
       "3    10.500000  3.472111  \n",
       "4    30.100000  3.247221  \n",
       "5     0.893717  0.012353  \n",
       "6     0.785917  0.060363  \n",
       "7     0.556977  0.051131  \n",
       "8     0.966550  0.011001  \n",
       "9     0.650260  0.045130  \n",
       "10    0.886270  0.013239  \n",
       "11    0.793784  0.025853  \n",
       "12    0.761772  0.026179  \n",
       "13    0.602919  0.050227  \n",
       "14    0.909860  0.008720  \n",
       "15    0.761772  0.026179  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test['std'] = mat_met_xgb_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_xgb0</th>\n",
       "      <th>y_pred_xgb1</th>\n",
       "      <th>y_pred_xgb2</th>\n",
       "      <th>y_pred_xgb3</th>\n",
       "      <th>y_pred_xgb4</th>\n",
       "      <th>y_pred_xgb_ave</th>\n",
       "      <th>y_pred_xgb_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL585939</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>-0.357235</td>\n",
       "      <td>-0.501857</td>\n",
       "      <td>-0.507951</td>\n",
       "      <td>-0.263064</td>\n",
       "      <td>-0.464698</td>\n",
       "      <td>-0.547468</td>\n",
       "      <td>0.300115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL96051</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.727524</td>\n",
       "      <td>1.862799</td>\n",
       "      <td>1.640337</td>\n",
       "      <td>1.785949</td>\n",
       "      <td>1.790880</td>\n",
       "      <td>1.597915</td>\n",
       "      <td>0.372008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3356916</td>\n",
       "      <td>2</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.487420</td>\n",
       "      <td>0.471336</td>\n",
       "      <td>0.415176</td>\n",
       "      <td>0.538762</td>\n",
       "      <td>0.493862</td>\n",
       "      <td>0.451093</td>\n",
       "      <td>0.076772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3907413</td>\n",
       "      <td>3</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.303023</td>\n",
       "      <td>0.653664</td>\n",
       "      <td>1.691334</td>\n",
       "      <td>0.611730</td>\n",
       "      <td>0.316920</td>\n",
       "      <td>0.757779</td>\n",
       "      <td>0.474323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2047704</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>-2.917163</td>\n",
       "      <td>-3.046047</td>\n",
       "      <td>-2.886846</td>\n",
       "      <td>-2.892398</td>\n",
       "      <td>-3.055449</td>\n",
       "      <td>-2.841317</td>\n",
       "      <td>0.273209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>CHEMBL1095136</td>\n",
       "      <td>1905</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>-1.076517</td>\n",
       "      <td>-0.989931</td>\n",
       "      <td>-1.098946</td>\n",
       "      <td>-0.951889</td>\n",
       "      <td>-1.076014</td>\n",
       "      <td>-1.065550</td>\n",
       "      <td>0.079644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>CHEMBL2012817</td>\n",
       "      <td>1906</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-1.129502</td>\n",
       "      <td>-0.784106</td>\n",
       "      <td>-0.393459</td>\n",
       "      <td>-0.061521</td>\n",
       "      <td>-0.859311</td>\n",
       "      <td>-1.027983</td>\n",
       "      <td>0.921114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>CHEMBL496511</td>\n",
       "      <td>1907</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.877123</td>\n",
       "      <td>1.243157</td>\n",
       "      <td>1.153667</td>\n",
       "      <td>0.979903</td>\n",
       "      <td>0.860010</td>\n",
       "      <td>0.985643</td>\n",
       "      <td>0.161556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>CHEMBL3940062</td>\n",
       "      <td>1908</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1.246309</td>\n",
       "      <td>1.276124</td>\n",
       "      <td>1.368273</td>\n",
       "      <td>1.200517</td>\n",
       "      <td>1.440004</td>\n",
       "      <td>1.466871</td>\n",
       "      <td>0.367749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>CHEMBL493749</td>\n",
       "      <td>1909</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.638456</td>\n",
       "      <td>0.607241</td>\n",
       "      <td>0.546827</td>\n",
       "      <td>0.566051</td>\n",
       "      <td>0.584690</td>\n",
       "      <td>0.545544</td>\n",
       "      <td>0.100708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1910 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_xgb0  y_pred_xgb1  \\\n",
       "0          CHEMBL585939            0    -1.19    -0.357235    -0.501857   \n",
       "1           CHEMBL96051            1     0.78     1.727524     1.862799   \n",
       "2         CHEMBL3356916            2     0.30     0.487420     0.471336   \n",
       "3         CHEMBL3907413            3     0.97     0.303023     0.653664   \n",
       "4         CHEMBL2047704            4    -2.25    -2.917163    -3.046047   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "1905      CHEMBL1095136         1905    -1.20    -1.076517    -0.989931   \n",
       "1906      CHEMBL2012817         1906    -2.94    -1.129502    -0.784106   \n",
       "1907       CHEMBL496511         1907     0.80     0.877123     1.243157   \n",
       "1908      CHEMBL3940062         1908     2.27     1.246309     1.276124   \n",
       "1909       CHEMBL493749         1909     0.33     0.638456     0.607241   \n",
       "\n",
       "      y_pred_xgb2  y_pred_xgb3  y_pred_xgb4  y_pred_xgb_ave  y_pred_xgb_std  \n",
       "0       -0.507951    -0.263064    -0.464698       -0.547468        0.300115  \n",
       "1        1.640337     1.785949     1.790880        1.597915        0.372008  \n",
       "2        0.415176     0.538762     0.493862        0.451093        0.076772  \n",
       "3        1.691334     0.611730     0.316920        0.757779        0.474323  \n",
       "4       -2.886846    -2.892398    -3.055449       -2.841317        0.273209  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "1905    -1.098946    -0.951889    -1.076014       -1.065550        0.079644  \n",
       "1906    -0.393459    -0.061521    -0.859311       -1.027983        0.921114  \n",
       "1907     1.153667     0.979903     0.860010        0.985643        0.161556  \n",
       "1908     1.368273     1.200517     1.440004        1.466871        0.367749  \n",
       "1909     0.546827     0.566051     0.584690        0.545544        0.100708  \n",
       "\n",
       "[1910 rows x 10 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"rmse\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_xgb_cat = np.where(((y_pred_optimized_xgb >= 2) | (y_pred_optimized_xgb <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_xgb_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_xgb))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "xgb_run0 = data_xgb[['y_test_idx0', 'y_test0', 'y_pred_xgb0']]\n",
    "xgb_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "xgb_run0.reset_index(inplace=True, drop=True)\n",
    "xgb_run1 = data_xgb[['y_test_idx1', 'y_test1', 'y_pred_xgb1']]\n",
    "xgb_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "xgb_run1.reset_index(inplace=True, drop=True)\n",
    "xgb_run2 = data_xgb[['y_test_idx2', 'y_test2', 'y_pred_xgb2']]\n",
    "xgb_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "xgb_run2.reset_index(inplace=True, drop=True)\n",
    "xgb_run3 = data_xgb[['y_test_idx3', 'y_test3', 'y_pred_xgb3']]\n",
    "xgb_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "xgb_run3.reset_index(inplace=True, drop=True)\n",
    "xgb_run4 = data_xgb[['y_test_idx4', 'y_test4', 'y_pred_xgb4']]\n",
    "xgb_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "xgb_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "xgb_5preds = pd.concat([chembl_id,xgb_run0, xgb_run1, xgb_run2, xgb_run3, xgb_run4], axis=1)\n",
    "xgb_5preds = xgb_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_xgb0', 'y_pred_xgb1', 'y_pred_xgb2', 'y_pred_xgb3', 'y_pred_xgb4']]\n",
    "xgb_5preds['y_pred_xgb_ave'] = xgb_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "xgb_5preds['y_pred_xgb_std'] = xgb_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "xgb_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c2883ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_xgb.to_csv(output/'mat_met_xgb_opt_withSemiSel.csv')\n",
    "xgb_5preds.to_csv(output/'xgb_5test_CV_result_withSemiSel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "02aaad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b19aca7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt  \n",
    "# retrieve performance metrics\n",
    "results = optimized_xgb_0.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "    \n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "pyplot.show()\n",
    "\n",
    " # plot classification error\n",
    "#fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "#ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "#ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "#ax.legend()\n",
    "    \n",
    "#pyplot.ylabel('Classification Error')\n",
    "#pyplot.title('XGBoost Classification Error')\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost baseline model r2_score 0.6925 with a standard deviation of 0.0498\n",
      "XGBoost optimized model r2_score 0.7344 with a standard deviation of 0.0365\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_reg, X, Y, cv=10, scoring=\"r2\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_xgb.joblib']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_reg, \"./xgb_reg.joblib\")\n",
    "#joblib.dump(optimized_xgb, \"./optimized_xgb.joblib\")\n",
    "joblib.dump(optimizedCV_xgb, \"./optimizedCV_xgb.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.701528     0.040884\n",
      "1                    TP        19.400000     2.270585\n",
      "2                    TN       152.400000     1.429841\n",
      "3                    FP         5.200000     1.549193\n",
      "4                    FN        14.000000     1.885618\n",
      "5              Accuracy         0.899476     0.010413\n",
      "6             Precision         0.790145     0.052428\n",
      "7           Sensitivity         0.580266     0.058661\n",
      "8           Specificity         0.967030     0.009732\n",
      "9              F1 score         0.667248     0.045515\n",
      "10  F1 score (weighted)         0.893044     0.011875\n",
      "11     F1 score (macro)         0.804005     0.025451\n",
      "12    Balanced Accuracy         0.773648     0.028602\n",
      "13                  MCC         0.621015     0.048188\n",
      "14                  NPV         0.916010     0.010094\n",
      "15              ROC_AUC         0.773648     0.028602\n",
      "CPU times: user 3.85 s, sys: 0 ns, total: 3.85 s\n",
      "Wall time: 3.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    \n",
    "    knn_reg.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=4)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=4)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:00:36,083]\u001b[0m A new study created in memory with name: KNNregressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:37,176]\u001b[0m Trial 0 finished with value: 0.5378400662549885 and parameters: {'n_neighbors': 19, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 52}. Best is trial 0 with value: 0.5378400662549885.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:38,271]\u001b[0m Trial 1 finished with value: 0.4969115229322877 and parameters: {'n_neighbors': 24, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 0 with value: 0.5378400662549885.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:39,370]\u001b[0m Trial 2 finished with value: 0.6246750719472615 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 49}. Best is trial 2 with value: 0.6246750719472615.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:40,476]\u001b[0m Trial 3 finished with value: 0.5734472894625996 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 2 with value: 0.6246750719472615.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:41,785]\u001b[0m Trial 4 finished with value: 0.573043477614104 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 2 with value: 0.6246750719472615.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:42,879]\u001b[0m Trial 5 finished with value: 0.4793024719321853 and parameters: {'n_neighbors': 27, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 67}. Best is trial 2 with value: 0.6246750719472615.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:43,974]\u001b[0m Trial 6 finished with value: 0.5651672337066268 and parameters: {'n_neighbors': 16, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 2 with value: 0.6246750719472615.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:45,071]\u001b[0m Trial 7 finished with value: 0.5840247185094333 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 2 with value: 0.6246750719472615.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:46,373]\u001b[0m Trial 8 finished with value: 0.4852871681676508 and parameters: {'n_neighbors': 26, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 33}. Best is trial 2 with value: 0.6246750719472615.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:47,678]\u001b[0m Trial 9 finished with value: 0.546554353865192 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 2 with value: 0.6246750719472615.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:48,615]\u001b[0m Trial 10 finished with value: 0.6859877854336454 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 10 with value: 0.6859877854336454.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:49,539]\u001b[0m Trial 11 finished with value: 0.6859877854336454 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 10 with value: 0.6859877854336454.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:50,465]\u001b[0m Trial 12 finished with value: 0.6974811656015723 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 12 with value: 0.6974811656015723.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:51,392]\u001b[0m Trial 13 finished with value: 0.6974811656015723 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 12 with value: 0.6974811656015723.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:52,488]\u001b[0m Trial 14 finished with value: 0.6630418739037454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 84}. Best is trial 12 with value: 0.6974811656015723.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:53,587]\u001b[0m Trial 15 finished with value: 0.6479489562460083 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 83}. Best is trial 12 with value: 0.6974811656015723.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:54,684]\u001b[0m Trial 16 finished with value: 0.6976612104022542 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 85}. Best is trial 16 with value: 0.6976612104022542.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:55,780]\u001b[0m Trial 17 finished with value: 0.6570193260483483 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 82}. Best is trial 16 with value: 0.6976612104022542.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:56,883]\u001b[0m Trial 18 finished with value: 0.6051638366960485 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 16 with value: 0.6976612104022542.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:57,813]\u001b[0m Trial 19 finished with value: 0.6700729448213995 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 89}. Best is trial 16 with value: 0.6976612104022542.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:58,906]\u001b[0m Trial 20 finished with value: 0.6431178354661449 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 74}. Best is trial 16 with value: 0.6976612104022542.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:00:59,831]\u001b[0m Trial 21 finished with value: 0.6974811656015723 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 92}. Best is trial 16 with value: 0.6976612104022542.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:00,778]\u001b[0m Trial 22 finished with value: 0.6764530318812133 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 92}. Best is trial 16 with value: 0.6976612104022542.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:01,718]\u001b[0m Trial 23 finished with value: 0.5174910027613682 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 16 with value: 0.6976612104022542.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:02,662]\u001b[0m Trial 24 finished with value: 0.6974811656015723 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 90}. Best is trial 16 with value: 0.6976612104022542.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:03,771]\u001b[0m Trial 25 finished with value: 0.6431178354661449 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 79}. Best is trial 16 with value: 0.6976612104022542.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:04,694]\u001b[0m Trial 26 finished with value: 0.6700729448213995 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 89}. Best is trial 16 with value: 0.6976612104022542.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:05,622]\u001b[0m Trial 27 finished with value: 0.5865951365929396 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 87}. Best is trial 16 with value: 0.6976612104022542.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:06,719]\u001b[0m Trial 28 finished with value: 0.6630418739037454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 16 with value: 0.6976612104022542.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:07,655]\u001b[0m Trial 29 finished with value: 0.7049748869598941 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 29 with value: 0.7049748869598941.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:08,754]\u001b[0m Trial 30 finished with value: 0.6612504012413206 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 29 with value: 0.7049748869598941.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:09,683]\u001b[0m Trial 31 finished with value: 0.7049748869598941 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 29 with value: 0.7049748869598941.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:10,607]\u001b[0m Trial 32 finished with value: 0.6899804944975274 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 29 with value: 0.7049748869598941.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:11,538]\u001b[0m Trial 33 finished with value: 0.6899804944975274 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 29 with value: 0.7049748869598941.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:12,467]\u001b[0m Trial 34 finished with value: 0.6247887477133524 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 29 with value: 0.7049748869598941.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:01:13,404]\u001b[0m Trial 35 finished with value: 0.7049748869598941 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 29 with value: 0.7049748869598941.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:14,518]\u001b[0m Trial 36 finished with value: 0.5085910039210749 and parameters: {'n_neighbors': 22, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 29 with value: 0.7049748869598941.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:15,622]\u001b[0m Trial 37 finished with value: 0.6864557006496794 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 29 with value: 0.7049748869598941.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:16,732]\u001b[0m Trial 38 finished with value: 0.558968684602925 and parameters: {'n_neighbors': 17, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 29 with value: 0.7049748869598941.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:18,049]\u001b[0m Trial 39 finished with value: 0.696299150204432 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 29 with value: 0.7049748869598941.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:18,979]\u001b[0m Trial 40 finished with value: 0.6770089121822996 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 29 with value: 0.7049748869598941.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:20,086]\u001b[0m Trial 41 finished with value: 0.7051489113968382 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:21,182]\u001b[0m Trial 42 finished with value: 0.6958698398467901 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:22,286]\u001b[0m Trial 43 finished with value: 0.6890751586700501 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:23,215]\u001b[0m Trial 44 finished with value: 0.7049748869598941 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:24,147]\u001b[0m Trial 45 finished with value: 0.635089654529963 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:25,244]\u001b[0m Trial 46 finished with value: 0.6958698398467901 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:26,349]\u001b[0m Trial 47 finished with value: 0.6532830720412343 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:27,447]\u001b[0m Trial 48 finished with value: 0.6864557006496794 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:28,379]\u001b[0m Trial 49 finished with value: 0.6806940785717066 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.7051\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 80\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNregressor\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.681130\n",
      "1                    TP   39.000000\n",
      "2                    TN  304.000000\n",
      "3                    FP    9.000000\n",
      "4                    FN   30.000000\n",
      "5              Accuracy    0.897906\n",
      "6             Precision    0.812500\n",
      "7           Sensitivity    0.565217\n",
      "8           Specificity    0.971200\n",
      "9              F1 score    0.666667\n",
      "10  F1 score (weighted)    0.890400\n",
      "11     F1 score (macro)    0.803194\n",
      "12    Balanced Accuracy    0.768232\n",
      "13                  MCC    0.622649\n",
      "14                  NPV    0.910200\n",
      "15              ROC_AUC    0.768232\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_knn_0)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_knn_0_cat = np.where(((y_pred_knn_0 >= 2) | (y_pred_knn_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_knn_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:01:29,723]\u001b[0m Trial 50 finished with value: 0.6818884664642351 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 85}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:30,651]\u001b[0m Trial 51 finished with value: 0.7014031756160014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:31,582]\u001b[0m Trial 52 finished with value: 0.7014031756160014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:32,511]\u001b[0m Trial 53 finished with value: 0.7026190576271137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:33,439]\u001b[0m Trial 54 finished with value: 0.7026190576271137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:34,365]\u001b[0m Trial 55 finished with value: 0.6937969447816363 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:35,299]\u001b[0m Trial 56 finished with value: 0.7026190576271137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:36,235]\u001b[0m Trial 57 finished with value: 0.6897837671289191 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:37,346]\u001b[0m Trial 58 finished with value: 0.6818884664642351 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 82}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:38,275]\u001b[0m Trial 59 finished with value: 0.6778272970118593 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:39,204]\u001b[0m Trial 60 finished with value: 0.6332193760234028 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:40,128]\u001b[0m Trial 61 finished with value: 0.7026190576271137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:41,057]\u001b[0m Trial 62 finished with value: 0.7014031756160014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:41,987]\u001b[0m Trial 63 finished with value: 0.7026190576271137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:42,921]\u001b[0m Trial 64 finished with value: 0.6961427465153656 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:43,854]\u001b[0m Trial 65 finished with value: 0.7014031756160014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:44,959]\u001b[0m Trial 66 finished with value: 0.6939744984323317 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:45,888]\u001b[0m Trial 67 finished with value: 0.6733286053369041 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 91}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:46,823]\u001b[0m Trial 68 finished with value: 0.7026190576271137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:47,927]\u001b[0m Trial 69 finished with value: 0.47393033892687997 and parameters: {'n_neighbors': 29, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:48,860]\u001b[0m Trial 70 finished with value: 0.7014031756160014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:49,783]\u001b[0m Trial 71 finished with value: 0.7026190576271137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:50,715]\u001b[0m Trial 72 finished with value: 0.6961427465153656 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:51,643]\u001b[0m Trial 73 finished with value: 0.7026190576271137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:52,576]\u001b[0m Trial 74 finished with value: 0.7014031756160014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:53,522]\u001b[0m Trial 75 finished with value: 0.6937969447816363 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:54,619]\u001b[0m Trial 76 finished with value: 0.6169920129910956 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:55,722]\u001b[0m Trial 77 finished with value: 0.6950337908212572 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:56,659]\u001b[0m Trial 78 finished with value: 0.6834212986206228 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:57,603]\u001b[0m Trial 79 finished with value: 0.6924870696089842 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:58,540]\u001b[0m Trial 80 finished with value: 0.678851242928265 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:01:59,469]\u001b[0m Trial 81 finished with value: 0.7026190576271137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:00,403]\u001b[0m Trial 82 finished with value: 0.7026190576271137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:01,332]\u001b[0m Trial 83 finished with value: 0.6961427465153656 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:02,264]\u001b[0m Trial 84 finished with value: 0.7026190576271137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:02:03,199]\u001b[0m Trial 85 finished with value: 0.7014031756160014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:04,140]\u001b[0m Trial 86 finished with value: 0.6937969447816363 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:05,083]\u001b[0m Trial 87 finished with value: 0.6053629301623984 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:06,031]\u001b[0m Trial 88 finished with value: 0.6961427465153656 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:06,963]\u001b[0m Trial 89 finished with value: 0.6897837671289191 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:07,899]\u001b[0m Trial 90 finished with value: 0.7014031756160014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:08,831]\u001b[0m Trial 91 finished with value: 0.7026190576271137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:09,764]\u001b[0m Trial 92 finished with value: 0.7026190576271137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:10,703]\u001b[0m Trial 93 finished with value: 0.6961427465153656 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:11,635]\u001b[0m Trial 94 finished with value: 0.7014031756160014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:12,571]\u001b[0m Trial 95 finished with value: 0.7026190576271137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:13,507]\u001b[0m Trial 96 finished with value: 0.6937969447816363 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:14,439]\u001b[0m Trial 97 finished with value: 0.6749310857284798 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:15,550]\u001b[0m Trial 98 finished with value: 0.7013707418476514 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:16,482]\u001b[0m Trial 99 finished with value: 0.6188072878588938 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7051\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 80\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.681130    0.709181\n",
      "1                    TP   39.000000   44.000000\n",
      "2                    TN  304.000000  301.000000\n",
      "3                    FP    9.000000   13.000000\n",
      "4                    FN   30.000000   24.000000\n",
      "5              Accuracy    0.897906    0.903141\n",
      "6             Precision    0.812500    0.771930\n",
      "7           Sensitivity    0.565217    0.647059\n",
      "8           Specificity    0.971200    0.958600\n",
      "9              F1 score    0.666667    0.704000\n",
      "10  F1 score (weighted)    0.890400    0.899713\n",
      "11     F1 score (macro)    0.803194    0.823049\n",
      "12    Balanced Accuracy    0.768232    0.802829\n",
      "13                  MCC    0.622649    0.650230\n",
      "14                  NPV    0.910200    0.926200\n",
      "15              ROC_AUC    0.768232    0.802829\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_knn_1)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet1_cat = np.where(((Y_testSet1>=2) | (Y_testSet1<=-2)), 1, 0) \n",
    "y_pred_knn_1_cat = np.where(((y_pred_knn_1 >= 2) | (y_pred_knn_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_knn_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:02:18,006]\u001b[0m Trial 100 finished with value: 0.6814126537788354 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:18,939]\u001b[0m Trial 101 finished with value: 0.6835151385985576 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:19,868]\u001b[0m Trial 102 finished with value: 0.6843715746624659 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:20,803]\u001b[0m Trial 103 finished with value: 0.6835151385985576 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:21,734]\u001b[0m Trial 104 finished with value: 0.6753967138747214 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:22,663]\u001b[0m Trial 105 finished with value: 0.681520363987518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:23,591]\u001b[0m Trial 106 finished with value: 0.6843715746624659 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:24,686]\u001b[0m Trial 107 finished with value: 0.6740247091705298 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 64}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:25,619]\u001b[0m Trial 108 finished with value: 0.6843715746624659 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:26,550]\u001b[0m Trial 109 finished with value: 0.681520363987518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:27,483]\u001b[0m Trial 110 finished with value: 0.6835151385985576 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:28,423]\u001b[0m Trial 111 finished with value: 0.6835151385985576 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:29,361]\u001b[0m Trial 112 finished with value: 0.6843715746624659 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:30,288]\u001b[0m Trial 113 finished with value: 0.6080474680576586 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:31,391]\u001b[0m Trial 114 finished with value: 0.6755625863287376 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:32,321]\u001b[0m Trial 115 finished with value: 0.681520363987518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:33,256]\u001b[0m Trial 116 finished with value: 0.6489878352473407 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:34,350]\u001b[0m Trial 117 finished with value: 0.6740247091705298 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 85}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:35,284]\u001b[0m Trial 118 finished with value: 0.6843715746624659 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:36,219]\u001b[0m Trial 119 finished with value: 0.6705307673123658 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:37,148]\u001b[0m Trial 120 finished with value: 0.681520363987518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:38,078]\u001b[0m Trial 121 finished with value: 0.6835151385985576 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:39,004]\u001b[0m Trial 122 finished with value: 0.6843715746624659 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:39,935]\u001b[0m Trial 123 finished with value: 0.6835151385985576 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:40,870]\u001b[0m Trial 124 finished with value: 0.6843715746624659 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:41,804]\u001b[0m Trial 125 finished with value: 0.681520363987518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:42,742]\u001b[0m Trial 126 finished with value: 0.6835151385985576 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:43,670]\u001b[0m Trial 127 finished with value: 0.6587898204427683 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:44,599]\u001b[0m Trial 128 finished with value: 0.6843715746624659 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:45,535]\u001b[0m Trial 129 finished with value: 0.681520363987518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:46,470]\u001b[0m Trial 130 finished with value: 0.6489878352473407 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:47,403]\u001b[0m Trial 131 finished with value: 0.6835151385985576 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:48,332]\u001b[0m Trial 132 finished with value: 0.6843715746624659 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:49,271]\u001b[0m Trial 133 finished with value: 0.6835151385985576 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:50,202]\u001b[0m Trial 134 finished with value: 0.6843715746624659 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:02:51,138]\u001b[0m Trial 135 finished with value: 0.681520363987518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:52,072]\u001b[0m Trial 136 finished with value: 0.6426128467145649 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:53,008]\u001b[0m Trial 137 finished with value: 0.6753967138747214 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:53,936]\u001b[0m Trial 138 finished with value: 0.6754002530857959 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 92}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:54,866]\u001b[0m Trial 139 finished with value: 0.6835151385985576 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:55,793]\u001b[0m Trial 140 finished with value: 0.6835151385985576 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:56,723]\u001b[0m Trial 141 finished with value: 0.6835151385985576 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:57,647]\u001b[0m Trial 142 finished with value: 0.681520363987518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:58,574]\u001b[0m Trial 143 finished with value: 0.6843715746624659 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:02:59,504]\u001b[0m Trial 144 finished with value: 0.681520363987518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:00,432]\u001b[0m Trial 145 finished with value: 0.6843715746624659 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:01,367]\u001b[0m Trial 146 finished with value: 0.6835151385985576 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:02,308]\u001b[0m Trial 147 finished with value: 0.6835151385985576 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:03,414]\u001b[0m Trial 148 finished with value: 0.6755625863287376 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:04,730]\u001b[0m Trial 149 finished with value: 0.6814126537788354 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7051\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 80\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.681130    0.709181    0.748912\n",
      "1                    TP   39.000000   44.000000   41.000000\n",
      "2                    TN  304.000000  301.000000  301.000000\n",
      "3                    FP    9.000000   13.000000   13.000000\n",
      "4                    FN   30.000000   24.000000   27.000000\n",
      "5              Accuracy    0.897906    0.903141    0.895288\n",
      "6             Precision    0.812500    0.771930    0.759259\n",
      "7           Sensitivity    0.565217    0.647059    0.602941\n",
      "8           Specificity    0.971200    0.958600    0.958600\n",
      "9              F1 score    0.666667    0.704000    0.672131\n",
      "10  F1 score (weighted)    0.890400    0.899713    0.890422\n",
      "11     F1 score (macro)    0.803194    0.823049    0.804913\n",
      "12    Balanced Accuracy    0.768232    0.802829    0.780770\n",
      "13                  MCC    0.622649    0.650230    0.616547\n",
      "14                  NPV    0.910200    0.926200    0.917700\n",
      "15              ROC_AUC    0.768232    0.802829    0.780770\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_knn_2)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet2_cat = np.where(((Y_testSet2>=2) | (Y_testSet2<=-2)), 1, 0) \n",
    "y_pred_knn_2_cat = np.where(((y_pred_knn_2 >= 2) | (y_pred_knn_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_knn_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:03:05,879]\u001b[0m Trial 150 finished with value: 0.6923289726330768 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:06,820]\u001b[0m Trial 151 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:07,754]\u001b[0m Trial 152 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:08,873]\u001b[0m Trial 153 finished with value: 0.7005097982407589 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:09,812]\u001b[0m Trial 154 finished with value: 0.6974860648978721 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:10,751]\u001b[0m Trial 155 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:11,692]\u001b[0m Trial 156 finished with value: 0.7005391119132771 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:12,627]\u001b[0m Trial 157 finished with value: 0.6786302980041203 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:13,572]\u001b[0m Trial 158 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:14,510]\u001b[0m Trial 159 finished with value: 0.6974860648978721 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:15,448]\u001b[0m Trial 160 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:16,382]\u001b[0m Trial 161 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:17,321]\u001b[0m Trial 162 finished with value: 0.7005391119132771 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:18,264]\u001b[0m Trial 163 finished with value: 0.5746615773627248 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:19,215]\u001b[0m Trial 164 finished with value: 0.6974860648978721 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:20,155]\u001b[0m Trial 165 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:21,255]\u001b[0m Trial 166 finished with value: 0.6921698134450318 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 82}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:22,188]\u001b[0m Trial 167 finished with value: 0.6974860648978721 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:23,119]\u001b[0m Trial 168 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:24,052]\u001b[0m Trial 169 finished with value: 0.5907708396881642 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:24,983]\u001b[0m Trial 170 finished with value: 0.7005391119132771 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:25,915]\u001b[0m Trial 171 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:26,843]\u001b[0m Trial 172 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:27,772]\u001b[0m Trial 173 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:28,698]\u001b[0m Trial 174 finished with value: 0.7005391119132771 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:29,630]\u001b[0m Trial 175 finished with value: 0.6974860648978721 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:30,571]\u001b[0m Trial 176 finished with value: 0.6632368222853665 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:31,879]\u001b[0m Trial 177 finished with value: 0.7010312601439402 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:32,812]\u001b[0m Trial 178 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:33,742]\u001b[0m Trial 179 finished with value: 0.6928768323286494 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:34,671]\u001b[0m Trial 180 finished with value: 0.6786302980041203 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:35,603]\u001b[0m Trial 181 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:36,527]\u001b[0m Trial 182 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:37,625]\u001b[0m Trial 183 finished with value: 0.6982127621392582 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:38,551]\u001b[0m Trial 184 finished with value: 0.6923289726330768 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:03:39,479]\u001b[0m Trial 185 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:40,579]\u001b[0m Trial 186 finished with value: 0.6982127621392582 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:41,506]\u001b[0m Trial 187 finished with value: 0.6378705675599932 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:42,434]\u001b[0m Trial 188 finished with value: 0.7005391119132771 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:43,358]\u001b[0m Trial 189 finished with value: 0.6974860648978721 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:44,292]\u001b[0m Trial 190 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:45,233]\u001b[0m Trial 191 finished with value: 0.7005391119132771 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:46,332]\u001b[0m Trial 192 finished with value: 0.6990369273325702 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:47,265]\u001b[0m Trial 193 finished with value: 0.6204058489345693 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:48,195]\u001b[0m Trial 194 finished with value: 0.7005391119132771 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:49,127]\u001b[0m Trial 195 finished with value: 0.7005391119132771 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:50,059]\u001b[0m Trial 196 finished with value: 0.6893822059325265 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:50,997]\u001b[0m Trial 197 finished with value: 0.7005391119132771 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:51,926]\u001b[0m Trial 198 finished with value: 0.6996942981253647 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:52,858]\u001b[0m Trial 199 finished with value: 0.6974860648978721 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7051\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 80\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.681130    0.709181    0.748912    0.678639\n",
      "1                    TP   39.000000   44.000000   41.000000   45.000000\n",
      "2                    TN  304.000000  301.000000  301.000000  290.000000\n",
      "3                    FP    9.000000   13.000000   13.000000   25.000000\n",
      "4                    FN   30.000000   24.000000   27.000000   22.000000\n",
      "5              Accuracy    0.897906    0.903141    0.895288    0.876963\n",
      "6             Precision    0.812500    0.771930    0.759259    0.642857\n",
      "7           Sensitivity    0.565217    0.647059    0.602941    0.671642\n",
      "8           Specificity    0.971200    0.958600    0.958600    0.920600\n",
      "9              F1 score    0.666667    0.704000    0.672131    0.656934\n",
      "10  F1 score (weighted)    0.890400    0.899713    0.890422    0.878016\n",
      "11     F1 score (macro)    0.803194    0.823049    0.804913    0.790987\n",
      "12    Balanced Accuracy    0.768232    0.802829    0.780770    0.796138\n",
      "13                  MCC    0.622649    0.650230    0.616547    0.582225\n",
      "14                  NPV    0.910200    0.926200    0.917700    0.929500\n",
      "15              ROC_AUC    0.768232    0.802829    0.780770    0.796138\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_knn_3)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet3_cat = np.where(((Y_testSet3>=2) | (Y_testSet3<=-2)), 1, 0) \n",
    "y_pred_knn_3_cat = np.where(((y_pred_knn_3 >= 2) | (y_pred_knn_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_knn_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:03:54,010]\u001b[0m Trial 200 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:54,943]\u001b[0m Trial 201 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:55,887]\u001b[0m Trial 202 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:56,819]\u001b[0m Trial 203 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:57,754]\u001b[0m Trial 204 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:58,678]\u001b[0m Trial 205 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:03:59,606]\u001b[0m Trial 206 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:00,538]\u001b[0m Trial 207 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:01,471]\u001b[0m Trial 208 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:02,411]\u001b[0m Trial 209 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:03,354]\u001b[0m Trial 210 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:04,283]\u001b[0m Trial 211 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:05,214]\u001b[0m Trial 212 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:06,148]\u001b[0m Trial 213 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:07,078]\u001b[0m Trial 214 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:08,011]\u001b[0m Trial 215 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:08,944]\u001b[0m Trial 216 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:09,873]\u001b[0m Trial 217 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:10,800]\u001b[0m Trial 218 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:11,731]\u001b[0m Trial 219 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:12,666]\u001b[0m Trial 220 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:13,592]\u001b[0m Trial 221 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:14,519]\u001b[0m Trial 222 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:15,450]\u001b[0m Trial 223 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:16,382]\u001b[0m Trial 224 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:17,320]\u001b[0m Trial 225 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:18,254]\u001b[0m Trial 226 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:19,185]\u001b[0m Trial 227 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:20,116]\u001b[0m Trial 228 finished with value: 0.6860878713335437 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:21,058]\u001b[0m Trial 229 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:21,993]\u001b[0m Trial 230 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:22,922]\u001b[0m Trial 231 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:23,851]\u001b[0m Trial 232 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:24,779]\u001b[0m Trial 233 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:25,706]\u001b[0m Trial 234 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:04:26,634]\u001b[0m Trial 235 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:27,566]\u001b[0m Trial 236 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:28,498]\u001b[0m Trial 237 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:29,426]\u001b[0m Trial 238 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:30,359]\u001b[0m Trial 239 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:31,291]\u001b[0m Trial 240 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:32,222]\u001b[0m Trial 241 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:33,172]\u001b[0m Trial 242 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:34,114]\u001b[0m Trial 243 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:35,047]\u001b[0m Trial 244 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:35,982]\u001b[0m Trial 245 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:36,915]\u001b[0m Trial 246 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:37,851]\u001b[0m Trial 247 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:38,783]\u001b[0m Trial 248 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:39,719]\u001b[0m Trial 249 finished with value: 0.7049024704177289 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7051\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 80\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681130    0.709181    0.748912    0.678639   \n",
      "1                    TP   39.000000   44.000000   41.000000   45.000000   \n",
      "2                    TN  304.000000  301.000000  301.000000  290.000000   \n",
      "3                    FP    9.000000   13.000000   13.000000   25.000000   \n",
      "4                    FN   30.000000   24.000000   27.000000   22.000000   \n",
      "5              Accuracy    0.897906    0.903141    0.895288    0.876963   \n",
      "6             Precision    0.812500    0.771930    0.759259    0.642857   \n",
      "7           Sensitivity    0.565217    0.647059    0.602941    0.671642   \n",
      "8           Specificity    0.971200    0.958600    0.958600    0.920600   \n",
      "9              F1 score    0.666667    0.704000    0.672131    0.656934   \n",
      "10  F1 score (weighted)    0.890400    0.899713    0.890422    0.878016   \n",
      "11     F1 score (macro)    0.803194    0.823049    0.804913    0.790987   \n",
      "12    Balanced Accuracy    0.768232    0.802829    0.780770    0.796138   \n",
      "13                  MCC    0.622649    0.650230    0.616547    0.582225   \n",
      "14                  NPV    0.910200    0.926200    0.917700    0.929500   \n",
      "15              ROC_AUC    0.768232    0.802829    0.780770    0.796138   \n",
      "\n",
      "          Set4  \n",
      "0     0.663838  \n",
      "1    40.000000  \n",
      "2   303.000000  \n",
      "3    12.000000  \n",
      "4    27.000000  \n",
      "5     0.897906  \n",
      "6     0.769231  \n",
      "7     0.597015  \n",
      "8     0.961900  \n",
      "9     0.672269  \n",
      "10    0.892658  \n",
      "11    0.805902  \n",
      "12    0.779460  \n",
      "13    0.619845  \n",
      "14    0.918200  \n",
      "15    0.779460  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_knn_4)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet4_cat = np.where(((Y_testSet4>=2) | (Y_testSet4<=-2)), 1, 0) \n",
    "y_pred_knn_4_cat = np.where(((y_pred_knn_4 >= 2) | (y_pred_knn_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_knn_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:04:40,887]\u001b[0m Trial 250 finished with value: 0.6965391348255034 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:41,815]\u001b[0m Trial 251 finished with value: 0.6930175471937943 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:42,751]\u001b[0m Trial 252 finished with value: 0.6965391348255034 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:43,683]\u001b[0m Trial 253 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:44,620]\u001b[0m Trial 254 finished with value: 0.6965391348255034 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:45,548]\u001b[0m Trial 255 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:46,482]\u001b[0m Trial 256 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:47,423]\u001b[0m Trial 257 finished with value: 0.6808967876070828 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:48,354]\u001b[0m Trial 258 finished with value: 0.6965391348255034 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:49,283]\u001b[0m Trial 259 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:50,214]\u001b[0m Trial 260 finished with value: 0.6930175471937943 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:51,319]\u001b[0m Trial 261 finished with value: 0.698321366828345 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:52,258]\u001b[0m Trial 262 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:53,194]\u001b[0m Trial 263 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:54,132]\u001b[0m Trial 264 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:55,075]\u001b[0m Trial 265 finished with value: 0.6965391348255034 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:56,017]\u001b[0m Trial 266 finished with value: 0.6545060739897085 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:56,959]\u001b[0m Trial 267 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:57,896]\u001b[0m Trial 268 finished with value: 0.6965391348255034 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:58,837]\u001b[0m Trial 269 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:04:59,944]\u001b[0m Trial 270 finished with value: 0.6753740700919832 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:00,880]\u001b[0m Trial 271 finished with value: 0.6965391348255034 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:01,821]\u001b[0m Trial 272 finished with value: 0.6191176706675285 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:02,758]\u001b[0m Trial 273 finished with value: 0.6930175471937943 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:03,695]\u001b[0m Trial 274 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:04,636]\u001b[0m Trial 275 finished with value: 0.6965391348255034 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:05,577]\u001b[0m Trial 276 finished with value: 0.6872746795406409 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:06,879]\u001b[0m Trial 277 finished with value: 0.6975837192509875 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:07,814]\u001b[0m Trial 278 finished with value: 0.6965391348255034 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:08,752]\u001b[0m Trial 279 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:09,694]\u001b[0m Trial 280 finished with value: 0.6965391348255034 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:10,636]\u001b[0m Trial 281 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:11,582]\u001b[0m Trial 282 finished with value: 0.6965391348255034 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:12,515]\u001b[0m Trial 283 finished with value: 0.6930175471937943 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:13,460]\u001b[0m Trial 284 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:05:14,404]\u001b[0m Trial 285 finished with value: 0.6965391348255034 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:15,347]\u001b[0m Trial 286 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:16,289]\u001b[0m Trial 287 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:17,231]\u001b[0m Trial 288 finished with value: 0.6965391348255034 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:18,167]\u001b[0m Trial 289 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:19,097]\u001b[0m Trial 290 finished with value: 0.6414270882005977 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:20,035]\u001b[0m Trial 291 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:21,126]\u001b[0m Trial 292 finished with value: 0.698321366828345 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:22,047]\u001b[0m Trial 293 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:22,977]\u001b[0m Trial 294 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:23,899]\u001b[0m Trial 295 finished with value: 0.6892427088603593 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:24,836]\u001b[0m Trial 296 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:25,922]\u001b[0m Trial 297 finished with value: 0.6826272179106204 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:26,857]\u001b[0m Trial 298 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:27,779]\u001b[0m Trial 299 finished with value: 0.6983421636907978 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7051\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 80\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681130    0.709181    0.748912    0.678639   \n",
      "1                    TP   39.000000   44.000000   41.000000   45.000000   \n",
      "2                    TN  304.000000  301.000000  301.000000  290.000000   \n",
      "3                    FP    9.000000   13.000000   13.000000   25.000000   \n",
      "4                    FN   30.000000   24.000000   27.000000   22.000000   \n",
      "5              Accuracy    0.897906    0.903141    0.895288    0.876963   \n",
      "6             Precision    0.812500    0.771930    0.759259    0.642857   \n",
      "7           Sensitivity    0.565217    0.647059    0.602941    0.671642   \n",
      "8           Specificity    0.971200    0.958600    0.958600    0.920600   \n",
      "9              F1 score    0.666667    0.704000    0.672131    0.656934   \n",
      "10  F1 score (weighted)    0.890400    0.899713    0.890422    0.878016   \n",
      "11     F1 score (macro)    0.803194    0.823049    0.804913    0.790987   \n",
      "12    Balanced Accuracy    0.768232    0.802829    0.780770    0.796138   \n",
      "13                  MCC    0.622649    0.650230    0.616547    0.582225   \n",
      "14                  NPV    0.910200    0.926200    0.917700    0.929500   \n",
      "15              ROC_AUC    0.768232    0.802829    0.780770    0.796138   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.663838    0.685186  \n",
      "1    40.000000   43.000000  \n",
      "2   303.000000  301.000000  \n",
      "3    12.000000   11.000000  \n",
      "4    27.000000   27.000000  \n",
      "5     0.897906    0.900524  \n",
      "6     0.769231    0.796296  \n",
      "7     0.597015    0.614286  \n",
      "8     0.961900    0.964700  \n",
      "9     0.672269    0.693548  \n",
      "10    0.892658    0.895349  \n",
      "11    0.805902    0.817087  \n",
      "12    0.779460    0.789515  \n",
      "13    0.619845    0.642973  \n",
      "14    0.918200    0.917700  \n",
      "15    0.779460    0.789515  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_knn_5)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet5_cat = np.where(((Y_testSet5>=2) | (Y_testSet5<=-2)), 1, 0) \n",
    "y_pred_knn_5_cat = np.where(((y_pred_knn_5 >= 2) | (y_pred_knn_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_knn_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:05:29,088]\u001b[0m Trial 300 finished with value: 0.6783702113965304 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:30,018]\u001b[0m Trial 301 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:30,950]\u001b[0m Trial 302 finished with value: 0.6785840182699687 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:31,875]\u001b[0m Trial 303 finished with value: 0.5627884001389457 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:32,802]\u001b[0m Trial 304 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:33,739]\u001b[0m Trial 305 finished with value: 0.6685416161011547 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:34,675]\u001b[0m Trial 306 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:35,613]\u001b[0m Trial 307 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:36,551]\u001b[0m Trial 308 finished with value: 0.6785840182699687 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:37,483]\u001b[0m Trial 309 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:38,426]\u001b[0m Trial 310 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:39,363]\u001b[0m Trial 311 finished with value: 0.6785840182699687 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:40,299]\u001b[0m Trial 312 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:41,239]\u001b[0m Trial 313 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:42,172]\u001b[0m Trial 314 finished with value: 0.6785840182699687 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:43,110]\u001b[0m Trial 315 finished with value: 0.648936929894046 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:44,048]\u001b[0m Trial 316 finished with value: 0.5763733852462181 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:44,987]\u001b[0m Trial 317 finished with value: 0.6721505596892559 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:45,915]\u001b[0m Trial 318 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:46,851]\u001b[0m Trial 319 finished with value: 0.6785840182699687 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:47,786]\u001b[0m Trial 320 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:48,726]\u001b[0m Trial 321 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:49,840]\u001b[0m Trial 322 finished with value: 0.6282682412004239 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:50,776]\u001b[0m Trial 323 finished with value: 0.6785840182699687 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:51,704]\u001b[0m Trial 324 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:52,815]\u001b[0m Trial 325 finished with value: 0.680208541712014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:53,755]\u001b[0m Trial 326 finished with value: 0.6785840182699687 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:54,684]\u001b[0m Trial 327 finished with value: 0.6685416161011547 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:55,619]\u001b[0m Trial 328 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:56,722]\u001b[0m Trial 329 finished with value: 0.680208541712014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:57,659]\u001b[0m Trial 330 finished with value: 0.6049345969490512 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:58,604]\u001b[0m Trial 331 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:05:59,539]\u001b[0m Trial 332 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:00,483]\u001b[0m Trial 333 finished with value: 0.6785840182699687 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:01,416]\u001b[0m Trial 334 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:06:02,355]\u001b[0m Trial 335 finished with value: 0.648936929894046 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:03,296]\u001b[0m Trial 336 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:04,226]\u001b[0m Trial 337 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:05,158]\u001b[0m Trial 338 finished with value: 0.6785840182699687 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:06,087]\u001b[0m Trial 339 finished with value: 0.6721505596892559 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:07,018]\u001b[0m Trial 340 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:07,946]\u001b[0m Trial 341 finished with value: 0.6785840182699687 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:08,889]\u001b[0m Trial 342 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:09,822]\u001b[0m Trial 343 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:10,759]\u001b[0m Trial 344 finished with value: 0.6785840182699687 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:11,691]\u001b[0m Trial 345 finished with value: 0.6806830036795903 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:12,623]\u001b[0m Trial 346 finished with value: 0.6785840182699687 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:13,554]\u001b[0m Trial 347 finished with value: 0.6447721501340549 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:14,488]\u001b[0m Trial 348 finished with value: 0.6785840182699687 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:15,416]\u001b[0m Trial 349 finished with value: 0.6721505596892559 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7051\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 80\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681130    0.709181    0.748912    0.678639   \n",
      "1                    TP   39.000000   44.000000   41.000000   45.000000   \n",
      "2                    TN  304.000000  301.000000  301.000000  290.000000   \n",
      "3                    FP    9.000000   13.000000   13.000000   25.000000   \n",
      "4                    FN   30.000000   24.000000   27.000000   22.000000   \n",
      "5              Accuracy    0.897906    0.903141    0.895288    0.876963   \n",
      "6             Precision    0.812500    0.771930    0.759259    0.642857   \n",
      "7           Sensitivity    0.565217    0.647059    0.602941    0.671642   \n",
      "8           Specificity    0.971200    0.958600    0.958600    0.920600   \n",
      "9              F1 score    0.666667    0.704000    0.672131    0.656934   \n",
      "10  F1 score (weighted)    0.890400    0.899713    0.890422    0.878016   \n",
      "11     F1 score (macro)    0.803194    0.823049    0.804913    0.790987   \n",
      "12    Balanced Accuracy    0.768232    0.802829    0.780770    0.796138   \n",
      "13                  MCC    0.622649    0.650230    0.616547    0.582225   \n",
      "14                  NPV    0.910200    0.926200    0.917700    0.929500   \n",
      "15              ROC_AUC    0.768232    0.802829    0.780770    0.796138   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.663838    0.685186    0.719665  \n",
      "1    40.000000   43.000000   37.000000  \n",
      "2   303.000000  301.000000  308.000000  \n",
      "3    12.000000   11.000000    7.000000  \n",
      "4    27.000000   27.000000   30.000000  \n",
      "5     0.897906    0.900524    0.903141  \n",
      "6     0.769231    0.796296    0.840909  \n",
      "7     0.597015    0.614286    0.552239  \n",
      "8     0.961900    0.964700    0.977800  \n",
      "9     0.672269    0.693548    0.666667  \n",
      "10    0.892658    0.895349    0.894812  \n",
      "11    0.805902    0.817087    0.805003  \n",
      "12    0.779460    0.789515    0.765008  \n",
      "13    0.619845    0.642973    0.631390  \n",
      "14    0.918200    0.917700    0.911200  \n",
      "15    0.779460    0.789515    0.765008  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_knn_6)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet6_cat = np.where(((Y_testSet6>=2) | (Y_testSet6<=-2)), 1, 0) \n",
    "y_pred_knn_6_cat = np.where(((y_pred_knn_6 >= 2) | (y_pred_knn_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_knn_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:06:16,574]\u001b[0m Trial 350 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:17,507]\u001b[0m Trial 351 finished with value: 0.695667662741496 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:18,608]\u001b[0m Trial 352 finished with value: 0.7012654097795504 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:19,535]\u001b[0m Trial 353 finished with value: 0.6149466593704113 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:20,644]\u001b[0m Trial 354 finished with value: 0.6988581026180031 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:21,586]\u001b[0m Trial 355 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:22,520]\u001b[0m Trial 356 finished with value: 0.698673177293386 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:23,624]\u001b[0m Trial 357 finished with value: 0.7012654097795504 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:24,566]\u001b[0m Trial 358 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:25,511]\u001b[0m Trial 359 finished with value: 0.698673177293386 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:26,451]\u001b[0m Trial 360 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:27,398]\u001b[0m Trial 361 finished with value: 0.698673177293386 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:28,326]\u001b[0m Trial 362 finished with value: 0.6944263268365931 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:29,270]\u001b[0m Trial 363 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:30,207]\u001b[0m Trial 364 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:31,144]\u001b[0m Trial 365 finished with value: 0.698673177293386 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:32,737]\u001b[0m Trial 366 finished with value: 0.6984920918563274 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:33,666]\u001b[0m Trial 367 finished with value: 0.695667662741496 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:34,789]\u001b[0m Trial 368 finished with value: 0.7012654097795504 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:35,763]\u001b[0m Trial 369 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:36,740]\u001b[0m Trial 370 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:37,718]\u001b[0m Trial 371 finished with value: 0.6890257765600516 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:38,695]\u001b[0m Trial 372 finished with value: 0.6706703731499527 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:39,677]\u001b[0m Trial 373 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:40,649]\u001b[0m Trial 374 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:41,633]\u001b[0m Trial 375 finished with value: 0.698673177293386 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:42,608]\u001b[0m Trial 376 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:43,581]\u001b[0m Trial 377 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:44,699]\u001b[0m Trial 378 finished with value: 0.6988581026180031 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:45,627]\u001b[0m Trial 379 finished with value: 0.6861726927590497 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:46,555]\u001b[0m Trial 380 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:47,489]\u001b[0m Trial 381 finished with value: 0.695667662741496 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:48,608]\u001b[0m Trial 382 finished with value: 0.7012654097795504 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:49,542]\u001b[0m Trial 383 finished with value: 0.698673177293386 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:50,467]\u001b[0m Trial 384 finished with value: 0.6944263268365931 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:06:51,407]\u001b[0m Trial 385 finished with value: 0.698673177293386 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:52,371]\u001b[0m Trial 386 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:53,305]\u001b[0m Trial 387 finished with value: 0.6656034352380242 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:54,251]\u001b[0m Trial 388 finished with value: 0.698673177293386 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:55,182]\u001b[0m Trial 389 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:56,116]\u001b[0m Trial 390 finished with value: 0.695667662741496 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:57,052]\u001b[0m Trial 391 finished with value: 0.6808110652185511 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:57,983]\u001b[0m Trial 392 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:58,919]\u001b[0m Trial 393 finished with value: 0.6093373624947047 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:06:59,848]\u001b[0m Trial 394 finished with value: 0.6890257765600516 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:00,778]\u001b[0m Trial 395 finished with value: 0.595046808516226 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:01,882]\u001b[0m Trial 396 finished with value: 0.6988581026180031 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:02,814]\u001b[0m Trial 397 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:03,755]\u001b[0m Trial 398 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:04,691]\u001b[0m Trial 399 finished with value: 0.7005737681105396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7051\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 80\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681130    0.709181    0.748912    0.678639   \n",
      "1                    TP   39.000000   44.000000   41.000000   45.000000   \n",
      "2                    TN  304.000000  301.000000  301.000000  290.000000   \n",
      "3                    FP    9.000000   13.000000   13.000000   25.000000   \n",
      "4                    FN   30.000000   24.000000   27.000000   22.000000   \n",
      "5              Accuracy    0.897906    0.903141    0.895288    0.876963   \n",
      "6             Precision    0.812500    0.771930    0.759259    0.642857   \n",
      "7           Sensitivity    0.565217    0.647059    0.602941    0.671642   \n",
      "8           Specificity    0.971200    0.958600    0.958600    0.920600   \n",
      "9              F1 score    0.666667    0.704000    0.672131    0.656934   \n",
      "10  F1 score (weighted)    0.890400    0.899713    0.890422    0.878016   \n",
      "11     F1 score (macro)    0.803194    0.823049    0.804913    0.790987   \n",
      "12    Balanced Accuracy    0.768232    0.802829    0.780770    0.796138   \n",
      "13                  MCC    0.622649    0.650230    0.616547    0.582225   \n",
      "14                  NPV    0.910200    0.926200    0.917700    0.929500   \n",
      "15              ROC_AUC    0.768232    0.802829    0.780770    0.796138   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.663838    0.685186    0.719665    0.701116  \n",
      "1    40.000000   43.000000   37.000000   38.000000  \n",
      "2   303.000000  301.000000  308.000000  302.000000  \n",
      "3    12.000000   11.000000    7.000000   12.000000  \n",
      "4    27.000000   27.000000   30.000000   30.000000  \n",
      "5     0.897906    0.900524    0.903141    0.890052  \n",
      "6     0.769231    0.796296    0.840909    0.760000  \n",
      "7     0.597015    0.614286    0.552239    0.558824  \n",
      "8     0.961900    0.964700    0.977800    0.961800  \n",
      "9     0.672269    0.693548    0.666667    0.644068  \n",
      "10    0.892658    0.895349    0.894812    0.883198  \n",
      "11    0.805902    0.817087    0.805003    0.789526  \n",
      "12    0.779460    0.789515    0.765008    0.760303  \n",
      "13    0.619845    0.642973    0.631390    0.590439  \n",
      "14    0.918200    0.917700    0.911200    0.909600  \n",
      "15    0.779460    0.789515    0.765008    0.760303  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_knn_7)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet7_cat = np.where(((Y_testSet7>=2) | (Y_testSet7<=-2)), 1, 0) \n",
    "y_pred_knn_7_cat = np.where(((y_pred_knn_7 >= 2) | (y_pred_knn_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_knn_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:07:05,846]\u001b[0m Trial 400 finished with value: 0.6942959054074137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:06,779]\u001b[0m Trial 401 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:07,714]\u001b[0m Trial 402 finished with value: 0.6942959054074137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:08,645]\u001b[0m Trial 403 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:09,584]\u001b[0m Trial 404 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:10,519]\u001b[0m Trial 405 finished with value: 0.6942959054074137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:11,446]\u001b[0m Trial 406 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:12,374]\u001b[0m Trial 407 finished with value: 0.691239020399601 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:13,310]\u001b[0m Trial 408 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:14,238]\u001b[0m Trial 409 finished with value: 0.6843816257699334 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:15,166]\u001b[0m Trial 410 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:16,091]\u001b[0m Trial 411 finished with value: 0.6771647751081125 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:17,017]\u001b[0m Trial 412 finished with value: 0.6942959054074137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:18,110]\u001b[0m Trial 413 finished with value: 0.6988902745648337 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:19,043]\u001b[0m Trial 414 finished with value: 0.691239020399601 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:19,970]\u001b[0m Trial 415 finished with value: 0.6294678896048771 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:20,903]\u001b[0m Trial 416 finished with value: 0.6843816257699334 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:21,839]\u001b[0m Trial 417 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:22,762]\u001b[0m Trial 418 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:23,691]\u001b[0m Trial 419 finished with value: 0.6942959054074137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:24,619]\u001b[0m Trial 420 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:25,543]\u001b[0m Trial 421 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:26,478]\u001b[0m Trial 422 finished with value: 0.6942959054074137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:27,421]\u001b[0m Trial 423 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:28,361]\u001b[0m Trial 424 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:29,462]\u001b[0m Trial 425 finished with value: 0.6912113991448539 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:30,774]\u001b[0m Trial 426 finished with value: 0.693605004370004 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:31,703]\u001b[0m Trial 427 finished with value: 0.6711128031184856 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:32,628]\u001b[0m Trial 428 finished with value: 0.6843816257699334 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:33,554]\u001b[0m Trial 429 finished with value: 0.6771647751081125 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:34,483]\u001b[0m Trial 430 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:35,414]\u001b[0m Trial 431 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:36,337]\u001b[0m Trial 432 finished with value: 0.6942959054074137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:37,263]\u001b[0m Trial 433 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:38,553]\u001b[0m Trial 434 finished with value: 0.693605004370004 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:07:39,486]\u001b[0m Trial 435 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:40,419]\u001b[0m Trial 436 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:41,354]\u001b[0m Trial 437 finished with value: 0.6453392589548249 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:42,299]\u001b[0m Trial 438 finished with value: 0.6942959054074137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:43,238]\u001b[0m Trial 439 finished with value: 0.6885692166510478 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:44,175]\u001b[0m Trial 440 finished with value: 0.6942959054074137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:45,106]\u001b[0m Trial 441 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:46,039]\u001b[0m Trial 442 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:46,991]\u001b[0m Trial 443 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:47,964]\u001b[0m Trial 444 finished with value: 0.5946752777518415 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:49,082]\u001b[0m Trial 445 finished with value: 0.6016693332043259 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:50,035]\u001b[0m Trial 446 finished with value: 0.6942959054074137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:50,994]\u001b[0m Trial 447 finished with value: 0.6967874653190076 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:51,958]\u001b[0m Trial 448 finished with value: 0.6942959054074137 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:52,919]\u001b[0m Trial 449 finished with value: 0.6654366924890744 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7051\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 80\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681130    0.709181    0.748912    0.678639   \n",
      "1                    TP   39.000000   44.000000   41.000000   45.000000   \n",
      "2                    TN  304.000000  301.000000  301.000000  290.000000   \n",
      "3                    FP    9.000000   13.000000   13.000000   25.000000   \n",
      "4                    FN   30.000000   24.000000   27.000000   22.000000   \n",
      "5              Accuracy    0.897906    0.903141    0.895288    0.876963   \n",
      "6             Precision    0.812500    0.771930    0.759259    0.642857   \n",
      "7           Sensitivity    0.565217    0.647059    0.602941    0.671642   \n",
      "8           Specificity    0.971200    0.958600    0.958600    0.920600   \n",
      "9              F1 score    0.666667    0.704000    0.672131    0.656934   \n",
      "10  F1 score (weighted)    0.890400    0.899713    0.890422    0.878016   \n",
      "11     F1 score (macro)    0.803194    0.823049    0.804913    0.790987   \n",
      "12    Balanced Accuracy    0.768232    0.802829    0.780770    0.796138   \n",
      "13                  MCC    0.622649    0.650230    0.616547    0.582225   \n",
      "14                  NPV    0.910200    0.926200    0.917700    0.929500   \n",
      "15              ROC_AUC    0.768232    0.802829    0.780770    0.796138   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.663838    0.685186    0.719665    0.701116    0.666298  \n",
      "1    40.000000   43.000000   37.000000   38.000000   39.000000  \n",
      "2   303.000000  301.000000  308.000000  302.000000  305.000000  \n",
      "3    12.000000   11.000000    7.000000   12.000000    8.000000  \n",
      "4    27.000000   27.000000   30.000000   30.000000   30.000000  \n",
      "5     0.897906    0.900524    0.903141    0.890052    0.900524  \n",
      "6     0.769231    0.796296    0.840909    0.760000    0.829787  \n",
      "7     0.597015    0.614286    0.552239    0.558824    0.565217  \n",
      "8     0.961900    0.964700    0.977800    0.961800    0.974400  \n",
      "9     0.672269    0.693548    0.666667    0.644068    0.672414  \n",
      "10    0.892658    0.895349    0.894812    0.883198    0.892779  \n",
      "11    0.805902    0.817087    0.805003    0.789526    0.806886  \n",
      "12    0.779460    0.789515    0.765008    0.760303    0.769829  \n",
      "13    0.619845    0.642973    0.631390    0.590439    0.632040  \n",
      "14    0.918200    0.917700    0.911200    0.909600    0.910400  \n",
      "15    0.779460    0.789515    0.765008    0.760303    0.769829  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_knn_8)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet8_cat = np.where(((Y_testSet8>=2) | (Y_testSet8<=-2)), 1, 0) \n",
    "y_pred_knn_8_cat = np.where(((y_pred_knn_8 >= 2) | (y_pred_knn_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_knn_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:07:54,279]\u001b[0m Trial 450 finished with value: 0.6864515315350059 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 64}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:55,243]\u001b[0m Trial 451 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:56,203]\u001b[0m Trial 452 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:57,169]\u001b[0m Trial 453 finished with value: 0.695275289414346 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:58,136]\u001b[0m Trial 454 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:07:59,267]\u001b[0m Trial 455 finished with value: 0.7032824152906996 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:00,591]\u001b[0m Trial 456 finished with value: 0.6959759935785906 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:01,558]\u001b[0m Trial 457 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:02,513]\u001b[0m Trial 458 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:03,452]\u001b[0m Trial 459 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:04,385]\u001b[0m Trial 460 finished with value: 0.6857533898986271 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 86}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:05,318]\u001b[0m Trial 461 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:06,249]\u001b[0m Trial 462 finished with value: 0.6919964177851614 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:07,180]\u001b[0m Trial 463 finished with value: 0.695275289414346 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:08,280]\u001b[0m Trial 464 finished with value: 0.7032824152906996 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:09,212]\u001b[0m Trial 465 finished with value: 0.695275289414346 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:10,144]\u001b[0m Trial 466 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:11,075]\u001b[0m Trial 467 finished with value: 0.6775659059753043 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:12,008]\u001b[0m Trial 468 finished with value: 0.695275289414346 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:12,949]\u001b[0m Trial 469 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:13,884]\u001b[0m Trial 470 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:14,821]\u001b[0m Trial 471 finished with value: 0.695275289414346 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:15,754]\u001b[0m Trial 472 finished with value: 0.6949923650385605 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:16,694]\u001b[0m Trial 473 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:17,633]\u001b[0m Trial 474 finished with value: 0.695275289414346 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:18,566]\u001b[0m Trial 475 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:19,503]\u001b[0m Trial 476 finished with value: 0.6919964177851614 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:20,436]\u001b[0m Trial 477 finished with value: 0.6273916371034021 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:21,371]\u001b[0m Trial 478 finished with value: 0.695275289414346 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:22,479]\u001b[0m Trial 479 finished with value: 0.7032824152906996 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:23,417]\u001b[0m Trial 480 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:24,356]\u001b[0m Trial 481 finished with value: 0.6100692931628402 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:25,283]\u001b[0m Trial 482 finished with value: 0.695275289414346 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:26,216]\u001b[0m Trial 483 finished with value: 0.6949923650385605 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:27,145]\u001b[0m Trial 484 finished with value: 0.695275289414346 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:08:28,078]\u001b[0m Trial 485 finished with value: 0.6590919634899776 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:29,009]\u001b[0m Trial 486 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:29,937]\u001b[0m Trial 487 finished with value: 0.695275289414346 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:30,872]\u001b[0m Trial 488 finished with value: 0.6775659059753043 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:31,810]\u001b[0m Trial 489 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:32,746]\u001b[0m Trial 490 finished with value: 0.5787774897836239 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:33,675]\u001b[0m Trial 491 finished with value: 0.695275289414346 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:34,603]\u001b[0m Trial 492 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:35,531]\u001b[0m Trial 493 finished with value: 0.6919964177851614 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:36,634]\u001b[0m Trial 494 finished with value: 0.6964654047460396 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 74}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:37,564]\u001b[0m Trial 495 finished with value: 0.695275289414346 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:38,495]\u001b[0m Trial 496 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:39,427]\u001b[0m Trial 497 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:40,358]\u001b[0m Trial 498 finished with value: 0.695275289414346 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:08:41,292]\u001b[0m Trial 499 finished with value: 0.7020400111466775 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 41 with value: 0.7051489113968382.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7051\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 80\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.681130    0.709181    0.748912    0.678639   \n",
      "1                    TP   39.000000   44.000000   41.000000   45.000000   \n",
      "2                    TN  304.000000  301.000000  301.000000  290.000000   \n",
      "3                    FP    9.000000   13.000000   13.000000   25.000000   \n",
      "4                    FN   30.000000   24.000000   27.000000   22.000000   \n",
      "5              Accuracy    0.897906    0.903141    0.895288    0.876963   \n",
      "6             Precision    0.812500    0.771930    0.759259    0.642857   \n",
      "7           Sensitivity    0.565217    0.647059    0.602941    0.671642   \n",
      "8           Specificity    0.971200    0.958600    0.958600    0.920600   \n",
      "9              F1 score    0.666667    0.704000    0.672131    0.656934   \n",
      "10  F1 score (weighted)    0.890400    0.899713    0.890422    0.878016   \n",
      "11     F1 score (macro)    0.803194    0.823049    0.804913    0.790987   \n",
      "12    Balanced Accuracy    0.768232    0.802829    0.780770    0.796138   \n",
      "13                  MCC    0.622649    0.650230    0.616547    0.582225   \n",
      "14                  NPV    0.910200    0.926200    0.917700    0.929500   \n",
      "15              ROC_AUC    0.768232    0.802829    0.780770    0.796138   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.663838    0.685186    0.719665    0.701116    0.666298    0.691001  \n",
      "1    40.000000   43.000000   37.000000   38.000000   39.000000   40.000000  \n",
      "2   303.000000  301.000000  308.000000  302.000000  305.000000  304.000000  \n",
      "3    12.000000   11.000000    7.000000   12.000000    8.000000   11.000000  \n",
      "4    27.000000   27.000000   30.000000   30.000000   30.000000   27.000000  \n",
      "5     0.897906    0.900524    0.903141    0.890052    0.900524    0.900524  \n",
      "6     0.769231    0.796296    0.840909    0.760000    0.829787    0.784314  \n",
      "7     0.597015    0.614286    0.552239    0.558824    0.565217    0.597015  \n",
      "8     0.961900    0.964700    0.977800    0.961800    0.974400    0.965100  \n",
      "9     0.672269    0.693548    0.666667    0.644068    0.672414    0.677966  \n",
      "10    0.892658    0.895349    0.894812    0.883198    0.892779    0.895011  \n",
      "11    0.805902    0.817087    0.805003    0.789526    0.806886    0.809571  \n",
      "12    0.779460    0.789515    0.765008    0.760303    0.769829    0.781047  \n",
      "13    0.619845    0.642973    0.631390    0.590439    0.632040    0.628496  \n",
      "14    0.918200    0.917700    0.911200    0.909600    0.910400    0.918400  \n",
      "15    0.779460    0.789515    0.765008    0.760303    0.769829    0.781047  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_knn_9)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet9_cat = np.where(((Y_testSet9>=2) | (Y_testSet9<=-2)), 1, 0) \n",
    "y_pred_knn_9_cat = np.where(((y_pred_knn_9 >= 2) | (y_pred_knn_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_knn_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.718181</td>\n",
       "      <td>0.045656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>20.300000</td>\n",
       "      <td>2.213594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>152.700000</td>\n",
       "      <td>1.828782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>1.595131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>1.523884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.905759</td>\n",
       "      <td>0.013518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.806179</td>\n",
       "      <td>0.059641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.606846</td>\n",
       "      <td>0.052366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.968910</td>\n",
       "      <td>0.010081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.691646</td>\n",
       "      <td>0.050393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.900291</td>\n",
       "      <td>0.014424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.817998</td>\n",
       "      <td>0.028882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.787880</td>\n",
       "      <td>0.028299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.646792</td>\n",
       "      <td>0.057581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.921050</td>\n",
       "      <td>0.008554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.787880</td>\n",
       "      <td>0.028299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.718181     0.045656\n",
       "1                    TP        20.300000     2.213594\n",
       "2                    TN       152.700000     1.828782\n",
       "3                    FP         4.900000     1.595131\n",
       "4                    FN        13.100000     1.523884\n",
       "5              Accuracy         0.905759     0.013518\n",
       "6             Precision         0.806179     0.059641\n",
       "7           Sensitivity         0.606846     0.052366\n",
       "8           Specificity         0.968910     0.010081\n",
       "9              F1 score         0.691646     0.050393\n",
       "10  F1 score (weighted)         0.900291     0.014424\n",
       "11     F1 score (macro)         0.817998     0.028882\n",
       "12    Balanced Accuracy         0.787880     0.028299\n",
       "13                  MCC         0.646792     0.057581\n",
       "14                  NPV         0.921050     0.008554\n",
       "15              ROC_AUC         0.787880     0.028299"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.681130</td>\n",
       "      <td>0.709181</td>\n",
       "      <td>0.748912</td>\n",
       "      <td>0.678639</td>\n",
       "      <td>0.663838</td>\n",
       "      <td>0.685186</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>0.701116</td>\n",
       "      <td>0.666298</td>\n",
       "      <td>0.691001</td>\n",
       "      <td>0.694497</td>\n",
       "      <td>0.026073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.600000</td>\n",
       "      <td>2.633122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>301.900000</td>\n",
       "      <td>4.724640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>4.976612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>2.756810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.876963</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.896597</td>\n",
       "      <td>0.007926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.776708</td>\n",
       "      <td>0.055015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.597145</td>\n",
       "      <td>0.039289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.971200</td>\n",
       "      <td>0.958600</td>\n",
       "      <td>0.958600</td>\n",
       "      <td>0.920600</td>\n",
       "      <td>0.961900</td>\n",
       "      <td>0.964700</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.961800</td>\n",
       "      <td>0.974400</td>\n",
       "      <td>0.965100</td>\n",
       "      <td>0.961470</td>\n",
       "      <td>0.015774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>0.656934</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.693548</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.672666</td>\n",
       "      <td>0.016944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.890400</td>\n",
       "      <td>0.899713</td>\n",
       "      <td>0.890422</td>\n",
       "      <td>0.878016</td>\n",
       "      <td>0.892658</td>\n",
       "      <td>0.895349</td>\n",
       "      <td>0.894812</td>\n",
       "      <td>0.883198</td>\n",
       "      <td>0.892779</td>\n",
       "      <td>0.895011</td>\n",
       "      <td>0.891236</td>\n",
       "      <td>0.006333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.803194</td>\n",
       "      <td>0.823049</td>\n",
       "      <td>0.804913</td>\n",
       "      <td>0.790987</td>\n",
       "      <td>0.805902</td>\n",
       "      <td>0.817087</td>\n",
       "      <td>0.805003</td>\n",
       "      <td>0.789526</td>\n",
       "      <td>0.806886</td>\n",
       "      <td>0.809571</td>\n",
       "      <td>0.805612</td>\n",
       "      <td>0.010179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.768232</td>\n",
       "      <td>0.802829</td>\n",
       "      <td>0.780770</td>\n",
       "      <td>0.796138</td>\n",
       "      <td>0.779460</td>\n",
       "      <td>0.789515</td>\n",
       "      <td>0.765008</td>\n",
       "      <td>0.760303</td>\n",
       "      <td>0.769829</td>\n",
       "      <td>0.781047</td>\n",
       "      <td>0.779313</td>\n",
       "      <td>0.013840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.622649</td>\n",
       "      <td>0.650230</td>\n",
       "      <td>0.616547</td>\n",
       "      <td>0.582225</td>\n",
       "      <td>0.619845</td>\n",
       "      <td>0.642973</td>\n",
       "      <td>0.631390</td>\n",
       "      <td>0.590439</td>\n",
       "      <td>0.632040</td>\n",
       "      <td>0.628496</td>\n",
       "      <td>0.621683</td>\n",
       "      <td>0.021291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.910200</td>\n",
       "      <td>0.926200</td>\n",
       "      <td>0.917700</td>\n",
       "      <td>0.929500</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.917700</td>\n",
       "      <td>0.911200</td>\n",
       "      <td>0.909600</td>\n",
       "      <td>0.910400</td>\n",
       "      <td>0.918400</td>\n",
       "      <td>0.916910</td>\n",
       "      <td>0.006859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.768232</td>\n",
       "      <td>0.802829</td>\n",
       "      <td>0.780770</td>\n",
       "      <td>0.796138</td>\n",
       "      <td>0.779460</td>\n",
       "      <td>0.789515</td>\n",
       "      <td>0.765008</td>\n",
       "      <td>0.760303</td>\n",
       "      <td>0.769829</td>\n",
       "      <td>0.781047</td>\n",
       "      <td>0.779313</td>\n",
       "      <td>0.013840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.681130    0.709181    0.748912    0.678639   \n",
       "1                    TP   39.000000   44.000000   41.000000   45.000000   \n",
       "2                    TN  304.000000  301.000000  301.000000  290.000000   \n",
       "3                    FP    9.000000   13.000000   13.000000   25.000000   \n",
       "4                    FN   30.000000   24.000000   27.000000   22.000000   \n",
       "5              Accuracy    0.897906    0.903141    0.895288    0.876963   \n",
       "6             Precision    0.812500    0.771930    0.759259    0.642857   \n",
       "7           Sensitivity    0.565217    0.647059    0.602941    0.671642   \n",
       "8           Specificity    0.971200    0.958600    0.958600    0.920600   \n",
       "9              F1 score    0.666667    0.704000    0.672131    0.656934   \n",
       "10  F1 score (weighted)    0.890400    0.899713    0.890422    0.878016   \n",
       "11     F1 score (macro)    0.803194    0.823049    0.804913    0.790987   \n",
       "12    Balanced Accuracy    0.768232    0.802829    0.780770    0.796138   \n",
       "13                  MCC    0.622649    0.650230    0.616547    0.582225   \n",
       "14                  NPV    0.910200    0.926200    0.917700    0.929500   \n",
       "15              ROC_AUC    0.768232    0.802829    0.780770    0.796138   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.663838    0.685186    0.719665    0.701116    0.666298    0.691001   \n",
       "1    40.000000   43.000000   37.000000   38.000000   39.000000   40.000000   \n",
       "2   303.000000  301.000000  308.000000  302.000000  305.000000  304.000000   \n",
       "3    12.000000   11.000000    7.000000   12.000000    8.000000   11.000000   \n",
       "4    27.000000   27.000000   30.000000   30.000000   30.000000   27.000000   \n",
       "5     0.897906    0.900524    0.903141    0.890052    0.900524    0.900524   \n",
       "6     0.769231    0.796296    0.840909    0.760000    0.829787    0.784314   \n",
       "7     0.597015    0.614286    0.552239    0.558824    0.565217    0.597015   \n",
       "8     0.961900    0.964700    0.977800    0.961800    0.974400    0.965100   \n",
       "9     0.672269    0.693548    0.666667    0.644068    0.672414    0.677966   \n",
       "10    0.892658    0.895349    0.894812    0.883198    0.892779    0.895011   \n",
       "11    0.805902    0.817087    0.805003    0.789526    0.806886    0.809571   \n",
       "12    0.779460    0.789515    0.765008    0.760303    0.769829    0.781047   \n",
       "13    0.619845    0.642973    0.631390    0.590439    0.632040    0.628496   \n",
       "14    0.918200    0.917700    0.911200    0.909600    0.910400    0.918400   \n",
       "15    0.779460    0.789515    0.765008    0.760303    0.769829    0.781047   \n",
       "\n",
       "           ave       std  \n",
       "0     0.694497  0.026073  \n",
       "1    40.600000  2.633122  \n",
       "2   301.900000  4.724640  \n",
       "3    12.100000  4.976612  \n",
       "4    27.400000  2.756810  \n",
       "5     0.896597  0.007926  \n",
       "6     0.776708  0.055015  \n",
       "7     0.597145  0.039289  \n",
       "8     0.961470  0.015774  \n",
       "9     0.672666  0.016944  \n",
       "10    0.891236  0.006333  \n",
       "11    0.805612  0.010179  \n",
       "12    0.779313  0.013840  \n",
       "13    0.621683  0.021291  \n",
       "14    0.916910  0.006859  \n",
       "15    0.779313  0.013840  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test['std'] = mat_met_knn_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_knn0</th>\n",
       "      <th>y_pred_knn1</th>\n",
       "      <th>y_pred_knn2</th>\n",
       "      <th>y_pred_knn3</th>\n",
       "      <th>y_pred_knn4</th>\n",
       "      <th>y_pred_knn_ave</th>\n",
       "      <th>y_pred_knn_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL585939</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>-0.565484</td>\n",
       "      <td>-0.661961</td>\n",
       "      <td>-0.729797</td>\n",
       "      <td>-0.661961</td>\n",
       "      <td>-0.574638</td>\n",
       "      <td>-0.730640</td>\n",
       "      <td>0.212939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL96051</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.769904</td>\n",
       "      <td>1.297770</td>\n",
       "      <td>2.200604</td>\n",
       "      <td>1.328309</td>\n",
       "      <td>1.691403</td>\n",
       "      <td>1.511332</td>\n",
       "      <td>0.445034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3356916</td>\n",
       "      <td>2</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.469412</td>\n",
       "      <td>0.469412</td>\n",
       "      <td>0.396812</td>\n",
       "      <td>0.481715</td>\n",
       "      <td>0.469412</td>\n",
       "      <td>0.431127</td>\n",
       "      <td>0.064972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3907413</td>\n",
       "      <td>3</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>1.920840</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.668473</td>\n",
       "      <td>0.614137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2047704</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>-2.961400</td>\n",
       "      <td>-2.907200</td>\n",
       "      <td>-2.907200</td>\n",
       "      <td>-2.907200</td>\n",
       "      <td>-2.907200</td>\n",
       "      <td>-2.806700</td>\n",
       "      <td>0.249749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>CHEMBL1095136</td>\n",
       "      <td>1905</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>-0.948582</td>\n",
       "      <td>-1.064139</td>\n",
       "      <td>-1.064139</td>\n",
       "      <td>-0.929614</td>\n",
       "      <td>-1.064139</td>\n",
       "      <td>-1.045102</td>\n",
       "      <td>0.089195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>CHEMBL2012817</td>\n",
       "      <td>1906</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-0.926517</td>\n",
       "      <td>-0.777176</td>\n",
       "      <td>-1.090648</td>\n",
       "      <td>-1.090648</td>\n",
       "      <td>-0.665715</td>\n",
       "      <td>-1.248451</td>\n",
       "      <td>0.772060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>CHEMBL496511</td>\n",
       "      <td>1907</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.778430</td>\n",
       "      <td>1.347715</td>\n",
       "      <td>1.131214</td>\n",
       "      <td>0.778430</td>\n",
       "      <td>1.022006</td>\n",
       "      <td>0.976299</td>\n",
       "      <td>0.213473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>CHEMBL3940062</td>\n",
       "      <td>1908</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1.849308</td>\n",
       "      <td>1.856509</td>\n",
       "      <td>1.746333</td>\n",
       "      <td>1.944615</td>\n",
       "      <td>1.849308</td>\n",
       "      <td>1.919345</td>\n",
       "      <td>0.166978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>CHEMBL493749</td>\n",
       "      <td>1909</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.391866</td>\n",
       "      <td>0.391866</td>\n",
       "      <td>0.485657</td>\n",
       "      <td>0.485657</td>\n",
       "      <td>0.391866</td>\n",
       "      <td>0.412818</td>\n",
       "      <td>0.055957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1910 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_knn0  y_pred_knn1  \\\n",
       "0          CHEMBL585939            0    -1.19    -0.565484    -0.661961   \n",
       "1           CHEMBL96051            1     0.78     1.769904     1.297770   \n",
       "2         CHEMBL3356916            2     0.30     0.469412     0.469412   \n",
       "3         CHEMBL3907413            3     0.97     0.280000     0.280000   \n",
       "4         CHEMBL2047704            4    -2.25    -2.961400    -2.907200   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "1905      CHEMBL1095136         1905    -1.20    -0.948582    -1.064139   \n",
       "1906      CHEMBL2012817         1906    -2.94    -0.926517    -0.777176   \n",
       "1907       CHEMBL496511         1907     0.80     0.778430     1.347715   \n",
       "1908      CHEMBL3940062         1908     2.27     1.849308     1.856509   \n",
       "1909       CHEMBL493749         1909     0.33     0.391866     0.391866   \n",
       "\n",
       "      y_pred_knn2  y_pred_knn3  y_pred_knn4  y_pred_knn_ave  y_pred_knn_std  \n",
       "0       -0.729797    -0.661961    -0.574638       -0.730640        0.212939  \n",
       "1        2.200604     1.328309     1.691403        1.511332        0.445034  \n",
       "2        0.396812     0.481715     0.469412        0.431127        0.064972  \n",
       "3        1.920840     0.280000     0.280000        0.668473        0.614137  \n",
       "4       -2.907200    -2.907200    -2.907200       -2.806700        0.249749  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "1905    -1.064139    -0.929614    -1.064139       -1.045102        0.089195  \n",
       "1906    -1.090648    -1.090648    -0.665715       -1.248451        0.772060  \n",
       "1907     1.131214     0.778430     1.022006        0.976299        0.213473  \n",
       "1908     1.746333     1.944615     1.849308        1.919345        0.166978  \n",
       "1909     0.485657     0.485657     0.391866        0.412818        0.055957  \n",
       "\n",
       "[1910 rows x 10 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_knn_cat = np.where(((y_pred_optimized_knn >= 2) | (y_pred_optimized_knn <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_knn_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_knn))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "knn_run0 = data_knn[['y_test_idx0', 'y_test0', 'y_pred_knn0']]\n",
    "knn_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "knn_run0.reset_index(inplace=True, drop=True)\n",
    "knn_run1 = data_knn[['y_test_idx1', 'y_test1', 'y_pred_knn1']]\n",
    "knn_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "knn_run1.reset_index(inplace=True, drop=True)\n",
    "knn_run2 = data_knn[['y_test_idx2', 'y_test2', 'y_pred_knn2']]\n",
    "knn_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "knn_run2.reset_index(inplace=True, drop=True)\n",
    "knn_run3 = data_knn[['y_test_idx3', 'y_test3', 'y_pred_knn3']]\n",
    "knn_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "knn_run3.reset_index(inplace=True, drop=True)\n",
    "knn_run4 = data_knn[['y_test_idx4', 'y_test4', 'y_pred_knn4']]\n",
    "knn_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "knn_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "knn_5preds = pd.concat([chembl_id,knn_run0, knn_run1, knn_run2, knn_run3, knn_run4], axis=1)\n",
    "knn_5preds = knn_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_knn0', 'y_pred_knn1', 'y_pred_knn2', 'y_pred_knn3', 'y_pred_knn4']]\n",
    "knn_5preds['y_pred_knn_ave'] = knn_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "knn_5preds['y_pred_knn_std'] = knn_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "knn_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c149767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_knn.to_csv(output/'mat_met_knn_opt_withSemiSel.csv')\n",
    "knn_5preds.to_csv(output/'knn_5test_CV_result_withSemiSel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0bc43db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model r2_score 0.6903 with a standard deviation of 0.0471\n",
      "KNN optimized model r2_score 0.7072 with a standard deviation of 0.0428\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"KNN baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_knn.joblib']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_reg, \"./knn_reg.joblib\")\n",
    "#joblib.dump(optimized_knn, \"./optimized_knn.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"./optimizedCV_knn.joblib\")\n",
    "#loaded_rf = joblib.load(\"./optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.709710     0.046285\n",
      "1                    TP        14.300000     2.496664\n",
      "2                    TN       155.400000     0.966092\n",
      "3                    FP         2.200000     1.032796\n",
      "4                    FN        19.100000     2.131770\n",
      "5              Accuracy         0.888482     0.012597\n",
      "6             Precision         0.865499     0.060138\n",
      "7           Sensitivity         0.427408     0.067799\n",
      "8           Specificity         0.986050     0.006498\n",
      "9              F1 score         0.570002     0.066974\n",
      "10  F1 score (weighted)         0.872064     0.016531\n",
      "11     F1 score (macro)         0.752952     0.036775\n",
      "12    Balanced Accuracy         0.706736     0.034654\n",
      "13                  MCC         0.558138     0.064658\n",
      "14                  NPV         0.890680     0.010926\n",
      "15              ROC_AUC         0.706736     0.034654\n",
      "CPU times: user 18.6 s, sys: 0 ns, total: 18.6 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    \n",
    "    svm_reg.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:09:18,630]\u001b[0m A new study created in memory with name: SVM_regressor_CV\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:09:30,801]\u001b[0m Trial 0 finished with value: 0.7205055534411337 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:09:42,995]\u001b[0m Trial 1 finished with value: 0.7037196683681222 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:09:56,102]\u001b[0m Trial 2 finished with value: 0.035888448674564956 and parameters: {'C': 1.0, 'gamma': 0.5}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:10:08,884]\u001b[0m Trial 3 finished with value: 0.6329223936873076 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:10:22,013]\u001b[0m Trial 4 finished with value: 0.026836659656775806 and parameters: {'C': 1.0, 'gamma': 1.0}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:10:35,209]\u001b[0m Trial 5 finished with value: 0.07612375547946613 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:10:48,414]\u001b[0m Trial 6 finished with value: 0.024696060139553444 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:11:01,629]\u001b[0m Trial 7 finished with value: 0.07612375547946616 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:11:14,672]\u001b[0m Trial 8 finished with value: 0.05831228292091134 and parameters: {'C': 0.25, 'gamma': 0.0001220703125}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:11:27,831]\u001b[0m Trial 9 finished with value: 0.003816142960886748 and parameters: {'C': 0.0078125, 'gamma': 0.00048828125}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:11:40,524]\u001b[0m Trial 10 finished with value: 0.220300288237378 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:11:52,708]\u001b[0m Trial 11 finished with value: 0.7037196683681222 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:12:04,900]\u001b[0m Trial 12 finished with value: 0.7037196683681222 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:12:17,066]\u001b[0m Trial 13 finished with value: 0.720452543429243 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:12:29,230]\u001b[0m Trial 14 finished with value: 0.720452543429243 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:12:41,177]\u001b[0m Trial 15 finished with value: 0.712718003216173 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:12:54,347]\u001b[0m Trial 16 finished with value: 0.0027265551876913706 and parameters: {'C': 0.125, 'gamma': 4.0}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:13:06,729]\u001b[0m Trial 17 finished with value: 0.5411971032618742 and parameters: {'C': 0.5, 'gamma': 0.00390625}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:13:20,401]\u001b[0m Trial 18 finished with value: 0.023230597796565187 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:13:32,929]\u001b[0m Trial 19 finished with value: 0.7005540276934719 and parameters: {'C': 64.0, 'gamma': 0.0078125}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:13:46,070]\u001b[0m Trial 20 finished with value: 0.0040877203900565266 and parameters: {'C': 0.0625, 'gamma': 6.103515625e-05}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:13:58,234]\u001b[0m Trial 21 finished with value: 0.720452543429243 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:14:10,619]\u001b[0m Trial 22 finished with value: 0.590589653586248 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:14:23,713]\u001b[0m Trial 23 finished with value: 0.0016814389532036622 and parameters: {'C': 0.015625, 'gamma': 0.125}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:14:36,055]\u001b[0m Trial 24 finished with value: 0.5881135222934188 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:14:49,000]\u001b[0m Trial 25 finished with value: 0.6513962971281544 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:15:01,168]\u001b[0m Trial 26 finished with value: 0.720452543429243 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:15:13,860]\u001b[0m Trial 27 finished with value: 0.393079774499631 and parameters: {'C': 2.0, 'gamma': 0.000244140625}. Best is trial 0 with value: 0.7205055534411337.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:15:25,947]\u001b[0m Trial 28 finished with value: 0.7229092165665432 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:15:38,031]\u001b[0m Trial 29 finished with value: 0.7229092165665432 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:15:50,116]\u001b[0m Trial 30 finished with value: 0.7229092165665432 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:16:02,199]\u001b[0m Trial 31 finished with value: 0.7229092165665432 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:16:14,289]\u001b[0m Trial 32 finished with value: 0.7229092165665432 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:16:27,525]\u001b[0m Trial 33 finished with value: 0.039560689759834254 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:16:39,617]\u001b[0m Trial 34 finished with value: 0.7229092165665432 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:16:51,964]\u001b[0m Trial 35 finished with value: 0.5957261480589695 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:17:04,063]\u001b[0m Trial 36 finished with value: 0.7229092165665432 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:17:17,266]\u001b[0m Trial 37 finished with value: 0.029057654476828544 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:17:30,480]\u001b[0m Trial 38 finished with value: 0.024693972466788318 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:17:42,946]\u001b[0m Trial 39 finished with value: 0.4829568500860869 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:17:55,986]\u001b[0m Trial 40 finished with value: 0.05831228292091134 and parameters: {'C': 0.25, 'gamma': 0.0001220703125}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:18:08,080]\u001b[0m Trial 41 finished with value: 0.7229092165665432 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:18:20,174]\u001b[0m Trial 42 finished with value: 0.7229092165665432 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:18:32,874]\u001b[0m Trial 43 finished with value: 0.3904899580327849 and parameters: {'C': 1.0, 'gamma': 0.00048828125}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:18:45,988]\u001b[0m Trial 44 finished with value: -0.0036003729840812905 and parameters: {'C': 0.0078125, 'gamma': 0.25}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:18:58,080]\u001b[0m Trial 45 finished with value: 0.7229092165665432 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:19:11,688]\u001b[0m Trial 46 finished with value: -0.0027516788613614063 and parameters: {'C': 0.03125, 'gamma': 8.0}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:19:23,809]\u001b[0m Trial 47 finished with value: 0.704051647214755 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:19:36,979]\u001b[0m Trial 48 finished with value: 0.0027265551876913706 and parameters: {'C': 0.125, 'gamma': 4.0}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:19:49,546]\u001b[0m Trial 49 finished with value: 0.47617668164117155 and parameters: {'C': 0.5, 'gamma': 0.001953125}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.7229\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_regressor_CV\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.728270\n",
      "1                    TP   36.000000\n",
      "2                    TN  304.000000\n",
      "3                    FP    9.000000\n",
      "4                    FN   33.000000\n",
      "5              Accuracy    0.890052\n",
      "6             Precision    0.800000\n",
      "7           Sensitivity    0.521739\n",
      "8           Specificity    0.971200\n",
      "9              F1 score    0.631579\n",
      "10  F1 score (weighted)    0.880509\n",
      "11     F1 score (macro)    0.783482\n",
      "12    Balanced Accuracy    0.746493\n",
      "13                  MCC    0.588314\n",
      "14                  NPV    0.902100\n",
      "15              ROC_AUC    0.746493\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_svm_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_svm_0_cat = np.where(((y_pred_svm_0 >= 2) | (y_pred_svm_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_svm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:20:04,024]\u001b[0m Trial 50 finished with value: 0.23824500216217018 and parameters: {'C': 0.0625, 'gamma': 0.00390625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:20:16,137]\u001b[0m Trial 51 finished with value: 0.7222959385637544 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:20:28,252]\u001b[0m Trial 52 finished with value: 0.7222959385637544 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:20:40,363]\u001b[0m Trial 53 finished with value: 0.7222959385637544 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:20:52,791]\u001b[0m Trial 54 finished with value: 0.5898982133230432 and parameters: {'C': 64.0, 'gamma': 6.103515625e-05}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:21:04,980]\u001b[0m Trial 55 finished with value: 0.7228650832155777 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:21:17,913]\u001b[0m Trial 56 finished with value: 0.28748276214062934 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:21:31,055]\u001b[0m Trial 57 finished with value: -0.0036071864895035554 and parameters: {'C': 0.015625, 'gamma': 3.0517578125e-05}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:21:43,250]\u001b[0m Trial 58 finished with value: 0.5826953650954583 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:21:55,255]\u001b[0m Trial 59 finished with value: 0.671850004222192 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:22:07,422]\u001b[0m Trial 60 finished with value: 0.7201976350163333 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:22:19,531]\u001b[0m Trial 61 finished with value: 0.7222959385637544 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:22:31,641]\u001b[0m Trial 62 finished with value: 0.7222959385637544 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:22:43,752]\u001b[0m Trial 63 finished with value: 0.7222959385637544 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:22:56,120]\u001b[0m Trial 64 finished with value: 0.5942313730815265 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:23:09,282]\u001b[0m Trial 65 finished with value: 0.01404335039704846 and parameters: {'C': 0.25, 'gamma': 1.0}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:23:21,281]\u001b[0m Trial 66 finished with value: 0.7123340682168238 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:23:34,373]\u001b[0m Trial 67 finished with value: -0.001381593080513077 and parameters: {'C': 0.03125, 'gamma': 0.5}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:23:47,467]\u001b[0m Trial 68 finished with value: -0.004088540618862224 and parameters: {'C': 0.0078125, 'gamma': 2.0}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:23:59,581]\u001b[0m Trial 69 finished with value: 0.7222959385637544 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:24:12,002]\u001b[0m Trial 70 finished with value: 0.5392277481359609 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:24:24,168]\u001b[0m Trial 71 finished with value: 0.7222959385637544 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:24:36,288]\u001b[0m Trial 72 finished with value: 0.7222959385637544 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:24:49,378]\u001b[0m Trial 73 finished with value: 0.029316040468309913 and parameters: {'C': 0.125, 'gamma': 0.0001220703125}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:25:01,507]\u001b[0m Trial 74 finished with value: 0.7222959385637544 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:25:14,595]\u001b[0m Trial 75 finished with value: 0.05088474436930279 and parameters: {'C': 0.5, 'gamma': 0.25}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:25:26,790]\u001b[0m Trial 76 finished with value: 0.7193481717167042 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:25:39,982]\u001b[0m Trial 77 finished with value: -0.0002529339783225226 and parameters: {'C': 0.0625, 'gamma': 4.0}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:25:51,919]\u001b[0m Trial 78 finished with value: 0.7037657233897191 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:26:04,835]\u001b[0m Trial 79 finished with value: 0.08640830506096242 and parameters: {'C': 0.015625, 'gamma': 0.00390625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:26:16,967]\u001b[0m Trial 80 finished with value: 0.7222959385637544 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:26:29,101]\u001b[0m Trial 81 finished with value: 0.7222959385637544 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:26:42,791]\u001b[0m Trial 82 finished with value: 0.031712242897049936 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:26:54,980]\u001b[0m Trial 83 finished with value: 0.7228650832155777 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:27:07,503]\u001b[0m Trial 84 finished with value: 0.4692078362488493 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:27:20,298]\u001b[0m Trial 85 finished with value: 0.28005741160801984 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 28 with value: 0.7229092165665432.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:27:32,464]\u001b[0m Trial 86 finished with value: 0.7250943070916923 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:27:45,388]\u001b[0m Trial 87 finished with value: 0.2865003684418993 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:27:57,755]\u001b[0m Trial 88 finished with value: 0.6545351388581186 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:28:10,530]\u001b[0m Trial 89 finished with value: 0.2804142112043483 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:28:22,785]\u001b[0m Trial 90 finished with value: 0.5317008200898949 and parameters: {'C': 1.0, 'gamma': 0.0625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:28:34,959]\u001b[0m Trial 91 finished with value: 0.7201976350163333 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:28:47,075]\u001b[0m Trial 92 finished with value: 0.7222959385637544 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:28:59,193]\u001b[0m Trial 93 finished with value: 0.7222959385637544 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:29:11,615]\u001b[0m Trial 94 finished with value: 0.5462390255815144 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:29:23,733]\u001b[0m Trial 95 finished with value: 0.6983685056131176 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:29:35,846]\u001b[0m Trial 96 finished with value: 0.7222959385637544 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:29:48,999]\u001b[0m Trial 97 finished with value: 0.052541851290079836 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:30:02,002]\u001b[0m Trial 98 finished with value: 0.07409021013644519 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:30:15,110]\u001b[0m Trial 99 finished with value: -0.0020720153170107223 and parameters: {'C': 0.03125, 'gamma': 1.0}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7251\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.728270    0.732425\n",
      "1                    TP   36.000000   36.000000\n",
      "2                    TN  304.000000  310.000000\n",
      "3                    FP    9.000000    4.000000\n",
      "4                    FN   33.000000   32.000000\n",
      "5              Accuracy    0.890052    0.905759\n",
      "6             Precision    0.800000    0.900000\n",
      "7           Sensitivity    0.521739    0.529412\n",
      "8           Specificity    0.971200    0.987300\n",
      "9              F1 score    0.631579    0.666667\n",
      "10  F1 score (weighted)    0.880509    0.895554\n",
      "11     F1 score (macro)    0.783482    0.805894\n",
      "12    Balanced Accuracy    0.746493    0.758336\n",
      "13                  MCC    0.588314    0.645494\n",
      "14                  NPV    0.902100    0.906400\n",
      "15              ROC_AUC    0.746493    0.758336\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_svm_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet1_cat = np.where(((Y_testSet1>=2) | (Y_testSet1<=-2)), 1, 0) \n",
    "y_pred_svm_1_cat = np.where(((y_pred_svm_1 >= 2) | (y_pred_svm_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_svm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:30:29,608]\u001b[0m Trial 100 finished with value: 0.17599749780465646 and parameters: {'C': 0.125, 'gamma': 0.0009765625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:30:41,768]\u001b[0m Trial 101 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:30:53,933]\u001b[0m Trial 102 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:31:06,093]\u001b[0m Trial 103 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:31:19,338]\u001b[0m Trial 104 finished with value: 0.019431011842076174 and parameters: {'C': 0.5, 'gamma': 2.0}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:31:31,501]\u001b[0m Trial 105 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:31:44,117]\u001b[0m Trial 106 finished with value: 0.47646756623903974 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:31:56,284]\u001b[0m Trial 107 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:32:09,409]\u001b[0m Trial 108 finished with value: 0.05763725857181491 and parameters: {'C': 0.0625, 'gamma': 0.00048828125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:32:22,542]\u001b[0m Trial 109 finished with value: -0.002266020755254039 and parameters: {'C': 0.015625, 'gamma': 0.25}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:32:34,701]\u001b[0m Trial 110 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:32:46,925]\u001b[0m Trial 111 finished with value: 0.71643203117338 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:32:59,151]\u001b[0m Trial 112 finished with value: 0.7148342816311603 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:33:11,399]\u001b[0m Trial 113 finished with value: 0.7148337797994494 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:33:24,717]\u001b[0m Trial 114 finished with value: 0.03176483686981657 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:33:36,893]\u001b[0m Trial 115 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:33:50,666]\u001b[0m Trial 116 finished with value: 0.03144871445966443 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:34:02,902]\u001b[0m Trial 117 finished with value: 0.71643203117338 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:34:15,366]\u001b[0m Trial 118 finished with value: 0.7038813445998542 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:34:27,996]\u001b[0m Trial 119 finished with value: 0.4757956217768065 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:34:40,350]\u001b[0m Trial 120 finished with value: 0.6027968268265606 and parameters: {'C': 1.0, 'gamma': 0.00390625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:34:52,565]\u001b[0m Trial 121 finished with value: 0.71643203117338 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:35:04,722]\u001b[0m Trial 122 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:35:16,938]\u001b[0m Trial 123 finished with value: 0.71643203117338 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:35:29,637]\u001b[0m Trial 124 finished with value: 0.6608226379345344 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:35:42,625]\u001b[0m Trial 125 finished with value: 0.28765210415609443 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:35:55,131]\u001b[0m Trial 126 finished with value: 0.5492436556119176 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:36:07,598]\u001b[0m Trial 127 finished with value: 0.5404070592489266 and parameters: {'C': 32.0, 'gamma': 6.103515625e-05}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:36:19,955]\u001b[0m Trial 128 finished with value: 0.5804080778215361 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:36:32,896]\u001b[0m Trial 129 finished with value: 0.11026832279287549 and parameters: {'C': 2.0, 'gamma': 3.0517578125e-05}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:36:45,111]\u001b[0m Trial 130 finished with value: 0.71643203117338 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:36:57,281]\u001b[0m Trial 131 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:37:09,439]\u001b[0m Trial 132 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:37:21,599]\u001b[0m Trial 133 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:37:34,708]\u001b[0m Trial 134 finished with value: 0.0758344907068004 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:37:47,490]\u001b[0m Trial 135 finished with value: 0.17108071325691498 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:37:59,651]\u001b[0m Trial 136 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:38:12,887]\u001b[0m Trial 137 finished with value: 0.051593192679713726 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:38:25,688]\u001b[0m Trial 138 finished with value: 0.17599749780465646 and parameters: {'C': 0.125, 'gamma': 0.0009765625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:38:37,847]\u001b[0m Trial 139 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:38:51,114]\u001b[0m Trial 140 finished with value: 0.022850174750369844 and parameters: {'C': 0.5, 'gamma': 1.0}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:39:03,272]\u001b[0m Trial 141 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:39:15,445]\u001b[0m Trial 142 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:39:27,606]\u001b[0m Trial 143 finished with value: 0.7182401474447313 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:39:40,858]\u001b[0m Trial 144 finished with value: 0.034020422491106615 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:39:53,503]\u001b[0m Trial 145 finished with value: 0.3201638713689837 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:40:05,730]\u001b[0m Trial 146 finished with value: 0.7148342816311603 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:40:18,135]\u001b[0m Trial 147 finished with value: 0.5967087174865149 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:40:30,890]\u001b[0m Trial 148 finished with value: 0.3839235223059724 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:40:43,742]\u001b[0m Trial 149 finished with value: 0.1359590841993581 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7251\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.728270    0.732425    0.740376\n",
      "1                    TP   36.000000   36.000000   33.000000\n",
      "2                    TN  304.000000  310.000000  301.000000\n",
      "3                    FP    9.000000    4.000000   13.000000\n",
      "4                    FN   33.000000   32.000000   35.000000\n",
      "5              Accuracy    0.890052    0.905759    0.874346\n",
      "6             Precision    0.800000    0.900000    0.717391\n",
      "7           Sensitivity    0.521739    0.529412    0.485294\n",
      "8           Specificity    0.971200    0.987300    0.958600\n",
      "9              F1 score    0.631579    0.666667    0.578947\n",
      "10  F1 score (weighted)    0.880509    0.895554    0.864347\n",
      "11     F1 score (macro)    0.783482    0.805894    0.752551\n",
      "12    Balanced Accuracy    0.746493    0.758336    0.721946\n",
      "13                  MCC    0.588314    0.645494    0.521734\n",
      "14                  NPV    0.902100    0.906400    0.895800\n",
      "15              ROC_AUC    0.746493    0.758336    0.721946\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_svm_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet2_cat = np.where(((Y_testSet2>=2) | (Y_testSet2<=-2)), 1, 0) \n",
    "y_pred_svm_2_cat = np.where(((y_pred_svm_2 >= 2) | (y_pred_svm_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_svm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:40:58,686]\u001b[0m Trial 150 finished with value: 0.0812115639718752 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:41:10,816]\u001b[0m Trial 151 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:41:22,944]\u001b[0m Trial 152 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:41:35,065]\u001b[0m Trial 153 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:41:47,337]\u001b[0m Trial 154 finished with value: 0.7071765835028508 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:41:59,467]\u001b[0m Trial 155 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:42:11,734]\u001b[0m Trial 156 finished with value: 0.7098737356680868 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:42:23,954]\u001b[0m Trial 157 finished with value: 0.7000759722160588 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:42:37,273]\u001b[0m Trial 158 finished with value: 0.026633691233335043 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:42:49,486]\u001b[0m Trial 159 finished with value: 0.6683502306228517 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:43:03,224]\u001b[0m Trial 160 finished with value: 0.02579029267978522 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:43:15,352]\u001b[0m Trial 161 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:43:27,479]\u001b[0m Trial 162 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:43:39,607]\u001b[0m Trial 163 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:43:52,234]\u001b[0m Trial 164 finished with value: 0.4778292469833099 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:44:04,209]\u001b[0m Trial 165 finished with value: 0.6790924609360423 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:44:16,311]\u001b[0m Trial 166 finished with value: 0.7066515815304817 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:44:29,485]\u001b[0m Trial 167 finished with value: 0.6511184392541632 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:44:41,755]\u001b[0m Trial 168 finished with value: 0.7098737356680868 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:44:54,927]\u001b[0m Trial 169 finished with value: 0.013062597844039524 and parameters: {'C': 0.25, 'gamma': 3.0517578125e-05}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:45:07,662]\u001b[0m Trial 170 finished with value: 0.28925477550318207 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:45:19,795]\u001b[0m Trial 171 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:45:31,925]\u001b[0m Trial 172 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:45:44,056]\u001b[0m Trial 173 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:45:57,041]\u001b[0m Trial 174 finished with value: 0.28521444400604234 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:46:10,179]\u001b[0m Trial 175 finished with value: 0.013359907561135021 and parameters: {'C': 0.0078125, 'gamma': 0.0625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:46:22,917]\u001b[0m Trial 176 finished with value: 0.22028855170583603 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:46:35,051]\u001b[0m Trial 177 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:46:47,183]\u001b[0m Trial 178 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:46:59,449]\u001b[0m Trial 179 finished with value: 0.6252533911026956 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:47:11,998]\u001b[0m Trial 180 finished with value: 0.38273568503368327 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:47:24,131]\u001b[0m Trial 181 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:47:36,285]\u001b[0m Trial 182 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:47:48,461]\u001b[0m Trial 183 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:48:00,632]\u001b[0m Trial 184 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:48:13,200]\u001b[0m Trial 185 finished with value: 0.6407100364613727 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:48:25,368]\u001b[0m Trial 186 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:48:38,590]\u001b[0m Trial 187 finished with value: -0.0004789633126028758 and parameters: {'C': 0.0625, 'gamma': 1.0}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:48:50,757]\u001b[0m Trial 188 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:49:03,977]\u001b[0m Trial 189 finished with value: 0.04196818666885636 and parameters: {'C': 64.0, 'gamma': 0.5}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:49:17,221]\u001b[0m Trial 190 finished with value: 0.027699299551882384 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:49:29,354]\u001b[0m Trial 191 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:49:41,488]\u001b[0m Trial 192 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:49:53,622]\u001b[0m Trial 193 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:50:05,755]\u001b[0m Trial 194 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:50:17,897]\u001b[0m Trial 195 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:50:31,122]\u001b[0m Trial 196 finished with value: 4.909923961750051e-05 and parameters: {'C': 0.015625, 'gamma': 0.0001220703125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:50:43,256]\u001b[0m Trial 197 finished with value: 0.7108496443522426 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:50:56,475]\u001b[0m Trial 198 finished with value: 0.08062738367881664 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:51:08,760]\u001b[0m Trial 199 finished with value: 0.7071765835028508 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7251\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.728270    0.732425    0.740376    0.756597\n",
      "1                    TP   36.000000   36.000000   33.000000   39.000000\n",
      "2                    TN  304.000000  310.000000  301.000000  307.000000\n",
      "3                    FP    9.000000    4.000000   13.000000    8.000000\n",
      "4                    FN   33.000000   32.000000   35.000000   28.000000\n",
      "5              Accuracy    0.890052    0.905759    0.874346    0.905759\n",
      "6             Precision    0.800000    0.900000    0.717391    0.829787\n",
      "7           Sensitivity    0.521739    0.529412    0.485294    0.582090\n",
      "8           Specificity    0.971200    0.987300    0.958600    0.974600\n",
      "9              F1 score    0.631579    0.666667    0.578947    0.684211\n",
      "10  F1 score (weighted)    0.880509    0.895554    0.864347    0.898942\n",
      "11     F1 score (macro)    0.783482    0.805894    0.752551    0.814413\n",
      "12    Balanced Accuracy    0.746493    0.758336    0.721946    0.778346\n",
      "13                  MCC    0.588314    0.645494    0.521734    0.644521\n",
      "14                  NPV    0.902100    0.906400    0.895800    0.916400\n",
      "15              ROC_AUC    0.746493    0.758336    0.721946    0.778346\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_svm_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet3_cat = np.where(((Y_testSet3>=2) | (Y_testSet3<=-2)), 1, 0) \n",
    "y_pred_svm_3_cat = np.where(((y_pred_svm_3 >= 2) | (y_pred_svm_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_svm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 09:51:22,937]\u001b[0m Trial 200 finished with value: 0.5566726084483908 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 86 with value: 0.7250943070916923.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:51:34,898]\u001b[0m Trial 201 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:51:46,857]\u001b[0m Trial 202 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:51:58,822]\u001b[0m Trial 203 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:52:10,784]\u001b[0m Trial 204 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:52:22,748]\u001b[0m Trial 205 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:52:34,714]\u001b[0m Trial 206 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:52:46,682]\u001b[0m Trial 207 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:52:58,656]\u001b[0m Trial 208 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:53:10,657]\u001b[0m Trial 209 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:53:22,656]\u001b[0m Trial 210 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:53:34,655]\u001b[0m Trial 211 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:53:46,635]\u001b[0m Trial 212 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:53:58,591]\u001b[0m Trial 213 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:54:10,551]\u001b[0m Trial 214 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:54:22,510]\u001b[0m Trial 215 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:54:34,468]\u001b[0m Trial 216 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:54:46,441]\u001b[0m Trial 217 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:54:58,405]\u001b[0m Trial 218 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:55:10,371]\u001b[0m Trial 219 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:55:22,336]\u001b[0m Trial 220 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:55:34,302]\u001b[0m Trial 221 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:55:46,269]\u001b[0m Trial 222 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:55:58,235]\u001b[0m Trial 223 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:56:10,233]\u001b[0m Trial 224 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:56:22,237]\u001b[0m Trial 225 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:56:34,242]\u001b[0m Trial 226 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:56:46,244]\u001b[0m Trial 227 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:56:58,213]\u001b[0m Trial 228 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:57:11,513]\u001b[0m Trial 229 finished with value: 0.030682762808722796 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:57:23,604]\u001b[0m Trial 230 finished with value: 0.720823481010106 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:57:35,566]\u001b[0m Trial 231 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:57:47,527]\u001b[0m Trial 232 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:57:59,489]\u001b[0m Trial 233 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:58:11,453]\u001b[0m Trial 234 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:58:25,216]\u001b[0m Trial 235 finished with value: 0.03029490419831481 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:58:37,205]\u001b[0m Trial 236 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:58:49,193]\u001b[0m Trial 237 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:59:01,453]\u001b[0m Trial 238 finished with value: 0.6917088983675599 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:59:13,419]\u001b[0m Trial 239 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:59:25,379]\u001b[0m Trial 240 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:59:37,339]\u001b[0m Trial 241 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 09:59:49,299]\u001b[0m Trial 242 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:00:01,258]\u001b[0m Trial 243 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:00:13,751]\u001b[0m Trial 244 finished with value: 0.49217645093189094 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 10:00:25,711]\u001b[0m Trial 245 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:00:38,062]\u001b[0m Trial 246 finished with value: 0.6588079715730446 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:00:50,042]\u001b[0m Trial 247 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:01:02,949]\u001b[0m Trial 248 finished with value: 0.11396820662391134 and parameters: {'C': 1.0, 'gamma': 6.103515625e-05}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:01:14,930]\u001b[0m Trial 249 finished with value: 0.732729100402606 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7327\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.728270    0.732425    0.740376    0.756597   \n",
      "1                    TP   36.000000   36.000000   33.000000   39.000000   \n",
      "2                    TN  304.000000  310.000000  301.000000  307.000000   \n",
      "3                    FP    9.000000    4.000000   13.000000    8.000000   \n",
      "4                    FN   33.000000   32.000000   35.000000   28.000000   \n",
      "5              Accuracy    0.890052    0.905759    0.874346    0.905759   \n",
      "6             Precision    0.800000    0.900000    0.717391    0.829787   \n",
      "7           Sensitivity    0.521739    0.529412    0.485294    0.582090   \n",
      "8           Specificity    0.971200    0.987300    0.958600    0.974600   \n",
      "9              F1 score    0.631579    0.666667    0.578947    0.684211   \n",
      "10  F1 score (weighted)    0.880509    0.895554    0.864347    0.898942   \n",
      "11     F1 score (macro)    0.783482    0.805894    0.752551    0.814413   \n",
      "12    Balanced Accuracy    0.746493    0.758336    0.721946    0.778346   \n",
      "13                  MCC    0.588314    0.645494    0.521734    0.644521   \n",
      "14                  NPV    0.902100    0.906400    0.895800    0.916400   \n",
      "15              ROC_AUC    0.746493    0.758336    0.721946    0.778346   \n",
      "\n",
      "          Set4  \n",
      "0     0.673235  \n",
      "1    35.000000  \n",
      "2   306.000000  \n",
      "3     9.000000  \n",
      "4    32.000000  \n",
      "5     0.892670  \n",
      "6     0.795455  \n",
      "7     0.522388  \n",
      "8     0.971400  \n",
      "9     0.630631  \n",
      "10    0.883441  \n",
      "11    0.783922  \n",
      "12    0.746908  \n",
      "13    0.588266  \n",
      "14    0.905300  \n",
      "15    0.746908  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_svm_4)\n",
    "# now convert the resuls to binary with cutoff 6.4\n",
    "Y_testSet4_cat = np.where(((Y_testSet4>=2) | (Y_testSet4<=-2)), 1, 0) \n",
    "y_pred_svm_4_cat = np.where(((y_pred_svm_4 >= 2) | (y_pred_svm_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_svm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 10:01:28,937]\u001b[0m Trial 250 finished with value: 0.5827370908995875 and parameters: {'C': 32.0, 'gamma': 0.0625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:01:41,936]\u001b[0m Trial 251 finished with value: 0.27557097982974155 and parameters: {'C': 2.0, 'gamma': 0.125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:01:54,811]\u001b[0m Trial 252 finished with value: 0.19605877693195445 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:02:07,170]\u001b[0m Trial 253 finished with value: 0.5595613265887528 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:02:19,231]\u001b[0m Trial 254 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:02:31,284]\u001b[0m Trial 255 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:02:43,338]\u001b[0m Trial 256 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:02:56,457]\u001b[0m Trial 257 finished with value: 0.05537725014866527 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:03:08,543]\u001b[0m Trial 258 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:03:20,627]\u001b[0m Trial 259 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:03:33,755]\u001b[0m Trial 260 finished with value: 0.06019707089463616 and parameters: {'C': 0.03125, 'gamma': 0.0009765625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:03:45,830]\u001b[0m Trial 261 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:03:59,100]\u001b[0m Trial 262 finished with value: 0.035565133935032286 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:04:11,675]\u001b[0m Trial 263 finished with value: 0.4489371077591902 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:04:24,939]\u001b[0m Trial 264 finished with value: 0.04760274324546874 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:04:36,998]\u001b[0m Trial 265 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:04:49,663]\u001b[0m Trial 266 finished with value: 0.3290257623872243 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:05:02,967]\u001b[0m Trial 267 finished with value: 0.029283591893088157 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:05:15,134]\u001b[0m Trial 268 finished with value: 0.6476570925261399 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:05:27,194]\u001b[0m Trial 269 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:05:39,312]\u001b[0m Trial 270 finished with value: 0.7134759458836286 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:05:51,693]\u001b[0m Trial 271 finished with value: 0.55303421917517 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:06:04,395]\u001b[0m Trial 272 finished with value: 0.3994793999761307 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:06:17,319]\u001b[0m Trial 273 finished with value: 0.14067713524433487 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:06:30,580]\u001b[0m Trial 274 finished with value: 0.08549915481166893 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:06:42,679]\u001b[0m Trial 275 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:06:54,745]\u001b[0m Trial 276 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:07:06,809]\u001b[0m Trial 277 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:07:20,164]\u001b[0m Trial 278 finished with value: 0.026579630079919113 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:07:32,221]\u001b[0m Trial 279 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:07:44,278]\u001b[0m Trial 280 finished with value: 0.7119439668512018 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:07:56,363]\u001b[0m Trial 281 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:08:10,205]\u001b[0m Trial 282 finished with value: 0.02687481941686972 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:08:22,384]\u001b[0m Trial 283 finished with value: 0.6840334409177519 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:08:34,446]\u001b[0m Trial 284 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:08:46,894]\u001b[0m Trial 285 finished with value: 0.6357502173505818 and parameters: {'C': 32.0, 'gamma': 0.000244140625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:08:58,923]\u001b[0m Trial 286 finished with value: 0.6996893494409646 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:09:10,989]\u001b[0m Trial 287 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:09:22,956]\u001b[0m Trial 288 finished with value: 0.7180939499413553 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:09:35,273]\u001b[0m Trial 289 finished with value: 0.6532611856565544 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:09:47,596]\u001b[0m Trial 290 finished with value: 0.5595613265887528 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:10:00,344]\u001b[0m Trial 291 finished with value: 0.29230166924802903 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:10:13,375]\u001b[0m Trial 292 finished with value: 0.28767483146000267 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:10:25,440]\u001b[0m Trial 293 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 10:10:38,597]\u001b[0m Trial 294 finished with value: 0.015074117122708142 and parameters: {'C': 0.0078125, 'gamma': 0.0625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:10:50,698]\u001b[0m Trial 295 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:11:02,797]\u001b[0m Trial 296 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:11:14,895]\u001b[0m Trial 297 finished with value: 0.7210682454360278 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:11:27,757]\u001b[0m Trial 298 finished with value: 0.17213203616409672 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:11:40,963]\u001b[0m Trial 299 finished with value: 0.005534916666438916 and parameters: {'C': 0.125, 'gamma': 3.0517578125e-05}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7327\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.728270    0.732425    0.740376    0.756597   \n",
      "1                    TP   36.000000   36.000000   33.000000   39.000000   \n",
      "2                    TN  304.000000  310.000000  301.000000  307.000000   \n",
      "3                    FP    9.000000    4.000000   13.000000    8.000000   \n",
      "4                    FN   33.000000   32.000000   35.000000   28.000000   \n",
      "5              Accuracy    0.890052    0.905759    0.874346    0.905759   \n",
      "6             Precision    0.800000    0.900000    0.717391    0.829787   \n",
      "7           Sensitivity    0.521739    0.529412    0.485294    0.582090   \n",
      "8           Specificity    0.971200    0.987300    0.958600    0.974600   \n",
      "9              F1 score    0.631579    0.666667    0.578947    0.684211   \n",
      "10  F1 score (weighted)    0.880509    0.895554    0.864347    0.898942   \n",
      "11     F1 score (macro)    0.783482    0.805894    0.752551    0.814413   \n",
      "12    Balanced Accuracy    0.746493    0.758336    0.721946    0.778346   \n",
      "13                  MCC    0.588314    0.645494    0.521734    0.644521   \n",
      "14                  NPV    0.902100    0.906400    0.895800    0.916400   \n",
      "15              ROC_AUC    0.746493    0.758336    0.721946    0.778346   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.673235    0.713100  \n",
      "1    35.000000   36.000000  \n",
      "2   306.000000  303.000000  \n",
      "3     9.000000    9.000000  \n",
      "4    32.000000   34.000000  \n",
      "5     0.892670    0.887435  \n",
      "6     0.795455    0.800000  \n",
      "7     0.522388    0.514286  \n",
      "8     0.971400    0.971200  \n",
      "9     0.630631    0.626087  \n",
      "10    0.883441    0.877367  \n",
      "11    0.783922    0.779916  \n",
      "12    0.746908    0.742720  \n",
      "13    0.588266    0.582559  \n",
      "14    0.905300    0.899100  \n",
      "15    0.746908    0.742720  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_svm_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "Y_testSet5_cat = np.where(((Y_testSet5>=2) | (Y_testSet5<=-2)), 1, 0) \n",
    "y_pred_svm_5_cat = np.where(((y_pred_svm_5 >= 2) | (y_pred_svm_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_svm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 10:11:54,748]\u001b[0m Trial 300 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:12:07,022]\u001b[0m Trial 301 finished with value: 0.637531721295862 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:12:19,106]\u001b[0m Trial 302 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:12:31,192]\u001b[0m Trial 303 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:12:43,553]\u001b[0m Trial 304 finished with value: 0.5970814959564432 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:12:56,111]\u001b[0m Trial 305 finished with value: 0.3331311784036584 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:13:08,294]\u001b[0m Trial 306 finished with value: 0.7124372804095528 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:13:21,548]\u001b[0m Trial 307 finished with value: 0.03813929358394036 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:13:34,834]\u001b[0m Trial 308 finished with value: 0.026907872416975388 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:13:47,041]\u001b[0m Trial 309 finished with value: 0.7121396687198553 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:13:59,157]\u001b[0m Trial 310 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:14:11,974]\u001b[0m Trial 311 finished with value: 0.1424063665424342 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:14:24,645]\u001b[0m Trial 312 finished with value: 0.3953629657227215 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:14:37,909]\u001b[0m Trial 313 finished with value: 0.02053576864801354 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:14:50,025]\u001b[0m Trial 314 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:15:02,441]\u001b[0m Trial 315 finished with value: 0.5464633595297237 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:15:14,555]\u001b[0m Trial 316 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:15:26,640]\u001b[0m Trial 317 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:15:38,725]\u001b[0m Trial 318 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:15:51,978]\u001b[0m Trial 319 finished with value: 0.07599707839411257 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:16:04,062]\u001b[0m Trial 320 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:16:16,147]\u001b[0m Trial 321 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:16:28,232]\u001b[0m Trial 322 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:16:41,518]\u001b[0m Trial 323 finished with value: 0.01814468922634078 and parameters: {'C': 32.0, 'gamma': 4.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:16:53,628]\u001b[0m Trial 324 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:17:05,762]\u001b[0m Trial 325 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:17:19,528]\u001b[0m Trial 326 finished with value: 0.017439633281600565 and parameters: {'C': 2.0, 'gamma': 8.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:17:31,693]\u001b[0m Trial 327 finished with value: 0.6551720279129181 and parameters: {'C': 1.0, 'gamma': 0.0078125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:17:43,776]\u001b[0m Trial 328 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:17:56,610]\u001b[0m Trial 329 finished with value: 0.11662216400664815 and parameters: {'C': 0.25, 'gamma': 0.000244140625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:18:08,788]\u001b[0m Trial 330 finished with value: 0.6669301022843417 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:18:21,448]\u001b[0m Trial 331 finished with value: 0.22730416333639497 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:18:34,549]\u001b[0m Trial 332 finished with value: 0.027293268444292418 and parameters: {'C': 0.0078125, 'gamma': 0.001953125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:18:47,209]\u001b[0m Trial 333 finished with value: 0.29266703903301633 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:18:59,303]\u001b[0m Trial 334 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:19:11,871]\u001b[0m Trial 335 finished with value: 0.44958928527902636 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:19:24,702]\u001b[0m Trial 336 finished with value: 0.1953353572285282 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:19:37,134]\u001b[0m Trial 337 finished with value: 0.5836945879597634 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:19:50,244]\u001b[0m Trial 338 finished with value: 0.14409981190038626 and parameters: {'C': 0.5, 'gamma': 0.125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:20:02,368]\u001b[0m Trial 339 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:20:14,493]\u001b[0m Trial 340 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:20:26,617]\u001b[0m Trial 341 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:20:39,222]\u001b[0m Trial 342 finished with value: 0.27328005159920477 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:20:51,434]\u001b[0m Trial 343 finished with value: 0.7124372804095528 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 10:21:03,545]\u001b[0m Trial 344 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:21:15,657]\u001b[0m Trial 345 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:21:29,658]\u001b[0m Trial 346 finished with value: 0.5804483721809923 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:21:41,737]\u001b[0m Trial 347 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:21:54,812]\u001b[0m Trial 348 finished with value: -0.0036631646227926586 and parameters: {'C': 0.015625, 'gamma': 1.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:22:06,899]\u001b[0m Trial 349 finished with value: 0.7127859797569068 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7327\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.728270    0.732425    0.740376    0.756597   \n",
      "1                    TP   36.000000   36.000000   33.000000   39.000000   \n",
      "2                    TN  304.000000  310.000000  301.000000  307.000000   \n",
      "3                    FP    9.000000    4.000000   13.000000    8.000000   \n",
      "4                    FN   33.000000   32.000000   35.000000   28.000000   \n",
      "5              Accuracy    0.890052    0.905759    0.874346    0.905759   \n",
      "6             Precision    0.800000    0.900000    0.717391    0.829787   \n",
      "7           Sensitivity    0.521739    0.529412    0.485294    0.582090   \n",
      "8           Specificity    0.971200    0.987300    0.958600    0.974600   \n",
      "9              F1 score    0.631579    0.666667    0.578947    0.684211   \n",
      "10  F1 score (weighted)    0.880509    0.895554    0.864347    0.898942   \n",
      "11     F1 score (macro)    0.783482    0.805894    0.752551    0.814413   \n",
      "12    Balanced Accuracy    0.746493    0.758336    0.721946    0.778346   \n",
      "13                  MCC    0.588314    0.645494    0.521734    0.644521   \n",
      "14                  NPV    0.902100    0.906400    0.895800    0.916400   \n",
      "15              ROC_AUC    0.746493    0.758336    0.721946    0.778346   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.673235    0.713100    0.733415  \n",
      "1    35.000000   36.000000   28.000000  \n",
      "2   306.000000  303.000000  306.000000  \n",
      "3     9.000000    9.000000    9.000000  \n",
      "4    32.000000   34.000000   39.000000  \n",
      "5     0.892670    0.887435    0.874346  \n",
      "6     0.795455    0.800000    0.756757  \n",
      "7     0.522388    0.514286    0.417910  \n",
      "8     0.971400    0.971200    0.971400  \n",
      "9     0.630631    0.626087    0.538462  \n",
      "10    0.883441    0.877367    0.859078  \n",
      "11    0.783922    0.779916    0.732867  \n",
      "12    0.746908    0.742720    0.694670  \n",
      "13    0.588266    0.582559    0.500622  \n",
      "14    0.905300    0.899100    0.887000  \n",
      "15    0.746908    0.742720    0.694670  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_svm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "Y_testSet6_cat = np.where(((Y_testSet6>=2) | (Y_testSet6<=-2)), 1, 0) \n",
    "y_pred_svm_6_cat = np.where(((y_pred_svm_6 >= 2) | (y_pred_svm_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_svm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 10:22:20,784]\u001b[0m Trial 350 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:22:34,032]\u001b[0m Trial 351 finished with value: 0.06056291571506519 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:22:46,156]\u001b[0m Trial 352 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:22:58,275]\u001b[0m Trial 353 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:23:11,539]\u001b[0m Trial 354 finished with value: 0.0399670870971234 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:23:23,657]\u001b[0m Trial 355 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:23:36,517]\u001b[0m Trial 356 finished with value: 0.38369483982152064 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:23:48,614]\u001b[0m Trial 357 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:24:01,152]\u001b[0m Trial 358 finished with value: 0.5436537970886641 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:24:13,276]\u001b[0m Trial 359 finished with value: 0.7156758801235592 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:24:26,489]\u001b[0m Trial 360 finished with value: 0.10220567799611024 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:24:38,597]\u001b[0m Trial 361 finished with value: 0.6885626546790831 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:24:50,661]\u001b[0m Trial 362 finished with value: 0.7119687688841134 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:25:02,759]\u001b[0m Trial 363 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:25:14,859]\u001b[0m Trial 364 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:25:27,354]\u001b[0m Trial 365 finished with value: 0.5535566571253148 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:25:39,546]\u001b[0m Trial 366 finished with value: 0.703692039943791 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:25:51,645]\u001b[0m Trial 367 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:26:04,861]\u001b[0m Trial 368 finished with value: -0.0041539072914475676 and parameters: {'C': 0.0078125, 'gamma': 4.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:26:18,593]\u001b[0m Trial 369 finished with value: 0.03686821912411139 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:26:31,495]\u001b[0m Trial 370 finished with value: 0.15253568826133193 and parameters: {'C': 0.03125, 'gamma': 0.00390625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:26:43,594]\u001b[0m Trial 371 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:26:55,683]\u001b[0m Trial 372 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:27:08,315]\u001b[0m Trial 373 finished with value: 0.47598041034963 and parameters: {'C': 4.0, 'gamma': 0.000244140625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:27:20,640]\u001b[0m Trial 374 finished with value: 0.6365919014033846 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:27:32,726]\u001b[0m Trial 375 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:27:45,568]\u001b[0m Trial 376 finished with value: 0.28033381484711173 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:27:58,412]\u001b[0m Trial 377 finished with value: 0.2591131029318149 and parameters: {'C': 0.125, 'gamma': 0.001953125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:28:10,533]\u001b[0m Trial 378 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:28:23,250]\u001b[0m Trial 379 finished with value: 0.320019805885756 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:28:36,327]\u001b[0m Trial 380 finished with value: 0.3048606550962735 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:28:48,496]\u001b[0m Trial 381 finished with value: 0.7156367996886558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:29:00,611]\u001b[0m Trial 382 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:29:12,708]\u001b[0m Trial 383 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:29:25,628]\u001b[0m Trial 384 finished with value: 0.13679602171545008 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:29:37,918]\u001b[0m Trial 385 finished with value: 0.7014849998694901 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:29:50,318]\u001b[0m Trial 386 finished with value: 0.5898907504093783 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:30:02,402]\u001b[0m Trial 387 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:30:15,287]\u001b[0m Trial 388 finished with value: 0.18602865989243855 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:30:27,396]\u001b[0m Trial 389 finished with value: 0.7189355882845796 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:30:40,620]\u001b[0m Trial 390 finished with value: 0.046970824824457816 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:30:53,096]\u001b[0m Trial 391 finished with value: 0.5981418709830156 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:31:06,404]\u001b[0m Trial 392 finished with value: 0.061088400160541845 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:31:18,571]\u001b[0m Trial 393 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 10:31:30,720]\u001b[0m Trial 394 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:31:42,852]\u001b[0m Trial 395 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:31:56,136]\u001b[0m Trial 396 finished with value: 0.039445007840804736 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:32:08,263]\u001b[0m Trial 397 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:32:20,396]\u001b[0m Trial 398 finished with value: 0.6885626546790831 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:32:32,520]\u001b[0m Trial 399 finished with value: 0.7185225521040166 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7327\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.728270    0.732425    0.740376    0.756597   \n",
      "1                    TP   36.000000   36.000000   33.000000   39.000000   \n",
      "2                    TN  304.000000  310.000000  301.000000  307.000000   \n",
      "3                    FP    9.000000    4.000000   13.000000    8.000000   \n",
      "4                    FN   33.000000   32.000000   35.000000   28.000000   \n",
      "5              Accuracy    0.890052    0.905759    0.874346    0.905759   \n",
      "6             Precision    0.800000    0.900000    0.717391    0.829787   \n",
      "7           Sensitivity    0.521739    0.529412    0.485294    0.582090   \n",
      "8           Specificity    0.971200    0.987300    0.958600    0.974600   \n",
      "9              F1 score    0.631579    0.666667    0.578947    0.684211   \n",
      "10  F1 score (weighted)    0.880509    0.895554    0.864347    0.898942   \n",
      "11     F1 score (macro)    0.783482    0.805894    0.752551    0.814413   \n",
      "12    Balanced Accuracy    0.746493    0.758336    0.721946    0.778346   \n",
      "13                  MCC    0.588314    0.645494    0.521734    0.644521   \n",
      "14                  NPV    0.902100    0.906400    0.895800    0.916400   \n",
      "15              ROC_AUC    0.746493    0.758336    0.721946    0.778346   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.673235    0.713100    0.733415    0.735893  \n",
      "1    35.000000   36.000000   28.000000   36.000000  \n",
      "2   306.000000  303.000000  306.000000  309.000000  \n",
      "3     9.000000    9.000000    9.000000    5.000000  \n",
      "4    32.000000   34.000000   39.000000   32.000000  \n",
      "5     0.892670    0.887435    0.874346    0.903141  \n",
      "6     0.795455    0.800000    0.756757    0.878049  \n",
      "7     0.522388    0.514286    0.417910    0.529412  \n",
      "8     0.971400    0.971200    0.971400    0.984100  \n",
      "9     0.630631    0.626087    0.538462    0.660550  \n",
      "10    0.883441    0.877367    0.859078    0.893141  \n",
      "11    0.783922    0.779916    0.732867    0.802031  \n",
      "12    0.746908    0.742720    0.694670    0.756744  \n",
      "13    0.588266    0.582559    0.500622    0.634572  \n",
      "14    0.905300    0.899100    0.887000    0.906200  \n",
      "15    0.746908    0.742720    0.694670    0.756744  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_svm_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "Y_testSet7_cat = np.where(((Y_testSet7>=2) | (Y_testSet7<=-2)), 1, 0) \n",
    "y_pred_svm_7_cat = np.where(((y_pred_svm_7 >= 2) | (y_pred_svm_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_svm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 10:32:46,364]\u001b[0m Trial 400 finished with value: 0.7121169002807722 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:32:59,069]\u001b[0m Trial 401 finished with value: 0.3977309906304868 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:33:11,276]\u001b[0m Trial 402 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:33:23,858]\u001b[0m Trial 403 finished with value: 0.5552624520641278 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:33:37,148]\u001b[0m Trial 404 finished with value: 0.02487285850184474 and parameters: {'C': 0.25, 'gamma': 0.25}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:33:49,355]\u001b[0m Trial 405 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:34:02,543]\u001b[0m Trial 406 finished with value: 0.07818094579538856 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:34:15,390]\u001b[0m Trial 407 finished with value: 0.2231325701860159 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:34:27,619]\u001b[0m Trial 408 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:34:39,830]\u001b[0m Trial 409 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:34:53,194]\u001b[0m Trial 410 finished with value: 0.024131677370935422 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:35:05,898]\u001b[0m Trial 411 finished with value: 0.44974885801449 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:35:18,112]\u001b[0m Trial 412 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:35:30,313]\u001b[0m Trial 413 finished with value: 0.7049125423157283 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:35:43,330]\u001b[0m Trial 414 finished with value: 0.18995161846639766 and parameters: {'C': 0.5, 'gamma': 0.000244140625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:35:57,149]\u001b[0m Trial 415 finished with value: 0.02376256629350466 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:36:10,076]\u001b[0m Trial 416 finished with value: 0.24786264740591638 and parameters: {'C': 0.0625, 'gamma': 0.00390625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:36:22,254]\u001b[0m Trial 417 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:36:34,772]\u001b[0m Trial 418 finished with value: 0.6049938242144505 and parameters: {'C': 64.0, 'gamma': 6.103515625e-05}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:36:46,953]\u001b[0m Trial 419 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:36:59,128]\u001b[0m Trial 420 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:37:12,177]\u001b[0m Trial 421 finished with value: 0.2808356548898338 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:37:25,993]\u001b[0m Trial 422 finished with value: 0.6273950672321115 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:37:38,983]\u001b[0m Trial 423 finished with value: 0.13955266628967594 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:37:51,172]\u001b[0m Trial 424 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:38:03,346]\u001b[0m Trial 425 finished with value: 0.7142375079094928 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:38:16,338]\u001b[0m Trial 426 finished with value: 0.1920287817160731 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:38:28,516]\u001b[0m Trial 427 finished with value: 0.7155933633933061 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:38:40,736]\u001b[0m Trial 428 finished with value: 0.6967940740870726 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:38:52,924]\u001b[0m Trial 429 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:39:05,109]\u001b[0m Trial 430 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:39:17,283]\u001b[0m Trial 431 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:39:29,769]\u001b[0m Trial 432 finished with value: 0.6093534103637622 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:39:42,144]\u001b[0m Trial 433 finished with value: 0.5775514209206628 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:39:54,329]\u001b[0m Trial 434 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:40:07,539]\u001b[0m Trial 435 finished with value: 0.038091985631283284 and parameters: {'C': 1.0, 'gamma': 0.5}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:40:20,847]\u001b[0m Trial 436 finished with value: 0.03233661583977658 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:40:33,032]\u001b[0m Trial 437 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:40:45,127]\u001b[0m Trial 438 finished with value: 0.7121169002807722 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:40:57,311]\u001b[0m Trial 439 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:41:09,500]\u001b[0m Trial 440 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:41:21,683]\u001b[0m Trial 441 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:41:34,797]\u001b[0m Trial 442 finished with value: 0.061538305339278176 and parameters: {'C': 0.25, 'gamma': 0.0001220703125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:41:46,979]\u001b[0m Trial 443 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 10:42:00,258]\u001b[0m Trial 444 finished with value: -0.003684830558219976 and parameters: {'C': 0.0078125, 'gamma': 2.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:42:12,439]\u001b[0m Trial 445 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:42:25,729]\u001b[0m Trial 446 finished with value: 0.02965355054937502 and parameters: {'C': 0.03125, 'gamma': 0.00048828125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:42:37,994]\u001b[0m Trial 447 finished with value: 0.7175024573436747 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:42:51,351]\u001b[0m Trial 448 finished with value: 0.07769387532640494 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:43:04,085]\u001b[0m Trial 449 finished with value: 0.44974885801449 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7327\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.728270    0.732425    0.740376    0.756597   \n",
      "1                    TP   36.000000   36.000000   33.000000   39.000000   \n",
      "2                    TN  304.000000  310.000000  301.000000  307.000000   \n",
      "3                    FP    9.000000    4.000000   13.000000    8.000000   \n",
      "4                    FN   33.000000   32.000000   35.000000   28.000000   \n",
      "5              Accuracy    0.890052    0.905759    0.874346    0.905759   \n",
      "6             Precision    0.800000    0.900000    0.717391    0.829787   \n",
      "7           Sensitivity    0.521739    0.529412    0.485294    0.582090   \n",
      "8           Specificity    0.971200    0.987300    0.958600    0.974600   \n",
      "9              F1 score    0.631579    0.666667    0.578947    0.684211   \n",
      "10  F1 score (weighted)    0.880509    0.895554    0.864347    0.898942   \n",
      "11     F1 score (macro)    0.783482    0.805894    0.752551    0.814413   \n",
      "12    Balanced Accuracy    0.746493    0.758336    0.721946    0.778346   \n",
      "13                  MCC    0.588314    0.645494    0.521734    0.644521   \n",
      "14                  NPV    0.902100    0.906400    0.895800    0.916400   \n",
      "15              ROC_AUC    0.746493    0.758336    0.721946    0.778346   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.673235    0.713100    0.733415    0.735893    0.696699  \n",
      "1    35.000000   36.000000   28.000000   36.000000   35.000000  \n",
      "2   306.000000  303.000000  306.000000  309.000000  309.000000  \n",
      "3     9.000000    9.000000    9.000000    5.000000    4.000000  \n",
      "4    32.000000   34.000000   39.000000   32.000000   34.000000  \n",
      "5     0.892670    0.887435    0.874346    0.903141    0.900524  \n",
      "6     0.795455    0.800000    0.756757    0.878049    0.897436  \n",
      "7     0.522388    0.514286    0.417910    0.529412    0.507246  \n",
      "8     0.971400    0.971200    0.971400    0.984100    0.987200  \n",
      "9     0.630631    0.626087    0.538462    0.660550    0.648148  \n",
      "10    0.883441    0.877367    0.859078    0.893141    0.888982  \n",
      "11    0.783922    0.779916    0.732867    0.802031    0.795111  \n",
      "12    0.746908    0.742720    0.694670    0.756744    0.747233  \n",
      "13    0.588266    0.582559    0.500622    0.634572    0.628282  \n",
      "14    0.905300    0.899100    0.887000    0.906200    0.900900  \n",
      "15    0.746908    0.742720    0.694670    0.756744    0.747233  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_svm_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "Y_testSet8_cat = np.where(((Y_testSet8>=2) | (Y_testSet8<=-2)), 1, 0) \n",
    "y_pred_svm_8_cat = np.where(((y_pred_svm_8 >= 2) | (y_pred_svm_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_svm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 10:43:19,222]\u001b[0m Trial 450 finished with value: 0.025818275072507092 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:43:31,378]\u001b[0m Trial 451 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:43:43,534]\u001b[0m Trial 452 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:43:55,883]\u001b[0m Trial 453 finished with value: 0.6732602697248062 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:44:08,248]\u001b[0m Trial 454 finished with value: 0.6384043446864224 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:44:21,468]\u001b[0m Trial 455 finished with value: 0.03255568945381253 and parameters: {'C': 0.0625, 'gamma': 0.000244140625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:44:33,614]\u001b[0m Trial 456 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:44:45,889]\u001b[0m Trial 457 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:44:58,251]\u001b[0m Trial 458 finished with value: 0.7049533669577431 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:45:12,205]\u001b[0m Trial 459 finished with value: 0.025549716787923848 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:45:24,557]\u001b[0m Trial 460 finished with value: 0.7188217829587469 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:45:37,775]\u001b[0m Trial 461 finished with value: 0.057086047963742295 and parameters: {'C': 0.015625, 'gamma': 0.001953125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:45:50,408]\u001b[0m Trial 462 finished with value: 0.6298607115768062 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:46:02,608]\u001b[0m Trial 463 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:46:14,880]\u001b[0m Trial 464 finished with value: 0.7208049730046089 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:46:27,066]\u001b[0m Trial 465 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:46:39,466]\u001b[0m Trial 466 finished with value: 0.5856490111426061 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:46:51,746]\u001b[0m Trial 467 finished with value: 0.7208719501577627 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:47:04,838]\u001b[0m Trial 468 finished with value: 0.29924408982225115 and parameters: {'C': 4.0, 'gamma': 0.125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:47:17,850]\u001b[0m Trial 469 finished with value: 0.19557132087417814 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:47:30,013]\u001b[0m Trial 470 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:47:42,162]\u001b[0m Trial 471 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:47:54,336]\u001b[0m Trial 472 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:48:06,536]\u001b[0m Trial 473 finished with value: 0.698899227561032 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:48:18,701]\u001b[0m Trial 474 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:48:31,184]\u001b[0m Trial 475 finished with value: 0.606165698159318 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:48:43,272]\u001b[0m Trial 476 finished with value: 0.6907377658003159 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:48:56,643]\u001b[0m Trial 477 finished with value: 0.03325761368466288 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:49:08,667]\u001b[0m Trial 478 finished with value: 0.7128301802791996 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:49:20,853]\u001b[0m Trial 479 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:49:33,056]\u001b[0m Trial 480 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:49:46,393]\u001b[0m Trial 481 finished with value: 0.009802530292093414 and parameters: {'C': 0.25, 'gamma': 2.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:49:58,597]\u001b[0m Trial 482 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:50:10,806]\u001b[0m Trial 483 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:50:23,031]\u001b[0m Trial 484 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:50:36,359]\u001b[0m Trial 485 finished with value: -0.0008394008256455754 and parameters: {'C': 0.03125, 'gamma': 0.5}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:50:49,291]\u001b[0m Trial 486 finished with value: 0.3970761594469545 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:51:01,868]\u001b[0m Trial 487 finished with value: 0.552439061236847 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:51:14,608]\u001b[0m Trial 488 finished with value: 0.4489306792112808 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:51:27,733]\u001b[0m Trial 489 finished with value: 0.0813986479801756 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:51:41,014]\u001b[0m Trial 490 finished with value: 0.08537109664162092 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:51:53,166]\u001b[0m Trial 491 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:52:05,545]\u001b[0m Trial 492 finished with value: 0.6384043446864224 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:52:17,686]\u001b[0m Trial 493 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 10:52:29,837]\u001b[0m Trial 494 finished with value: 0.7177577993441846 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:52:43,211]\u001b[0m Trial 495 finished with value: 0.025821283879391765 and parameters: {'C': 64.0, 'gamma': 4.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:52:57,039]\u001b[0m Trial 496 finished with value: 0.025549716787923848 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:53:09,260]\u001b[0m Trial 497 finished with value: 0.7049533669577431 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:53:22,246]\u001b[0m Trial 498 finished with value: 0.14356463590454216 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 10:53:34,931]\u001b[0m Trial 499 finished with value: 0.681047575304757 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 201 with value: 0.732729100402606.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7327\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.728270    0.732425    0.740376    0.756597   \n",
      "1                    TP   36.000000   36.000000   33.000000   39.000000   \n",
      "2                    TN  304.000000  310.000000  301.000000  307.000000   \n",
      "3                    FP    9.000000    4.000000   13.000000    8.000000   \n",
      "4                    FN   33.000000   32.000000   35.000000   28.000000   \n",
      "5              Accuracy    0.890052    0.905759    0.874346    0.905759   \n",
      "6             Precision    0.800000    0.900000    0.717391    0.829787   \n",
      "7           Sensitivity    0.521739    0.529412    0.485294    0.582090   \n",
      "8           Specificity    0.971200    0.987300    0.958600    0.974600   \n",
      "9              F1 score    0.631579    0.666667    0.578947    0.684211   \n",
      "10  F1 score (weighted)    0.880509    0.895554    0.864347    0.898942   \n",
      "11     F1 score (macro)    0.783482    0.805894    0.752551    0.814413   \n",
      "12    Balanced Accuracy    0.746493    0.758336    0.721946    0.778346   \n",
      "13                  MCC    0.588314    0.645494    0.521734    0.644521   \n",
      "14                  NPV    0.902100    0.906400    0.895800    0.916400   \n",
      "15              ROC_AUC    0.746493    0.758336    0.721946    0.778346   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.673235    0.713100    0.733415    0.735893    0.696699    0.717591  \n",
      "1    35.000000   36.000000   28.000000   36.000000   35.000000   29.000000  \n",
      "2   306.000000  303.000000  306.000000  309.000000  309.000000  311.000000  \n",
      "3     9.000000    9.000000    9.000000    5.000000    4.000000    4.000000  \n",
      "4    32.000000   34.000000   39.000000   32.000000   34.000000   38.000000  \n",
      "5     0.892670    0.887435    0.874346    0.903141    0.900524    0.890052  \n",
      "6     0.795455    0.800000    0.756757    0.878049    0.897436    0.878788  \n",
      "7     0.522388    0.514286    0.417910    0.529412    0.507246    0.432836  \n",
      "8     0.971400    0.971200    0.971400    0.984100    0.987200    0.987300  \n",
      "9     0.630631    0.626087    0.538462    0.660550    0.648148    0.580000  \n",
      "10    0.883441    0.877367    0.859078    0.893141    0.888982    0.874176  \n",
      "11    0.783922    0.779916    0.732867    0.802031    0.795111    0.758373  \n",
      "12    0.746908    0.742720    0.694670    0.756744    0.747233    0.710069  \n",
      "13    0.588266    0.582559    0.500622    0.634572    0.628282    0.568741  \n",
      "14    0.905300    0.899100    0.887000    0.906200    0.900900    0.891100  \n",
      "15    0.746908    0.742720    0.694670    0.756744    0.747233    0.710069  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_svm_9)\n",
    "# now convert the resuls to binary with cutoff 6.9\n",
    "Y_testSet9_cat = np.where(((Y_testSet9>=2) | (Y_testSet9<=-2)), 1, 0) \n",
    "y_pred_svm_9_cat = np.where(((y_pred_svm_9 >= 2) | (y_pred_svm_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_svm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7b0e56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7327\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "#plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.740221</td>\n",
       "      <td>0.038667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>2.347576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>154.100000</td>\n",
       "      <td>1.286684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.080123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>16.600000</td>\n",
       "      <td>1.577621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.894764</td>\n",
       "      <td>0.012931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.825413</td>\n",
       "      <td>0.056121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.501781</td>\n",
       "      <td>0.057208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.977810</td>\n",
       "      <td>0.006806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.623558</td>\n",
       "      <td>0.058286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.883866</td>\n",
       "      <td>0.015011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.781181</td>\n",
       "      <td>0.032574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.739792</td>\n",
       "      <td>0.030949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.590727</td>\n",
       "      <td>0.062543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.902810</td>\n",
       "      <td>0.008544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.739792</td>\n",
       "      <td>0.030949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.740221     0.038667\n",
       "1                    TP        16.800000     2.347576\n",
       "2                    TN       154.100000     1.286684\n",
       "3                    FP         3.500000     1.080123\n",
       "4                    FN        16.600000     1.577621\n",
       "5              Accuracy         0.894764     0.012931\n",
       "6             Precision         0.825413     0.056121\n",
       "7           Sensitivity         0.501781     0.057208\n",
       "8           Specificity         0.977810     0.006806\n",
       "9              F1 score         0.623558     0.058286\n",
       "10  F1 score (weighted)         0.883866     0.015011\n",
       "11     F1 score (macro)         0.781181     0.032574\n",
       "12    Balanced Accuracy         0.739792     0.030949\n",
       "13                  MCC         0.590727     0.062543\n",
       "14                  NPV         0.902810     0.008544\n",
       "15              ROC_AUC         0.739792     0.030949"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.728270</td>\n",
       "      <td>0.732425</td>\n",
       "      <td>0.740376</td>\n",
       "      <td>0.756597</td>\n",
       "      <td>0.673235</td>\n",
       "      <td>0.713100</td>\n",
       "      <td>0.733415</td>\n",
       "      <td>0.735893</td>\n",
       "      <td>0.696699</td>\n",
       "      <td>0.717591</td>\n",
       "      <td>0.722760</td>\n",
       "      <td>0.023844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>34.300000</td>\n",
       "      <td>3.400980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>306.600000</td>\n",
       "      <td>3.238655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>3.025815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>33.700000</td>\n",
       "      <td>3.164034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.905759</td>\n",
       "      <td>0.874346</td>\n",
       "      <td>0.905759</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.887435</td>\n",
       "      <td>0.874346</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.892408</td>\n",
       "      <td>0.011671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.825366</td>\n",
       "      <td>0.062326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.432836</td>\n",
       "      <td>0.504261</td>\n",
       "      <td>0.048286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.971200</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>0.958600</td>\n",
       "      <td>0.974600</td>\n",
       "      <td>0.971400</td>\n",
       "      <td>0.971200</td>\n",
       "      <td>0.971400</td>\n",
       "      <td>0.984100</td>\n",
       "      <td>0.987200</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>0.976430</td>\n",
       "      <td>0.009648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.630631</td>\n",
       "      <td>0.626087</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.660550</td>\n",
       "      <td>0.648148</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.624528</td>\n",
       "      <td>0.045644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.880509</td>\n",
       "      <td>0.895554</td>\n",
       "      <td>0.864347</td>\n",
       "      <td>0.898942</td>\n",
       "      <td>0.883441</td>\n",
       "      <td>0.877367</td>\n",
       "      <td>0.859078</td>\n",
       "      <td>0.893141</td>\n",
       "      <td>0.888982</td>\n",
       "      <td>0.874176</td>\n",
       "      <td>0.881554</td>\n",
       "      <td>0.013189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.783482</td>\n",
       "      <td>0.805894</td>\n",
       "      <td>0.752551</td>\n",
       "      <td>0.814413</td>\n",
       "      <td>0.783922</td>\n",
       "      <td>0.779916</td>\n",
       "      <td>0.732867</td>\n",
       "      <td>0.802031</td>\n",
       "      <td>0.795111</td>\n",
       "      <td>0.758373</td>\n",
       "      <td>0.780856</td>\n",
       "      <td>0.025887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.746493</td>\n",
       "      <td>0.758336</td>\n",
       "      <td>0.721946</td>\n",
       "      <td>0.778346</td>\n",
       "      <td>0.746908</td>\n",
       "      <td>0.742720</td>\n",
       "      <td>0.694670</td>\n",
       "      <td>0.756744</td>\n",
       "      <td>0.747233</td>\n",
       "      <td>0.710069</td>\n",
       "      <td>0.740347</td>\n",
       "      <td>0.024748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.588314</td>\n",
       "      <td>0.645494</td>\n",
       "      <td>0.521734</td>\n",
       "      <td>0.644521</td>\n",
       "      <td>0.588266</td>\n",
       "      <td>0.582559</td>\n",
       "      <td>0.500622</td>\n",
       "      <td>0.634572</td>\n",
       "      <td>0.628282</td>\n",
       "      <td>0.568741</td>\n",
       "      <td>0.590310</td>\n",
       "      <td>0.050183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.902100</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>0.895800</td>\n",
       "      <td>0.916400</td>\n",
       "      <td>0.905300</td>\n",
       "      <td>0.899100</td>\n",
       "      <td>0.887000</td>\n",
       "      <td>0.906200</td>\n",
       "      <td>0.900900</td>\n",
       "      <td>0.891100</td>\n",
       "      <td>0.901030</td>\n",
       "      <td>0.008418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.746493</td>\n",
       "      <td>0.758336</td>\n",
       "      <td>0.721946</td>\n",
       "      <td>0.778346</td>\n",
       "      <td>0.746908</td>\n",
       "      <td>0.742720</td>\n",
       "      <td>0.694670</td>\n",
       "      <td>0.756744</td>\n",
       "      <td>0.747233</td>\n",
       "      <td>0.710069</td>\n",
       "      <td>0.740347</td>\n",
       "      <td>0.024748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.728270    0.732425    0.740376    0.756597   \n",
       "1                    TP   36.000000   36.000000   33.000000   39.000000   \n",
       "2                    TN  304.000000  310.000000  301.000000  307.000000   \n",
       "3                    FP    9.000000    4.000000   13.000000    8.000000   \n",
       "4                    FN   33.000000   32.000000   35.000000   28.000000   \n",
       "5              Accuracy    0.890052    0.905759    0.874346    0.905759   \n",
       "6             Precision    0.800000    0.900000    0.717391    0.829787   \n",
       "7           Sensitivity    0.521739    0.529412    0.485294    0.582090   \n",
       "8           Specificity    0.971200    0.987300    0.958600    0.974600   \n",
       "9              F1 score    0.631579    0.666667    0.578947    0.684211   \n",
       "10  F1 score (weighted)    0.880509    0.895554    0.864347    0.898942   \n",
       "11     F1 score (macro)    0.783482    0.805894    0.752551    0.814413   \n",
       "12    Balanced Accuracy    0.746493    0.758336    0.721946    0.778346   \n",
       "13                  MCC    0.588314    0.645494    0.521734    0.644521   \n",
       "14                  NPV    0.902100    0.906400    0.895800    0.916400   \n",
       "15              ROC_AUC    0.746493    0.758336    0.721946    0.778346   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.673235    0.713100    0.733415    0.735893    0.696699    0.717591   \n",
       "1    35.000000   36.000000   28.000000   36.000000   35.000000   29.000000   \n",
       "2   306.000000  303.000000  306.000000  309.000000  309.000000  311.000000   \n",
       "3     9.000000    9.000000    9.000000    5.000000    4.000000    4.000000   \n",
       "4    32.000000   34.000000   39.000000   32.000000   34.000000   38.000000   \n",
       "5     0.892670    0.887435    0.874346    0.903141    0.900524    0.890052   \n",
       "6     0.795455    0.800000    0.756757    0.878049    0.897436    0.878788   \n",
       "7     0.522388    0.514286    0.417910    0.529412    0.507246    0.432836   \n",
       "8     0.971400    0.971200    0.971400    0.984100    0.987200    0.987300   \n",
       "9     0.630631    0.626087    0.538462    0.660550    0.648148    0.580000   \n",
       "10    0.883441    0.877367    0.859078    0.893141    0.888982    0.874176   \n",
       "11    0.783922    0.779916    0.732867    0.802031    0.795111    0.758373   \n",
       "12    0.746908    0.742720    0.694670    0.756744    0.747233    0.710069   \n",
       "13    0.588266    0.582559    0.500622    0.634572    0.628282    0.568741   \n",
       "14    0.905300    0.899100    0.887000    0.906200    0.900900    0.891100   \n",
       "15    0.746908    0.742720    0.694670    0.756744    0.747233    0.710069   \n",
       "\n",
       "           ave       std  \n",
       "0     0.722760  0.023844  \n",
       "1    34.300000  3.400980  \n",
       "2   306.600000  3.238655  \n",
       "3     7.400000  3.025815  \n",
       "4    33.700000  3.164034  \n",
       "5     0.892408  0.011671  \n",
       "6     0.825366  0.062326  \n",
       "7     0.504261  0.048286  \n",
       "8     0.976430  0.009648  \n",
       "9     0.624528  0.045644  \n",
       "10    0.881554  0.013189  \n",
       "11    0.780856  0.025887  \n",
       "12    0.740347  0.024748  \n",
       "13    0.590310  0.050183  \n",
       "14    0.901030  0.008418  \n",
       "15    0.740347  0.024748  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test['std'] = mat_met_svm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_svm0</th>\n",
       "      <th>y_pred_svm1</th>\n",
       "      <th>y_pred_svm2</th>\n",
       "      <th>y_pred_svm3</th>\n",
       "      <th>y_pred_svm4</th>\n",
       "      <th>y_pred_svm_ave</th>\n",
       "      <th>y_pred_svm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL585939</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>-0.256169</td>\n",
       "      <td>-0.286499</td>\n",
       "      <td>-0.472578</td>\n",
       "      <td>-0.274509</td>\n",
       "      <td>-0.135059</td>\n",
       "      <td>-0.435802</td>\n",
       "      <td>0.351470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL96051</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.599326</td>\n",
       "      <td>1.434846</td>\n",
       "      <td>1.453320</td>\n",
       "      <td>1.441484</td>\n",
       "      <td>1.562506</td>\n",
       "      <td>1.378580</td>\n",
       "      <td>0.274944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3356916</td>\n",
       "      <td>2</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.382826</td>\n",
       "      <td>0.340030</td>\n",
       "      <td>0.204492</td>\n",
       "      <td>0.330517</td>\n",
       "      <td>0.398427</td>\n",
       "      <td>0.326049</td>\n",
       "      <td>0.063420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3907413</td>\n",
       "      <td>3</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.379836</td>\n",
       "      <td>0.380389</td>\n",
       "      <td>1.515301</td>\n",
       "      <td>0.380123</td>\n",
       "      <td>0.379929</td>\n",
       "      <td>0.667596</td>\n",
       "      <td>0.436031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2047704</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>-2.843864</td>\n",
       "      <td>-2.909972</td>\n",
       "      <td>-2.920186</td>\n",
       "      <td>-2.853222</td>\n",
       "      <td>-2.929457</td>\n",
       "      <td>-2.784450</td>\n",
       "      <td>0.241213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>CHEMBL1095136</td>\n",
       "      <td>1905</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>-0.898888</td>\n",
       "      <td>-0.915047</td>\n",
       "      <td>-0.953986</td>\n",
       "      <td>-0.842478</td>\n",
       "      <td>-0.903765</td>\n",
       "      <td>-0.952361</td>\n",
       "      <td>0.115481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>CHEMBL2012817</td>\n",
       "      <td>1906</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-0.732434</td>\n",
       "      <td>-0.561669</td>\n",
       "      <td>-0.635710</td>\n",
       "      <td>-0.350524</td>\n",
       "      <td>-0.580081</td>\n",
       "      <td>-0.966736</td>\n",
       "      <td>0.889901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>CHEMBL496511</td>\n",
       "      <td>1907</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.774021</td>\n",
       "      <td>0.895353</td>\n",
       "      <td>0.827289</td>\n",
       "      <td>0.807763</td>\n",
       "      <td>0.839926</td>\n",
       "      <td>0.824059</td>\n",
       "      <td>0.038064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>CHEMBL3940062</td>\n",
       "      <td>1908</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1.249047</td>\n",
       "      <td>1.219714</td>\n",
       "      <td>1.172657</td>\n",
       "      <td>1.179153</td>\n",
       "      <td>1.248406</td>\n",
       "      <td>1.389830</td>\n",
       "      <td>0.394759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>CHEMBL493749</td>\n",
       "      <td>1909</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.631599</td>\n",
       "      <td>0.652479</td>\n",
       "      <td>0.577952</td>\n",
       "      <td>0.524184</td>\n",
       "      <td>0.704652</td>\n",
       "      <td>0.570144</td>\n",
       "      <td>0.121443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1910 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_svm0  y_pred_svm1  \\\n",
       "0          CHEMBL585939            0    -1.19    -0.256169    -0.286499   \n",
       "1           CHEMBL96051            1     0.78     1.599326     1.434846   \n",
       "2         CHEMBL3356916            2     0.30     0.382826     0.340030   \n",
       "3         CHEMBL3907413            3     0.97     0.379836     0.380389   \n",
       "4         CHEMBL2047704            4    -2.25    -2.843864    -2.909972   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "1905      CHEMBL1095136         1905    -1.20    -0.898888    -0.915047   \n",
       "1906      CHEMBL2012817         1906    -2.94    -0.732434    -0.561669   \n",
       "1907       CHEMBL496511         1907     0.80     0.774021     0.895353   \n",
       "1908      CHEMBL3940062         1908     2.27     1.249047     1.219714   \n",
       "1909       CHEMBL493749         1909     0.33     0.631599     0.652479   \n",
       "\n",
       "      y_pred_svm2  y_pred_svm3  y_pred_svm4  y_pred_svm_ave  y_pred_svm_std  \n",
       "0       -0.472578    -0.274509    -0.135059       -0.435802        0.351470  \n",
       "1        1.453320     1.441484     1.562506        1.378580        0.274944  \n",
       "2        0.204492     0.330517     0.398427        0.326049        0.063420  \n",
       "3        1.515301     0.380123     0.379929        0.667596        0.436031  \n",
       "4       -2.920186    -2.853222    -2.929457       -2.784450        0.241213  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "1905    -0.953986    -0.842478    -0.903765       -0.952361        0.115481  \n",
       "1906    -0.635710    -0.350524    -0.580081       -0.966736        0.889901  \n",
       "1907     0.827289     0.807763     0.839926        0.824059        0.038064  \n",
       "1908     1.172657     1.179153     1.248406        1.389830        0.394759  \n",
       "1909     0.577952     0.524184     0.704652        0.570144        0.121443  \n",
       "\n",
       "[1910 rows x 10 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_svm_cat = np.where(((y_pred_optimized_svm >= 2) | (y_pred_optimized_svm <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_svm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_svm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "svm_run0 = data_svm[['y_test_idx0', 'y_test0', 'y_pred_svm0']]\n",
    "svm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "svm_run0.reset_index(inplace=True, drop=True)\n",
    "svm_run1 = data_svm[['y_test_idx1', 'y_test1', 'y_pred_svm1']]\n",
    "svm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "svm_run1.reset_index(inplace=True, drop=True)\n",
    "svm_run2 = data_svm[['y_test_idx2', 'y_test2', 'y_pred_svm2']]\n",
    "svm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "svm_run2.reset_index(inplace=True, drop=True)\n",
    "svm_run3 = data_svm[['y_test_idx3', 'y_test3', 'y_pred_svm3']]\n",
    "svm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "svm_run3.reset_index(inplace=True, drop=True)\n",
    "svm_run4 = data_svm[['y_test_idx4', 'y_test4', 'y_pred_svm4']]\n",
    "svm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "svm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "svm_5preds = pd.concat([chembl_id,svm_run0, svm_run1, svm_run2, svm_run3, svm_run4], axis=1)\n",
    "svm_5preds = svm_5preds[['molecule_chembl_id','y_test_idx0', 'y_test0', 'y_pred_svm0', 'y_pred_svm1', 'y_pred_svm2', 'y_pred_svm3', 'y_pred_svm4']]\n",
    "svm_5preds['y_pred_svm_ave'] = svm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "svm_5preds['y_pred_svm_std'] = svm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "svm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6394fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_svm.to_csv(output/'mat_met_svm_opt_withSemiSel.csv')\n",
    "svm_5preds.to_csv(output/'svm_5test_CV_result_withSemiSel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2869d8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model r2_score 0.7033 with a standard deviation of 0.0479\n",
      "SVM optimized model r2_score 0.7298 with a standard deviation of 0.0450\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVR \n",
    "svm_baseline_CVscore = cross_val_score(svm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"SVM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_svm.joblib']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_reg, output/\"svm_reg.joblib\")\n",
    "#joblib.dump(optimized_svm, output/\"optimized_svm.joblib\")\n",
    "joblib.dump(optimizedCV_svm, output/\"optimizedCV_svm.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
