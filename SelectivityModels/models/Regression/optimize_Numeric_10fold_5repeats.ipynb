{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/miniforge3/envs/teachopencadd/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "HDAC1and6 = Path(HERE).resolve().parents[1]/'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4443998</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[374334, 1061278, 963989, 218190, 2667007, 152...</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL1934906</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[8732066, 19118050, 11111289, 9391761, 235748,...</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL2311793</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[7319467, 16948577, 3844537, 7125875, 1422872,...</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4539004</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5976924, 4613032, 4703614, 11956864, 1293667,...</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4089964</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4681989, 2448961, 8033062, 26668239, 8519057,...</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4443998  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL1934906  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL2311793  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL4539004  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL4089964  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...   \n",
       "3  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  SelectivityWindow  \n",
       "0  [374334, 1061278, 963989, 218190, 2667007, 152...               0.11  \n",
       "1  [8732066, 19118050, 11111289, 9391761, 235748,...               0.73  \n",
       "2  [7319467, 16948577, 3844537, 7125875, 1422872,...               0.57  \n",
       "3  [5976924, 4613032, 4703614, 11956864, 1293667,...              -0.01  \n",
       "4  [4681989, 2448961, 8033062, 26668239, 8519057,...               0.24  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC1and6/\"HDAC1and6_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>type_HDAC1</th>\n",
       "      <th>Standard_Value_HDAC1</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "      <th>type_HDAC6</th>\n",
       "      <th>Standard_Value_HDAC6</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "      <th>SelectivityRatio</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4098975</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>109.647820</td>\n",
       "      <td>6.96</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.141254</td>\n",
       "      <td>9.85</td>\n",
       "      <td>776.247117</td>\n",
       "      <td>2.89</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3912061</td>\n",
       "      <td>CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12</td>\n",
       "      <td>IC50</td>\n",
       "      <td>616.595002</td>\n",
       "      <td>6.21</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.169824</td>\n",
       "      <td>9.77</td>\n",
       "      <td>3630.780548</td>\n",
       "      <td>3.56</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4243347</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1.995262</td>\n",
       "      <td>8.70</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.199526</td>\n",
       "      <td>9.70</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4247128</td>\n",
       "      <td>C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>83.176377</td>\n",
       "      <td>7.08</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.251189</td>\n",
       "      <td>9.60</td>\n",
       "      <td>331.131122</td>\n",
       "      <td>2.52</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4126811</td>\n",
       "      <td>CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>436.515832</td>\n",
       "      <td>6.36</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.331131</td>\n",
       "      <td>9.48</td>\n",
       "      <td>1318.256739</td>\n",
       "      <td>3.12</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>CHEMBL4167599</td>\n",
       "      <td>NCCCCNCCCCNCCCN1C(=O)c2ccc3c4c(ccc(c24)C1=O)C(...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>4073.802778</td>\n",
       "      <td>5.39</td>\n",
       "      <td>IC50</td>\n",
       "      <td>50.118723</td>\n",
       "      <td>7.30</td>\n",
       "      <td>81.283052</td>\n",
       "      <td>1.91</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>CHEMBL4282471</td>\n",
       "      <td>CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(NC(=O)CCCCC(=O)NO)...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>3388.441561</td>\n",
       "      <td>5.47</td>\n",
       "      <td>IC50</td>\n",
       "      <td>117.489756</td>\n",
       "      <td>6.93</td>\n",
       "      <td>28.840315</td>\n",
       "      <td>1.46</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>CHEMBL470843</td>\n",
       "      <td>O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO</td>\n",
       "      <td>IC50</td>\n",
       "      <td>6309.573445</td>\n",
       "      <td>5.20</td>\n",
       "      <td>IC50</td>\n",
       "      <td>173.780083</td>\n",
       "      <td>6.76</td>\n",
       "      <td>36.307805</td>\n",
       "      <td>1.56</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>CHEMBL3215861</td>\n",
       "      <td>CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC</td>\n",
       "      <td>Ki</td>\n",
       "      <td>28.183829</td>\n",
       "      <td>7.55</td>\n",
       "      <td>Ki</td>\n",
       "      <td>245.470892</td>\n",
       "      <td>6.61</td>\n",
       "      <td>0.114815</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>CHEMBL3233708</td>\n",
       "      <td>O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>354.813389</td>\n",
       "      <td>6.45</td>\n",
       "      <td>IC50</td>\n",
       "      <td>295.120923</td>\n",
       "      <td>6.53</td>\n",
       "      <td>1.202264</td>\n",
       "      <td>0.08</td>\n",
       "      <td>Non-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1910 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                             smiles  \\\n",
       "0         CHEMBL4098975  O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...   \n",
       "1         CHEMBL3912061     CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12   \n",
       "2         CHEMBL4243347  O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...   \n",
       "3         CHEMBL4247128  C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...   \n",
       "4         CHEMBL4126811  CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...   \n",
       "...                 ...                                                ...   \n",
       "1905      CHEMBL4167599  NCCCCNCCCCNCCCN1C(=O)c2ccc3c4c(ccc(c24)C1=O)C(...   \n",
       "1906      CHEMBL4282471  CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(NC(=O)CCCCC(=O)NO)...   \n",
       "1907       CHEMBL470843          O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO   \n",
       "1908      CHEMBL3215861           CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC   \n",
       "1909      CHEMBL3233708  O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...   \n",
       "\n",
       "     type_HDAC1  Standard_Value_HDAC1  pChEMBL_HDAC1 type_HDAC6  \\\n",
       "0          IC50            109.647820           6.96       IC50   \n",
       "1          IC50            616.595002           6.21       IC50   \n",
       "2          IC50              1.995262           8.70       IC50   \n",
       "3          IC50             83.176377           7.08       IC50   \n",
       "4          IC50            436.515832           6.36       IC50   \n",
       "...         ...                   ...            ...        ...   \n",
       "1905       IC50           4073.802778           5.39       IC50   \n",
       "1906       IC50           3388.441561           5.47       IC50   \n",
       "1907       IC50           6309.573445           5.20       IC50   \n",
       "1908         Ki             28.183829           7.55         Ki   \n",
       "1909       IC50            354.813389           6.45       IC50   \n",
       "\n",
       "      Standard_Value_HDAC6  pChEMBL_HDAC6  SelectivityRatio  \\\n",
       "0                 0.141254           9.85        776.247117   \n",
       "1                 0.169824           9.77       3630.780548   \n",
       "2                 0.199526           9.70         10.000000   \n",
       "3                 0.251189           9.60        331.131122   \n",
       "4                 0.331131           9.48       1318.256739   \n",
       "...                    ...            ...               ...   \n",
       "1905             50.118723           7.30         81.283052   \n",
       "1906            117.489756           6.93         28.840315   \n",
       "1907            173.780083           6.76         36.307805   \n",
       "1908            245.470892           6.61          0.114815   \n",
       "1909            295.120923           6.53          1.202264   \n",
       "\n",
       "      SelectivityWindow            label  \n",
       "0                  2.89  HDAC6-selective  \n",
       "1                  3.56  HDAC6-selective  \n",
       "2                  1.00      Dual-binder  \n",
       "3                  2.52  HDAC6-selective  \n",
       "4                  3.12  HDAC6-selective  \n",
       "...                 ...              ...  \n",
       "1905               1.91   Semi-selective  \n",
       "1906               1.46   Semi-selective  \n",
       "1907               1.56   Semi-selective  \n",
       "1908              -0.94      Dual-binder  \n",
       "1909               0.08       Non-binder  \n",
       "\n",
       "[1910 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC1and6/\"HDAC1and6_SemiSel_dataset.csv\", )\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca3dbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label'] == 'Dual-binder']['SelectivityWindow'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4443998</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[374334, 1061278, 963989, 218190, 2667007, 152...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL1934906</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[8732066, 19118050, 11111289, 9391761, 235748,...</td>\n",
       "      <td>0.73</td>\n",
       "      <td>Non-binder</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL2311793</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[7319467, 16948577, 3844537, 7125875, 1422872,...</td>\n",
       "      <td>0.57</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4539004</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5976924, 4613032, 4703614, 11956864, 1293667,...</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4443998  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL1934906  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL2311793  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL4539004  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...   \n",
       "3  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  SelectivityWindow  \\\n",
       "0  [374334, 1061278, 963989, 218190, 2667007, 152...               0.11   \n",
       "1  [8732066, 19118050, 11111289, 9391761, 235748,...               0.73   \n",
       "2  [7319467, 16948577, 3844537, 7125875, 1422872,...               0.57   \n",
       "3  [5976924, 4613032, 4703614, 11956864, 1293667,...              -0.01   \n",
       "\n",
       "         label  Class  \n",
       "0  Dual-binder    3.0  \n",
       "1   Non-binder    4.0  \n",
       "2  Dual-binder    3.0  \n",
       "3  Dual-binder    3.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'hDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'hDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'hDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for selectivity\n",
    "df[\"selectivity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as selective if SelectivityWindow is >=2 or >=-2, 0 otherwise\n",
    "df.loc[df[df.SelectivityWindow >= 2.0].index, \"selectivity\"] = 1.0\n",
    "df.loc[df[df.SelectivityWindow <= -2.0].index, \"selectivity\"] = 1.0\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "indices =  np.array(df.index)\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y = df[\"SelectivityWindow\"].values\n",
    "Y_cat =  df[\"selectivity\"].values\n",
    "Y_class = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['SelectivityWindow'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_cat\"] = np.array(list(df.iloc[index_tr_all[i]]['selectivity'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['SelectivityWindow'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_cat\"] = np.array(list(df.iloc[index_te_all[i]]['selectivity'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.704545     0.058799\n",
      "1                    TP        18.400000     3.533962\n",
      "2                    TN        98.800000     1.475730\n",
      "3                    FP         1.700000     1.251666\n",
      "4                    FN        15.000000     3.590110\n",
      "5              Accuracy         0.875300     0.030404\n",
      "6             Precision         0.914945     0.056620\n",
      "7           Sensitivity         0.551097     0.106164\n",
      "8           Specificity         0.983090     0.012450\n",
      "9              F1 score         0.683340     0.090786\n",
      "10  F1 score (weighted)         0.862668     0.036160\n",
      "11     F1 score (macro)         0.802801     0.054241\n",
      "12    Balanced Accuracy         0.767093     0.054957\n",
      "13                  MCC         0.645476     0.091915\n",
      "14                  NPV         0.868870     0.028182\n",
      "15              ROC_AUC         0.767093     0.054957\n",
      "CPU times: user 59.6 s, sys: 112 ms, total: 59.7 s\n",
      "Wall time: 8.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_reg =  RandomForestRegressor(random_state=1121218, max_features = None, n_jobs=8,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_reg.fit(x_train, y_train)\n",
    "        y_pred = rf_reg.predict(x_test)  \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=8, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=8, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 19:06:06,018] A new study created in memory with name: RFRegressor\n",
      "[I 2023-12-11 19:06:56,546] Trial 0 finished with value: 0.6799747384321797 and parameters: {'n_estimators': 795}. Best is trial 0 with value: 0.6799747384321797.\n",
      "[I 2023-12-11 19:07:10,854] Trial 1 finished with value: 0.6798790433692264 and parameters: {'n_estimators': 216}. Best is trial 0 with value: 0.6799747384321797.\n",
      "[I 2023-12-11 19:07:35,907] Trial 2 finished with value: 0.6813571811065954 and parameters: {'n_estimators': 384}. Best is trial 2 with value: 0.6813571811065954.\n",
      "[I 2023-12-11 19:08:07,567] Trial 3 finished with value: 0.6811102064049138 and parameters: {'n_estimators': 488}. Best is trial 2 with value: 0.6813571811065954.\n",
      "[I 2023-12-11 19:08:20,085] Trial 4 finished with value: 0.6805670030833525 and parameters: {'n_estimators': 184}. Best is trial 2 with value: 0.6813571811065954.\n",
      "[I 2023-12-11 19:08:36,492] Trial 5 finished with value: 0.6799622300592321 and parameters: {'n_estimators': 246}. Best is trial 2 with value: 0.6813571811065954.\n",
      "[I 2023-12-11 19:09:12,360] Trial 6 finished with value: 0.6802714625483104 and parameters: {'n_estimators': 554}. Best is trial 2 with value: 0.6813571811065954.\n",
      "[I 2023-12-11 19:09:28,380] Trial 7 finished with value: 0.6801092056070397 and parameters: {'n_estimators': 240}. Best is trial 2 with value: 0.6813571811065954.\n",
      "[I 2023-12-11 19:09:37,418] Trial 8 finished with value: 0.6803576399810141 and parameters: {'n_estimators': 133}. Best is trial 2 with value: 0.6813571811065954.\n",
      "[I 2023-12-11 19:10:34,104] Trial 9 finished with value: 0.6799619884559048 and parameters: {'n_estimators': 882}. Best is trial 2 with value: 0.6813571811065954.\n",
      "[I 2023-12-11 19:11:04,519] Trial 10 finished with value: 0.6811599356432863 and parameters: {'n_estimators': 466}. Best is trial 2 with value: 0.6813571811065954.\n",
      "[I 2023-12-11 19:11:35,789] Trial 11 finished with value: 0.6812356504413543 and parameters: {'n_estimators': 463}. Best is trial 2 with value: 0.6813571811065954.\n",
      "[I 2023-12-11 19:12:18,048] Trial 12 finished with value: 0.6807983837654099 and parameters: {'n_estimators': 640}. Best is trial 2 with value: 0.6813571811065954.\n",
      "[I 2023-12-11 19:12:43,940] Trial 13 finished with value: 0.6815250673833065 and parameters: {'n_estimators': 392}. Best is trial 13 with value: 0.6815250673833065.\n",
      "[I 2023-12-11 19:13:07,494] Trial 14 finished with value: 0.6808713518497449 and parameters: {'n_estimators': 354}. Best is trial 13 with value: 0.6815250673833065.\n",
      "[I 2023-12-11 19:13:30,874] Trial 15 finished with value: 0.6808285875788758 and parameters: {'n_estimators': 338}. Best is trial 13 with value: 0.6815250673833065.\n",
      "[I 2023-12-11 19:14:21,187] Trial 16 finished with value: 0.6804791167939188 and parameters: {'n_estimators': 662}. Best is trial 13 with value: 0.6815250673833065.\n",
      "[I 2023-12-11 19:14:49,060] Trial 17 finished with value: 0.6806122289469162 and parameters: {'n_estimators': 347}. Best is trial 13 with value: 0.6815250673833065.\n",
      "[I 2023-12-11 19:15:36,796] Trial 18 finished with value: 0.6804509206934529 and parameters: {'n_estimators': 621}. Best is trial 13 with value: 0.6815250673833065.\n",
      "[I 2023-12-11 19:16:36,900] Trial 19 finished with value: 0.6800810140254807 and parameters: {'n_estimators': 768}. Best is trial 13 with value: 0.6815250673833065.\n",
      "[I 2023-12-11 19:17:08,248] Trial 20 finished with value: 0.6814757632341883 and parameters: {'n_estimators': 391}. Best is trial 13 with value: 0.6815250673833065.\n",
      "[I 2023-12-11 19:17:37,540] Trial 21 finished with value: 0.6812819075005399 and parameters: {'n_estimators': 375}. Best is trial 13 with value: 0.6815250673833065.\n",
      "[I 2023-12-11 19:18:10,216] Trial 22 finished with value: 0.6817676708007995 and parameters: {'n_estimators': 415}. Best is trial 22 with value: 0.6817676708007995.\n",
      "[I 2023-12-11 19:18:56,213] Trial 23 finished with value: 0.680477511538984 and parameters: {'n_estimators': 547}. Best is trial 22 with value: 0.6817676708007995.\n",
      "[I 2023-12-11 19:19:21,010] Trial 24 finished with value: 0.6806858744959814 and parameters: {'n_estimators': 284}. Best is trial 22 with value: 0.6817676708007995.\n",
      "[I 2023-12-11 19:20:00,033] Trial 25 finished with value: 0.6810800886102669 and parameters: {'n_estimators': 454}. Best is trial 22 with value: 0.6817676708007995.\n",
      "[I 2023-12-11 19:20:43,949] Trial 26 finished with value: 0.6804597900596626 and parameters: {'n_estimators': 541}. Best is trial 22 with value: 0.6817676708007995.\n",
      "[I 2023-12-11 19:20:53,206] Trial 27 finished with value: 0.6786853777230059 and parameters: {'n_estimators': 107}. Best is trial 22 with value: 0.6817676708007995.\n",
      "[I 2023-12-11 19:21:28,390] Trial 28 finished with value: 0.6817676708007995 and parameters: {'n_estimators': 415}. Best is trial 22 with value: 0.6817676708007995.\n",
      "[I 2023-12-11 19:21:53,387] Trial 29 finished with value: 0.6808924675279285 and parameters: {'n_estimators': 294}. Best is trial 22 with value: 0.6817676708007995.\n",
      "[I 2023-12-11 19:22:55,863] Trial 30 finished with value: 0.6803319152638866 and parameters: {'n_estimators': 743}. Best is trial 22 with value: 0.6817676708007995.\n",
      "[I 2023-12-11 19:23:30,522] Trial 31 finished with value: 0.6820459731716213 and parameters: {'n_estimators': 417}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:24:07,037] Trial 32 finished with value: 0.6817091308379135 and parameters: {'n_estimators': 425}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:24:42,363] Trial 33 finished with value: 0.6817804992013379 and parameters: {'n_estimators': 424}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:25:24,704] Trial 34 finished with value: 0.6801837816470288 and parameters: {'n_estimators': 513}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:25:52,899] Trial 35 finished with value: 0.6807885364341548 and parameters: {'n_estimators': 301}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:26:42,505] Trial 36 finished with value: 0.6801729281227796 and parameters: {'n_estimators': 584}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:26:58,301] Trial 37 finished with value: 0.6806565028171881 and parameters: {'n_estimators': 188}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:27:33,909] Trial 38 finished with value: 0.6817091308379135 and parameters: {'n_estimators': 425}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:28:30,987] Trial 39 finished with value: 0.680366439077269 and parameters: {'n_estimators': 703}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:29:15,885] Trial 40 finished with value: 0.6802148573929587 and parameters: {'n_estimators': 514}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:29:51,493] Trial 41 finished with value: 0.6819153886253849 and parameters: {'n_estimators': 420}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:30:26,657] Trial 42 finished with value: 0.6817091308379135 and parameters: {'n_estimators': 425}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:31:07,282] Trial 43 finished with value: 0.6810633545804258 and parameters: {'n_estimators': 485}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:32:17,690] Trial 44 finished with value: 0.6796026458286871 and parameters: {'n_estimators': 916}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:32:45,248] Trial 45 finished with value: 0.6807093409945801 and parameters: {'n_estimators': 318}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:33:01,975] Trial 46 finished with value: 0.6794799679958382 and parameters: {'n_estimators': 222}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:33:35,209] Trial 47 finished with value: 0.6819153886253849 and parameters: {'n_estimators': 420}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:34:21,842] Trial 48 finished with value: 0.6803866413921715 and parameters: {'n_estimators': 590}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:34:43,801] Trial 49 finished with value: 0.6805004438212613 and parameters: {'n_estimators': 268}. Best is trial 31 with value: 0.6820459731716213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 417\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFRegressor\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.706222\n",
      "1                    TP   33.000000\n",
      "2                    TN  197.000000\n",
      "3                    FP    3.000000\n",
      "4                    FN   35.000000\n",
      "5              Accuracy    0.858209\n",
      "6             Precision    0.916667\n",
      "7           Sensitivity    0.485294\n",
      "8           Specificity    0.985000\n",
      "9              F1 score    0.634615\n",
      "10  F1 score (weighted)    0.841646\n",
      "11     F1 score (macro)    0.773326\n",
      "12    Balanced Accuracy    0.735147\n",
      "13                  MCC    0.600128\n",
      "14                  NPV    0.849100\n",
      "15              ROC_AUC    0.735147\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_rf_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet0 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_0_cat = np.where(((y_pred_rf_0 >= 2) | (y_pred_rf_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_rf_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 19:35:22,052] Trial 50 finished with value: 0.6748904680192761 and parameters: {'n_estimators': 461}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:35:55,138] Trial 51 finished with value: 0.6740520408534876 and parameters: {'n_estimators': 410}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:36:23,297] Trial 52 finished with value: 0.6745683315045274 and parameters: {'n_estimators': 357}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:36:58,139] Trial 53 finished with value: 0.6742761018020899 and parameters: {'n_estimators': 437}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:37:35,789] Trial 54 finished with value: 0.6752815405115765 and parameters: {'n_estimators': 507}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:38:05,390] Trial 55 finished with value: 0.6748989809020655 and parameters: {'n_estimators': 382}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:38:32,848] Trial 56 finished with value: 0.6738236074946977 and parameters: {'n_estimators': 335}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:39:12,472] Trial 57 finished with value: 0.6750684507305388 and parameters: {'n_estimators': 477}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:39:31,856] Trial 58 finished with value: 0.6737124868292896 and parameters: {'n_estimators': 254}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:40:02,125] Trial 59 finished with value: 0.6748636467265261 and parameters: {'n_estimators': 389}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:40:47,191] Trial 60 finished with value: 0.6745570621269622 and parameters: {'n_estimators': 563}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:41:19,007] Trial 61 finished with value: 0.6740520408534876 and parameters: {'n_estimators': 410}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:41:56,103] Trial 62 finished with value: 0.6749654121143293 and parameters: {'n_estimators': 464}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:42:25,348] Trial 63 finished with value: 0.6746266895784963 and parameters: {'n_estimators': 369}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:42:50,319] Trial 64 finished with value: 0.6735535140296217 and parameters: {'n_estimators': 325}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:43:25,266] Trial 65 finished with value: 0.67427610180209 and parameters: {'n_estimators': 437}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:43:56,948] Trial 66 finished with value: 0.6740221955951384 and parameters: {'n_estimators': 414}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:45:01,259] Trial 67 finished with value: 0.6746950541400538 and parameters: {'n_estimators': 838}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:45:41,843] Trial 68 finished with value: 0.6751232257364277 and parameters: {'n_estimators': 511}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:46:56,128] Trial 69 finished with value: 0.6744030085683491 and parameters: {'n_estimators': 986}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:47:23,884] Trial 70 finished with value: 0.674616697528594 and parameters: {'n_estimators': 361}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:47:57,213] Trial 71 finished with value: 0.6742972178299997 and parameters: {'n_estimators': 438}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:48:36,626] Trial 72 finished with value: 0.6753477155653125 and parameters: {'n_estimators': 496}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:49:07,539] Trial 73 finished with value: 0.6745522538625979 and parameters: {'n_estimators': 403}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:49:49,873] Trial 74 finished with value: 0.674583851238945 and parameters: {'n_estimators': 540}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:50:23,792] Trial 75 finished with value: 0.6743191019127941 and parameters: {'n_estimators': 446}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:50:54,549] Trial 76 finished with value: 0.6747479572319485 and parameters: {'n_estimators': 377}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:51:18,973] Trial 77 finished with value: 0.6732806031707607 and parameters: {'n_estimators': 313}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:51:57,732] Trial 78 finished with value: 0.6749901032508816 and parameters: {'n_estimators': 469}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:52:26,174] Trial 79 finished with value: 0.6743129618156147 and parameters: {'n_estimators': 348}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:52:57,365] Trial 80 finished with value: 0.6740520408534876 and parameters: {'n_estimators': 410}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:53:32,588] Trial 81 finished with value: 0.67427610180209 and parameters: {'n_estimators': 437}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:54:07,727] Trial 82 finished with value: 0.6740874479163963 and parameters: {'n_estimators': 423}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:54:48,923] Trial 83 finished with value: 0.6753938447612756 and parameters: {'n_estimators': 493}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:55:10,674] Trial 84 finished with value: 0.6725759049660328 and parameters: {'n_estimators': 281}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:55:52,497] Trial 85 finished with value: 0.6746791989380627 and parameters: {'n_estimators': 527}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:56:23,284] Trial 86 finished with value: 0.6748989809020655 and parameters: {'n_estimators': 382}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:57:01,226] Trial 87 finished with value: 0.6750684507305388 and parameters: {'n_estimators': 477}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:57:36,018] Trial 88 finished with value: 0.6741015313285326 and parameters: {'n_estimators': 425}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:58:03,911] Trial 89 finished with value: 0.6741595905883234 and parameters: {'n_estimators': 340}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:58:51,389] Trial 90 finished with value: 0.6749001864486921 and parameters: {'n_estimators': 574}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:59:23,607] Trial 91 finished with value: 0.6748826325938798 and parameters: {'n_estimators': 394}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 19:59:56,209] Trial 92 finished with value: 0.6748174369132661 and parameters: {'n_estimators': 398}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:00:32,031] Trial 93 finished with value: 0.6743928039491407 and parameters: {'n_estimators': 455}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:01:07,953] Trial 94 finished with value: 0.6740757421845955 and parameters: {'n_estimators': 427}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:01:38,663] Trial 95 finished with value: 0.6746266895784963 and parameters: {'n_estimators': 369}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:02:14,163] Trial 96 finished with value: 0.6746712760201986 and parameters: {'n_estimators': 459}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:03:01,989] Trial 97 finished with value: 0.6747767907916812 and parameters: {'n_estimators': 607}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:03:25,885] Trial 98 finished with value: 0.6733531050611583 and parameters: {'n_estimators': 298}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:04:02,424] Trial 99 finished with value: 0.6752461518348312 and parameters: {'n_estimators': 488}. Best is trial 31 with value: 0.6820459731716213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 417\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.706222    0.682656\n",
      "1                    TP   33.000000   37.000000\n",
      "2                    TN  197.000000  199.000000\n",
      "3                    FP    3.000000    3.000000\n",
      "4                    FN   35.000000   29.000000\n",
      "5              Accuracy    0.858209    0.880597\n",
      "6             Precision    0.916667    0.925000\n",
      "7           Sensitivity    0.485294    0.560606\n",
      "8           Specificity    0.985000    0.985100\n",
      "9              F1 score    0.634615    0.698113\n",
      "10  F1 score (weighted)    0.841646    0.869563\n",
      "11     F1 score (macro)    0.773326    0.811847\n",
      "12    Balanced Accuracy    0.735147    0.772877\n",
      "13                  MCC    0.600128    0.659854\n",
      "14                  NPV    0.849100    0.872800\n",
      "15              ROC_AUC    0.735147    0.772877\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_rf_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_1_cat = np.where(((y_pred_rf_1 >= 2) | (y_pred_rf_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_rf_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 20:04:34,136] Trial 100 finished with value: 0.6762309069935344 and parameters: {'n_estimators': 362}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:05:03,451] Trial 101 finished with value: 0.6764376828482542 and parameters: {'n_estimators': 404}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:05:29,476] Trial 102 finished with value: 0.6751199687596062 and parameters: {'n_estimators': 331}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:05:58,256] Trial 103 finished with value: 0.6767519659310344 and parameters: {'n_estimators': 385}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:06:30,548] Trial 104 finished with value: 0.6766481549003471 and parameters: {'n_estimators': 430}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:07:05,757] Trial 105 finished with value: 0.6777740309692721 and parameters: {'n_estimators': 448}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:07:46,192] Trial 106 finished with value: 0.6784260805003438 and parameters: {'n_estimators': 528}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:08:13,295] Trial 107 finished with value: 0.6758145605855204 and parameters: {'n_estimators': 353}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:08:37,546] Trial 108 finished with value: 0.6758548056932276 and parameters: {'n_estimators': 314}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:09:09,651] Trial 109 finished with value: 0.6761978247732745 and parameters: {'n_estimators': 419}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:09:39,018] Trial 110 finished with value: 0.6769074803836831 and parameters: {'n_estimators': 390}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:10:05,058] Trial 111 finished with value: 0.6762515154276257 and parameters: {'n_estimators': 369}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:10:40,831] Trial 112 finished with value: 0.6781240282129269 and parameters: {'n_estimators': 452}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:11:16,092] Trial 113 finished with value: 0.6778857368939477 and parameters: {'n_estimators': 478}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:11:45,953] Trial 114 finished with value: 0.6768815306573593 and parameters: {'n_estimators': 395}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:12:16,805] Trial 115 finished with value: 0.6761734466977976 and parameters: {'n_estimators': 417}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:12:43,105] Trial 116 finished with value: 0.6753722847111338 and parameters: {'n_estimators': 340}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:13:20,228] Trial 117 finished with value: 0.6777215774465633 and parameters: {'n_estimators': 501}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:13:53,721] Trial 118 finished with value: 0.6777740309692721 and parameters: {'n_estimators': 448}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:14:25,879] Trial 119 finished with value: 0.6763245413610394 and parameters: {'n_estimators': 409}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:15:00,794] Trial 120 finished with value: 0.6778215867825798 and parameters: {'n_estimators': 468}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:15:29,582] Trial 121 finished with value: 0.6765048445365155 and parameters: {'n_estimators': 375}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:16:03,455] Trial 122 finished with value: 0.676842954733197 and parameters: {'n_estimators': 434}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:16:29,693] Trial 123 finished with value: 0.675859583026455 and parameters: {'n_estimators': 350}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:16:59,180] Trial 124 finished with value: 0.6768546013117289 and parameters: {'n_estimators': 394}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:17:31,544] Trial 125 finished with value: 0.6764145729070499 and parameters: {'n_estimators': 422}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:17:59,653] Trial 126 finished with value: 0.6767046748552191 and parameters: {'n_estimators': 379}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:18:30,387] Trial 127 finished with value: 0.6764857003121648 and parameters: {'n_estimators': 405}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:18:57,904] Trial 128 finished with value: 0.6762544095325852 and parameters: {'n_estimators': 363}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:19:24,704] Trial 129 finished with value: 0.6753908710901333 and parameters: {'n_estimators': 325}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:19:35,430] Trial 130 finished with value: 0.6704374288550614 and parameters: {'n_estimators': 147}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:20:08,777] Trial 131 finished with value: 0.6771484771806838 and parameters: {'n_estimators': 439}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:20:45,719] Trial 132 finished with value: 0.6779407138472934 and parameters: {'n_estimators': 464}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:21:17,350] Trial 133 finished with value: 0.6763589936144501 and parameters: {'n_estimators': 408}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:21:45,852] Trial 134 finished with value: 0.6768386255746871 and parameters: {'n_estimators': 384}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:22:22,373] Trial 135 finished with value: 0.6777760332494033 and parameters: {'n_estimators': 483}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:22:54,568] Trial 136 finished with value: 0.6766939394987979 and parameters: {'n_estimators': 429}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:23:29,878] Trial 137 finished with value: 0.6777546368604286 and parameters: {'n_estimators': 449}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:23:55,227] Trial 138 finished with value: 0.6759077687483913 and parameters: {'n_estimators': 354}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:24:26,561] Trial 139 finished with value: 0.6761978247732745 and parameters: {'n_estimators': 419}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:24:56,372] Trial 140 finished with value: 0.6769233936230533 and parameters: {'n_estimators': 392}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:25:31,463] Trial 141 finished with value: 0.6780633749368 and parameters: {'n_estimators': 458}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:26:11,934] Trial 142 finished with value: 0.6783127818328895 and parameters: {'n_estimators': 525}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:26:48,481] Trial 143 finished with value: 0.6775509262842265 and parameters: {'n_estimators': 495}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:27:20,734] Trial 144 finished with value: 0.6769008113263493 and parameters: {'n_estimators': 436}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:27:55,703] Trial 145 finished with value: 0.6778215867825798 and parameters: {'n_estimators': 468}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:28:27,538] Trial 146 finished with value: 0.6763589936144501 and parameters: {'n_estimators': 408}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:28:55,115] Trial 147 finished with value: 0.6762515154276257 and parameters: {'n_estimators': 369}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:29:42,450] Trial 148 finished with value: 0.6792742540837202 and parameters: {'n_estimators': 659}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:30:15,653] Trial 149 finished with value: 0.6772942015228176 and parameters: {'n_estimators': 442}. Best is trial 31 with value: 0.6820459731716213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 417\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.706222    0.682656    0.670451\n",
      "1                    TP   33.000000   37.000000   36.000000\n",
      "2                    TN  197.000000  199.000000  198.000000\n",
      "3                    FP    3.000000    3.000000    2.000000\n",
      "4                    FN   35.000000   29.000000   32.000000\n",
      "5              Accuracy    0.858209    0.880597    0.873134\n",
      "6             Precision    0.916667    0.925000    0.947368\n",
      "7           Sensitivity    0.485294    0.560606    0.529412\n",
      "8           Specificity    0.985000    0.985100    0.990000\n",
      "9              F1 score    0.634615    0.698113    0.679245\n",
      "10  F1 score (weighted)    0.841646    0.869563    0.859607\n",
      "11     F1 score (macro)    0.773326    0.811847    0.800088\n",
      "12    Balanced Accuracy    0.735147    0.772877    0.759706\n",
      "13                  MCC    0.600128    0.659854    0.647926\n",
      "14                  NPV    0.849100    0.872800    0.860900\n",
      "15              ROC_AUC    0.735147    0.772877    0.759706\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_rf_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_2_cat = np.where(((y_pred_rf_2 >= 2) | (y_pred_rf_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_rf_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 20:30:50,858] Trial 150 finished with value: 0.6655770153371754 and parameters: {'n_estimators': 391}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:31:29,021] Trial 151 finished with value: 0.6655631381508655 and parameters: {'n_estimators': 506}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:32:06,198] Trial 152 finished with value: 0.6654257542919917 and parameters: {'n_estimators': 481}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:32:41,107] Trial 153 finished with value: 0.6654389514288439 and parameters: {'n_estimators': 421}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:33:16,430] Trial 154 finished with value: 0.6658107875833345 and parameters: {'n_estimators': 456}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:34:00,001] Trial 155 finished with value: 0.6658823395054895 and parameters: {'n_estimators': 558}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:34:31,716] Trial 156 finished with value: 0.6656970499646325 and parameters: {'n_estimators': 401}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:35:05,406] Trial 157 finished with value: 0.6655370783528409 and parameters: {'n_estimators': 425}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:35:35,588] Trial 158 finished with value: 0.6661881518546624 and parameters: {'n_estimators': 371}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:36:13,919] Trial 159 finished with value: 0.6653819967427153 and parameters: {'n_estimators': 473}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:36:48,803] Trial 160 finished with value: 0.6661123318062561 and parameters: {'n_estimators': 445}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:37:21,320] Trial 161 finished with value: 0.66568247769999 and parameters: {'n_estimators': 413}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:37:56,010] Trial 162 finished with value: 0.6658359365074369 and parameters: {'n_estimators': 436}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:38:33,447] Trial 163 finished with value: 0.665700813199355 and parameters: {'n_estimators': 455}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:39:03,681] Trial 164 finished with value: 0.6656136814593243 and parameters: {'n_estimators': 385}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:39:41,346] Trial 165 finished with value: 0.6655540993601272 and parameters: {'n_estimators': 492}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:40:15,117] Trial 166 finished with value: 0.6657095412743146 and parameters: {'n_estimators': 400}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:40:47,323] Trial 167 finished with value: 0.665494552802669 and parameters: {'n_estimators': 423}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:41:14,007] Trial 168 finished with value: 0.6658404006190934 and parameters: {'n_estimators': 343}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:41:50,167] Trial 169 finished with value: 0.6655006965630811 and parameters: {'n_estimators': 477}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:42:25,835] Trial 170 finished with value: 0.666134489713148 and parameters: {'n_estimators': 441}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:43:06,008] Trial 171 finished with value: 0.6657869725174858 and parameters: {'n_estimators': 525}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:43:43,584] Trial 172 finished with value: 0.6655706347407971 and parameters: {'n_estimators': 467}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:44:14,999] Trial 173 finished with value: 0.665794749288993 and parameters: {'n_estimators': 411}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:44:54,230] Trial 174 finished with value: 0.665523435139111 and parameters: {'n_estimators': 511}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:45:23,834] Trial 175 finished with value: 0.6656641479048359 and parameters: {'n_estimators': 386}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:46:02,804] Trial 176 finished with value: 0.6654067863337978 and parameters: {'n_estimators': 491}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:46:46,881] Trial 177 finished with value: 0.6658072432055638 and parameters: {'n_estimators': 545}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:47:23,111] Trial 178 finished with value: 0.6657932491635816 and parameters: {'n_estimators': 459}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:47:58,149] Trial 179 finished with value: 0.6660948421424282 and parameters: {'n_estimators': 432}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:48:29,581] Trial 180 finished with value: 0.6656970499646325 and parameters: {'n_estimators': 401}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:49:02,058] Trial 181 finished with value: 0.6656047988723621 and parameters: {'n_estimators': 426}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:49:20,276] Trial 182 finished with value: 0.6650496316020374 and parameters: {'n_estimators': 222}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:50:16,533] Trial 183 finished with value: 0.6653375345853901 and parameters: {'n_estimators': 728}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:50:39,317] Trial 184 finished with value: 0.6655108848605515 and parameters: {'n_estimators': 289}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:51:13,004] Trial 185 finished with value: 0.6660541425502021 and parameters: {'n_estimators': 448}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:51:32,813] Trial 186 finished with value: 0.6653951784069332 and parameters: {'n_estimators': 262}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:51:52,186] Trial 187 finished with value: 0.6647960305234564 and parameters: {'n_estimators': 237}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:52:20,068] Trial 188 finished with value: 0.6661978567176204 and parameters: {'n_estimators': 356}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:52:33,862] Trial 189 finished with value: 0.6642707829896702 and parameters: {'n_estimators': 183}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:53:05,473] Trial 190 finished with value: 0.6660527833669005 and parameters: {'n_estimators': 378}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:53:38,402] Trial 191 finished with value: 0.6656848773280484 and parameters: {'n_estimators': 414}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:54:02,801] Trial 192 finished with value: 0.6658306499412836 and parameters: {'n_estimators': 311}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:54:31,257] Trial 193 finished with value: 0.6660808426513881 and parameters: {'n_estimators': 365}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:55:02,596] Trial 194 finished with value: 0.6655713120095638 and parameters: {'n_estimators': 397}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:55:28,496] Trial 195 finished with value: 0.665756678607637 and parameters: {'n_estimators': 330}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:56:05,051] Trial 196 finished with value: 0.6662117817849957 and parameters: {'n_estimators': 442}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:56:37,486] Trial 197 finished with value: 0.6656473661705258 and parameters: {'n_estimators': 417}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:57:05,380] Trial 198 finished with value: 0.6659689367132675 and parameters: {'n_estimators': 380}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:57:41,825] Trial 199 finished with value: 0.6654198717466467 and parameters: {'n_estimators': 472}. Best is trial 31 with value: 0.6820459731716213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 417\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.706222    0.682656    0.670451    0.723950\n",
      "1                    TP   33.000000   37.000000   36.000000   32.000000\n",
      "2                    TN  197.000000  199.000000  198.000000  199.000000\n",
      "3                    FP    3.000000    3.000000    2.000000    2.000000\n",
      "4                    FN   35.000000   29.000000   32.000000   35.000000\n",
      "5              Accuracy    0.858209    0.880597    0.873134    0.861940\n",
      "6             Precision    0.916667    0.925000    0.947368    0.941176\n",
      "7           Sensitivity    0.485294    0.560606    0.529412    0.477612\n",
      "8           Specificity    0.985000    0.985100    0.990000    0.990000\n",
      "9              F1 score    0.634615    0.698113    0.679245    0.633663\n",
      "10  F1 score (weighted)    0.841646    0.869563    0.859607    0.844623\n",
      "11     F1 score (macro)    0.773326    0.811847    0.800088    0.774303\n",
      "12    Balanced Accuracy    0.735147    0.772877    0.759706    0.733831\n",
      "13                  MCC    0.600128    0.659854    0.647926    0.608443\n",
      "14                  NPV    0.849100    0.872800    0.860900    0.850400\n",
      "15              ROC_AUC    0.735147    0.772877    0.759706    0.733831\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_rf_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_3_cat = np.where(((y_pred_rf_3 >= 2) | (y_pred_rf_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_rf_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 20:58:20,274] Trial 200 finished with value: 0.6750280829732906 and parameters: {'n_estimators': 430}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:58:46,409] Trial 201 finished with value: 0.6753481589041488 and parameters: {'n_estimators': 342}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:59:10,340] Trial 202 finished with value: 0.6758567429784568 and parameters: {'n_estimators': 312}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:59:32,196] Trial 203 finished with value: 0.6760627172173247 and parameters: {'n_estimators': 277}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 20:59:58,649] Trial 204 finished with value: 0.6758690902369802 and parameters: {'n_estimators': 356}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:00:30,965] Trial 205 finished with value: 0.6754561343857339 and parameters: {'n_estimators': 402}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:01:00,132] Trial 206 finished with value: 0.6758710407640371 and parameters: {'n_estimators': 374}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:01:35,611] Trial 207 finished with value: 0.6750630942543949 and parameters: {'n_estimators': 460}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:02:07,051] Trial 208 finished with value: 0.6754768741543422 and parameters: {'n_estimators': 399}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:02:32,605] Trial 209 finished with value: 0.6751679149069043 and parameters: {'n_estimators': 325}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:03:48,938] Trial 210 finished with value: 0.676682269151565 and parameters: {'n_estimators': 995}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:04:51,059] Trial 211 finished with value: 0.6763356770756606 and parameters: {'n_estimators': 817}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:05:42,255] Trial 212 finished with value: 0.676224376015751 and parameters: {'n_estimators': 665}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:06:14,624] Trial 213 finished with value: 0.6750186642878739 and parameters: {'n_estimators': 426}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:07:01,805] Trial 214 finished with value: 0.676121657959491 and parameters: {'n_estimators': 624}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:07:37,708] Trial 215 finished with value: 0.6749208955438188 and parameters: {'n_estimators': 448}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:08:09,339] Trial 216 finished with value: 0.6753234149331283 and parameters: {'n_estimators': 413}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:08:37,875] Trial 217 finished with value: 0.6757300235233029 and parameters: {'n_estimators': 378}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:09:01,017] Trial 218 finished with value: 0.6760794822331002 and parameters: {'n_estimators': 298}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:09:33,748] Trial 219 finished with value: 0.6750518238942189 and parameters: {'n_estimators': 427}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:10:11,925] Trial 220 finished with value: 0.6753867619588922 and parameters: {'n_estimators': 481}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:10:35,908] Trial 221 finished with value: 0.6761546440506008 and parameters: {'n_estimators': 299}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:11:01,994] Trial 222 finished with value: 0.6754276740313051 and parameters: {'n_estimators': 347}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:11:33,946] Trial 223 finished with value: 0.6756154229917486 and parameters: {'n_estimators': 392}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:12:05,676] Trial 224 finished with value: 0.6751547918685273 and parameters: {'n_estimators': 437}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:12:37,430] Trial 225 finished with value: 0.675265324989716 and parameters: {'n_estimators': 409}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:13:04,595] Trial 226 finished with value: 0.6753814100383331 and parameters: {'n_estimators': 329}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:13:39,286] Trial 227 finished with value: 0.6749977449104563 and parameters: {'n_estimators': 451}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:14:01,815] Trial 228 finished with value: 0.6761066301564935 and parameters: {'n_estimators': 278}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:15:11,886] Trial 229 finished with value: 0.6769667599878951 and parameters: {'n_estimators': 910}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:15:40,617] Trial 230 finished with value: 0.6757848547237507 and parameters: {'n_estimators': 362}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:16:05,172] Trial 231 finished with value: 0.676084941256633 and parameters: {'n_estimators': 308}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:16:31,126] Trial 232 finished with value: 0.6753459432585369 and parameters: {'n_estimators': 333}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:17:15,410] Trial 233 finished with value: 0.675814299709903 and parameters: {'n_estimators': 592}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:17:36,600] Trial 234 finished with value: 0.6762874844625217 and parameters: {'n_estimators': 290}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:18:07,958] Trial 235 finished with value: 0.6757105725644282 and parameters: {'n_estimators': 387}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:18:27,037] Trial 236 finished with value: 0.6763490556089605 and parameters: {'n_estimators': 255}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:19:05,772] Trial 237 finished with value: 0.6754994310304774 and parameters: {'n_estimators': 503}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:19:39,750] Trial 238 finished with value: 0.6750671690453889 and parameters: {'n_estimators': 415}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:20:15,150] Trial 239 finished with value: 0.6749179635914747 and parameters: {'n_estimators': 463}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:20:42,333] Trial 240 finished with value: 0.6758529538556407 and parameters: {'n_estimators': 355}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:21:07,017] Trial 241 finished with value: 0.6759507581242842 and parameters: {'n_estimators': 311}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:21:29,335] Trial 242 finished with value: 0.6761733547636302 and parameters: {'n_estimators': 282}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:21:48,949] Trial 243 finished with value: 0.6765747917562547 and parameters: {'n_estimators': 269}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:22:14,500] Trial 244 finished with value: 0.675556499808569 and parameters: {'n_estimators': 320}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:22:48,624] Trial 245 finished with value: 0.6750280829732906 and parameters: {'n_estimators': 430}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:23:19,999] Trial 246 finished with value: 0.6752628457347212 and parameters: {'n_estimators': 406}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:23:50,255] Trial 247 finished with value: 0.6756426015319936 and parameters: {'n_estimators': 391}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:24:24,103] Trial 248 finished with value: 0.6753184933465842 and parameters: {'n_estimators': 441}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:24:43,480] Trial 249 finished with value: 0.676534472458395 and parameters: {'n_estimators': 240}. Best is trial 31 with value: 0.6820459731716213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 417\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77894dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.706222    0.682656    0.670451    0.723950   \n",
      "1                    TP   33.000000   37.000000   36.000000   32.000000   \n",
      "2                    TN  197.000000  199.000000  198.000000  199.000000   \n",
      "3                    FP    3.000000    3.000000    2.000000    2.000000   \n",
      "4                    FN   35.000000   29.000000   32.000000   35.000000   \n",
      "5              Accuracy    0.858209    0.880597    0.873134    0.861940   \n",
      "6             Precision    0.916667    0.925000    0.947368    0.941176   \n",
      "7           Sensitivity    0.485294    0.560606    0.529412    0.477612   \n",
      "8           Specificity    0.985000    0.985100    0.990000    0.990000   \n",
      "9              F1 score    0.634615    0.698113    0.679245    0.633663   \n",
      "10  F1 score (weighted)    0.841646    0.869563    0.859607    0.844623   \n",
      "11     F1 score (macro)    0.773326    0.811847    0.800088    0.774303   \n",
      "12    Balanced Accuracy    0.735147    0.772877    0.759706    0.733831   \n",
      "13                  MCC    0.600128    0.659854    0.647926    0.608443   \n",
      "14                  NPV    0.849100    0.872800    0.860900    0.850400   \n",
      "15              ROC_AUC    0.735147    0.772877    0.759706    0.733831   \n",
      "\n",
      "          Set4  \n",
      "0     0.661065  \n",
      "1    38.000000  \n",
      "2   201.000000  \n",
      "3     0.000000  \n",
      "4    29.000000  \n",
      "5     0.891791  \n",
      "6     1.000000  \n",
      "7     0.567164  \n",
      "8     1.000000  \n",
      "9     0.723810  \n",
      "10    0.880488  \n",
      "11    0.828262  \n",
      "12    0.783582  \n",
      "13    0.704026  \n",
      "14    0.873900  \n",
      "15    0.783582  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_rf_4)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_4_cat = np.where(((y_pred_rf_4 >= 2) | (y_pred_rf_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_rf_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 21:25:15,889] Trial 250 finished with value: 0.6723376765036715 and parameters: {'n_estimators': 369}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:26:08,194] Trial 251 finished with value: 0.6731146579109636 and parameters: {'n_estimators': 682}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:26:33,113] Trial 252 finished with value: 0.6717585365164535 and parameters: {'n_estimators': 343}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:27:31,042] Trial 253 finished with value: 0.6726586708914984 and parameters: {'n_estimators': 762}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:28:02,990] Trial 254 finished with value: 0.6726148622208468 and parameters: {'n_estimators': 420}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:28:26,429] Trial 255 finished with value: 0.6709879999939026 and parameters: {'n_estimators': 297}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:29:03,343] Trial 256 finished with value: 0.6728204409727192 and parameters: {'n_estimators': 479}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:29:39,383] Trial 257 finished with value: 0.6721239482704924 and parameters: {'n_estimators': 447}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:30:10,058] Trial 258 finished with value: 0.6722175760417078 and parameters: {'n_estimators': 396}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:30:57,598] Trial 259 finished with value: 0.6731695425824422 and parameters: {'n_estimators': 626}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:31:33,863] Trial 260 finished with value: 0.6725428034955678 and parameters: {'n_estimators': 462}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:32:06,744] Trial 261 finished with value: 0.6725615709957735 and parameters: {'n_estimators': 421}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:32:38,432] Trial 262 finished with value: 0.6721538101540904 and parameters: {'n_estimators': 378}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:33:08,811] Trial 263 finished with value: 0.6722196483267637 and parameters: {'n_estimators': 406}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:33:46,479] Trial 264 finished with value: 0.672937543706132 and parameters: {'n_estimators': 491}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:34:12,110] Trial 265 finished with value: 0.6712644675468602 and parameters: {'n_estimators': 324}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:34:48,414] Trial 266 finished with value: 0.6723931333768456 and parameters: {'n_estimators': 434}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:35:16,905] Trial 267 finished with value: 0.6724103798017668 and parameters: {'n_estimators': 366}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:35:51,930] Trial 268 finished with value: 0.6725684980595686 and parameters: {'n_estimators': 457}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:36:19,206] Trial 269 finished with value: 0.671600159927418 and parameters: {'n_estimators': 342}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:36:50,149] Trial 270 finished with value: 0.6723178596131235 and parameters: {'n_estimators': 392}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:37:23,669] Trial 271 finished with value: 0.6724873487211225 and parameters: {'n_estimators': 414}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:37:45,780] Trial 272 finished with value: 0.6710458180638019 and parameters: {'n_estimators': 302}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:38:05,718] Trial 273 finished with value: 0.6712368178719292 and parameters: {'n_estimators': 268}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:38:37,318] Trial 274 finished with value: 0.6723633317335929 and parameters: {'n_estimators': 438}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:39:11,070] Trial 275 finished with value: 0.6726336238661255 and parameters: {'n_estimators': 472}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:39:38,687] Trial 276 finished with value: 0.672571271993267 and parameters: {'n_estimators': 384}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:40:08,850] Trial 277 finished with value: 0.6725951370765105 and parameters: {'n_estimators': 425}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:40:46,671] Trial 278 finished with value: 0.6727114929448612 and parameters: {'n_estimators': 528}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:41:12,519] Trial 279 finished with value: 0.6723378932216603 and parameters: {'n_estimators': 359}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:41:41,639] Trial 280 finished with value: 0.6721740950338544 and parameters: {'n_estimators': 403}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:42:17,548] Trial 281 finished with value: 0.6725148441653486 and parameters: {'n_estimators': 508}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:42:48,848] Trial 282 finished with value: 0.6723852538205998 and parameters: {'n_estimators': 439}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:43:11,501] Trial 283 finished with value: 0.6712644675468602 and parameters: {'n_estimators': 324}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:43:31,933] Trial 284 finished with value: 0.6713184169097826 and parameters: {'n_estimators': 283}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:44:04,715] Trial 285 finished with value: 0.672592595233326 and parameters: {'n_estimators': 455}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:44:55,947] Trial 286 finished with value: 0.6727348644175559 and parameters: {'n_estimators': 725}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:45:23,052] Trial 287 finished with value: 0.6719935886083248 and parameters: {'n_estimators': 374}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:45:52,826] Trial 288 finished with value: 0.6722196483267637 and parameters: {'n_estimators': 406}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:46:22,379] Trial 289 finished with value: 0.6726148622208468 and parameters: {'n_estimators': 420}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:46:57,109] Trial 290 finished with value: 0.6727456000339023 and parameters: {'n_estimators': 486}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:47:21,511] Trial 291 finished with value: 0.6715725820755732 and parameters: {'n_estimators': 341}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:47:49,633] Trial 292 finished with value: 0.6722810645013616 and parameters: {'n_estimators': 390}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:48:21,117] Trial 293 finished with value: 0.6722433331896239 and parameters: {'n_estimators': 443}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:48:43,576] Trial 294 finished with value: 0.6709127057487436 and parameters: {'n_estimators': 306}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:49:14,051] Trial 295 finished with value: 0.6723883829590226 and parameters: {'n_estimators': 432}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:49:46,747] Trial 296 finished with value: 0.6725428034955678 and parameters: {'n_estimators': 462}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:50:11,824] Trial 297 finished with value: 0.6721387123901144 and parameters: {'n_estimators': 355}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:50:40,546] Trial 298 finished with value: 0.6722822303003106 and parameters: {'n_estimators': 410}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:51:07,795] Trial 299 finished with value: 0.6720009160701517 and parameters: {'n_estimators': 375}. Best is trial 31 with value: 0.6820459731716213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 417\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.706222    0.682656    0.670451    0.723950   \n",
      "1                    TP   33.000000   37.000000   36.000000   32.000000   \n",
      "2                    TN  197.000000  199.000000  198.000000  199.000000   \n",
      "3                    FP    3.000000    3.000000    2.000000    2.000000   \n",
      "4                    FN   35.000000   29.000000   32.000000   35.000000   \n",
      "5              Accuracy    0.858209    0.880597    0.873134    0.861940   \n",
      "6             Precision    0.916667    0.925000    0.947368    0.941176   \n",
      "7           Sensitivity    0.485294    0.560606    0.529412    0.477612   \n",
      "8           Specificity    0.985000    0.985100    0.990000    0.990000   \n",
      "9              F1 score    0.634615    0.698113    0.679245    0.633663   \n",
      "10  F1 score (weighted)    0.841646    0.869563    0.859607    0.844623   \n",
      "11     F1 score (macro)    0.773326    0.811847    0.800088    0.774303   \n",
      "12    Balanced Accuracy    0.735147    0.772877    0.759706    0.733831   \n",
      "13                  MCC    0.600128    0.659854    0.647926    0.608443   \n",
      "14                  NPV    0.849100    0.872800    0.860900    0.850400   \n",
      "15              ROC_AUC    0.735147    0.772877    0.759706    0.733831   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.661065    0.698594  \n",
      "1    38.000000   38.000000  \n",
      "2   201.000000  198.000000  \n",
      "3     0.000000    2.000000  \n",
      "4    29.000000   30.000000  \n",
      "5     0.891791    0.880597  \n",
      "6     1.000000    0.950000  \n",
      "7     0.567164    0.558824  \n",
      "8     1.000000    0.990000  \n",
      "9     0.723810    0.703704  \n",
      "10    0.880488    0.869025  \n",
      "11    0.828262    0.814469  \n",
      "12    0.783582    0.774412  \n",
      "13    0.704026    0.670201  \n",
      "14    0.873900    0.868400  \n",
      "15    0.783582    0.774412  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_rf_5)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_5_cat = np.where(((y_pred_rf_5 >= 2) | (y_pred_rf_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_rf_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 21:51:29,269] Trial 300 finished with value: 0.6668088952491356 and parameters: {'n_estimators': 248}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:51:58,230] Trial 301 finished with value: 0.6666586967966233 and parameters: {'n_estimators': 398}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:52:32,325] Trial 302 finished with value: 0.6675673564379438 and parameters: {'n_estimators': 476}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:53:02,701] Trial 303 finished with value: 0.6667862907555445 and parameters: {'n_estimators': 426}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:53:23,677] Trial 304 finished with value: 0.6674593870235337 and parameters: {'n_estimators': 291}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:53:55,641] Trial 305 finished with value: 0.6674692493479814 and parameters: {'n_estimators': 450}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:54:19,280] Trial 306 finished with value: 0.666391397458931 and parameters: {'n_estimators': 322}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:55:00,070] Trial 307 finished with value: 0.6670366440478366 and parameters: {'n_estimators': 576}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:55:27,911] Trial 308 finished with value: 0.6669334575888335 and parameters: {'n_estimators': 386}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:56:03,600] Trial 309 finished with value: 0.6673064762373218 and parameters: {'n_estimators': 499}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:56:33,110] Trial 310 finished with value: 0.6667260533165583 and parameters: {'n_estimators': 412}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:56:58,115] Trial 311 finished with value: 0.6672802023026849 and parameters: {'n_estimators': 345}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:57:28,848] Trial 312 finished with value: 0.6668012447563598 and parameters: {'n_estimators': 432}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:57:55,624] Trial 313 finished with value: 0.6673076817199133 and parameters: {'n_estimators': 369}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:58:28,115] Trial 314 finished with value: 0.6674268039088568 and parameters: {'n_estimators': 455}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:58:56,994] Trial 315 finished with value: 0.6669270081383212 and parameters: {'n_estimators': 393}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 21:59:30,922] Trial 316 finished with value: 0.6674959108250468 and parameters: {'n_estimators': 473}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:00:00,371] Trial 317 finished with value: 0.6665972467177667 and parameters: {'n_estimators': 415}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:00:22,852] Trial 318 finished with value: 0.666845917665071 and parameters: {'n_estimators': 309}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:00:54,913] Trial 319 finished with value: 0.667218077383821 and parameters: {'n_estimators': 445}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:01:20,932] Trial 320 finished with value: 0.6670781905784977 and parameters: {'n_estimators': 359}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:01:57,982] Trial 321 finished with value: 0.6672038877974349 and parameters: {'n_estimators': 518}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:02:43,579] Trial 322 finished with value: 0.6666160139212278 and parameters: {'n_estimators': 646}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:03:13,845] Trial 323 finished with value: 0.666848155438849 and parameters: {'n_estimators': 425}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:03:34,118] Trial 324 finished with value: 0.6673522741935726 and parameters: {'n_estimators': 274}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:04:12,240] Trial 325 finished with value: 0.6673333114203122 and parameters: {'n_estimators': 541}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:04:40,937] Trial 326 finished with value: 0.6668380244331404 and parameters: {'n_estimators': 403}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:05:14,336] Trial 327 finished with value: 0.6673114955750442 and parameters: {'n_estimators': 467}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:05:38,787] Trial 328 finished with value: 0.6670953319379841 and parameters: {'n_estimators': 333}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:06:06,715] Trial 329 finished with value: 0.6670433371655904 and parameters: {'n_estimators': 385}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:06:38,952] Trial 330 finished with value: 0.6669811672645937 and parameters: {'n_estimators': 439}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:07:29,080] Trial 331 finished with value: 0.6665687086056635 and parameters: {'n_estimators': 694}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:07:57,874] Trial 332 finished with value: 0.6665600201818406 and parameters: {'n_estimators': 411}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:08:35,965] Trial 333 finished with value: 0.6674644526127037 and parameters: {'n_estimators': 484}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:09:16,523] Trial 334 finished with value: 0.6671630530208239 and parameters: {'n_estimators': 374}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:09:44,950] Trial 335 finished with value: 0.6671352751017711 and parameters: {'n_estimators': 263}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:10:15,933] Trial 336 finished with value: 0.6676268037395822 and parameters: {'n_estimators': 287}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:11:01,702] Trial 337 finished with value: 0.6664040233911821 and parameters: {'n_estimators': 430}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:11:44,162] Trial 338 finished with value: 0.6665689650706766 and parameters: {'n_estimators': 396}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:12:21,910] Trial 339 finished with value: 0.6671669692978625 and parameters: {'n_estimators': 347}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:12:55,745] Trial 340 finished with value: 0.6665897955687144 and parameters: {'n_estimators': 311}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:13:40,307] Trial 341 finished with value: 0.6674692493479812 and parameters: {'n_estimators': 450}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:14:10,158] Trial 342 finished with value: 0.6668814113298931 and parameters: {'n_estimators': 418}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:14:36,398] Trial 343 finished with value: 0.6673551362493406 and parameters: {'n_estimators': 368}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:15:15,777] Trial 344 finished with value: 0.6674136793855536 and parameters: {'n_estimators': 561}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:15:51,015] Trial 345 finished with value: 0.6672076604953909 and parameters: {'n_estimators': 503}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:16:15,118] Trial 346 finished with value: 0.6666807503805798 and parameters: {'n_estimators': 329}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:16:47,690] Trial 347 finished with value: 0.6675583380297121 and parameters: {'n_estimators': 462}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:17:15,246] Trial 348 finished with value: 0.6670622974877907 and parameters: {'n_estimators': 389}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:17:46,375] Trial 349 finished with value: 0.6669429041643745 and parameters: {'n_estimators': 442}. Best is trial 31 with value: 0.6820459731716213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 417\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.706222    0.682656    0.670451    0.723950   \n",
      "1                    TP   33.000000   37.000000   36.000000   32.000000   \n",
      "2                    TN  197.000000  199.000000  198.000000  199.000000   \n",
      "3                    FP    3.000000    3.000000    2.000000    2.000000   \n",
      "4                    FN   35.000000   29.000000   32.000000   35.000000   \n",
      "5              Accuracy    0.858209    0.880597    0.873134    0.861940   \n",
      "6             Precision    0.916667    0.925000    0.947368    0.941176   \n",
      "7           Sensitivity    0.485294    0.560606    0.529412    0.477612   \n",
      "8           Specificity    0.985000    0.985100    0.990000    0.990000   \n",
      "9              F1 score    0.634615    0.698113    0.679245    0.633663   \n",
      "10  F1 score (weighted)    0.841646    0.869563    0.859607    0.844623   \n",
      "11     F1 score (macro)    0.773326    0.811847    0.800088    0.774303   \n",
      "12    Balanced Accuracy    0.735147    0.772877    0.759706    0.733831   \n",
      "13                  MCC    0.600128    0.659854    0.647926    0.608443   \n",
      "14                  NPV    0.849100    0.872800    0.860900    0.850400   \n",
      "15              ROC_AUC    0.735147    0.772877    0.759706    0.733831   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.661065    0.698594    0.737843  \n",
      "1    38.000000   38.000000   36.000000  \n",
      "2   201.000000  198.000000  199.000000  \n",
      "3     0.000000    2.000000    2.000000  \n",
      "4    29.000000   30.000000   31.000000  \n",
      "5     0.891791    0.880597    0.876866  \n",
      "6     1.000000    0.950000    0.947368  \n",
      "7     0.567164    0.558824    0.537313  \n",
      "8     1.000000    0.990000    0.990000  \n",
      "9     0.723810    0.703704    0.685714  \n",
      "10    0.880488    0.869025    0.864004  \n",
      "11    0.828262    0.814469    0.804574  \n",
      "12    0.783582    0.774412    0.763682  \n",
      "13    0.704026    0.670201    0.654620  \n",
      "14    0.873900    0.868400    0.865200  \n",
      "15    0.783582    0.774412    0.763682  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_rf_6)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_6_cat = np.where(((y_pred_rf_6 >= 2) | (y_pred_rf_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_rf_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 22:18:31,837] Trial 350 finished with value: 0.6748992953586777 and parameters: {'n_estimators': 596}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:19:00,986] Trial 351 finished with value: 0.6735762533314416 and parameters: {'n_estimators': 408}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:19:31,415] Trial 352 finished with value: 0.6743254689684512 and parameters: {'n_estimators': 427}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:19:52,839] Trial 353 finished with value: 0.6735917179204993 and parameters: {'n_estimators': 295}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:20:18,337] Trial 354 finished with value: 0.674195763462002 and parameters: {'n_estimators': 353}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:20:45,683] Trial 355 finished with value: 0.6737879519443167 and parameters: {'n_estimators': 379}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:21:20,723] Trial 356 finished with value: 0.6750764258930072 and parameters: {'n_estimators': 484}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:21:49,549] Trial 357 finished with value: 0.6733716542557656 and parameters: {'n_estimators': 402}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:22:22,181] Trial 358 finished with value: 0.6749383941639675 and parameters: {'n_estimators': 460}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:22:53,288] Trial 359 finished with value: 0.6743824945741885 and parameters: {'n_estimators': 431}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:23:16,421] Trial 360 finished with value: 0.6740259923720214 and parameters: {'n_estimators': 316}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:23:46,375] Trial 361 finished with value: 0.6737860827763122 and parameters: {'n_estimators': 420}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:24:18,969] Trial 362 finished with value: 0.6746116815220172 and parameters: {'n_estimators': 450}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:24:47,227] Trial 363 finished with value: 0.6736311730241196 and parameters: {'n_estimators': 394}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:25:13,935] Trial 364 finished with value: 0.6738536164183648 and parameters: {'n_estimators': 365}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:25:38,118] Trial 365 finished with value: 0.6740635691361241 and parameters: {'n_estimators': 338}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:26:07,503] Trial 366 finished with value: 0.6738047446753408 and parameters: {'n_estimators': 415}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:26:40,940] Trial 367 finished with value: 0.6748406913957055 and parameters: {'n_estimators': 471}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:27:08,384] Trial 368 finished with value: 0.6737878644821802 and parameters: {'n_estimators': 382}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:27:40,386] Trial 369 finished with value: 0.6746218340455904 and parameters: {'n_estimators': 440}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:28:09,300] Trial 370 finished with value: 0.6734288047336664 and parameters: {'n_estimators': 403}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:28:39,447] Trial 371 finished with value: 0.6740210175267348 and parameters: {'n_estimators': 425}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:29:37,725] Trial 372 finished with value: 0.6749029235884572 and parameters: {'n_estimators': 840}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:29:53,736] Trial 373 finished with value: 0.6724585882256464 and parameters: {'n_estimators': 221}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:30:28,117] Trial 374 finished with value: 0.6748429220218702 and parameters: {'n_estimators': 488}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:30:49,763] Trial 375 finished with value: 0.6737585939745019 and parameters: {'n_estimators': 301}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:31:16,083] Trial 376 finished with value: 0.6737870606060363 and parameters: {'n_estimators': 362}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:31:52,123] Trial 377 finished with value: 0.6744276787269622 and parameters: {'n_estimators': 511}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:32:10,081] Trial 378 finished with value: 0.6730506710534135 and parameters: {'n_estimators': 252}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:33:15,945] Trial 379 finished with value: 0.6745676483087955 and parameters: {'n_estimators': 946}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:33:48,329] Trial 380 finished with value: 0.674693334815574 and parameters: {'n_estimators': 448}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:34:16,807] Trial 381 finished with value: 0.6733404602230781 and parameters: {'n_estimators': 398}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:34:36,854] Trial 382 finished with value: 0.6735238971911716 and parameters: {'n_estimators': 277}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:35:01,572] Trial 383 finished with value: 0.6740103826165958 and parameters: {'n_estimators': 344}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:35:34,572] Trial 384 finished with value: 0.6748166446659151 and parameters: {'n_estimators': 469}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:36:04,467] Trial 385 finished with value: 0.6738047446753408 and parameters: {'n_estimators': 415}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:36:27,347] Trial 386 finished with value: 0.674135723936953 and parameters: {'n_estimators': 322}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:36:54,611] Trial 387 finished with value: 0.6737879519443167 and parameters: {'n_estimators': 379}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:37:25,885] Trial 388 finished with value: 0.674742072454881 and parameters: {'n_estimators': 442}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:37:55,886] Trial 389 finished with value: 0.6739027507896894 and parameters: {'n_estimators': 421}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:38:25,048] Trial 390 finished with value: 0.6737000162375223 and parameters: {'n_estimators': 393}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:38:57,528] Trial 391 finished with value: 0.6749383941639675 and parameters: {'n_estimators': 460}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:39:24,372] Trial 392 finished with value: 0.6737455716336919 and parameters: {'n_estimators': 367}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:40:01,804] Trial 393 finished with value: 0.67424142447316 and parameters: {'n_estimators': 531}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:40:33,042] Trial 394 finished with value: 0.6744859185208503 and parameters: {'n_estimators': 432}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:40:54,689] Trial 395 finished with value: 0.6735826823197706 and parameters: {'n_estimators': 296}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:41:18,667] Trial 396 finished with value: 0.6741327022619077 and parameters: {'n_estimators': 331}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:41:28,124] Trial 397 finished with value: 0.6714461122746026 and parameters: {'n_estimators': 130}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:41:57,424] Trial 398 finished with value: 0.6735915134488067 and parameters: {'n_estimators': 407}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:42:24,960] Trial 399 finished with value: 0.6737317292240127 and parameters: {'n_estimators': 383}. Best is trial 31 with value: 0.6820459731716213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 417\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.706222    0.682656    0.670451    0.723950   \n",
      "1                    TP   33.000000   37.000000   36.000000   32.000000   \n",
      "2                    TN  197.000000  199.000000  198.000000  199.000000   \n",
      "3                    FP    3.000000    3.000000    2.000000    2.000000   \n",
      "4                    FN   35.000000   29.000000   32.000000   35.000000   \n",
      "5              Accuracy    0.858209    0.880597    0.873134    0.861940   \n",
      "6             Precision    0.916667    0.925000    0.947368    0.941176   \n",
      "7           Sensitivity    0.485294    0.560606    0.529412    0.477612   \n",
      "8           Specificity    0.985000    0.985100    0.990000    0.990000   \n",
      "9              F1 score    0.634615    0.698113    0.679245    0.633663   \n",
      "10  F1 score (weighted)    0.841646    0.869563    0.859607    0.844623   \n",
      "11     F1 score (macro)    0.773326    0.811847    0.800088    0.774303   \n",
      "12    Balanced Accuracy    0.735147    0.772877    0.759706    0.733831   \n",
      "13                  MCC    0.600128    0.659854    0.647926    0.608443   \n",
      "14                  NPV    0.849100    0.872800    0.860900    0.850400   \n",
      "15              ROC_AUC    0.735147    0.772877    0.759706    0.733831   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.661065    0.698594    0.737843    0.693690  \n",
      "1    38.000000   38.000000   36.000000   37.000000  \n",
      "2   201.000000  198.000000  199.000000  199.000000  \n",
      "3     0.000000    2.000000    2.000000    2.000000  \n",
      "4    29.000000   30.000000   31.000000   30.000000  \n",
      "5     0.891791    0.880597    0.876866    0.880597  \n",
      "6     1.000000    0.950000    0.947368    0.948718  \n",
      "7     0.567164    0.558824    0.537313    0.552239  \n",
      "8     1.000000    0.990000    0.990000    0.990000  \n",
      "9     0.723810    0.703704    0.685714    0.698113  \n",
      "10    0.880488    0.869025    0.864004    0.868714  \n",
      "11    0.828262    0.814469    0.804574    0.811847  \n",
      "12    0.783582    0.774412    0.763682    0.771144  \n",
      "13    0.704026    0.670201    0.654620    0.665910  \n",
      "14    0.873900    0.868400    0.865200    0.869000  \n",
      "15    0.783582    0.774412    0.763682    0.771144  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_rf_7)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_7_cat = np.where(((y_pred_rf_7 >= 2) | (y_pred_rf_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_rf_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 22:43:03,028] Trial 400 finished with value: 0.6672626433561376 and parameters: {'n_estimators': 498}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:43:56,187] Trial 401 finished with value: 0.6675490487200154 and parameters: {'n_estimators': 779}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:44:29,414] Trial 402 finished with value: 0.667108183249456 and parameters: {'n_estimators': 477}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:44:43,719] Trial 403 finished with value: 0.6639796663673231 and parameters: {'n_estimators': 195}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:45:08,941] Trial 404 finished with value: 0.6665938240040692 and parameters: {'n_estimators': 354}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:45:40,814] Trial 405 finished with value: 0.6670068976778449 and parameters: {'n_estimators': 453}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:46:13,880] Trial 406 finished with value: 0.6668362905090294 and parameters: {'n_estimators': 433}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:46:42,727] Trial 407 finished with value: 0.6663886419079791 and parameters: {'n_estimators': 409}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:47:04,906] Trial 408 finished with value: 0.6659354580973459 and parameters: {'n_estimators': 314}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:47:24,305] Trial 409 finished with value: 0.6656216398277844 and parameters: {'n_estimators': 271}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:47:51,157] Trial 410 finished with value: 0.666362734499944 and parameters: {'n_estimators': 385}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:48:21,512] Trial 411 finished with value: 0.6668362905090294 and parameters: {'n_estimators': 433}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:48:49,737] Trial 412 finished with value: 0.6667336212655 and parameters: {'n_estimators': 400}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:49:21,434] Trial 413 finished with value: 0.6669652107660313 and parameters: {'n_estimators': 452}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:49:47,077] Trial 414 finished with value: 0.6665938240040692 and parameters: {'n_estimators': 354}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:50:16,361] Trial 415 finished with value: 0.6665275807162659 and parameters: {'n_estimators': 416}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:50:59,215] Trial 416 finished with value: 0.6673247506504799 and parameters: {'n_estimators': 615}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:51:48,683] Trial 417 finished with value: 0.6674238513777262 and parameters: {'n_estimators': 720}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:52:21,696] Trial 418 finished with value: 0.6668714215025847 and parameters: {'n_estimators': 471}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:52:55,600] Trial 419 finished with value: 0.6671822075153542 and parameters: {'n_estimators': 491}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:53:19,538] Trial 420 finished with value: 0.6672180545642369 and parameters: {'n_estimators': 337}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:53:46,252] Trial 421 finished with value: 0.666392375382653 and parameters: {'n_estimators': 376}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:54:16,207] Trial 422 finished with value: 0.666560088808013 and parameters: {'n_estimators': 424}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:54:37,701] Trial 423 finished with value: 0.6653673494257214 and parameters: {'n_estimators': 298}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:55:08,832] Trial 424 finished with value: 0.6667097618882155 and parameters: {'n_estimators': 443}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:55:37,140] Trial 425 finished with value: 0.6667336212655 and parameters: {'n_estimators': 400}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:56:02,407] Trial 426 finished with value: 0.6664561730180169 and parameters: {'n_estimators': 363}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:56:47,886] Trial 427 finished with value: 0.6675740818459486 and parameters: {'n_estimators': 654}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:57:17,927] Trial 428 finished with value: 0.6665137681994416 and parameters: {'n_estimators': 421}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:57:50,426] Trial 429 finished with value: 0.6668619635056741 and parameters: {'n_estimators': 463}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:58:18,022] Trial 430 finished with value: 0.6662878657794183 and parameters: {'n_estimators': 390}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:58:39,886] Trial 431 finished with value: 0.666008058256874 and parameters: {'n_estimators': 313}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:58:59,819] Trial 432 finished with value: 0.6655930213068966 and parameters: {'n_estimators': 280}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 22:59:36,325] Trial 433 finished with value: 0.6675527987795608 and parameters: {'n_estimators': 517}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:00:07,496] Trial 434 finished with value: 0.6668253867556757 and parameters: {'n_estimators': 446}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:00:31,523] Trial 435 finished with value: 0.6667464544840668 and parameters: {'n_estimators': 328}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:01:02,195] Trial 436 finished with value: 0.6664467366695253 and parameters: {'n_estimators': 411}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:01:28,867] Trial 437 finished with value: 0.666343033743511 and parameters: {'n_estimators': 377}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:01:59,249] Trial 438 finished with value: 0.6668296769247319 and parameters: {'n_estimators': 437}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:02:38,348] Trial 439 finished with value: 0.6672281619573017 and parameters: {'n_estimators': 552}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:03:12,259] Trial 440 finished with value: 0.6670784883198502 and parameters: {'n_estimators': 485}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:03:40,380] Trial 441 finished with value: 0.6664715080659438 and parameters: {'n_estimators': 398}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:03:58,793] Trial 442 finished with value: 0.6651747599734559 and parameters: {'n_estimators': 255}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:04:23,927] Trial 443 finished with value: 0.6667095755775635 and parameters: {'n_estimators': 352}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:04:53,380] Trial 444 finished with value: 0.6665888868172916 and parameters: {'n_estimators': 415}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:05:25,546] Trial 445 finished with value: 0.6668619635056741 and parameters: {'n_estimators': 463}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:05:56,077] Trial 446 finished with value: 0.6667807997718544 and parameters: {'n_estimators': 432}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:06:22,971] Trial 447 finished with value: 0.6662403949118963 and parameters: {'n_estimators': 382}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:06:43,537] Trial 448 finished with value: 0.6655678607467582 and parameters: {'n_estimators': 284}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:07:07,077] Trial 449 finished with value: 0.6671727515314908 and parameters: {'n_estimators': 335}. Best is trial 31 with value: 0.6820459731716213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 417\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.706222    0.682656    0.670451    0.723950   \n",
      "1                    TP   33.000000   37.000000   36.000000   32.000000   \n",
      "2                    TN  197.000000  199.000000  198.000000  199.000000   \n",
      "3                    FP    3.000000    3.000000    2.000000    2.000000   \n",
      "4                    FN   35.000000   29.000000   32.000000   35.000000   \n",
      "5              Accuracy    0.858209    0.880597    0.873134    0.861940   \n",
      "6             Precision    0.916667    0.925000    0.947368    0.941176   \n",
      "7           Sensitivity    0.485294    0.560606    0.529412    0.477612   \n",
      "8           Specificity    0.985000    0.985100    0.990000    0.990000   \n",
      "9              F1 score    0.634615    0.698113    0.679245    0.633663   \n",
      "10  F1 score (weighted)    0.841646    0.869563    0.859607    0.844623   \n",
      "11     F1 score (macro)    0.773326    0.811847    0.800088    0.774303   \n",
      "12    Balanced Accuracy    0.735147    0.772877    0.759706    0.733831   \n",
      "13                  MCC    0.600128    0.659854    0.647926    0.608443   \n",
      "14                  NPV    0.849100    0.872800    0.860900    0.850400   \n",
      "15              ROC_AUC    0.735147    0.772877    0.759706    0.733831   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.661065    0.698594    0.737843    0.693690    0.713538  \n",
      "1    38.000000   38.000000   36.000000   37.000000   29.000000  \n",
      "2   201.000000  198.000000  199.000000  199.000000  199.000000  \n",
      "3     0.000000    2.000000    2.000000    2.000000    3.000000  \n",
      "4    29.000000   30.000000   31.000000   30.000000   37.000000  \n",
      "5     0.891791    0.880597    0.876866    0.880597    0.850746  \n",
      "6     1.000000    0.950000    0.947368    0.948718    0.906250  \n",
      "7     0.567164    0.558824    0.537313    0.552239    0.439394  \n",
      "8     1.000000    0.990000    0.990000    0.990000    0.985100  \n",
      "9     0.723810    0.703704    0.685714    0.698113    0.591837  \n",
      "10    0.880488    0.869025    0.864004    0.868714    0.830648  \n",
      "11    0.828262    0.814469    0.804574    0.811847    0.750256  \n",
      "12    0.783582    0.774412    0.763682    0.771144    0.712271  \n",
      "13    0.704026    0.670201    0.654620    0.665910    0.564076  \n",
      "14    0.873900    0.868400    0.865200    0.869000    0.843200  \n",
      "15    0.783582    0.774412    0.763682    0.771144    0.712271  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_rf_8)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_8_cat = np.where(((y_pred_rf_8 >= 2) | (y_pred_rf_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_rf_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:07:42,830] Trial 450 finished with value: 0.6757316783176892 and parameters: {'n_estimators': 453}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:08:11,814] Trial 451 finished with value: 0.6756185966549747 and parameters: {'n_estimators': 406}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:08:38,165] Trial 452 finished with value: 0.6754994339030908 and parameters: {'n_estimators': 367}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:09:09,580] Trial 453 finished with value: 0.6758995223359423 and parameters: {'n_estimators': 431}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:09:44,325] Trial 454 finished with value: 0.676435725520486 and parameters: {'n_estimators': 481}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:10:06,876] Trial 455 finished with value: 0.6751932365113189 and parameters: {'n_estimators': 307}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:10:35,561] Trial 456 finished with value: 0.6754051131083764 and parameters: {'n_estimators': 395}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:11:07,849] Trial 457 finished with value: 0.6760765625613906 and parameters: {'n_estimators': 420}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:11:34,497] Trial 458 finished with value: 0.6750037289297892 and parameters: {'n_estimators': 350}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:12:10,463] Trial 459 finished with value: 0.6761960498151078 and parameters: {'n_estimators': 500}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:12:42,715] Trial 460 finished with value: 0.6758849685827526 and parameters: {'n_estimators': 442}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:13:16,335] Trial 461 finished with value: 0.6761561626994367 and parameters: {'n_estimators': 468}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:13:44,373] Trial 462 finished with value: 0.6755437596023355 and parameters: {'n_estimators': 382}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:14:12,924] Trial 463 finished with value: 0.6754614257303809 and parameters: {'n_estimators': 404}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:14:34,462] Trial 464 finished with value: 0.6754698859707735 and parameters: {'n_estimators': 294}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:15:06,472] Trial 465 finished with value: 0.6758440482930389 and parameters: {'n_estimators': 450}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:15:24,193] Trial 466 finished with value: 0.6756943568780589 and parameters: {'n_estimators': 241}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:15:46,994] Trial 467 finished with value: 0.6750907910508579 and parameters: {'n_estimators': 318}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:16:17,687] Trial 468 finished with value: 0.6760585716452845 and parameters: {'n_estimators': 425}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:16:44,705] Trial 469 finished with value: 0.6754994339030908 and parameters: {'n_estimators': 367}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:17:38,623] Trial 470 finished with value: 0.6783987254928153 and parameters: {'n_estimators': 752}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:18:06,952] Trial 471 finished with value: 0.6754285579300302 and parameters: {'n_estimators': 393}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:18:55,074] Trial 472 finished with value: 0.6780481547620713 and parameters: {'n_estimators': 671}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:19:25,360] Trial 473 finished with value: 0.6758703690027145 and parameters: {'n_estimators': 417}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:19:49,267] Trial 474 finished with value: 0.6755619768987742 and parameters: {'n_estimators': 335}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:20:09,095] Trial 475 finished with value: 0.6754090368266669 and parameters: {'n_estimators': 269}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:20:42,258] Trial 476 finished with value: 0.6757706310662976 and parameters: {'n_estimators': 464}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:21:12,951] Trial 477 finished with value: 0.6759034597951568 and parameters: {'n_estimators': 432}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:21:42,262] Trial 478 finished with value: 0.6756206242512777 and parameters: {'n_estimators': 408}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:22:23,915] Trial 479 finished with value: 0.6769612479311955 and parameters: {'n_estimators': 574}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:22:59,213] Trial 480 finished with value: 0.6762471945336204 and parameters: {'n_estimators': 495}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:23:25,104] Trial 481 finished with value: 0.6750597445078693 and parameters: {'n_estimators': 360}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:23:57,152] Trial 482 finished with value: 0.6759332806496534 and parameters: {'n_estimators': 447}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:24:24,165] Trial 483 finished with value: 0.6756009589658716 and parameters: {'n_estimators': 379}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:24:52,846] Trial 484 finished with value: 0.6753480360483002 and parameters: {'n_estimators': 396}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:25:26,471] Trial 485 finished with value: 0.6763885858616951 and parameters: {'n_estimators': 473}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:26:04,052] Trial 486 finished with value: 0.6764675177316385 and parameters: {'n_estimators': 527}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:26:35,034] Trial 487 finished with value: 0.6759034597951568 and parameters: {'n_estimators': 432}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:26:59,781] Trial 488 finished with value: 0.6754497196990715 and parameters: {'n_estimators': 343}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:27:29,282] Trial 489 finished with value: 0.6759190996253059 and parameters: {'n_estimators': 414}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:27:51,502] Trial 490 finished with value: 0.6753870717375617 and parameters: {'n_estimators': 309}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:28:24,251] Trial 491 finished with value: 0.6760837945031897 and parameters: {'n_estimators': 457}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:28:51,542] Trial 492 finished with value: 0.6756129056361019 and parameters: {'n_estimators': 383}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:29:23,315] Trial 493 finished with value: 0.676057614310401 and parameters: {'n_estimators': 439}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:29:46,463] Trial 494 finished with value: 0.6752781883715608 and parameters: {'n_estimators': 324}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:30:08,509] Trial 495 finished with value: 0.6754698859707734 and parameters: {'n_estimators': 294}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:30:59,419] Trial 496 finished with value: 0.6785340723725974 and parameters: {'n_estimators': 708}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:31:30,218] Trial 497 finished with value: 0.6759307021028799 and parameters: {'n_estimators': 416}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:32:04,409] Trial 498 finished with value: 0.676435725520486 and parameters: {'n_estimators': 481}. Best is trial 31 with value: 0.6820459731716213.\n",
      "[I 2023-12-11 23:32:41,163] Trial 499 finished with value: 0.6766108363404137 and parameters: {'n_estimators': 514}. Best is trial 31 with value: 0.6820459731716213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 417\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.706222    0.682656    0.670451    0.723950   \n",
      "1                    TP   33.000000   37.000000   36.000000   32.000000   \n",
      "2                    TN  197.000000  199.000000  198.000000  199.000000   \n",
      "3                    FP    3.000000    3.000000    2.000000    2.000000   \n",
      "4                    FN   35.000000   29.000000   32.000000   35.000000   \n",
      "5              Accuracy    0.858209    0.880597    0.873134    0.861940   \n",
      "6             Precision    0.916667    0.925000    0.947368    0.941176   \n",
      "7           Sensitivity    0.485294    0.560606    0.529412    0.477612   \n",
      "8           Specificity    0.985000    0.985100    0.990000    0.990000   \n",
      "9              F1 score    0.634615    0.698113    0.679245    0.633663   \n",
      "10  F1 score (weighted)    0.841646    0.869563    0.859607    0.844623   \n",
      "11     F1 score (macro)    0.773326    0.811847    0.800088    0.774303   \n",
      "12    Balanced Accuracy    0.735147    0.772877    0.759706    0.733831   \n",
      "13                  MCC    0.600128    0.659854    0.647926    0.608443   \n",
      "14                  NPV    0.849100    0.872800    0.860900    0.850400   \n",
      "15              ROC_AUC    0.735147    0.772877    0.759706    0.733831   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.661065    0.698594    0.737843    0.693690    0.713538    0.653350  \n",
      "1    38.000000   38.000000   36.000000   37.000000   29.000000   35.000000  \n",
      "2   201.000000  198.000000  199.000000  199.000000  199.000000  198.000000  \n",
      "3     0.000000    2.000000    2.000000    2.000000    3.000000    4.000000  \n",
      "4    29.000000   30.000000   31.000000   30.000000   37.000000   31.000000  \n",
      "5     0.891791    0.880597    0.876866    0.880597    0.850746    0.869403  \n",
      "6     1.000000    0.950000    0.947368    0.948718    0.906250    0.897436  \n",
      "7     0.567164    0.558824    0.537313    0.552239    0.439394    0.530303  \n",
      "8     1.000000    0.990000    0.990000    0.990000    0.985100    0.980200  \n",
      "9     0.723810    0.703704    0.685714    0.698113    0.591837    0.666667  \n",
      "10    0.880488    0.869025    0.864004    0.868714    0.830648    0.856703  \n",
      "11    0.828262    0.814469    0.804574    0.811847    0.750256    0.792730  \n",
      "12    0.783582    0.774412    0.763682    0.771144    0.712271    0.755251  \n",
      "13    0.704026    0.670201    0.654620    0.665910    0.564076    0.623727  \n",
      "14    0.873900    0.868400    0.865200    0.869000    0.843200    0.864600  \n",
      "15    0.783582    0.774412    0.763682    0.771144    0.712271    0.755251  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_rf_9)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_9_cat = np.where(((y_pred_rf_9 >= 2) | (y_pred_rf_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_rf_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6820\n",
      "\tBest params:\n",
      "\t\tn_estimators: 417\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.709113</td>\n",
       "      <td>0.058466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>3.835507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>98.900000</td>\n",
       "      <td>1.286684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.173788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3.887301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.876046</td>\n",
       "      <td>0.031633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.918767</td>\n",
       "      <td>0.052872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.551097</td>\n",
       "      <td>0.115375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.984100</td>\n",
       "      <td>0.011625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.683306</td>\n",
       "      <td>0.100811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.863085</td>\n",
       "      <td>0.038856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.803060</td>\n",
       "      <td>0.059466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.767598</td>\n",
       "      <td>0.059055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.647321</td>\n",
       "      <td>0.097602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.869130</td>\n",
       "      <td>0.029914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.767598</td>\n",
       "      <td>0.059055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.709113     0.058466\n",
       "1                    TP        18.400000     3.835507\n",
       "2                    TN        98.900000     1.286684\n",
       "3                    FP         1.600000     1.173788\n",
       "4                    FN        15.000000     3.887301\n",
       "5              Accuracy         0.876046     0.031633\n",
       "6             Precision         0.918767     0.052872\n",
       "7           Sensitivity         0.551097     0.115375\n",
       "8           Specificity         0.984100     0.011625\n",
       "9              F1 score         0.683306     0.100811\n",
       "10  F1 score (weighted)         0.863085     0.038856\n",
       "11     F1 score (macro)         0.803060     0.059466\n",
       "12    Balanced Accuracy         0.767598     0.059055\n",
       "13                  MCC         0.647321     0.097602\n",
       "14                  NPV         0.869130     0.029914\n",
       "15              ROC_AUC         0.767598     0.059055"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.706222</td>\n",
       "      <td>0.682656</td>\n",
       "      <td>0.670451</td>\n",
       "      <td>0.723950</td>\n",
       "      <td>0.661065</td>\n",
       "      <td>0.698594</td>\n",
       "      <td>0.737843</td>\n",
       "      <td>0.693690</td>\n",
       "      <td>0.713538</td>\n",
       "      <td>0.653350</td>\n",
       "      <td>0.694136</td>\n",
       "      <td>0.027471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.100000</td>\n",
       "      <td>2.923088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.700000</td>\n",
       "      <td>1.059350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1.059350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.900000</td>\n",
       "      <td>2.806738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.861940</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.869403</td>\n",
       "      <td>0.872388</td>\n",
       "      <td>0.012413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.937998</td>\n",
       "      <td>0.028994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.537313</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.523816</td>\n",
       "      <td>0.042515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>0.988540</td>\n",
       "      <td>0.005247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.633663</td>\n",
       "      <td>0.723810</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.671548</td>\n",
       "      <td>0.040296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.841646</td>\n",
       "      <td>0.869563</td>\n",
       "      <td>0.859607</td>\n",
       "      <td>0.844623</td>\n",
       "      <td>0.880488</td>\n",
       "      <td>0.869025</td>\n",
       "      <td>0.864004</td>\n",
       "      <td>0.868714</td>\n",
       "      <td>0.830648</td>\n",
       "      <td>0.856703</td>\n",
       "      <td>0.858502</td>\n",
       "      <td>0.015305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.773326</td>\n",
       "      <td>0.811847</td>\n",
       "      <td>0.800088</td>\n",
       "      <td>0.774303</td>\n",
       "      <td>0.828262</td>\n",
       "      <td>0.814469</td>\n",
       "      <td>0.804574</td>\n",
       "      <td>0.811847</td>\n",
       "      <td>0.750256</td>\n",
       "      <td>0.792730</td>\n",
       "      <td>0.796170</td>\n",
       "      <td>0.023716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.735147</td>\n",
       "      <td>0.772877</td>\n",
       "      <td>0.759706</td>\n",
       "      <td>0.733831</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.774412</td>\n",
       "      <td>0.763682</td>\n",
       "      <td>0.771144</td>\n",
       "      <td>0.712271</td>\n",
       "      <td>0.755251</td>\n",
       "      <td>0.756190</td>\n",
       "      <td>0.022424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.600128</td>\n",
       "      <td>0.659854</td>\n",
       "      <td>0.647926</td>\n",
       "      <td>0.608443</td>\n",
       "      <td>0.704026</td>\n",
       "      <td>0.670201</td>\n",
       "      <td>0.654620</td>\n",
       "      <td>0.665910</td>\n",
       "      <td>0.564076</td>\n",
       "      <td>0.623727</td>\n",
       "      <td>0.639891</td>\n",
       "      <td>0.040782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.849100</td>\n",
       "      <td>0.872800</td>\n",
       "      <td>0.860900</td>\n",
       "      <td>0.850400</td>\n",
       "      <td>0.873900</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>0.865200</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.843200</td>\n",
       "      <td>0.864600</td>\n",
       "      <td>0.861750</td>\n",
       "      <td>0.010647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.735147</td>\n",
       "      <td>0.772877</td>\n",
       "      <td>0.759706</td>\n",
       "      <td>0.733831</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.774412</td>\n",
       "      <td>0.763682</td>\n",
       "      <td>0.771144</td>\n",
       "      <td>0.712271</td>\n",
       "      <td>0.755251</td>\n",
       "      <td>0.756190</td>\n",
       "      <td>0.022424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.706222    0.682656    0.670451    0.723950   \n",
       "1                    TP   33.000000   37.000000   36.000000   32.000000   \n",
       "2                    TN  197.000000  199.000000  198.000000  199.000000   \n",
       "3                    FP    3.000000    3.000000    2.000000    2.000000   \n",
       "4                    FN   35.000000   29.000000   32.000000   35.000000   \n",
       "5              Accuracy    0.858209    0.880597    0.873134    0.861940   \n",
       "6             Precision    0.916667    0.925000    0.947368    0.941176   \n",
       "7           Sensitivity    0.485294    0.560606    0.529412    0.477612   \n",
       "8           Specificity    0.985000    0.985100    0.990000    0.990000   \n",
       "9              F1 score    0.634615    0.698113    0.679245    0.633663   \n",
       "10  F1 score (weighted)    0.841646    0.869563    0.859607    0.844623   \n",
       "11     F1 score (macro)    0.773326    0.811847    0.800088    0.774303   \n",
       "12    Balanced Accuracy    0.735147    0.772877    0.759706    0.733831   \n",
       "13                  MCC    0.600128    0.659854    0.647926    0.608443   \n",
       "14                  NPV    0.849100    0.872800    0.860900    0.850400   \n",
       "15              ROC_AUC    0.735147    0.772877    0.759706    0.733831   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.661065    0.698594    0.737843    0.693690    0.713538    0.653350   \n",
       "1    38.000000   38.000000   36.000000   37.000000   29.000000   35.000000   \n",
       "2   201.000000  198.000000  199.000000  199.000000  199.000000  198.000000   \n",
       "3     0.000000    2.000000    2.000000    2.000000    3.000000    4.000000   \n",
       "4    29.000000   30.000000   31.000000   30.000000   37.000000   31.000000   \n",
       "5     0.891791    0.880597    0.876866    0.880597    0.850746    0.869403   \n",
       "6     1.000000    0.950000    0.947368    0.948718    0.906250    0.897436   \n",
       "7     0.567164    0.558824    0.537313    0.552239    0.439394    0.530303   \n",
       "8     1.000000    0.990000    0.990000    0.990000    0.985100    0.980200   \n",
       "9     0.723810    0.703704    0.685714    0.698113    0.591837    0.666667   \n",
       "10    0.880488    0.869025    0.864004    0.868714    0.830648    0.856703   \n",
       "11    0.828262    0.814469    0.804574    0.811847    0.750256    0.792730   \n",
       "12    0.783582    0.774412    0.763682    0.771144    0.712271    0.755251   \n",
       "13    0.704026    0.670201    0.654620    0.665910    0.564076    0.623727   \n",
       "14    0.873900    0.868400    0.865200    0.869000    0.843200    0.864600   \n",
       "15    0.783582    0.774412    0.763682    0.771144    0.712271    0.755251   \n",
       "\n",
       "           ave       std  \n",
       "0     0.694136  0.027471  \n",
       "1    35.100000  2.923088  \n",
       "2   198.700000  1.059350  \n",
       "3     2.300000  1.059350  \n",
       "4    31.900000  2.806738  \n",
       "5     0.872388  0.012413  \n",
       "6     0.937998  0.028994  \n",
       "7     0.523816  0.042515  \n",
       "8     0.988540  0.005247  \n",
       "9     0.671548  0.040296  \n",
       "10    0.858502  0.015305  \n",
       "11    0.796170  0.023716  \n",
       "12    0.756190  0.022424  \n",
       "13    0.639891  0.040782  \n",
       "14    0.861750  0.010647  \n",
       "15    0.756190  0.022424  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test['std'] = mat_met_rf_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.701785</td>\n",
       "      <td>0.055399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.871403</td>\n",
       "      <td>0.019290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.926850</td>\n",
       "      <td>0.053458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.529251</td>\n",
       "      <td>0.075741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.985264</td>\n",
       "      <td>0.011791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.669681</td>\n",
       "      <td>0.063286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.857534</td>\n",
       "      <td>0.024697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.794878</td>\n",
       "      <td>0.037013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.757257</td>\n",
       "      <td>0.036379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.636140</td>\n",
       "      <td>0.058405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.863310</td>\n",
       "      <td>0.019923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.757257</td>\n",
       "      <td>0.036379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.701785     0.055399\n",
       "1              Accuracy         0.871403     0.019290\n",
       "2             Precision         0.926850     0.053458\n",
       "3           Sensitivity         0.529251     0.075741\n",
       "4           Specificity         0.985264     0.011791\n",
       "5              F1 score         0.669681     0.063286\n",
       "6   F1 score (weighted)         0.857534     0.024697\n",
       "7      F1 score (macro)         0.794878     0.037013\n",
       "8     Balanced Accuracy         0.757257     0.036379\n",
       "9                   MCC         0.636140     0.058405\n",
       "10                  NPV         0.863310     0.019923\n",
       "11              ROC_AUC         0.757257     0.036379"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_rf_cat = np.where(((y_pred_optimized_rf >= 2) | (y_pred_optimized_rf <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_rf_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_rf))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "rf_run0 = data_rf[['y_test_idx0', 'y_test0', 'y_pred_rf0']]\n",
    "rf_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "rf_run0.reset_index(inplace=True, drop=True)\n",
    "rf_run1 = data_rf[['y_test_idx1', 'y_test1', 'y_pred_rf1']]\n",
    "rf_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "rf_run1.reset_index(inplace=True, drop=True)\n",
    "rf_run2 = data_rf[['y_test_idx2', 'y_test2', 'y_pred_rf2']]\n",
    "rf_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "rf_run2.reset_index(inplace=True, drop=True)\n",
    "rf_run3 = data_rf[['y_test_idx3', 'y_test3', 'y_pred_rf3']]\n",
    "rf_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "rf_run3.reset_index(inplace=True, drop=True)\n",
    "rf_run4 = data_rf[['y_test_idx4', 'y_test4', 'y_pred_rf4']]\n",
    "rf_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "rf_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "rf_5preds = pd.concat([chembl_id, rf_run0, rf_run1, rf_run2, rf_run3, rf_run4], axis=1)\n",
    "rf_5preds = rf_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_rf0', 'y_pred_rf1', 'y_pred_rf2', 'y_pred_rf3', 'y_pred_rf4']]\n",
    "rf_5preds['y_pred_rf_ave'] = rf_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "rf_5preds['y_pred_rf_std'] = rf_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "mat_met_optimized_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfc78124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (RF)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model r2_score 0.7069 with a standard deviation of 0.0452\n",
      "RF optimized model r2_score 0.7113 with a standard deviation of 0.0433\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"RF baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_rf.joblib']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf_reg, \"OUTPUT/rf_reg.joblib\")\n",
    "#joblib.dump(optimized_rf, \"OUTPUT/optimized_rf.joblib\") # fitted to whole training set with last random_state selected\n",
    "joblib.dump(optimizedCV_rf, \"OUTPUT/optimizedCV_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.684434     0.064796\n",
      "1                    TP        19.600000     3.405877\n",
      "2                    TN        98.400000     1.646545\n",
      "3                    FP         2.100000     1.595131\n",
      "4                    FN        13.800000     3.326660\n",
      "5              Accuracy         0.881270     0.027061\n",
      "6             Precision         0.905579     0.062960\n",
      "7           Sensitivity         0.586689     0.100430\n",
      "8           Specificity         0.979130     0.015815\n",
      "9              F1 score         0.707011     0.086160\n",
      "10  F1 score (weighted)         0.871004     0.032861\n",
      "11     F1 score (macro)         0.816207     0.050708\n",
      "12    Balanced Accuracy         0.782909     0.050668\n",
      "13                  MCC         0.663943     0.084263\n",
      "14                  NPV         0.877670     0.025265\n",
      "15              ROC_AUC         0.782909     0.050668\n",
      "CPU times: user 12.3 s, sys: 124 ms, total: 12.5 s\n",
      "Wall time: 1.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_reg = lgbm.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=8,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_reg.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"rmse\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_reg.predict(X_test) \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "         # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:37:38,341] A new study created in memory with name: lgbmRegressor\n",
      "[I 2023-12-11 23:37:40,959] Trial 0 finished with value: 0.6582687456989093 and parameters: {'n_estimators': 603, 'learning_rate': 0.02543797079697695, 'max_depth': 8, 'max_bin': 172, 'num_leaves': 448}. Best is trial 0 with value: 0.6582687456989093.\n",
      "[I 2023-12-11 23:37:42,032] Trial 1 finished with value: 0.670073764171445 and parameters: {'n_estimators': 617, 'learning_rate': 0.1171253741191396, 'max_depth': 7, 'max_bin': 180, 'num_leaves': 72}. Best is trial 1 with value: 0.670073764171445.\n",
      "[I 2023-12-11 23:37:43,434] Trial 2 finished with value: 0.6643417435452081 and parameters: {'n_estimators': 782, 'learning_rate': 0.0794380352135593, 'max_depth': 7, 'max_bin': 187, 'num_leaves': 248}. Best is trial 1 with value: 0.670073764171445.\n",
      "[I 2023-12-11 23:37:44,222] Trial 3 finished with value: 0.6316215171112444 and parameters: {'n_estimators': 437, 'learning_rate': 0.1226061881116231, 'max_depth': 3, 'max_bin': 253, 'num_leaves': 381}. Best is trial 1 with value: 0.670073764171445.\n",
      "[I 2023-12-11 23:37:45,502] Trial 4 finished with value: 0.6725188109745284 and parameters: {'n_estimators': 450, 'learning_rate': 0.10971118515158323, 'max_depth': 7, 'max_bin': 291, 'num_leaves': 83}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:37:46,682] Trial 5 finished with value: 0.6422975840200401 and parameters: {'n_estimators': 206, 'learning_rate': 0.037190812295901446, 'max_depth': 7, 'max_bin': 153, 'num_leaves': 485}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:37:48,194] Trial 6 finished with value: 0.6672572326384397 and parameters: {'n_estimators': 710, 'learning_rate': 0.1691112271512196, 'max_depth': 12, 'max_bin': 168, 'num_leaves': 544}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:37:49,007] Trial 7 finished with value: 0.6428762403886917 and parameters: {'n_estimators': 804, 'learning_rate': 0.15363534413355678, 'max_depth': 3, 'max_bin': 182, 'num_leaves': 737}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:37:49,695] Trial 8 finished with value: 0.6340023209898856 and parameters: {'n_estimators': 431, 'learning_rate': 0.19509516710857955, 'max_depth': 3, 'max_bin': 234, 'num_leaves': 67}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:37:52,658] Trial 9 finished with value: 0.6688296844757202 and parameters: {'n_estimators': 722, 'learning_rate': 0.03680595982410435, 'max_depth': 12, 'max_bin': 179, 'num_leaves': 444}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:37:53,844] Trial 10 finished with value: 0.6592935110949525 and parameters: {'n_estimators': 171, 'learning_rate': 0.07662122310571942, 'max_depth': 9, 'max_bin': 297, 'num_leaves': 222}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:37:55,059] Trial 11 finished with value: 0.6604455580901006 and parameters: {'n_estimators': 543, 'learning_rate': 0.11731800139967483, 'max_depth': 5, 'max_bin': 288, 'num_leaves': 36}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:37:56,722] Trial 12 finished with value: 0.6704197343577494 and parameters: {'n_estimators': 343, 'learning_rate': 0.09936469102284264, 'max_depth': 10, 'max_bin': 217, 'num_leaves': 171}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:37:58,359] Trial 13 finished with value: 0.6630482297929603 and parameters: {'n_estimators': 313, 'learning_rate': 0.08349665820660221, 'max_depth': 10, 'max_bin': 213, 'num_leaves': 217}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:38:00,193] Trial 14 finished with value: 0.6702736092992059 and parameters: {'n_estimators': 330, 'learning_rate': 0.05961391065291617, 'max_depth': 10, 'max_bin': 270, 'num_leaves': 163}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:38:00,752] Trial 15 finished with value: 0.6271726099766971 and parameters: {'n_estimators': 83, 'learning_rate': 0.10140570254980712, 'max_depth': 5, 'max_bin': 211, 'num_leaves': 309}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:38:03,470] Trial 16 finished with value: 0.6278924032545312 and parameters: {'n_estimators': 345, 'learning_rate': 0.010030248746590642, 'max_depth': 10, 'max_bin': 237, 'num_leaves': 128}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:38:04,382] Trial 17 finished with value: 0.6541353558206046 and parameters: {'n_estimators': 522, 'learning_rate': 0.1405230415422104, 'max_depth': 5, 'max_bin': 270, 'num_leaves': 344}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:38:05,617] Trial 18 finished with value: 0.66736778015743 and parameters: {'n_estimators': 267, 'learning_rate': 0.1045351201924273, 'max_depth': 9, 'max_bin': 204, 'num_leaves': 162}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:38:07,185] Trial 19 finished with value: 0.668272666415081 and parameters: {'n_estimators': 899, 'learning_rate': 0.13592797675844093, 'max_depth': 11, 'max_bin': 252, 'num_leaves': 619}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:38:08,523] Trial 20 finished with value: 0.6659812244271319 and parameters: {'n_estimators': 395, 'learning_rate': 0.06435702700258035, 'max_depth': 6, 'max_bin': 226, 'num_leaves': 288}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:38:09,957] Trial 21 finished with value: 0.6614387175371897 and parameters: {'n_estimators': 248, 'learning_rate': 0.0631718694961416, 'max_depth': 9, 'max_bin': 275, 'num_leaves': 141}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:38:11,317] Trial 22 finished with value: 0.66163953642715 and parameters: {'n_estimators': 361, 'learning_rate': 0.09429610690274291, 'max_depth': 10, 'max_bin': 276, 'num_leaves': 176}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:38:12,207] Trial 23 finished with value: 0.6457330081817066 and parameters: {'n_estimators': 124, 'learning_rate': 0.054573087891398205, 'max_depth': 8, 'max_bin': 261, 'num_leaves': 104}. Best is trial 4 with value: 0.6725188109745284.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:38:13,877] Trial 24 finished with value: 0.6705632102963215 and parameters: {'n_estimators': 492, 'learning_rate': 0.09405407613295111, 'max_depth': 11, 'max_bin': 298, 'num_leaves': 31}. Best is trial 4 with value: 0.6725188109745284.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:38:15,496] Trial 25 finished with value: 0.669531412085982 and parameters: {'n_estimators': 491, 'learning_rate': 0.09709537501466547, 'max_depth': 11, 'max_bin': 292, 'num_leaves': 30}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:38:17,147] Trial 26 finished with value: 0.6677753174102786 and parameters: {'n_estimators': 460, 'learning_rate': 0.08831210923985215, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 95}. Best is trial 4 with value: 0.6725188109745284.\n",
      "[I 2023-12-11 23:38:18,732] Trial 27 finished with value: 0.6741812500775242 and parameters: {'n_estimators': 578, 'learning_rate': 0.10709014237811537, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 206}. Best is trial 27 with value: 0.6741812500775242.\n",
      "[I 2023-12-11 23:38:20,326] Trial 28 finished with value: 0.6739226295964728 and parameters: {'n_estimators': 623, 'learning_rate': 0.11335303147121842, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 280}. Best is trial 27 with value: 0.6741812500775242.\n",
      "[I 2023-12-11 23:38:22,075] Trial 29 finished with value: 0.6795929933316216 and parameters: {'n_estimators': 593, 'learning_rate': 0.11435344439283482, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 271}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:23,592] Trial 30 finished with value: 0.6721402179508196 and parameters: {'n_estimators': 619, 'learning_rate': 0.12314200790426699, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 292}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:25,105] Trial 31 finished with value: 0.6706843948393898 and parameters: {'n_estimators': 569, 'learning_rate': 0.10988781716157117, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 244}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:26,421] Trial 32 finished with value: 0.6670786935504668 and parameters: {'n_estimators': 669, 'learning_rate': 0.13299478032656487, 'max_depth': 8, 'max_bin': 257, 'num_leaves': 351}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:27,458] Trial 33 finished with value: 0.6580425401773894 and parameters: {'n_estimators': 638, 'learning_rate': 0.11522691051049765, 'max_depth': 6, 'max_bin': 265, 'num_leaves': 202}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:29,018] Trial 34 finished with value: 0.6690015571338026 and parameters: {'n_estimators': 569, 'learning_rate': 0.1094866117909653, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 262}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:30,670] Trial 35 finished with value: 0.6714402637959644 and parameters: {'n_estimators': 683, 'learning_rate': 0.11825889214579466, 'max_depth': 11, 'max_bin': 282, 'num_leaves': 416}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:31,840] Trial 36 finished with value: 0.658793106922315 and parameters: {'n_estimators': 775, 'learning_rate': 0.12535453841304628, 'max_depth': 6, 'max_bin': 289, 'num_leaves': 341}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:32,876] Trial 37 finished with value: 0.6596507524912439 and parameters: {'n_estimators': 590, 'learning_rate': 0.14417494981311688, 'max_depth': 7, 'max_bin': 277, 'num_leaves': 383}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:34,297] Trial 38 finished with value: 0.6759558231395995 and parameters: {'n_estimators': 524, 'learning_rate': 0.10732457688662786, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 255}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:35,896] Trial 39 finished with value: 0.6677897011292933 and parameters: {'n_estimators': 750, 'learning_rate': 0.07891091868534608, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 254}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:37,458] Trial 40 finished with value: 0.6722373710009164 and parameters: {'n_estimators': 643, 'learning_rate': 0.15207221251135405, 'max_depth': 11, 'max_bin': 265, 'num_leaves': 524}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:39,102] Trial 41 finished with value: 0.6761655636751174 and parameters: {'n_estimators': 525, 'learning_rate': 0.10802901210141155, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 309}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:40,707] Trial 42 finished with value: 0.6725434233795429 and parameters: {'n_estimators': 512, 'learning_rate': 0.10822128490403184, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 314}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:42,239] Trial 43 finished with value: 0.6664471873340003 and parameters: {'n_estimators': 555, 'learning_rate': 0.1272393673141651, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 267}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:43,748] Trial 44 finished with value: 0.6633938147830827 and parameters: {'n_estimators': 430, 'learning_rate': 0.08920961150453019, 'max_depth': 11, 'max_bin': 294, 'num_leaves': 206}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:45,187] Trial 45 finished with value: 0.6781832581940843 and parameters: {'n_estimators': 602, 'learning_rate': 0.1173116091963003, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 405}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:46,582] Trial 46 finished with value: 0.6666391167989447 and parameters: {'n_estimators': 590, 'learning_rate': 0.1270036269462513, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 418}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:48,097] Trial 47 finished with value: 0.6673917499018852 and parameters: {'n_estimators': 682, 'learning_rate': 0.10259339237089385, 'max_depth': 11, 'max_bin': 262, 'num_leaves': 462}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:49,992] Trial 48 finished with value: 0.674816794184958 and parameters: {'n_estimators': 537, 'learning_rate': 0.11713455718001811, 'max_depth': 12, 'max_bin': 151, 'num_leaves': 372}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:51,559] Trial 49 finished with value: 0.665446468559168 and parameters: {'n_estimators': 531, 'learning_rate': 0.1194454139253431, 'max_depth': 11, 'max_bin': 153, 'num_leaves': 411}. Best is trial 29 with value: 0.6795929933316216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6796\n",
      "\tBest params:\n",
      "\t\tn_estimators: 593\n",
      "\t\tlearning_rate: 0.11435344439283482\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 280\n",
      "\t\tnum_leaves: 271\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.688832\n",
      "1                    TP   31.000000\n",
      "2                    TN  198.000000\n",
      "3                    FP    2.000000\n",
      "4                    FN   37.000000\n",
      "5              Accuracy    0.854478\n",
      "6             Precision    0.939394\n",
      "7           Sensitivity    0.455882\n",
      "8           Specificity    0.990000\n",
      "9              F1 score    0.613861\n",
      "10  F1 score (weighted)    0.835118\n",
      "11     F1 score (macro)    0.762103\n",
      "12    Balanced Accuracy    0.722941\n",
      "13                  MCC    0.590471\n",
      "14                  NPV    0.842600\n",
      "15              ROC_AUC    0.722941\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_lgbm_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_0_cat = np.where(((y_pred_lgbm_0 >= 2) | (y_pred_lgbm_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:38:53,114] Trial 50 finished with value: 0.6645977512252371 and parameters: {'n_estimators': 412, 'learning_rate': 0.13095371570926673, 'max_depth': 10, 'max_bin': 161, 'num_leaves': 509}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:54,909] Trial 51 finished with value: 0.6717756318791117 and parameters: {'n_estimators': 472, 'learning_rate': 0.10640892668434217, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 363}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:56,577] Trial 52 finished with value: 0.6701872348763079 and parameters: {'n_estimators': 598, 'learning_rate': 0.1163515556424046, 'max_depth': 12, 'max_bin': 189, 'num_leaves': 330}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:38:58,310] Trial 53 finished with value: 0.6731840799680158 and parameters: {'n_estimators': 527, 'learning_rate': 0.10080161668797695, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 385}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:39:00,033] Trial 54 finished with value: 0.6735759352692754 and parameters: {'n_estimators': 721, 'learning_rate': 0.12181890983517506, 'max_depth': 11, 'max_bin': 243, 'num_leaves': 587}. Best is trial 29 with value: 0.6795929933316216.\n",
      "[I 2023-12-11 23:39:02,170] Trial 55 finished with value: 0.6799844334120557 and parameters: {'n_estimators': 500, 'learning_rate': 0.08466662721875205, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 233}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:03,032] Trial 56 finished with value: 0.6324687894841363 and parameters: {'n_estimators': 375, 'learning_rate': 0.0853006098221675, 'max_depth': 4, 'max_bin': 254, 'num_leaves': 237}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:04,591] Trial 57 finished with value: 0.6646758709299665 and parameters: {'n_estimators': 446, 'learning_rate': 0.07320250257684688, 'max_depth': 9, 'max_bin': 228, 'num_leaves': 314}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:06,058] Trial 58 finished with value: 0.6690191344053384 and parameters: {'n_estimators': 502, 'learning_rate': 0.09245799317950193, 'max_depth': 10, 'max_bin': 238, 'num_leaves': 459}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:07,913] Trial 59 finished with value: 0.6742427094150367 and parameters: {'n_estimators': 549, 'learning_rate': 0.09907006205707407, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 303}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:09,800] Trial 60 finished with value: 0.6712064760476547 and parameters: {'n_estimators': 844, 'learning_rate': 0.08242220139045259, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 189}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:11,452] Trial 61 finished with value: 0.6700242583478814 and parameters: {'n_estimators': 542, 'learning_rate': 0.09912266120260098, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 300}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:13,325] Trial 62 finished with value: 0.6720237464701333 and parameters: {'n_estimators': 471, 'learning_rate': 0.11206446049373091, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 365}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:14,825] Trial 63 finished with value: 0.6716794802372108 and parameters: {'n_estimators': 547, 'learning_rate': 0.09622144628733952, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 225}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:16,753] Trial 64 finished with value: 0.6776334832949839 and parameters: {'n_estimators': 486, 'learning_rate': 0.08953272662753245, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 328}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:18,629] Trial 65 finished with value: 0.671704928534123 and parameters: {'n_estimators': 411, 'learning_rate': 0.07464540942673129, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 432}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:20,415] Trial 66 finished with value: 0.6726144844206841 and parameters: {'n_estimators': 656, 'learning_rate': 0.09187942029919573, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 325}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:22,233] Trial 67 finished with value: 0.6681975606765214 and parameters: {'n_estimators': 504, 'learning_rate': 0.08664176439459662, 'max_depth': 12, 'max_bin': 174, 'num_leaves': 270}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:23,777] Trial 68 finished with value: 0.6710243570158809 and parameters: {'n_estimators': 482, 'learning_rate': 0.10438143590668544, 'max_depth': 11, 'max_bin': 205, 'num_leaves': 396}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:25,276] Trial 69 finished with value: 0.6736920163601436 and parameters: {'n_estimators': 603, 'learning_rate': 0.11420165515616401, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 732}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:26,549] Trial 70 finished with value: 0.6637278036089937 and parameters: {'n_estimators': 443, 'learning_rate': 0.13534575566197865, 'max_depth': 10, 'max_bin': 242, 'num_leaves': 148}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:28,211] Trial 71 finished with value: 0.6741491799456355 and parameters: {'n_estimators': 561, 'learning_rate': 0.09755323407518106, 'max_depth': 11, 'max_bin': 254, 'num_leaves': 360}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:30,040] Trial 72 finished with value: 0.6736979260546034 and parameters: {'n_estimators': 617, 'learning_rate': 0.10197144826039471, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 290}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:31,828] Trial 73 finished with value: 0.6785488823284199 and parameters: {'n_estimators': 517, 'learning_rate': 0.11278188257343119, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 233}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:33,362] Trial 74 finished with value: 0.6674307096419991 and parameters: {'n_estimators': 527, 'learning_rate': 0.12031975364090113, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 235}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:35,232] Trial 75 finished with value: 0.6740222160154776 and parameters: {'n_estimators': 491, 'learning_rate': 0.10931895087476023, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 183}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:37,071] Trial 76 finished with value: 0.6683730841389711 and parameters: {'n_estimators': 584, 'learning_rate': 0.11462020245827209, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 252}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:38,744] Trial 77 finished with value: 0.6714554683749382 and parameters: {'n_estimators': 512, 'learning_rate': 0.12530178819371718, 'max_depth': 11, 'max_bin': 271, 'num_leaves': 334}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:40,582] Trial 78 finished with value: 0.6749904907296076 and parameters: {'n_estimators': 459, 'learning_rate': 0.08986939835501259, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 279}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:42,195] Trial 79 finished with value: 0.6741625979972273 and parameters: {'n_estimators': 457, 'learning_rate': 0.09092678191145497, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 214}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:44,044] Trial 80 finished with value: 0.6729734372068009 and parameters: {'n_estimators': 382, 'learning_rate': 0.08010982949837034, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 277}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:46,067] Trial 81 finished with value: 0.6696619903006673 and parameters: {'n_estimators': 408, 'learning_rate': 0.10476477709500337, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 283}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:48,002] Trial 82 finished with value: 0.6715066434519639 and parameters: {'n_estimators': 476, 'learning_rate': 0.1108885369478851, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 259}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:50,061] Trial 83 finished with value: 0.6775261586233985 and parameters: {'n_estimators': 572, 'learning_rate': 0.09411259514600022, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 372}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:52,036] Trial 84 finished with value: 0.6714797722807913 and parameters: {'n_estimators': 628, 'learning_rate': 0.08635858222180556, 'max_depth': 11, 'max_bin': 288, 'num_leaves': 324}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:53,982] Trial 85 finished with value: 0.6764555728170027 and parameters: {'n_estimators': 573, 'learning_rate': 0.09424775582328504, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 400}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:55,930] Trial 86 finished with value: 0.6760618898062829 and parameters: {'n_estimators': 569, 'learning_rate': 0.09474113107290932, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 393}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:39:57,786] Trial 87 finished with value: 0.6690562194418108 and parameters: {'n_estimators': 567, 'learning_rate': 0.0952293959146918, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 482}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:40:00,147] Trial 88 finished with value: 0.6699991295982904 and parameters: {'n_estimators': 663, 'learning_rate': 0.08145746049785084, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 398}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:40:02,101] Trial 89 finished with value: 0.6647958341574581 and parameters: {'n_estimators': 609, 'learning_rate': 0.07051087216051918, 'max_depth': 11, 'max_bin': 241, 'num_leaves': 441}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:40:04,023] Trial 90 finished with value: 0.66854375827442 and parameters: {'n_estimators': 579, 'learning_rate': 0.09290306991080696, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 347}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:40:06,137] Trial 91 finished with value: 0.676587432401686 and parameters: {'n_estimators': 523, 'learning_rate': 0.10533193606082751, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 428}. Best is trial 55 with value: 0.6799844334120557.\n",
      "[I 2023-12-11 23:40:07,874] Trial 92 finished with value: 0.6809963460320942 and parameters: {'n_estimators': 695, 'learning_rate': 0.10244023588381922, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 382}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:09,555] Trial 93 finished with value: 0.6719683844615114 and parameters: {'n_estimators': 739, 'learning_rate': 0.1010547831716877, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 427}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:11,508] Trial 94 finished with value: 0.6733262732187668 and parameters: {'n_estimators': 632, 'learning_rate': 0.10662058167802045, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 408}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:13,172] Trial 95 finished with value: 0.6748441668411199 and parameters: {'n_estimators': 683, 'learning_rate': 0.11972491302208584, 'max_depth': 11, 'max_bin': 257, 'num_leaves': 378}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:15,007] Trial 96 finished with value: 0.6735609435660278 and parameters: {'n_estimators': 703, 'learning_rate': 0.11031011151947516, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 451}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:16,898] Trial 97 finished with value: 0.6798659868696916 and parameters: {'n_estimators': 428, 'learning_rate': 0.08570746936201998, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 469}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:18,726] Trial 98 finished with value: 0.6706045783864643 and parameters: {'n_estimators': 433, 'learning_rate': 0.08537523565999013, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 491}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:20,896] Trial 99 finished with value: 0.6775701985913709 and parameters: {'n_estimators': 499, 'learning_rate': 0.07646322495504106, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 555}. Best is trial 92 with value: 0.6809963460320942.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6810\n",
      "\tBest params:\n",
      "\t\tn_estimators: 695\n",
      "\t\tlearning_rate: 0.10244023588381922\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 255\n",
      "\t\tnum_leaves: 382\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.688832    0.664005\n",
      "1                    TP   31.000000   42.000000\n",
      "2                    TN  198.000000  198.000000\n",
      "3                    FP    2.000000    4.000000\n",
      "4                    FN   37.000000   24.000000\n",
      "5              Accuracy    0.854478    0.895522\n",
      "6             Precision    0.939394    0.913043\n",
      "7           Sensitivity    0.455882    0.636364\n",
      "8           Specificity    0.990000    0.980200\n",
      "9              F1 score    0.613861    0.750000\n",
      "10  F1 score (weighted)    0.835118    0.888658\n",
      "11     F1 score (macro)    0.762103    0.841981\n",
      "12    Balanced Accuracy    0.722941    0.808281\n",
      "13                  MCC    0.590471    0.704480\n",
      "14                  NPV    0.842600    0.891900\n",
      "15              ROC_AUC    0.722941    0.808281\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_lgbm_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_1_cat = np.where(((y_pred_lgbm_1 >= 2) | (y_pred_lgbm_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:40:23,081] Trial 100 finished with value: 0.6603124439026666 and parameters: {'n_estimators': 502, 'learning_rate': 0.06931867220774704, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 590}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:25,201] Trial 101 finished with value: 0.6625917018800268 and parameters: {'n_estimators': 310, 'learning_rate': 0.0760135317256683, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 546}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:27,070] Trial 102 finished with value: 0.6574021844023126 and parameters: {'n_estimators': 551, 'learning_rate': 0.08185761747907017, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 464}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:29,268] Trial 103 finished with value: 0.662782925424065 and parameters: {'n_estimators': 650, 'learning_rate': 0.08923037232981929, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 520}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:31,181] Trial 104 finished with value: 0.6592894736300203 and parameters: {'n_estimators': 491, 'learning_rate': 0.09834028184854884, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 433}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:32,571] Trial 105 finished with value: 0.6498431958336232 and parameters: {'n_estimators': 816, 'learning_rate': 0.10368768471746805, 'max_depth': 8, 'max_bin': 241, 'num_leaves': 671}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:33,545] Trial 106 finished with value: 0.6260959797448928 and parameters: {'n_estimators': 425, 'learning_rate': 0.07685813287401523, 'max_depth': 3, 'max_bin': 257, 'num_leaves': 414}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:35,362] Trial 107 finished with value: 0.6569478707394094 and parameters: {'n_estimators': 518, 'learning_rate': 0.09059403839668802, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 475}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:36,482] Trial 108 finished with value: 0.6398709299659117 and parameters: {'n_estimators': 540, 'learning_rate': 0.08421351820998446, 'max_depth': 4, 'max_bin': 248, 'num_leaves': 560}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:38,373] Trial 109 finished with value: 0.6633661025514538 and parameters: {'n_estimators': 595, 'learning_rate': 0.09575567639329491, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 359}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:40,258] Trial 110 finished with value: 0.6659263489855577 and parameters: {'n_estimators': 467, 'learning_rate': 0.10158665885682851, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 375}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:41,908] Trial 111 finished with value: 0.659646578515029 and parameters: {'n_estimators': 511, 'learning_rate': 0.11031211045587334, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 347}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:43,399] Trial 112 finished with value: 0.661124955362172 and parameters: {'n_estimators': 534, 'learning_rate': 0.11410759873246314, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 503}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:45,030] Trial 113 finished with value: 0.6629879394102921 and parameters: {'n_estimators': 489, 'learning_rate': 0.10565068938215876, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 309}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:46,732] Trial 114 finished with value: 0.6575982736582289 and parameters: {'n_estimators': 558, 'learning_rate': 0.07862928170699232, 'max_depth': 11, 'max_bin': 260, 'num_leaves': 227}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:48,406] Trial 115 finished with value: 0.665185830097066 and parameters: {'n_estimators': 584, 'learning_rate': 0.11769799335524582, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 403}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:50,423] Trial 116 finished with value: 0.66733443912311 and parameters: {'n_estimators': 450, 'learning_rate': 0.08725194681671385, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 637}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:52,050] Trial 117 finished with value: 0.6563379178493916 and parameters: {'n_estimators': 607, 'learning_rate': 0.09741408943579134, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 389}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:53,977] Trial 118 finished with value: 0.6597236234133024 and parameters: {'n_estimators': 526, 'learning_rate': 0.10003733594443806, 'max_depth': 11, 'max_bin': 254, 'num_leaves': 320}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:55,562] Trial 119 finished with value: 0.6585122149719105 and parameters: {'n_estimators': 483, 'learning_rate': 0.12334317663428451, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 427}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:57,519] Trial 120 finished with value: 0.6622536928356185 and parameters: {'n_estimators': 557, 'learning_rate': 0.09213908644198296, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 446}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:40:59,415] Trial 121 finished with value: 0.660308409988889 and parameters: {'n_estimators': 580, 'learning_rate': 0.10719226108778386, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 394}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:01,433] Trial 122 finished with value: 0.6660427729448921 and parameters: {'n_estimators': 571, 'learning_rate': 0.09425210572565994, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 341}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:03,251] Trial 123 finished with value: 0.6561358388239228 and parameters: {'n_estimators': 542, 'learning_rate': 0.08361230177874583, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 371}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:04,999] Trial 124 finished with value: 0.6642648826054545 and parameters: {'n_estimators': 505, 'learning_rate': 0.09550861475117861, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 294}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:06,157] Trial 125 finished with value: 0.6536520545507478 and parameters: {'n_estimators': 167, 'learning_rate': 0.11360748248469951, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 198}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:07,651] Trial 126 finished with value: 0.6547223683769963 and parameters: {'n_estimators': 617, 'learning_rate': 0.08842363873812505, 'max_depth': 9, 'max_bin': 254, 'num_leaves': 386}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:09,034] Trial 127 finished with value: 0.6583516984572675 and parameters: {'n_estimators': 526, 'learning_rate': 0.10314864450046808, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 245}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:11,051] Trial 128 finished with value: 0.6640451796583517 and parameters: {'n_estimators': 392, 'learning_rate': 0.07984311496879648, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 356}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:12,655] Trial 129 finished with value: 0.6582405228547674 and parameters: {'n_estimators': 566, 'learning_rate': 0.10781564811592746, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 420}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:14,168] Trial 130 finished with value: 0.6560377005161693 and parameters: {'n_estimators': 640, 'learning_rate': 0.11664198693163645, 'max_depth': 11, 'max_bin': 267, 'num_leaves': 333}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:15,767] Trial 131 finished with value: 0.6575620047713138 and parameters: {'n_estimators': 515, 'learning_rate': 0.09972238646140513, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 219}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:17,404] Trial 132 finished with value: 0.6640503713798238 and parameters: {'n_estimators': 471, 'learning_rate': 0.105039493997183, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 254}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:18,972] Trial 133 finished with value: 0.6632550482914483 and parameters: {'n_estimators': 600, 'learning_rate': 0.11215391201898635, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 268}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:20,561] Trial 134 finished with value: 0.657566887788332 and parameters: {'n_estimators': 494, 'learning_rate': 0.09371303347879764, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 237}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:21,721] Trial 135 finished with value: 0.6586319298845005 and parameters: {'n_estimators': 536, 'learning_rate': 0.12742766212031595, 'max_depth': 7, 'max_bin': 263, 'num_leaves': 302}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:23,133] Trial 136 finished with value: 0.6640907457350258 and parameters: {'n_estimators': 555, 'learning_rate': 0.10856918704179401, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 376}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:24,625] Trial 137 finished with value: 0.6643381389662029 and parameters: {'n_estimators': 764, 'learning_rate': 0.08784986547705047, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 407}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:25,489] Trial 138 finished with value: 0.6401968185021744 and parameters: {'n_estimators': 422, 'learning_rate': 0.09677563406237295, 'max_depth': 6, 'max_bin': 256, 'num_leaves': 442}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:26,543] Trial 139 finished with value: 0.6650090973792693 and parameters: {'n_estimators': 449, 'learning_rate': 0.12118273031420786, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 172}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:27,815] Trial 140 finished with value: 0.6598623380073645 and parameters: {'n_estimators': 519, 'learning_rate': 0.08382690604492986, 'max_depth': 11, 'max_bin': 252, 'num_leaves': 465}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:29,211] Trial 141 finished with value: 0.661337038475813 and parameters: {'n_estimators': 463, 'learning_rate': 0.09308521661360868, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 269}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:30,808] Trial 142 finished with value: 0.656602858315233 and parameters: {'n_estimators': 503, 'learning_rate': 0.09079187147039597, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 285}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:32,611] Trial 143 finished with value: 0.6636169035237025 and parameters: {'n_estimators': 476, 'learning_rate': 0.1015625312722754, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 279}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:34,517] Trial 144 finished with value: 0.6608787091599578 and parameters: {'n_estimators': 588, 'learning_rate': 0.08941610036424542, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 255}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:36,324] Trial 145 finished with value: 0.6596948832704725 and parameters: {'n_estimators': 453, 'learning_rate': 0.07263518988454895, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 321}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:38,327] Trial 146 finished with value: 0.6671464129638571 and parameters: {'n_estimators': 438, 'learning_rate': 0.07812535492183063, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 399}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:39,950] Trial 147 finished with value: 0.6595676211121304 and parameters: {'n_estimators': 542, 'learning_rate': 0.098313456757837, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 357}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:41,406] Trial 148 finished with value: 0.6628259399877955 and parameters: {'n_estimators': 574, 'learning_rate': 0.10466341538915604, 'max_depth': 11, 'max_bin': 259, 'num_leaves': 312}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:43,010] Trial 149 finished with value: 0.6591433192200131 and parameters: {'n_estimators': 700, 'learning_rate': 0.11049383150644387, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 230}. Best is trial 92 with value: 0.6809963460320942.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6810\n",
      "\tBest params:\n",
      "\t\tn_estimators: 695\n",
      "\t\tlearning_rate: 0.10244023588381922\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 255\n",
      "\t\tnum_leaves: 382\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.688832    0.664005    0.671613\n",
      "1                    TP   31.000000   42.000000   36.000000\n",
      "2                    TN  198.000000  198.000000  197.000000\n",
      "3                    FP    2.000000    4.000000    3.000000\n",
      "4                    FN   37.000000   24.000000   32.000000\n",
      "5              Accuracy    0.854478    0.895522    0.869403\n",
      "6             Precision    0.939394    0.913043    0.923077\n",
      "7           Sensitivity    0.455882    0.636364    0.529412\n",
      "8           Specificity    0.990000    0.980200    0.985000\n",
      "9              F1 score    0.613861    0.750000    0.672897\n",
      "10  F1 score (weighted)    0.835118    0.888658    0.856119\n",
      "11     F1 score (macro)    0.762103    0.841981    0.795656\n",
      "12    Balanced Accuracy    0.722941    0.808281    0.757206\n",
      "13                  MCC    0.590471    0.704480    0.634790\n",
      "14                  NPV    0.842600    0.891900    0.860300\n",
      "15              ROC_AUC    0.722941    0.808281    0.757206\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_lgbm_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_2_cat = np.where(((y_pred_lgbm_2 >= 2) | (y_pred_lgbm_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:41:45,220] Trial 150 finished with value: 0.661109442744966 and parameters: {'n_estimators': 360, 'learning_rate': 0.08630266001673892, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 208}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:47,072] Trial 151 finished with value: 0.6606923549115814 and parameters: {'n_estimators': 729, 'learning_rate': 0.11627602624192145, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 383}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:48,638] Trial 152 finished with value: 0.6652659007825715 and parameters: {'n_estimators': 692, 'learning_rate': 0.11911230840725326, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 417}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:50,079] Trial 153 finished with value: 0.6552473299677742 and parameters: {'n_estimators': 683, 'learning_rate': 0.12051853281753905, 'max_depth': 11, 'max_bin': 254, 'num_leaves': 366}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:51,769] Trial 154 finished with value: 0.6616256568659311 and parameters: {'n_estimators': 659, 'learning_rate': 0.11316159943382541, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 340}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:53,420] Trial 155 finished with value: 0.6571855098456097 and parameters: {'n_estimators': 624, 'learning_rate': 0.1074287742390165, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 376}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:55,018] Trial 156 finished with value: 0.6583140133540286 and parameters: {'n_estimators': 483, 'learning_rate': 0.10205700567773968, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 576}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:57,076] Trial 157 finished with value: 0.6550271102408481 and parameters: {'n_estimators': 520, 'learning_rate': 0.0828469631194296, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 294}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:41:59,115] Trial 158 finished with value: 0.656451455508279 and parameters: {'n_estimators': 674, 'learning_rate': 0.09581096362983021, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 389}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:00,602] Trial 159 finished with value: 0.6565180319587296 and parameters: {'n_estimators': 712, 'learning_rate': 0.12984393037079037, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 429}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:03,072] Trial 160 finished with value: 0.6607020681583174 and parameters: {'n_estimators': 552, 'learning_rate': 0.0746447325207921, 'max_depth': 11, 'max_bin': 196, 'num_leaves': 624}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:04,956] Trial 161 finished with value: 0.6626290607124823 and parameters: {'n_estimators': 536, 'learning_rate': 0.12495257601337491, 'max_depth': 12, 'max_bin': 181, 'num_leaves': 401}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:06,527] Trial 162 finished with value: 0.6573256027591245 and parameters: {'n_estimators': 507, 'learning_rate': 0.11739127808230049, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 369}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:08,208] Trial 163 finished with value: 0.6557268016464138 and parameters: {'n_estimators': 567, 'learning_rate': 0.1109970773501012, 'max_depth': 12, 'max_bin': 171, 'num_leaves': 346}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:09,756] Trial 164 finished with value: 0.6636878114048186 and parameters: {'n_estimators': 598, 'learning_rate': 0.1153906805208188, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 327}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:11,546] Trial 165 finished with value: 0.6665553170473044 and parameters: {'n_estimators': 493, 'learning_rate': 0.10484636374598205, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 244}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:13,269] Trial 166 finished with value: 0.6585613411648418 and parameters: {'n_estimators': 535, 'learning_rate': 0.12333562083616992, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 411}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:15,478] Trial 167 finished with value: 0.6652573986734891 and parameters: {'n_estimators': 467, 'learning_rate': 0.06746059613066677, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 386}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:17,283] Trial 168 finished with value: 0.6614915586417658 and parameters: {'n_estimators': 520, 'learning_rate': 0.09932377843787239, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 359}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:19,178] Trial 169 finished with value: 0.6664681323835281 and parameters: {'n_estimators': 556, 'learning_rate': 0.09087163874351999, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 280}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:20,753] Trial 170 finished with value: 0.6564737036695626 and parameters: {'n_estimators': 504, 'learning_rate': 0.12015641349774306, 'max_depth': 11, 'max_bin': 186, 'num_leaves': 264}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:22,295] Trial 171 finished with value: 0.6621778090328958 and parameters: {'n_estimators': 576, 'learning_rate': 0.09888659608523667, 'max_depth': 10, 'max_bin': 249, 'num_leaves': 295}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:23,892] Trial 172 finished with value: 0.6599145385464251 and parameters: {'n_estimators': 545, 'learning_rate': 0.10875357115082004, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 308}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:25,796] Trial 173 finished with value: 0.6541485008632796 and parameters: {'n_estimators': 613, 'learning_rate': 0.09447014751353221, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 531}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:27,422] Trial 174 finished with value: 0.6648664012960375 and parameters: {'n_estimators': 526, 'learning_rate': 0.08709479347491908, 'max_depth': 10, 'max_bin': 236, 'num_leaves': 335}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:29,251] Trial 175 finished with value: 0.6605831557176736 and parameters: {'n_estimators': 587, 'learning_rate': 0.1027145707990558, 'max_depth': 11, 'max_bin': 257, 'num_leaves': 376}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:29,956] Trial 176 finished with value: 0.6345804352659281 and parameters: {'n_estimators': 72, 'learning_rate': 0.11254024012563439, 'max_depth': 9, 'max_bin': 245, 'num_leaves': 437}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:32,246] Trial 177 finished with value: 0.6637536898849022 and parameters: {'n_estimators': 801, 'learning_rate': 0.08020735808765499, 'max_depth': 11, 'max_bin': 261, 'num_leaves': 401}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:34,560] Trial 178 finished with value: 0.6653645374660877 and parameters: {'n_estimators': 486, 'learning_rate': 0.08943210662194867, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 309}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:36,467] Trial 179 finished with value: 0.6627506068571095 and parameters: {'n_estimators': 548, 'learning_rate': 0.10644008438971517, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 682}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:38,574] Trial 180 finished with value: 0.6636199077973421 and parameters: {'n_estimators': 569, 'learning_rate': 0.0945607777744652, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 251}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:40,412] Trial 181 finished with value: 0.6680156506620836 and parameters: {'n_estimators': 586, 'learning_rate': 0.11484363070171445, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 217}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:42,380] Trial 182 finished with value: 0.6658629211867555 and parameters: {'n_estimators': 555, 'learning_rate': 0.09903293057707399, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 184}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:44,053] Trial 183 finished with value: 0.6648755357276692 and parameters: {'n_estimators': 512, 'learning_rate': 0.10925038101312792, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 204}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:45,764] Trial 184 finished with value: 0.6652767847768681 and parameters: {'n_estimators': 605, 'learning_rate': 0.10491220551561505, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 227}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:47,526] Trial 185 finished with value: 0.6597093741113261 and parameters: {'n_estimators': 647, 'learning_rate': 0.11780049313018913, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 60}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:49,427] Trial 186 finished with value: 0.6536785523821829 and parameters: {'n_estimators': 531, 'learning_rate': 0.10117664914271171, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 158}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:51,219] Trial 187 finished with value: 0.6538200185402399 and parameters: {'n_estimators': 494, 'learning_rate': 0.0837262889228484, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 269}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:52,799] Trial 188 finished with value: 0.6590220543104622 and parameters: {'n_estimators': 404, 'learning_rate': 0.11385436606203828, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 280}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:54,493] Trial 189 finished with value: 0.6572928983457941 and parameters: {'n_estimators': 464, 'learning_rate': 0.09633495935186853, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 242}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:56,387] Trial 190 finished with value: 0.6627102658376414 and parameters: {'n_estimators': 571, 'learning_rate': 0.09053042957261503, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 423}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:58,187] Trial 191 finished with value: 0.6616839839655058 and parameters: {'n_estimators': 435, 'learning_rate': 0.09332713129604707, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 196}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:42:59,787] Trial 192 finished with value: 0.6605502384876112 and parameters: {'n_estimators': 479, 'learning_rate': 0.0854946510023245, 'max_depth': 11, 'max_bin': 296, 'num_leaves': 221}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:01,250] Trial 193 finished with value: 0.6607268481227988 and parameters: {'n_estimators': 509, 'learning_rate': 0.09127221361068295, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 209}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:02,871] Trial 194 finished with value: 0.6623956327888039 and parameters: {'n_estimators': 453, 'learning_rate': 0.10892800089137765, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 353}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:04,523] Trial 195 finished with value: 0.6619952792565549 and parameters: {'n_estimators': 541, 'learning_rate': 0.10299506095127096, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 234}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:05,992] Trial 196 finished with value: 0.6514998723298263 and parameters: {'n_estimators': 420, 'learning_rate': 0.12118301915913114, 'max_depth': 11, 'max_bin': 245, 'num_leaves': 388}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:08,283] Trial 197 finished with value: 0.6610990662699475 and parameters: {'n_estimators': 631, 'learning_rate': 0.07597980060464884, 'max_depth': 12, 'max_bin': 249, 'num_leaves': 457}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:10,553] Trial 198 finished with value: 0.663002374767047 and parameters: {'n_estimators': 523, 'learning_rate': 0.0972973807402377, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 496}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:12,526] Trial 199 finished with value: 0.6541405916995295 and parameters: {'n_estimators': 744, 'learning_rate': 0.08780648474764413, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 371}. Best is trial 92 with value: 0.6809963460320942.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6810\n",
      "\tBest params:\n",
      "\t\tn_estimators: 695\n",
      "\t\tlearning_rate: 0.10244023588381922\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 255\n",
      "\t\tnum_leaves: 382\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.688832    0.664005    0.671613    0.713928\n",
      "1                    TP   31.000000   42.000000   36.000000   34.000000\n",
      "2                    TN  198.000000  198.000000  197.000000  199.000000\n",
      "3                    FP    2.000000    4.000000    3.000000    2.000000\n",
      "4                    FN   37.000000   24.000000   32.000000   33.000000\n",
      "5              Accuracy    0.854478    0.895522    0.869403    0.869403\n",
      "6             Precision    0.939394    0.913043    0.923077    0.944444\n",
      "7           Sensitivity    0.455882    0.636364    0.529412    0.507463\n",
      "8           Specificity    0.990000    0.980200    0.985000    0.990000\n",
      "9              F1 score    0.613861    0.750000    0.672897    0.660194\n",
      "10  F1 score (weighted)    0.835118    0.888658    0.856119    0.854425\n",
      "11     F1 score (macro)    0.762103    0.841981    0.795656    0.789681\n",
      "12    Balanced Accuracy    0.722941    0.808281    0.757206    0.748756\n",
      "13                  MCC    0.590471    0.704480    0.634790    0.631748\n",
      "14                  NPV    0.842600    0.891900    0.860300    0.857800\n",
      "15              ROC_AUC    0.722941    0.808281    0.757206    0.748756\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_lgbm_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_3_cat = np.where(((y_pred_lgbm_3 >= 2) | (y_pred_lgbm_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:43:14,163] Trial 200 finished with value: 0.6675536448169309 and parameters: {'n_estimators': 560, 'learning_rate': 0.08051301959096323, 'max_depth': 10, 'max_bin': 259, 'num_leaves': 260}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:15,615] Trial 201 finished with value: 0.6693561076171783 and parameters: {'n_estimators': 567, 'learning_rate': 0.10087061574164959, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 356}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:17,322] Trial 202 finished with value: 0.6697332448614148 and parameters: {'n_estimators': 589, 'learning_rate': 0.093049072793043, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 328}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:18,967] Trial 203 finished with value: 0.6680074795354101 and parameters: {'n_estimators': 547, 'learning_rate': 0.10665386356876772, 'max_depth': 11, 'max_bin': 253, 'num_leaves': 393}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:20,569] Trial 204 finished with value: 0.6660356530486501 and parameters: {'n_estimators': 531, 'learning_rate': 0.09687952231919977, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 409}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:22,054] Trial 205 finished with value: 0.6724325892188164 and parameters: {'n_estimators': 495, 'learning_rate': 0.11165710696329807, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 370}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:23,779] Trial 206 finished with value: 0.6704569988641624 and parameters: {'n_estimators': 459, 'learning_rate': 0.19107398529508623, 'max_depth': 11, 'max_bin': 247, 'num_leaves': 345}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:25,511] Trial 207 finished with value: 0.6668143044620226 and parameters: {'n_estimators': 578, 'learning_rate': 0.12831208093678817, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 296}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:26,833] Trial 208 finished with value: 0.6718948552411435 and parameters: {'n_estimators': 599, 'learning_rate': 0.10322978649185023, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 384}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:28,100] Trial 209 finished with value: 0.6665937833841983 and parameters: {'n_estimators': 557, 'learning_rate': 0.08969732537538543, 'max_depth': 11, 'max_bin': 272, 'num_leaves': 317}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:29,607] Trial 210 finished with value: 0.6683352111937599 and parameters: {'n_estimators': 478, 'learning_rate': 0.083661157176801, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 362}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:31,053] Trial 211 finished with value: 0.6715307881371178 and parameters: {'n_estimators': 504, 'learning_rate': 0.11116577382249657, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 124}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:32,361] Trial 212 finished with value: 0.6684444419760378 and parameters: {'n_estimators': 517, 'learning_rate': 0.10779560882800944, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 182}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:33,559] Trial 213 finished with value: 0.6632540513198307 and parameters: {'n_estimators': 493, 'learning_rate': 0.11857484100161103, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 211}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:35,011] Trial 214 finished with value: 0.666709122556277 and parameters: {'n_estimators': 445, 'learning_rate': 0.11494928892806668, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 186}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:36,780] Trial 215 finished with value: 0.6747832667886763 and parameters: {'n_estimators': 535, 'learning_rate': 0.10068581626436596, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 243}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:38,744] Trial 216 finished with value: 0.6779710126407219 and parameters: {'n_estimators': 536, 'learning_rate': 0.09892064819526848, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 251}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:40,457] Trial 217 finished with value: 0.6713507781417464 and parameters: {'n_estimators': 530, 'learning_rate': 0.09928038890059819, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 249}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:41,936] Trial 218 finished with value: 0.6709299968584782 and parameters: {'n_estimators': 543, 'learning_rate': 0.09449239017421739, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 239}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:43,482] Trial 219 finished with value: 0.6708102945892493 and parameters: {'n_estimators': 519, 'learning_rate': 0.10525840971236959, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 254}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:45,414] Trial 220 finished with value: 0.6721255167211801 and parameters: {'n_estimators': 547, 'learning_rate': 0.046466983334176384, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 272}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:47,257] Trial 221 finished with value: 0.6760395626377832 and parameters: {'n_estimators': 891, 'learning_rate': 0.09842694438257593, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 223}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:48,549] Trial 222 finished with value: 0.6680980868565772 and parameters: {'n_estimators': 862, 'learning_rate': 0.09978780756105649, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 241}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:50,348] Trial 223 finished with value: 0.6738197638621293 and parameters: {'n_estimators': 830, 'learning_rate': 0.09211438275147209, 'max_depth': 12, 'max_bin': 295, 'num_leaves': 223}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:51,716] Trial 224 finished with value: 0.673556659974143 and parameters: {'n_estimators': 766, 'learning_rate': 0.10343595654080233, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 229}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:53,187] Trial 225 finished with value: 0.6690873896022659 and parameters: {'n_estimators': 801, 'learning_rate': 0.09791281460979645, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 199}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:54,501] Trial 226 finished with value: 0.6672668270393148 and parameters: {'n_estimators': 869, 'learning_rate': 0.0888932079067077, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 265}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:43:55,846] Trial 227 finished with value: 0.6678223958978866 and parameters: {'n_estimators': 567, 'learning_rate': 0.12416288996600724, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 287}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:00,448] Trial 228 finished with value: 0.6385375240490637 and parameters: {'n_estimators': 535, 'learning_rate': 0.0058417754267033745, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 222}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:03,586] Trial 229 finished with value: 0.6627915817358186 and parameters: {'n_estimators': 616, 'learning_rate': 0.019672046352411182, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 256}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:05,195] Trial 230 finished with value: 0.6738473706207262 and parameters: {'n_estimators': 891, 'learning_rate': 0.09477513712689475, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 237}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:06,107] Trial 231 finished with value: 0.6517326855138346 and parameters: {'n_estimators': 564, 'learning_rate': 0.10209039061461858, 'max_depth': 4, 'max_bin': 262, 'num_leaves': 399}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:07,547] Trial 232 finished with value: 0.6697329102152403 and parameters: {'n_estimators': 581, 'learning_rate': 0.09675177834052873, 'max_depth': 12, 'max_bin': 249, 'num_leaves': 416}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:09,039] Trial 233 finished with value: 0.6700240416141192 and parameters: {'n_estimators': 549, 'learning_rate': 0.10620671574902299, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 380}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:10,352] Trial 234 finished with value: 0.6636445070227692 and parameters: {'n_estimators': 514, 'learning_rate': 0.09172155005794506, 'max_depth': 8, 'max_bin': 216, 'num_leaves': 209}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:12,204] Trial 235 finished with value: 0.669198153123949 and parameters: {'n_estimators': 504, 'learning_rate': 0.08478988743668506, 'max_depth': 12, 'max_bin': 196, 'num_leaves': 279}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:13,744] Trial 236 finished with value: 0.6716659204573148 and parameters: {'n_estimators': 595, 'learning_rate': 0.09958038269206271, 'max_depth': 10, 'max_bin': 268, 'num_leaves': 393}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:15,107] Trial 237 finished with value: 0.6710825413357581 and parameters: {'n_estimators': 482, 'learning_rate': 0.11046550848259447, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 368}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:16,655] Trial 238 finished with value: 0.675787650867394 and parameters: {'n_estimators': 533, 'learning_rate': 0.08769205588600233, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 299}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:18,539] Trial 239 finished with value: 0.6745735700753249 and parameters: {'n_estimators': 533, 'learning_rate': 0.08721142853388146, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 311}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:20,504] Trial 240 finished with value: 0.673304888173978 and parameters: {'n_estimators': 525, 'learning_rate': 0.07978386125265007, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 295}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:22,155] Trial 241 finished with value: 0.6708452718420078 and parameters: {'n_estimators': 532, 'learning_rate': 0.08675182237080653, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 305}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:23,802] Trial 242 finished with value: 0.6747685613782344 and parameters: {'n_estimators': 503, 'learning_rate': 0.09026041810516884, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 314}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:25,375] Trial 243 finished with value: 0.6700460055306395 and parameters: {'n_estimators': 509, 'learning_rate': 0.0870833595891159, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 309}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:26,962] Trial 244 finished with value: 0.6690962111240719 and parameters: {'n_estimators': 540, 'learning_rate': 0.07700259752723823, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 328}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:28,404] Trial 245 finished with value: 0.6697559212115627 and parameters: {'n_estimators': 500, 'learning_rate': 0.08310260912858528, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 292}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:30,167] Trial 246 finished with value: 0.6713296969265852 and parameters: {'n_estimators': 554, 'learning_rate': 0.09179462824535317, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 276}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:31,780] Trial 247 finished with value: 0.6696736239851573 and parameters: {'n_estimators': 528, 'learning_rate': 0.09422162082623274, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 318}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:33,400] Trial 248 finished with value: 0.6674408396701098 and parameters: {'n_estimators': 517, 'learning_rate': 0.07187806207348772, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 267}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:34,759] Trial 249 finished with value: 0.6676126570421406 and parameters: {'n_estimators': 575, 'learning_rate': 0.08727694412589326, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 304}. Best is trial 92 with value: 0.6809963460320942.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6810\n",
      "\tBest params:\n",
      "\t\tn_estimators: 695\n",
      "\t\tlearning_rate: 0.10244023588381922\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 255\n",
      "\t\tnum_leaves: 382\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.688832    0.664005    0.671613    0.713928   \n",
      "1                    TP   31.000000   42.000000   36.000000   34.000000   \n",
      "2                    TN  198.000000  198.000000  197.000000  199.000000   \n",
      "3                    FP    2.000000    4.000000    3.000000    2.000000   \n",
      "4                    FN   37.000000   24.000000   32.000000   33.000000   \n",
      "5              Accuracy    0.854478    0.895522    0.869403    0.869403   \n",
      "6             Precision    0.939394    0.913043    0.923077    0.944444   \n",
      "7           Sensitivity    0.455882    0.636364    0.529412    0.507463   \n",
      "8           Specificity    0.990000    0.980200    0.985000    0.990000   \n",
      "9              F1 score    0.613861    0.750000    0.672897    0.660194   \n",
      "10  F1 score (weighted)    0.835118    0.888658    0.856119    0.854425   \n",
      "11     F1 score (macro)    0.762103    0.841981    0.795656    0.789681   \n",
      "12    Balanced Accuracy    0.722941    0.808281    0.757206    0.748756   \n",
      "13                  MCC    0.590471    0.704480    0.634790    0.631748   \n",
      "14                  NPV    0.842600    0.891900    0.860300    0.857800   \n",
      "15              ROC_AUC    0.722941    0.808281    0.757206    0.748756   \n",
      "\n",
      "          Set4  \n",
      "0     0.633686  \n",
      "1    38.000000  \n",
      "2   199.000000  \n",
      "3     2.000000  \n",
      "4    29.000000  \n",
      "5     0.884328  \n",
      "6     0.950000  \n",
      "7     0.567164  \n",
      "8     0.990000  \n",
      "9     0.710280  \n",
      "10    0.873374  \n",
      "11    0.819010  \n",
      "12    0.778607  \n",
      "13    0.677111  \n",
      "14    0.872800  \n",
      "15    0.778607  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_lgbm_4)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_4_cat = np.where(((y_pred_lgbm_4 >= 2) | (y_pred_lgbm_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:44:36,536] Trial 250 finished with value: 0.6574735737395787 and parameters: {'n_estimators': 495, 'learning_rate': 0.11247634507705882, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 285}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:38,353] Trial 251 finished with value: 0.6652306810159436 and parameters: {'n_estimators': 551, 'learning_rate': 0.1171018851524291, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 251}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:39,940] Trial 252 finished with value: 0.6628467248690483 and parameters: {'n_estimators': 478, 'learning_rate': 0.08899432941334648, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 474}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:41,492] Trial 253 finished with value: 0.6613866367165875 and parameters: {'n_estimators': 259, 'learning_rate': 0.09617505992892335, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 317}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:42,877] Trial 254 finished with value: 0.662217828795355 and parameters: {'n_estimators': 537, 'learning_rate': 0.13417151746563657, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 335}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:44,842] Trial 255 finished with value: 0.6743488505490409 and parameters: {'n_estimators': 518, 'learning_rate': 0.08217295760118787, 'max_depth': 12, 'max_bin': 177, 'num_leaves': 302}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:46,697] Trial 256 finished with value: 0.6621487904072989 and parameters: {'n_estimators': 513, 'learning_rate': 0.08322218016831026, 'max_depth': 12, 'max_bin': 175, 'num_leaves': 308}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:48,350] Trial 257 finished with value: 0.666087182149995 and parameters: {'n_estimators': 784, 'learning_rate': 0.08117724504371297, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 294}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:50,167] Trial 258 finished with value: 0.6664309678714787 and parameters: {'n_estimators': 519, 'learning_rate': 0.07807322512856765, 'max_depth': 12, 'max_bin': 178, 'num_leaves': 606}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:51,735] Trial 259 finished with value: 0.6702759885205521 and parameters: {'n_estimators': 495, 'learning_rate': 0.07340881695646459, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 277}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:53,582] Trial 260 finished with value: 0.6683734103283923 and parameters: {'n_estimators': 536, 'learning_rate': 0.06490282778560544, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 564}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:55,263] Trial 261 finished with value: 0.6620570015964139 and parameters: {'n_estimators': 506, 'learning_rate': 0.0906533342712179, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 342}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:57,174] Trial 262 finished with value: 0.665150309584366 and parameters: {'n_estimators': 558, 'learning_rate': 0.0849130502969219, 'max_depth': 12, 'max_bin': 166, 'num_leaves': 326}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:44:58,800] Trial 263 finished with value: 0.6686106442117595 and parameters: {'n_estimators': 485, 'learning_rate': 0.12183805417060015, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 408}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:00,288] Trial 264 finished with value: 0.6599712664924963 and parameters: {'n_estimators': 526, 'learning_rate': 0.1010067790105596, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 301}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:01,896] Trial 265 finished with value: 0.6603580526907953 and parameters: {'n_estimators': 469, 'learning_rate': 0.09415213435744754, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 437}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:02,697] Trial 266 finished with value: 0.6416012678237546 and parameters: {'n_estimators': 546, 'learning_rate': 0.14896810571156216, 'max_depth': 5, 'max_bin': 161, 'num_leaves': 256}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:04,529] Trial 267 finished with value: 0.6639009616986397 and parameters: {'n_estimators': 514, 'learning_rate': 0.0815056015112919, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 282}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:06,413] Trial 268 finished with value: 0.661598035713157 and parameters: {'n_estimators': 565, 'learning_rate': 0.08783432945614573, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 422}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:07,711] Trial 269 finished with value: 0.6598938407831643 and parameters: {'n_estimators': 532, 'learning_rate': 0.10607449819828502, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 319}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:09,623] Trial 270 finished with value: 0.669584477086977 and parameters: {'n_estimators': 723, 'learning_rate': 0.09708697938630241, 'max_depth': 12, 'max_bin': 165, 'num_leaves': 266}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:10,682] Trial 271 finished with value: 0.650411555439226 and parameters: {'n_estimators': 493, 'learning_rate': 0.11524531191802975, 'max_depth': 7, 'max_bin': 157, 'num_leaves': 385}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:12,715] Trial 272 finished with value: 0.6706912092840708 and parameters: {'n_estimators': 588, 'learning_rate': 0.058904346581473394, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 296}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:14,235] Trial 273 finished with value: 0.6615088384019727 and parameters: {'n_estimators': 522, 'learning_rate': 0.09074229403538944, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 352}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:15,615] Trial 274 finished with value: 0.6616072649393038 and parameters: {'n_estimators': 464, 'learning_rate': 0.10355537518117507, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 240}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:17,397] Trial 275 finished with value: 0.6687198142978598 and parameters: {'n_estimators': 556, 'learning_rate': 0.07645040409867739, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 401}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:19,224] Trial 276 finished with value: 0.6607251846210288 and parameters: {'n_estimators': 429, 'learning_rate': 0.08625179796380966, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 336}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:20,939] Trial 277 finished with value: 0.6670995228831585 and parameters: {'n_estimators': 501, 'learning_rate': 0.10865138494114171, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 377}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:22,459] Trial 278 finished with value: 0.6645304118100765 and parameters: {'n_estimators': 540, 'learning_rate': 0.09284974003539616, 'max_depth': 12, 'max_bin': 194, 'num_leaves': 286}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:23,943] Trial 279 finished with value: 0.6538634211117004 and parameters: {'n_estimators': 607, 'learning_rate': 0.09939156832731003, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 311}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:25,515] Trial 280 finished with value: 0.6697100173624178 and parameters: {'n_estimators': 569, 'learning_rate': 0.11810287184832137, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 518}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:27,473] Trial 281 finished with value: 0.6765958420079972 and parameters: {'n_estimators': 480, 'learning_rate': 0.08198325993529887, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 258}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:29,068] Trial 282 finished with value: 0.6663030441828609 and parameters: {'n_estimators': 449, 'learning_rate': 0.08006194169389348, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 252}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:30,186] Trial 283 finished with value: 0.6540231872755264 and parameters: {'n_estimators': 118, 'learning_rate': 0.06828029874547037, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 231}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:31,827] Trial 284 finished with value: 0.6662438442660259 and parameters: {'n_estimators': 466, 'learning_rate': 0.08397042462298762, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 264}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:33,404] Trial 285 finished with value: 0.6655532134434262 and parameters: {'n_estimators': 478, 'learning_rate': 0.07479748807764301, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 268}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:34,985] Trial 286 finished with value: 0.6648900019690622 and parameters: {'n_estimators': 489, 'learning_rate': 0.08120072012375794, 'max_depth': 12, 'max_bin': 296, 'num_leaves': 233}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:36,544] Trial 287 finished with value: 0.6586578327347422 and parameters: {'n_estimators': 298, 'learning_rate': 0.08852723433297413, 'max_depth': 12, 'max_bin': 187, 'num_leaves': 450}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:37,928] Trial 288 finished with value: 0.6712757046879161 and parameters: {'n_estimators': 504, 'learning_rate': 0.12911688582793213, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 542}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:39,512] Trial 289 finished with value: 0.664781854729506 and parameters: {'n_estimators': 396, 'learning_rate': 0.12445910799564756, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 244}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:40,893] Trial 290 finished with value: 0.6644196643318973 and parameters: {'n_estimators': 518, 'learning_rate': 0.165575763451618, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 394}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:42,453] Trial 291 finished with value: 0.6692017483965562 and parameters: {'n_estimators': 488, 'learning_rate': 0.11122191673195433, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 358}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:44,332] Trial 292 finished with value: 0.6668601520046484 and parameters: {'n_estimators': 637, 'learning_rate': 0.08523158282383686, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 409}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:46,084] Trial 293 finished with value: 0.6688841533346273 and parameters: {'n_estimators': 669, 'learning_rate': 0.07720928532085478, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 254}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:47,919] Trial 294 finished with value: 0.6639056443038304 and parameters: {'n_estimators': 508, 'learning_rate': 0.09270041686729992, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 672}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:49,335] Trial 295 finished with value: 0.6652003395283571 and parameters: {'n_estimators': 532, 'learning_rate': 0.12058489868699766, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 376}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:50,635] Trial 296 finished with value: 0.6582365167998171 and parameters: {'n_estimators': 463, 'learning_rate': 0.14047183855657888, 'max_depth': 12, 'max_bin': 297, 'num_leaves': 277}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:52,133] Trial 297 finished with value: 0.6653633184363317 and parameters: {'n_estimators': 546, 'learning_rate': 0.11462257020607781, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 423}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:53,683] Trial 298 finished with value: 0.6588947940000629 and parameters: {'n_estimators': 476, 'learning_rate': 0.09729834973305014, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 225}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:55,321] Trial 299 finished with value: 0.6666217176794607 and parameters: {'n_estimators': 586, 'learning_rate': 0.08912420542888402, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 261}. Best is trial 92 with value: 0.6809963460320942.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6810\n",
      "\tBest params:\n",
      "\t\tn_estimators: 695\n",
      "\t\tlearning_rate: 0.10244023588381922\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 255\n",
      "\t\tnum_leaves: 382\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.688832    0.664005    0.671613    0.713928   \n",
      "1                    TP   31.000000   42.000000   36.000000   34.000000   \n",
      "2                    TN  198.000000  198.000000  197.000000  199.000000   \n",
      "3                    FP    2.000000    4.000000    3.000000    2.000000   \n",
      "4                    FN   37.000000   24.000000   32.000000   33.000000   \n",
      "5              Accuracy    0.854478    0.895522    0.869403    0.869403   \n",
      "6             Precision    0.939394    0.913043    0.923077    0.944444   \n",
      "7           Sensitivity    0.455882    0.636364    0.529412    0.507463   \n",
      "8           Specificity    0.990000    0.980200    0.985000    0.990000   \n",
      "9              F1 score    0.613861    0.750000    0.672897    0.660194   \n",
      "10  F1 score (weighted)    0.835118    0.888658    0.856119    0.854425   \n",
      "11     F1 score (macro)    0.762103    0.841981    0.795656    0.789681   \n",
      "12    Balanced Accuracy    0.722941    0.808281    0.757206    0.748756   \n",
      "13                  MCC    0.590471    0.704480    0.634790    0.631748   \n",
      "14                  NPV    0.842600    0.891900    0.860300    0.857800   \n",
      "15              ROC_AUC    0.722941    0.808281    0.757206    0.748756   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.633686    0.682160  \n",
      "1    38.000000   41.000000  \n",
      "2   199.000000  198.000000  \n",
      "3     2.000000    2.000000  \n",
      "4    29.000000   27.000000  \n",
      "5     0.884328    0.891791  \n",
      "6     0.950000    0.953488  \n",
      "7     0.567164    0.602941  \n",
      "8     0.990000    0.990000  \n",
      "9     0.710280    0.738739  \n",
      "10    0.873374    0.882788  \n",
      "11    0.819010    0.835252  \n",
      "12    0.778607    0.796471  \n",
      "13    0.677111    0.703000  \n",
      "14    0.872800    0.880000  \n",
      "15    0.778607    0.796471  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_lgbm_5)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_5_cat = np.where(((y_pred_lgbm_5 >= 2) | (y_pred_lgbm_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:45:57,155] Trial 300 finished with value: 0.6598956733753731 and parameters: {'n_estimators': 439, 'learning_rate': 0.10308784672464241, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 291}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:45:58,513] Trial 301 finished with value: 0.6595003177631744 and parameters: {'n_estimators': 515, 'learning_rate': 0.10869800211962588, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 345}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:00,092] Trial 302 finished with value: 0.6609154581267243 and parameters: {'n_estimators': 528, 'learning_rate': 0.08308738187226035, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 397}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:02,242] Trial 303 finished with value: 0.6607784217381133 and parameters: {'n_estimators': 417, 'learning_rate': 0.07201404773205589, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 241}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:03,826] Trial 304 finished with value: 0.6647280405359428 and parameters: {'n_estimators': 501, 'learning_rate': 0.09480521531144899, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 485}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:05,981] Trial 305 finished with value: 0.6591746240266207 and parameters: {'n_estimators': 555, 'learning_rate': 0.07981555485093353, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 700}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:07,786] Trial 306 finished with value: 0.6678501117286161 and parameters: {'n_estimators': 570, 'learning_rate': 0.08634903544880686, 'max_depth': 12, 'max_bin': 170, 'num_leaves': 324}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:09,525] Trial 307 finished with value: 0.6692736723202065 and parameters: {'n_estimators': 539, 'learning_rate': 0.09088517678814363, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 301}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:12,235] Trial 308 finished with value: 0.6685415131435775 and parameters: {'n_estimators': 491, 'learning_rate': 0.038831030662569536, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 646}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:13,609] Trial 309 finished with value: 0.6621210950422267 and parameters: {'n_estimators': 229, 'learning_rate': 0.10577114191227376, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 218}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:15,275] Trial 310 finished with value: 0.6623352316720145 and parameters: {'n_estimators': 784, 'learning_rate': 0.1005717452458423, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 384}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:16,815] Trial 311 finished with value: 0.6618601736722323 and parameters: {'n_estimators': 523, 'learning_rate': 0.11762615574942992, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 275}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:18,165] Trial 312 finished with value: 0.6601518667586181 and parameters: {'n_estimators': 693, 'learning_rate': 0.11355202186179036, 'max_depth': 11, 'max_bin': 156, 'num_leaves': 371}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:19,711] Trial 313 finished with value: 0.6627815119800902 and parameters: {'n_estimators': 618, 'learning_rate': 0.09628482308186294, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 417}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:21,424] Trial 314 finished with value: 0.65878296683256 and parameters: {'n_estimators': 457, 'learning_rate': 0.09015683865818287, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 248}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:23,090] Trial 315 finished with value: 0.665074597035158 and parameters: {'n_estimators': 595, 'learning_rate': 0.10999261429765358, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 358}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:24,598] Trial 316 finished with value: 0.6561180247630485 and parameters: {'n_estimators': 507, 'learning_rate': 0.12492647543585149, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 265}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:26,088] Trial 317 finished with value: 0.6632049292095646 and parameters: {'n_estimators': 577, 'learning_rate': 0.08341882168045385, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 317}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:27,625] Trial 318 finished with value: 0.6566079405733105 and parameters: {'n_estimators': 480, 'learning_rate': 0.10394867221221105, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 285}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:29,512] Trial 319 finished with value: 0.6640192849658308 and parameters: {'n_estimators': 559, 'learning_rate': 0.07841882802452087, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 398}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:30,799] Trial 320 finished with value: 0.6618343839119046 and parameters: {'n_estimators': 537, 'learning_rate': 0.09384547924952759, 'max_depth': 9, 'max_bin': 250, 'num_leaves': 300}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:33,934] Trial 321 finished with value: 0.6627014185765374 and parameters: {'n_estimators': 751, 'learning_rate': 0.02390064818860868, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 431}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:35,124] Trial 322 finished with value: 0.6626768582881374 and parameters: {'n_estimators': 513, 'learning_rate': 0.10052673837218473, 'max_depth': 11, 'max_bin': 241, 'num_leaves': 332}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:36,957] Trial 323 finished with value: 0.6668142187960242 and parameters: {'n_estimators': 492, 'learning_rate': 0.08653297482487907, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 247}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:38,513] Trial 324 finished with value: 0.6572347443319722 and parameters: {'n_estimators': 525, 'learning_rate': 0.11194696966472921, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 368}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:40,463] Trial 325 finished with value: 0.664810210841661 and parameters: {'n_estimators': 841, 'learning_rate': 0.06826358976196499, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 234}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:42,034] Trial 326 finished with value: 0.6568111243377699 and parameters: {'n_estimators': 377, 'learning_rate': 0.12067882041769709, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 274}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:42,953] Trial 327 finished with value: 0.6494802693581458 and parameters: {'n_estimators': 545, 'learning_rate': 0.10690066892742864, 'max_depth': 5, 'max_bin': 261, 'num_leaves': 408}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:44,827] Trial 328 finished with value: 0.6681546015899535 and parameters: {'n_estimators': 444, 'learning_rate': 0.09726978105291821, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 216}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:46,974] Trial 329 finished with value: 0.6596911527765085 and parameters: {'n_estimators': 473, 'learning_rate': 0.07388102873341262, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 348}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:48,376] Trial 330 finished with value: 0.6635333683263456 and parameters: {'n_estimators': 890, 'learning_rate': 0.08780837672806327, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 384}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:49,799] Trial 331 finished with value: 0.6563126268941222 and parameters: {'n_estimators': 506, 'learning_rate': 0.09162242632081723, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 507}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:51,477] Trial 332 finished with value: 0.6672375721843192 and parameters: {'n_estimators': 559, 'learning_rate': 0.11616666665876496, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 313}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:53,373] Trial 333 finished with value: 0.6604613086545399 and parameters: {'n_estimators': 526, 'learning_rate': 0.08155067593027582, 'max_depth': 12, 'max_bin': 183, 'num_leaves': 297}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:54,707] Trial 334 finished with value: 0.6542613453189304 and parameters: {'n_estimators': 597, 'learning_rate': 0.12978882747187095, 'max_depth': 12, 'max_bin': 192, 'num_leaves': 748}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:55,875] Trial 335 finished with value: 0.6498112654129731 and parameters: {'n_estimators': 573, 'learning_rate': 0.10192378449209417, 'max_depth': 6, 'max_bin': 263, 'num_leaves': 262}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:46:57,713] Trial 336 finished with value: 0.6643365855933295 and parameters: {'n_estimators': 546, 'learning_rate': 0.09456349318582216, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 469}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:03,811] Trial 337 finished with value: 0.6238099211532975 and parameters: {'n_estimators': 711, 'learning_rate': 0.004270136156388549, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 440}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:05,784] Trial 338 finished with value: 0.6602968423485102 and parameters: {'n_estimators': 494, 'learning_rate': 0.05979592041118257, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 281}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:07,642] Trial 339 finished with value: 0.669615963098636 and parameters: {'n_estimators': 520, 'learning_rate': 0.08484682717488211, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 235}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:09,277] Trial 340 finished with value: 0.6608719849918139 and parameters: {'n_estimators': 650, 'learning_rate': 0.10887775871560806, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 392}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:11,263] Trial 341 finished with value: 0.663257491660043 and parameters: {'n_estimators': 462, 'learning_rate': 0.09034539009281095, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 601}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:12,868] Trial 342 finished with value: 0.6611377417146188 and parameters: {'n_estimators': 532, 'learning_rate': 0.09949298143717432, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 201}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:14,330] Trial 343 finished with value: 0.6537064772844479 and parameters: {'n_estimators': 820, 'learning_rate': 0.11368536278848437, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 336}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:16,047] Trial 344 finished with value: 0.6577565257593087 and parameters: {'n_estimators': 627, 'learning_rate': 0.10548731005144604, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 250}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:17,531] Trial 345 finished with value: 0.6566118486661477 and parameters: {'n_estimators': 481, 'learning_rate': 0.13331805185035317, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 374}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:19,398] Trial 346 finished with value: 0.6689494675512776 and parameters: {'n_estimators': 504, 'learning_rate': 0.07626889184087574, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 304}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:20,702] Trial 347 finished with value: 0.6554218114817157 and parameters: {'n_estimators': 580, 'learning_rate': 0.1194371914558587, 'max_depth': 11, 'max_bin': 291, 'num_leaves': 288}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:22,126] Trial 348 finished with value: 0.6617050380321812 and parameters: {'n_estimators': 561, 'learning_rate': 0.0973957403347011, 'max_depth': 8, 'max_bin': 257, 'num_leaves': 419}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:24,061] Trial 349 finished with value: 0.6605166416733471 and parameters: {'n_estimators': 546, 'learning_rate': 0.08181223803244939, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 361}. Best is trial 92 with value: 0.6809963460320942.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.680996\n",
      "\tBest params:\n",
      "\t\tn_estimators: 695\n",
      "\t\tlearning_rate: 0.10244023588381922\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 255\n",
      "\t\tnum_leaves: 382\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.688832    0.664005    0.671613    0.713928   \n",
      "1                    TP   31.000000   42.000000   36.000000   34.000000   \n",
      "2                    TN  198.000000  198.000000  197.000000  199.000000   \n",
      "3                    FP    2.000000    4.000000    3.000000    2.000000   \n",
      "4                    FN   37.000000   24.000000   32.000000   33.000000   \n",
      "5              Accuracy    0.854478    0.895522    0.869403    0.869403   \n",
      "6             Precision    0.939394    0.913043    0.923077    0.944444   \n",
      "7           Sensitivity    0.455882    0.636364    0.529412    0.507463   \n",
      "8           Specificity    0.990000    0.980200    0.985000    0.990000   \n",
      "9              F1 score    0.613861    0.750000    0.672897    0.660194   \n",
      "10  F1 score (weighted)    0.835118    0.888658    0.856119    0.854425   \n",
      "11     F1 score (macro)    0.762103    0.841981    0.795656    0.789681   \n",
      "12    Balanced Accuracy    0.722941    0.808281    0.757206    0.748756   \n",
      "13                  MCC    0.590471    0.704480    0.634790    0.631748   \n",
      "14                  NPV    0.842600    0.891900    0.860300    0.857800   \n",
      "15              ROC_AUC    0.722941    0.808281    0.757206    0.748756   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.633686    0.682160    0.706744  \n",
      "1    38.000000   41.000000   43.000000  \n",
      "2   199.000000  198.000000  198.000000  \n",
      "3     2.000000    2.000000    3.000000  \n",
      "4    29.000000   27.000000   24.000000  \n",
      "5     0.884328    0.891791    0.899254  \n",
      "6     0.950000    0.953488    0.934783  \n",
      "7     0.567164    0.602941    0.641791  \n",
      "8     0.990000    0.990000    0.985100  \n",
      "9     0.710280    0.738739    0.761062  \n",
      "10    0.873374    0.882788    0.892393  \n",
      "11    0.819010    0.835252    0.848616  \n",
      "12    0.778607    0.796471    0.813433  \n",
      "13    0.677111    0.703000    0.719871  \n",
      "14    0.872800    0.880000    0.891900  \n",
      "15    0.778607    0.796471    0.813433  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_lgbm_6)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_6_cat = np.where(((y_pred_lgbm_6 >= 2) | (y_pred_lgbm_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:47:25,737] Trial 350 finished with value: 0.6538274011792573 and parameters: {'n_estimators': 420, 'learning_rate': 0.08762215683344832, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 324}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:26,990] Trial 351 finished with value: 0.6477036722933328 and parameters: {'n_estimators': 608, 'learning_rate': 0.12524881190167342, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 224}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:28,727] Trial 352 finished with value: 0.6533410350172882 and parameters: {'n_estimators': 524, 'learning_rate': 0.09360668619244307, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 260}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:30,218] Trial 353 finished with value: 0.6566988856956321 and parameters: {'n_estimators': 502, 'learning_rate': 0.10273655876649443, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 398}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:31,687] Trial 354 finished with value: 0.6532620482059546 and parameters: {'n_estimators': 446, 'learning_rate': 0.07933796849556138, 'max_depth': 12, 'max_bin': 175, 'num_leaves': 309}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:33,160] Trial 355 finished with value: 0.6550614418175685 and parameters: {'n_estimators': 482, 'learning_rate': 0.11091318250511378, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 454}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:34,712] Trial 356 finished with value: 0.6530744319034618 and parameters: {'n_estimators': 548, 'learning_rate': 0.08516876543920907, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 270}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:36,269] Trial 357 finished with value: 0.660077208028812 and parameters: {'n_estimators': 514, 'learning_rate': 0.07124225151250302, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 410}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:37,745] Trial 358 finished with value: 0.6556255751308974 and parameters: {'n_estimators': 581, 'learning_rate': 0.09149541749424357, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 244}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:39,286] Trial 359 finished with value: 0.6564757028118648 and parameters: {'n_estimators': 537, 'learning_rate': 0.09684743583050252, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 292}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:40,536] Trial 360 finished with value: 0.659573852559848 and parameters: {'n_estimators': 496, 'learning_rate': 0.11694502433836404, 'max_depth': 12, 'max_bin': 164, 'num_leaves': 340}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:41,560] Trial 361 finished with value: 0.6522851222582126 and parameters: {'n_estimators': 563, 'learning_rate': 0.19928606260817933, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 383}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:42,697] Trial 362 finished with value: 0.6490539821704207 and parameters: {'n_estimators': 472, 'learning_rate': 0.13891756027538646, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 357}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:44,208] Trial 363 finished with value: 0.6576541003754232 and parameters: {'n_estimators': 523, 'learning_rate': 0.10656652962762896, 'max_depth': 12, 'max_bin': 200, 'num_leaves': 257}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:46,174] Trial 364 finished with value: 0.6534262026774194 and parameters: {'n_estimators': 513, 'learning_rate': 0.05213189113304552, 'max_depth': 12, 'max_bin': 296, 'num_leaves': 573}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:47,408] Trial 365 finished with value: 0.6566046565267445 and parameters: {'n_estimators': 590, 'learning_rate': 0.18268546597906804, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 215}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:48,932] Trial 366 finished with value: 0.6593410675131843 and parameters: {'n_estimators': 535, 'learning_rate': 0.08833151097572721, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 276}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:50,494] Trial 367 finished with value: 0.6523440435548713 and parameters: {'n_estimators': 733, 'learning_rate': 0.10046705464476535, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 327}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:52,014] Trial 368 finished with value: 0.6579358926893234 and parameters: {'n_estimators': 490, 'learning_rate': 0.09426027412975253, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 429}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:53,583] Trial 369 finished with value: 0.6561619744412963 and parameters: {'n_estimators': 560, 'learning_rate': 0.12207008155860508, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 378}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:54,942] Trial 370 finished with value: 0.6566973880903341 and parameters: {'n_estimators': 453, 'learning_rate': 0.1140340094767979, 'max_depth': 12, 'max_bin': 153, 'num_leaves': 533}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:56,513] Trial 371 finished with value: 0.6522130879617465 and parameters: {'n_estimators': 547, 'learning_rate': 0.08314229012804102, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 234}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:57,930] Trial 372 finished with value: 0.6511167230604408 and parameters: {'n_estimators': 609, 'learning_rate': 0.10328096616934357, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 312}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:47:58,654] Trial 373 finished with value: 0.6100076316612572 and parameters: {'n_estimators': 673, 'learning_rate': 0.10818858060368822, 'max_depth': 3, 'max_bin': 264, 'num_leaves': 395}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:00,056] Trial 374 finished with value: 0.6549643202967633 and parameters: {'n_estimators': 509, 'learning_rate': 0.089407671846174, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 285}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:01,486] Trial 375 finished with value: 0.6497956499714304 and parameters: {'n_estimators': 432, 'learning_rate': 0.09861207044496391, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 247}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:02,951] Trial 376 finished with value: 0.6557715581429161 and parameters: {'n_estimators': 531, 'learning_rate': 0.07739122300045591, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 364}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:04,071] Trial 377 finished with value: 0.6474573639703136 and parameters: {'n_estimators': 476, 'learning_rate': 0.1273404931752561, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 227}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:05,695] Trial 378 finished with value: 0.6601763318019174 and parameters: {'n_estimators': 869, 'learning_rate': 0.0929588495120016, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 347}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:07,000] Trial 379 finished with value: 0.6535684444229495 and parameters: {'n_estimators': 571, 'learning_rate': 0.1186760078897145, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 410}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:07,990] Trial 380 finished with value: 0.6399891468334442 and parameters: {'n_estimators': 509, 'learning_rate': 0.11146620740866406, 'max_depth': 7, 'max_bin': 251, 'num_leaves': 552}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:09,478] Trial 381 finished with value: 0.6571953015004531 and parameters: {'n_estimators': 547, 'learning_rate': 0.08534848356229242, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 268}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:10,825] Trial 382 finished with value: 0.6556857141000396 and parameters: {'n_estimators': 402, 'learning_rate': 0.10473939732586515, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 299}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:12,278] Trial 383 finished with value: 0.6534512172111846 and parameters: {'n_estimators': 496, 'learning_rate': 0.07906718036537996, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 257}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:13,511] Trial 384 finished with value: 0.6508640634709162 and parameters: {'n_estimators': 524, 'learning_rate': 0.15149353810280017, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 386}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:14,888] Trial 385 finished with value: 0.6526030989906387 and parameters: {'n_estimators': 462, 'learning_rate': 0.09757057021391662, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 316}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:20,953] Trial 386 finished with value: 0.5907858052910337 and parameters: {'n_estimators': 755, 'learning_rate': 0.002780580170492916, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 196}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:22,691] Trial 387 finished with value: 0.6567594432008194 and parameters: {'n_estimators': 582, 'learning_rate': 0.09132031660157285, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 284}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:24,392] Trial 388 finished with value: 0.6522841556232051 and parameters: {'n_estimators': 538, 'learning_rate': 0.08673327768393342, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 329}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:25,809] Trial 389 finished with value: 0.6519749221059876 and parameters: {'n_estimators': 485, 'learning_rate': 0.07418672426827932, 'max_depth': 10, 'max_bin': 260, 'num_leaves': 443}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:26,584] Trial 390 finished with value: 0.6213306114328998 and parameters: {'n_estimators': 630, 'learning_rate': 0.1144260605169326, 'max_depth': 4, 'max_bin': 240, 'num_leaves': 213}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:27,973] Trial 391 finished with value: 0.6559694166245595 and parameters: {'n_estimators': 560, 'learning_rate': 0.15613479513593434, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 494}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:29,454] Trial 392 finished with value: 0.6546316807111239 and parameters: {'n_estimators': 172, 'learning_rate': 0.06576222623860035, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 240}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:31,133] Trial 393 finished with value: 0.6550473352566286 and parameters: {'n_estimators': 516, 'learning_rate': 0.08157354806799601, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 372}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:32,610] Trial 394 finished with value: 0.6523734613995386 and parameters: {'n_estimators': 498, 'learning_rate': 0.10166777209325965, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 274}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:33,950] Trial 395 finished with value: 0.6563421155679345 and parameters: {'n_estimators': 530, 'learning_rate': 0.12245685657752192, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 401}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:35,440] Trial 396 finished with value: 0.6572342389194152 and parameters: {'n_estimators': 552, 'learning_rate': 0.0945645881473745, 'max_depth': 11, 'max_bin': 288, 'num_leaves': 347}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:36,952] Trial 397 finished with value: 0.6554485321042118 and parameters: {'n_estimators': 604, 'learning_rate': 0.10632170950136895, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 297}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:39,711] Trial 398 finished with value: 0.6514323787184975 and parameters: {'n_estimators': 652, 'learning_rate': 0.027231849114281587, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 266}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:41,285] Trial 399 finished with value: 0.6603130352181991 and parameters: {'n_estimators': 575, 'learning_rate': 0.08961405400183872, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 419}. Best is trial 92 with value: 0.6809963460320942.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6809963\n",
      "\tBest params:\n",
      "\t\tn_estimators: 695\n",
      "\t\tlearning_rate: 0.10244023588381922\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 255\n",
      "\t\tnum_leaves: 382\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.688832    0.664005    0.671613    0.713928   \n",
      "1                    TP   31.000000   42.000000   36.000000   34.000000   \n",
      "2                    TN  198.000000  198.000000  197.000000  199.000000   \n",
      "3                    FP    2.000000    4.000000    3.000000    2.000000   \n",
      "4                    FN   37.000000   24.000000   32.000000   33.000000   \n",
      "5              Accuracy    0.854478    0.895522    0.869403    0.869403   \n",
      "6             Precision    0.939394    0.913043    0.923077    0.944444   \n",
      "7           Sensitivity    0.455882    0.636364    0.529412    0.507463   \n",
      "8           Specificity    0.990000    0.980200    0.985000    0.990000   \n",
      "9              F1 score    0.613861    0.750000    0.672897    0.660194   \n",
      "10  F1 score (weighted)    0.835118    0.888658    0.856119    0.854425   \n",
      "11     F1 score (macro)    0.762103    0.841981    0.795656    0.789681   \n",
      "12    Balanced Accuracy    0.722941    0.808281    0.757206    0.748756   \n",
      "13                  MCC    0.590471    0.704480    0.634790    0.631748   \n",
      "14                  NPV    0.842600    0.891900    0.860300    0.857800   \n",
      "15              ROC_AUC    0.722941    0.808281    0.757206    0.748756   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.633686    0.682160    0.706744    0.686708  \n",
      "1    38.000000   41.000000   43.000000   42.000000  \n",
      "2   199.000000  198.000000  198.000000  195.000000  \n",
      "3     2.000000    2.000000    3.000000    6.000000  \n",
      "4    29.000000   27.000000   24.000000   25.000000  \n",
      "5     0.884328    0.891791    0.899254    0.884328  \n",
      "6     0.950000    0.953488    0.934783    0.875000  \n",
      "7     0.567164    0.602941    0.641791    0.626866  \n",
      "8     0.990000    0.990000    0.985100    0.970100  \n",
      "9     0.710280    0.738739    0.761062    0.730435  \n",
      "10    0.873374    0.882788    0.892393    0.877383  \n",
      "11    0.819010    0.835252    0.848616    0.828400  \n",
      "12    0.778607    0.796471    0.813433    0.798507  \n",
      "13    0.677111    0.703000    0.719871    0.674200  \n",
      "14    0.872800    0.880000    0.891900    0.886400  \n",
      "15    0.778607    0.796471    0.813433    0.798507  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_lgbm_7)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_7_cat = np.where(((y_pred_lgbm_7 >= 2) | (y_pred_lgbm_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:48:43,291] Trial 400 finished with value: 0.6742757939211564 and parameters: {'n_estimators': 472, 'learning_rate': 0.11135739482950632, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 306}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:44,883] Trial 401 finished with value: 0.6749499227084392 and parameters: {'n_estimators': 518, 'learning_rate': 0.09784713265516093, 'max_depth': 12, 'max_bin': 168, 'num_leaves': 230}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:46,608] Trial 402 finished with value: 0.6717739697273697 and parameters: {'n_estimators': 501, 'learning_rate': 0.09597799130088737, 'max_depth': 11, 'max_bin': 155, 'num_leaves': 224}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:48,196] Trial 403 finished with value: 0.6751925217031703 and parameters: {'n_estimators': 432, 'learning_rate': 0.10377820261107756, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 243}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:49,936] Trial 404 finished with value: 0.6755367720295673 and parameters: {'n_estimators': 429, 'learning_rate': 0.10098168081454822, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 206}. Best is trial 92 with value: 0.6809963460320942.\n",
      "[I 2023-12-11 23:48:51,452] Trial 405 finished with value: 0.6810369815149631 and parameters: {'n_estimators': 405, 'learning_rate': 0.10222037123920603, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 217}. Best is trial 405 with value: 0.6810369815149631.\n",
      "[I 2023-12-11 23:48:53,248] Trial 406 finished with value: 0.6804794205222132 and parameters: {'n_estimators': 383, 'learning_rate': 0.10404267614178492, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 200}. Best is trial 405 with value: 0.6810369815149631.\n",
      "[I 2023-12-11 23:48:54,461] Trial 407 finished with value: 0.6626433348076084 and parameters: {'n_estimators': 374, 'learning_rate': 0.10414642299529046, 'max_depth': 6, 'max_bin': 222, 'num_leaves': 187}. Best is trial 405 with value: 0.6810369815149631.\n",
      "[I 2023-12-11 23:48:56,067] Trial 408 finished with value: 0.679521259661308 and parameters: {'n_estimators': 409, 'learning_rate': 0.10769848311684448, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 167}. Best is trial 405 with value: 0.6810369815149631.\n",
      "[I 2023-12-11 23:48:57,748] Trial 409 finished with value: 0.6769398764151158 and parameters: {'n_estimators': 399, 'learning_rate': 0.1084461086183966, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 158}. Best is trial 405 with value: 0.6810369815149631.\n",
      "[I 2023-12-11 23:48:59,695] Trial 410 finished with value: 0.6808668621561054 and parameters: {'n_estimators': 336, 'learning_rate': 0.10816456787019582, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 157}. Best is trial 405 with value: 0.6810369815149631.\n",
      "[I 2023-12-11 23:49:01,640] Trial 411 finished with value: 0.6799657081468622 and parameters: {'n_estimators': 359, 'learning_rate': 0.10811941817775782, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 142}. Best is trial 405 with value: 0.6810369815149631.\n",
      "[I 2023-12-11 23:49:03,108] Trial 412 finished with value: 0.6751817242583049 and parameters: {'n_estimators': 348, 'learning_rate': 0.10848753767284425, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 147}. Best is trial 405 with value: 0.6810369815149631.\n",
      "[I 2023-12-11 23:49:04,878] Trial 413 finished with value: 0.680874999919266 and parameters: {'n_estimators': 381, 'learning_rate': 0.10838113040377371, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 174}. Best is trial 405 with value: 0.6810369815149631.\n",
      "[I 2023-12-11 23:49:06,254] Trial 414 finished with value: 0.6715218415019629 and parameters: {'n_estimators': 331, 'learning_rate': 0.10882649538721301, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 127}. Best is trial 405 with value: 0.6810369815149631.\n",
      "[I 2023-12-11 23:49:07,705] Trial 415 finished with value: 0.6782126616323356 and parameters: {'n_estimators': 371, 'learning_rate': 0.10774880377537138, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 147}. Best is trial 405 with value: 0.6810369815149631.\n",
      "[I 2023-12-11 23:49:09,348] Trial 416 finished with value: 0.6715119919927419 and parameters: {'n_estimators': 381, 'learning_rate': 0.10667347110261934, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 159}. Best is trial 405 with value: 0.6810369815149631.\n",
      "[I 2023-12-11 23:49:11,037] Trial 417 finished with value: 0.6817600514650656 and parameters: {'n_estimators': 363, 'learning_rate': 0.11181105382293557, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 164}. Best is trial 417 with value: 0.6817600514650656.\n",
      "[I 2023-12-11 23:49:12,656] Trial 418 finished with value: 0.6818755737924482 and parameters: {'n_estimators': 341, 'learning_rate': 0.11063968222612378, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 138}. Best is trial 418 with value: 0.6818755737924482.\n",
      "[I 2023-12-11 23:49:14,170] Trial 419 finished with value: 0.6753632795328237 and parameters: {'n_estimators': 349, 'learning_rate': 0.11310417900326898, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 106}. Best is trial 418 with value: 0.6818755737924482.\n",
      "[I 2023-12-11 23:49:15,908] Trial 420 finished with value: 0.6794559120354559 and parameters: {'n_estimators': 369, 'learning_rate': 0.11067222362322489, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 165}. Best is trial 418 with value: 0.6818755737924482.\n",
      "[I 2023-12-11 23:49:17,500] Trial 421 finished with value: 0.6834939477325415 and parameters: {'n_estimators': 311, 'learning_rate': 0.11051475460927779, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 171}. Best is trial 421 with value: 0.6834939477325415.\n",
      "[I 2023-12-11 23:49:19,105] Trial 422 finished with value: 0.680253935137175 and parameters: {'n_estimators': 305, 'learning_rate': 0.1106799336627149, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 167}. Best is trial 421 with value: 0.6834939477325415.\n",
      "[I 2023-12-11 23:49:20,681] Trial 423 finished with value: 0.6761617463583471 and parameters: {'n_estimators': 296, 'learning_rate': 0.11162016104751196, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 175}. Best is trial 421 with value: 0.6834939477325415.\n",
      "[I 2023-12-11 23:49:22,386] Trial 424 finished with value: 0.684733949809419 and parameters: {'n_estimators': 320, 'learning_rate': 0.10959816383100612, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 157}. Best is trial 424 with value: 0.684733949809419.\n",
      "[I 2023-12-11 23:49:23,899] Trial 425 finished with value: 0.6743255087979917 and parameters: {'n_estimators': 322, 'learning_rate': 0.10965191156922381, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 139}. Best is trial 424 with value: 0.684733949809419.\n",
      "[I 2023-12-11 23:49:25,358] Trial 426 finished with value: 0.6732439392121756 and parameters: {'n_estimators': 361, 'learning_rate': 0.11537595518883534, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 165}. Best is trial 424 with value: 0.684733949809419.\n",
      "[I 2023-12-11 23:49:26,809] Trial 427 finished with value: 0.6778990991806724 and parameters: {'n_estimators': 295, 'learning_rate': 0.11086190930176672, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 145}. Best is trial 424 with value: 0.684733949809419.\n",
      "[I 2023-12-11 23:49:28,388] Trial 428 finished with value: 0.6790590870064619 and parameters: {'n_estimators': 272, 'learning_rate': 0.11035114968018515, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 154}. Best is trial 424 with value: 0.684733949809419.\n",
      "[I 2023-12-11 23:49:29,590] Trial 429 finished with value: 0.6702030991929404 and parameters: {'n_estimators': 279, 'learning_rate': 0.11336699435587409, 'max_depth': 9, 'max_bin': 209, 'num_leaves': 141}. Best is trial 424 with value: 0.684733949809419.\n",
      "[I 2023-12-11 23:49:31,017] Trial 430 finished with value: 0.675089680478599 and parameters: {'n_estimators': 330, 'learning_rate': 0.11682189749640749, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 120}. Best is trial 424 with value: 0.684733949809419.\n",
      "[I 2023-12-11 23:49:32,353] Trial 431 finished with value: 0.6783191052387054 and parameters: {'n_estimators': 306, 'learning_rate': 0.1103093334661132, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 163}. Best is trial 424 with value: 0.684733949809419.\n",
      "[I 2023-12-11 23:49:33,907] Trial 432 finished with value: 0.6729128062548445 and parameters: {'n_estimators': 306, 'learning_rate': 0.11146451814646316, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 171}. Best is trial 424 with value: 0.684733949809419.\n",
      "[I 2023-12-11 23:49:35,385] Trial 433 finished with value: 0.6790394461677436 and parameters: {'n_estimators': 269, 'learning_rate': 0.110468292092219, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 147}. Best is trial 424 with value: 0.684733949809419.\n",
      "[I 2023-12-11 23:49:36,837] Trial 434 finished with value: 0.6732106097867357 and parameters: {'n_estimators': 283, 'learning_rate': 0.11019519162398769, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 148}. Best is trial 424 with value: 0.684733949809419.\n",
      "[I 2023-12-11 23:49:38,187] Trial 435 finished with value: 0.6725009409837515 and parameters: {'n_estimators': 245, 'learning_rate': 0.11733262514576316, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 138}. Best is trial 424 with value: 0.684733949809419.\n",
      "[I 2023-12-11 23:49:39,676] Trial 436 finished with value: 0.6754872236757563 and parameters: {'n_estimators': 364, 'learning_rate': 0.11266674215029207, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 158}. Best is trial 424 with value: 0.684733949809419.\n",
      "[I 2023-12-11 23:49:41,201] Trial 437 finished with value: 0.6763655170705702 and parameters: {'n_estimators': 338, 'learning_rate': 0.10685070003784079, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 175}. Best is trial 424 with value: 0.684733949809419.\n",
      "[I 2023-12-11 23:49:42,758] Trial 438 finished with value: 0.6713702693952289 and parameters: {'n_estimators': 314, 'learning_rate': 0.11425295495435164, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 115}. Best is trial 424 with value: 0.684733949809419.\n",
      "[I 2023-12-11 23:49:44,407] Trial 439 finished with value: 0.6853683617399914 and parameters: {'n_estimators': 271, 'learning_rate': 0.10911691677480986, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 135}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:49:46,160] Trial 440 finished with value: 0.6809326869566512 and parameters: {'n_estimators': 273, 'learning_rate': 0.1089610937481909, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 133}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:49:47,846] Trial 441 finished with value: 0.6815609310473675 and parameters: {'n_estimators': 283, 'learning_rate': 0.10716875360863187, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 134}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:49:49,649] Trial 442 finished with value: 0.681723775578638 and parameters: {'n_estimators': 269, 'learning_rate': 0.10815300038695101, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 92}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:49:50,953] Trial 443 finished with value: 0.6773963984563578 and parameters: {'n_estimators': 260, 'learning_rate': 0.10773840790504942, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 60}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:49:52,574] Trial 444 finished with value: 0.6797698340642709 and parameters: {'n_estimators': 272, 'learning_rate': 0.10777636859968569, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 84}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:49:54,012] Trial 445 finished with value: 0.6778869267639676 and parameters: {'n_estimators': 275, 'learning_rate': 0.10591159383707949, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 94}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:49:55,544] Trial 446 finished with value: 0.6818043684464608 and parameters: {'n_estimators': 242, 'learning_rate': 0.11060308218399367, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 94}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:49:56,745] Trial 447 finished with value: 0.6677914857812496 and parameters: {'n_estimators': 261, 'learning_rate': 0.11408955767045614, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 108}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:49:57,982] Trial 448 finished with value: 0.67402796321881 and parameters: {'n_estimators': 233, 'learning_rate': 0.10546557976397151, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 78}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:49:59,354] Trial 449 finished with value: 0.6693227733296221 and parameters: {'n_estimators': 207, 'learning_rate': 0.11674850734742487, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 127}. Best is trial 439 with value: 0.6853683617399914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.68536836\n",
      "\tBest params:\n",
      "\t\tn_estimators: 271\n",
      "\t\tlearning_rate: 0.10911691677480986\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 225\n",
      "\t\tnum_leaves: 135\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.688832    0.664005    0.671613    0.713928   \n",
      "1                    TP   31.000000   42.000000   36.000000   34.000000   \n",
      "2                    TN  198.000000  198.000000  197.000000  199.000000   \n",
      "3                    FP    2.000000    4.000000    3.000000    2.000000   \n",
      "4                    FN   37.000000   24.000000   32.000000   33.000000   \n",
      "5              Accuracy    0.854478    0.895522    0.869403    0.869403   \n",
      "6             Precision    0.939394    0.913043    0.923077    0.944444   \n",
      "7           Sensitivity    0.455882    0.636364    0.529412    0.507463   \n",
      "8           Specificity    0.990000    0.980200    0.985000    0.990000   \n",
      "9              F1 score    0.613861    0.750000    0.672897    0.660194   \n",
      "10  F1 score (weighted)    0.835118    0.888658    0.856119    0.854425   \n",
      "11     F1 score (macro)    0.762103    0.841981    0.795656    0.789681   \n",
      "12    Balanced Accuracy    0.722941    0.808281    0.757206    0.748756   \n",
      "13                  MCC    0.590471    0.704480    0.634790    0.631748   \n",
      "14                  NPV    0.842600    0.891900    0.860300    0.857800   \n",
      "15              ROC_AUC    0.722941    0.808281    0.757206    0.748756   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.633686    0.682160    0.706744    0.686708    0.670744  \n",
      "1    38.000000   41.000000   43.000000   42.000000   33.000000  \n",
      "2   199.000000  198.000000  198.000000  195.000000  198.000000  \n",
      "3     2.000000    2.000000    3.000000    6.000000    4.000000  \n",
      "4    29.000000   27.000000   24.000000   25.000000   33.000000  \n",
      "5     0.884328    0.891791    0.899254    0.884328    0.861940  \n",
      "6     0.950000    0.953488    0.934783    0.875000    0.891892  \n",
      "7     0.567164    0.602941    0.641791    0.626866    0.500000  \n",
      "8     0.990000    0.990000    0.985100    0.970100    0.980200  \n",
      "9     0.710280    0.738739    0.761062    0.730435    0.640777  \n",
      "10    0.873374    0.882788    0.892393    0.877383    0.847128  \n",
      "11    0.819010    0.835252    0.848616    0.828400    0.777663  \n",
      "12    0.778607    0.796471    0.813433    0.798507    0.740099  \n",
      "13    0.677111    0.703000    0.719871    0.674200    0.599737  \n",
      "14    0.872800    0.880000    0.891900    0.886400    0.857100  \n",
      "15    0.778607    0.796471    0.813433    0.798507    0.740099  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_lgbm_8)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_8_cat = np.where(((y_pred_lgbm_8 >= 2) | (y_pred_lgbm_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:50:01,123] Trial 450 finished with value: 0.6809300227695658 and parameters: {'n_estimators': 281, 'learning_rate': 0.10923544845296695, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 48}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:02,627] Trial 451 finished with value: 0.6775618087981025 and parameters: {'n_estimators': 283, 'learning_rate': 0.10949347646175564, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 48}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:04,199] Trial 452 finished with value: 0.6804304235511276 and parameters: {'n_estimators': 255, 'learning_rate': 0.10455714118306164, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 98}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:05,530] Trial 453 finished with value: 0.6730776098531653 and parameters: {'n_estimators': 238, 'learning_rate': 0.10465931445541025, 'max_depth': 10, 'max_bin': 217, 'num_leaves': 80}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:06,912] Trial 454 finished with value: 0.6703323323360836 and parameters: {'n_estimators': 346, 'learning_rate': 0.10470685204238275, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 85}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:08,371] Trial 455 finished with value: 0.6792640832057766 and parameters: {'n_estimators': 216, 'learning_rate': 0.10872880139720298, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 39}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:09,794] Trial 456 finished with value: 0.6706881078530753 and parameters: {'n_estimators': 209, 'learning_rate': 0.10673989062033634, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 41}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:11,257] Trial 457 finished with value: 0.668876333802771 and parameters: {'n_estimators': 246, 'learning_rate': 0.1137329642325086, 'max_depth': 10, 'max_bin': 229, 'num_leaves': 96}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:12,725] Trial 458 finished with value: 0.6760276107039433 and parameters: {'n_estimators': 218, 'learning_rate': 0.10341767175353483, 'max_depth': 11, 'max_bin': 215, 'num_leaves': 53}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:14,362] Trial 459 finished with value: 0.6784015034973203 and parameters: {'n_estimators': 321, 'learning_rate': 0.10757511662676432, 'max_depth': 11, 'max_bin': 222, 'num_leaves': 131}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:15,765] Trial 460 finished with value: 0.6773616696607535 and parameters: {'n_estimators': 390, 'learning_rate': 0.11585206393661265, 'max_depth': 11, 'max_bin': 226, 'num_leaves': 68}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:17,456] Trial 461 finished with value: 0.675749787036018 and parameters: {'n_estimators': 296, 'learning_rate': 0.10382533542695185, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 179}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:19,325] Trial 462 finished with value: 0.6725152981596138 and parameters: {'n_estimators': 356, 'learning_rate': 0.10959316128039298, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 112}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:20,862] Trial 463 finished with value: 0.6697853636456118 and parameters: {'n_estimators': 288, 'learning_rate': 0.11216379679997751, 'max_depth': 10, 'max_bin': 223, 'num_leaves': 34}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:22,292] Trial 464 finished with value: 0.6768345814212376 and parameters: {'n_estimators': 319, 'learning_rate': 0.10341396030102006, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 98}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:23,882] Trial 465 finished with value: 0.6769174108501481 and parameters: {'n_estimators': 252, 'learning_rate': 0.118598722716778, 'max_depth': 11, 'max_bin': 212, 'num_leaves': 172}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:25,213] Trial 466 finished with value: 0.6765431689555493 and parameters: {'n_estimators': 183, 'learning_rate': 0.10737392612860357, 'max_depth': 11, 'max_bin': 232, 'num_leaves': 131}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:26,721] Trial 467 finished with value: 0.6735304916170618 and parameters: {'n_estimators': 337, 'learning_rate': 0.11321780924230251, 'max_depth': 11, 'max_bin': 222, 'num_leaves': 84}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:28,086] Trial 468 finished with value: 0.6733046443084931 and parameters: {'n_estimators': 381, 'learning_rate': 0.10780634081161587, 'max_depth': 11, 'max_bin': 225, 'num_leaves': 187}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:29,508] Trial 469 finished with value: 0.6732843249154362 and parameters: {'n_estimators': 272, 'learning_rate': 0.11944834005655283, 'max_depth': 11, 'max_bin': 217, 'num_leaves': 120}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:30,771] Trial 470 finished with value: 0.6744353540120971 and parameters: {'n_estimators': 310, 'learning_rate': 0.11452619995838657, 'max_depth': 9, 'max_bin': 214, 'num_leaves': 164}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:32,342] Trial 471 finished with value: 0.6792658924412827 and parameters: {'n_estimators': 364, 'learning_rate': 0.10200484613672092, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 133}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:33,835] Trial 472 finished with value: 0.668694736016218 and parameters: {'n_estimators': 360, 'learning_rate': 0.10260077941626927, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 136}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:35,266] Trial 473 finished with value: 0.674263797280706 and parameters: {'n_estimators': 397, 'learning_rate': 0.10284177062338212, 'max_depth': 10, 'max_bin': 228, 'num_leaves': 107}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:36,802] Trial 474 finished with value: 0.6625437098779884 and parameters: {'n_estimators': 339, 'learning_rate': 0.10207445569203395, 'max_depth': 8, 'max_bin': 225, 'num_leaves': 165}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:38,384] Trial 475 finished with value: 0.6743444026008324 and parameters: {'n_estimators': 372, 'learning_rate': 0.10586148122845132, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 188}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:39,958] Trial 476 finished with value: 0.6774170707470375 and parameters: {'n_estimators': 254, 'learning_rate': 0.11189543353951742, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 119}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:41,348] Trial 477 finished with value: 0.674349226579584 and parameters: {'n_estimators': 354, 'learning_rate': 0.11636675760790759, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 150}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:43,001] Trial 478 finished with value: 0.6763325967045605 and parameters: {'n_estimators': 408, 'learning_rate': 0.10710738943436551, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 137}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:44,560] Trial 479 finished with value: 0.6716116831730143 and parameters: {'n_estimators': 386, 'learning_rate': 0.11059599945015795, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 71}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:46,220] Trial 480 finished with value: 0.6783902656580576 and parameters: {'n_estimators': 327, 'learning_rate': 0.10187540988839995, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 93}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:47,550] Trial 481 finished with value: 0.6715256925794453 and parameters: {'n_estimators': 286, 'learning_rate': 0.12034557165428834, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 160}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:49,151] Trial 482 finished with value: 0.6773739004563877 and parameters: {'n_estimators': 302, 'learning_rate': 0.10593205685107747, 'max_depth': 11, 'max_bin': 225, 'num_leaves': 183}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:50,649] Trial 483 finished with value: 0.6769926050107921 and parameters: {'n_estimators': 266, 'learning_rate': 0.11337491252301965, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 126}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:52,188] Trial 484 finished with value: 0.6816571048647819 and parameters: {'n_estimators': 344, 'learning_rate': 0.10919438437838179, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 173}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:53,688] Trial 485 finished with value: 0.6764122811293066 and parameters: {'n_estimators': 316, 'learning_rate': 0.11097683544212514, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 179}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:55,072] Trial 486 finished with value: 0.6701833761063496 and parameters: {'n_estimators': 341, 'learning_rate': 0.11805396609486135, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 195}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:56,441] Trial 487 finished with value: 0.6756910837590961 and parameters: {'n_estimators': 282, 'learning_rate': 0.10852472894209371, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 174}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:57,724] Trial 488 finished with value: 0.6812182589624692 and parameters: {'n_estimators': 237, 'learning_rate': 0.11519359813477975, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 153}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:58,611] Trial 489 finished with value: 0.6544566520921098 and parameters: {'n_estimators': 249, 'learning_rate': 0.11591924943802853, 'max_depth': 5, 'max_bin': 215, 'num_leaves': 103}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:50:59,856] Trial 490 finished with value: 0.6730347454914521 and parameters: {'n_estimators': 231, 'learning_rate': 0.12003490072960167, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 148}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:51:01,054] Trial 491 finished with value: 0.6726874063847377 and parameters: {'n_estimators': 307, 'learning_rate': 0.12308113611620948, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 161}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:51:02,591] Trial 492 finished with value: 0.6759730971588878 and parameters: {'n_estimators': 291, 'learning_rate': 0.11459278780600324, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 149}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:51:04,020] Trial 493 finished with value: 0.6790515983398033 and parameters: {'n_estimators': 260, 'learning_rate': 0.1057593014623159, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 114}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:51:05,541] Trial 494 finished with value: 0.6762591700077398 and parameters: {'n_estimators': 332, 'learning_rate': 0.11202717267370654, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 195}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:51:06,886] Trial 495 finished with value: 0.6740538107202056 and parameters: {'n_estimators': 235, 'learning_rate': 0.10845959301917922, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 171}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:51:08,263] Trial 496 finished with value: 0.6752466165434933 and parameters: {'n_estimators': 267, 'learning_rate': 0.11572116485960285, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 67}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:51:11,300] Trial 497 finished with value: 0.10471982898691727 and parameters: {'n_estimators': 323, 'learning_rate': 0.0003537825612348888, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 133}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:51:12,782] Trial 498 finished with value: 0.6726773846295522 and parameters: {'n_estimators': 406, 'learning_rate': 0.10478719871704242, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 155}. Best is trial 439 with value: 0.6853683617399914.\n",
      "[I 2023-12-11 23:51:13,948] Trial 499 finished with value: 0.6691395501193332 and parameters: {'n_estimators': 298, 'learning_rate': 0.17532553034694798, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 177}. Best is trial 439 with value: 0.6853683617399914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.685368362\n",
      "\tBest params:\n",
      "\t\tn_estimators: 271\n",
      "\t\tlearning_rate: 0.10911691677480986\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 225\n",
      "\t\tnum_leaves: 135\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.688832    0.664005    0.671613    0.713928   \n",
      "1                    TP   31.000000   42.000000   36.000000   34.000000   \n",
      "2                    TN  198.000000  198.000000  197.000000  199.000000   \n",
      "3                    FP    2.000000    4.000000    3.000000    2.000000   \n",
      "4                    FN   37.000000   24.000000   32.000000   33.000000   \n",
      "5              Accuracy    0.854478    0.895522    0.869403    0.869403   \n",
      "6             Precision    0.939394    0.913043    0.923077    0.944444   \n",
      "7           Sensitivity    0.455882    0.636364    0.529412    0.507463   \n",
      "8           Specificity    0.990000    0.980200    0.985000    0.990000   \n",
      "9              F1 score    0.613861    0.750000    0.672897    0.660194   \n",
      "10  F1 score (weighted)    0.835118    0.888658    0.856119    0.854425   \n",
      "11     F1 score (macro)    0.762103    0.841981    0.795656    0.789681   \n",
      "12    Balanced Accuracy    0.722941    0.808281    0.757206    0.748756   \n",
      "13                  MCC    0.590471    0.704480    0.634790    0.631748   \n",
      "14                  NPV    0.842600    0.891900    0.860300    0.857800   \n",
      "15              ROC_AUC    0.722941    0.808281    0.757206    0.748756   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.633686    0.682160    0.706744    0.686708    0.670744    0.665545  \n",
      "1    38.000000   41.000000   43.000000   42.000000   33.000000   34.000000  \n",
      "2   199.000000  198.000000  198.000000  195.000000  198.000000  199.000000  \n",
      "3     2.000000    2.000000    3.000000    6.000000    4.000000    3.000000  \n",
      "4    29.000000   27.000000   24.000000   25.000000   33.000000   32.000000  \n",
      "5     0.884328    0.891791    0.899254    0.884328    0.861940    0.869403  \n",
      "6     0.950000    0.953488    0.934783    0.875000    0.891892    0.918919  \n",
      "7     0.567164    0.602941    0.641791    0.626866    0.500000    0.515152  \n",
      "8     0.990000    0.990000    0.985100    0.970100    0.980200    0.985100  \n",
      "9     0.710280    0.738739    0.761062    0.730435    0.640777    0.660194  \n",
      "10    0.873374    0.882788    0.892393    0.877383    0.847128    0.855391  \n",
      "11    0.819010    0.835252    0.848616    0.828400    0.777663    0.789681  \n",
      "12    0.778607    0.796471    0.813433    0.798507    0.740099    0.750150  \n",
      "13    0.677111    0.703000    0.719871    0.674200    0.599737    0.624844  \n",
      "14    0.872800    0.880000    0.891900    0.886400    0.857100    0.861500  \n",
      "15    0.778607    0.796471    0.813433    0.798507    0.740099    0.750150  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_lgbm_9)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_9_cat = np.where(((y_pred_lgbm_9 >= 2) | (y_pred_lgbm_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.692604     0.061554\n",
      "1                    TP        18.100000     3.314949\n",
      "2                    TN        98.700000     1.828782\n",
      "3                    FP         1.800000     1.549193\n",
      "4                    FN        15.300000     3.267687\n",
      "5              Accuracy         0.872304     0.027300\n",
      "6             Precision         0.912338     0.063576\n",
      "7           Sensitivity         0.541827     0.098431\n",
      "8           Specificity         0.982080     0.015396\n",
      "9              F1 score         0.674942     0.085955\n",
      "10  F1 score (weighted)         0.859219     0.033065\n",
      "11     F1 score (macro)         0.797673     0.050642\n",
      "12    Balanced Accuracy         0.761952     0.049939\n",
      "13                  MCC         0.636930     0.084322\n",
      "14                  NPV         0.866370     0.024909\n",
      "15              ROC_AUC         0.761952     0.049939\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.688832</td>\n",
       "      <td>0.664005</td>\n",
       "      <td>0.671613</td>\n",
       "      <td>0.713928</td>\n",
       "      <td>0.633686</td>\n",
       "      <td>0.682160</td>\n",
       "      <td>0.706744</td>\n",
       "      <td>0.686708</td>\n",
       "      <td>0.670744</td>\n",
       "      <td>0.665545</td>\n",
       "      <td>0.678396</td>\n",
       "      <td>0.022935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>37.400000</td>\n",
       "      <td>4.376706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>197.900000</td>\n",
       "      <td>1.197219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1.286684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>4.477102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.854478</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.869403</td>\n",
       "      <td>0.869403</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.861940</td>\n",
       "      <td>0.869403</td>\n",
       "      <td>0.877985</td>\n",
       "      <td>0.015136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.924404</td>\n",
       "      <td>0.025590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.558303</td>\n",
       "      <td>0.065833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.970100</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.984570</td>\n",
       "      <td>0.006379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.613861</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.672897</td>\n",
       "      <td>0.660194</td>\n",
       "      <td>0.710280</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.761062</td>\n",
       "      <td>0.730435</td>\n",
       "      <td>0.640777</td>\n",
       "      <td>0.660194</td>\n",
       "      <td>0.693844</td>\n",
       "      <td>0.050786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.835118</td>\n",
       "      <td>0.888658</td>\n",
       "      <td>0.856119</td>\n",
       "      <td>0.854425</td>\n",
       "      <td>0.873374</td>\n",
       "      <td>0.882788</td>\n",
       "      <td>0.892393</td>\n",
       "      <td>0.877383</td>\n",
       "      <td>0.847128</td>\n",
       "      <td>0.855391</td>\n",
       "      <td>0.866278</td>\n",
       "      <td>0.019233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.762103</td>\n",
       "      <td>0.841981</td>\n",
       "      <td>0.795656</td>\n",
       "      <td>0.789681</td>\n",
       "      <td>0.819010</td>\n",
       "      <td>0.835252</td>\n",
       "      <td>0.848616</td>\n",
       "      <td>0.828400</td>\n",
       "      <td>0.777663</td>\n",
       "      <td>0.789681</td>\n",
       "      <td>0.808804</td>\n",
       "      <td>0.029680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.722941</td>\n",
       "      <td>0.808281</td>\n",
       "      <td>0.757206</td>\n",
       "      <td>0.748756</td>\n",
       "      <td>0.778607</td>\n",
       "      <td>0.796471</td>\n",
       "      <td>0.813433</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.740099</td>\n",
       "      <td>0.750150</td>\n",
       "      <td>0.771445</td>\n",
       "      <td>0.031686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.590471</td>\n",
       "      <td>0.704480</td>\n",
       "      <td>0.634790</td>\n",
       "      <td>0.631748</td>\n",
       "      <td>0.677111</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>0.719871</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.599737</td>\n",
       "      <td>0.624844</td>\n",
       "      <td>0.656025</td>\n",
       "      <td>0.045806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.842600</td>\n",
       "      <td>0.891900</td>\n",
       "      <td>0.860300</td>\n",
       "      <td>0.857800</td>\n",
       "      <td>0.872800</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.891900</td>\n",
       "      <td>0.886400</td>\n",
       "      <td>0.857100</td>\n",
       "      <td>0.861500</td>\n",
       "      <td>0.870230</td>\n",
       "      <td>0.016891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.722941</td>\n",
       "      <td>0.808281</td>\n",
       "      <td>0.757206</td>\n",
       "      <td>0.748756</td>\n",
       "      <td>0.778607</td>\n",
       "      <td>0.796471</td>\n",
       "      <td>0.813433</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.740099</td>\n",
       "      <td>0.750150</td>\n",
       "      <td>0.771445</td>\n",
       "      <td>0.031686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.688832    0.664005    0.671613    0.713928   \n",
       "1                    TP   31.000000   42.000000   36.000000   34.000000   \n",
       "2                    TN  198.000000  198.000000  197.000000  199.000000   \n",
       "3                    FP    2.000000    4.000000    3.000000    2.000000   \n",
       "4                    FN   37.000000   24.000000   32.000000   33.000000   \n",
       "5              Accuracy    0.854478    0.895522    0.869403    0.869403   \n",
       "6             Precision    0.939394    0.913043    0.923077    0.944444   \n",
       "7           Sensitivity    0.455882    0.636364    0.529412    0.507463   \n",
       "8           Specificity    0.990000    0.980200    0.985000    0.990000   \n",
       "9              F1 score    0.613861    0.750000    0.672897    0.660194   \n",
       "10  F1 score (weighted)    0.835118    0.888658    0.856119    0.854425   \n",
       "11     F1 score (macro)    0.762103    0.841981    0.795656    0.789681   \n",
       "12    Balanced Accuracy    0.722941    0.808281    0.757206    0.748756   \n",
       "13                  MCC    0.590471    0.704480    0.634790    0.631748   \n",
       "14                  NPV    0.842600    0.891900    0.860300    0.857800   \n",
       "15              ROC_AUC    0.722941    0.808281    0.757206    0.748756   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.633686    0.682160    0.706744    0.686708    0.670744    0.665545   \n",
       "1    38.000000   41.000000   43.000000   42.000000   33.000000   34.000000   \n",
       "2   199.000000  198.000000  198.000000  195.000000  198.000000  199.000000   \n",
       "3     2.000000    2.000000    3.000000    6.000000    4.000000    3.000000   \n",
       "4    29.000000   27.000000   24.000000   25.000000   33.000000   32.000000   \n",
       "5     0.884328    0.891791    0.899254    0.884328    0.861940    0.869403   \n",
       "6     0.950000    0.953488    0.934783    0.875000    0.891892    0.918919   \n",
       "7     0.567164    0.602941    0.641791    0.626866    0.500000    0.515152   \n",
       "8     0.990000    0.990000    0.985100    0.970100    0.980200    0.985100   \n",
       "9     0.710280    0.738739    0.761062    0.730435    0.640777    0.660194   \n",
       "10    0.873374    0.882788    0.892393    0.877383    0.847128    0.855391   \n",
       "11    0.819010    0.835252    0.848616    0.828400    0.777663    0.789681   \n",
       "12    0.778607    0.796471    0.813433    0.798507    0.740099    0.750150   \n",
       "13    0.677111    0.703000    0.719871    0.674200    0.599737    0.624844   \n",
       "14    0.872800    0.880000    0.891900    0.886400    0.857100    0.861500   \n",
       "15    0.778607    0.796471    0.813433    0.798507    0.740099    0.750150   \n",
       "\n",
       "           ave       std  \n",
       "0     0.678396  0.022935  \n",
       "1    37.400000  4.376706  \n",
       "2   197.900000  1.197219  \n",
       "3     3.100000  1.286684  \n",
       "4    29.600000  4.477102  \n",
       "5     0.877985  0.015136  \n",
       "6     0.924404  0.025590  \n",
       "7     0.558303  0.065833  \n",
       "8     0.984570  0.006379  \n",
       "9     0.693844  0.050786  \n",
       "10    0.866278  0.019233  \n",
       "11    0.808804  0.029680  \n",
       "12    0.771445  0.031686  \n",
       "13    0.656025  0.045806  \n",
       "14    0.870230  0.016891  \n",
       "15    0.771445  0.031686  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test['std'] = mat_met_lgbm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.689087</td>\n",
       "      <td>0.067320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.878560</td>\n",
       "      <td>0.022412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.932511</td>\n",
       "      <td>0.058446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.556650</td>\n",
       "      <td>0.084047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.985652</td>\n",
       "      <td>0.013386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.692522</td>\n",
       "      <td>0.070599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.866377</td>\n",
       "      <td>0.027647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.808378</td>\n",
       "      <td>0.041618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.771151</td>\n",
       "      <td>0.041402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.658181</td>\n",
       "      <td>0.067117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.870374</td>\n",
       "      <td>0.021905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.771151</td>\n",
       "      <td>0.041402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.689087     0.067320\n",
       "1              Accuracy         0.878560     0.022412\n",
       "2             Precision         0.932511     0.058446\n",
       "3           Sensitivity         0.556650     0.084047\n",
       "4           Specificity         0.985652     0.013386\n",
       "5              F1 score         0.692522     0.070599\n",
       "6   F1 score (weighted)         0.866377     0.027647\n",
       "7      F1 score (macro)         0.808378     0.041618\n",
       "8     Balanced Accuracy         0.771151     0.041402\n",
       "9                   MCC         0.658181     0.067117\n",
       "10                  NPV         0.870374     0.021905\n",
       "11              ROC_AUC         0.771151     0.041402"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_lgbm_cat = np.where(((y_pred_optimized_lgbm >= 2) | (y_pred_optimized_lgbm <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_lgbm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_lgbm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "lgbm_run0 = data_lgbm[['y_test_idx0', 'y_test0', 'y_pred_lgbm0']]\n",
    "lgbm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "lgbm_run0.reset_index(inplace=True, drop=True)\n",
    "lgbm_run1 = data_lgbm[['y_test_idx1', 'y_test1', 'y_pred_lgbm1']]\n",
    "lgbm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "lgbm_run1.reset_index(inplace=True, drop=True)\n",
    "lgbm_run2 = data_lgbm[['y_test_idx2', 'y_test2', 'y_pred_lgbm2']]\n",
    "lgbm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "lgbm_run2.reset_index(inplace=True, drop=True)\n",
    "lgbm_run3 = data_lgbm[['y_test_idx3', 'y_test3', 'y_pred_lgbm3']]\n",
    "lgbm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "lgbm_run3.reset_index(inplace=True, drop=True)\n",
    "lgbm_run4 = data_lgbm[['y_test_idx4', 'y_test4', 'y_pred_lgbm4']]\n",
    "lgbm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "lgbm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "lgbm_5preds = pd.concat([chembl_id, lgbm_run0, lgbm_run1, lgbm_run2, lgbm_run3, lgbm_run4], axis=1)\n",
    "lgbm_5preds = lgbm_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_lgbm0', 'y_pred_lgbm1', 'y_pred_lgbm2', 'y_pred_lgbm3', 'y_pred_lgbm4']]\n",
    "lgbm_5preds['y_pred_lgbm_ave'] = lgbm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "lgbm_5preds['y_pred_lgbm_std'] = lgbm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "\n",
    "mat_met_optimized_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db4ac315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model r2_score 0.7103 with a standard deviation of 0.0390\n",
      "LightGBM optimized model r2_score 0.7116 with a standard deviation of 0.0395\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#r2_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"r2\")\n",
    "r2_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"r2\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)r2_score %0.4f with a standard deviation of %0.4f\" % (r2_cv_lgbm_opt_testSet.mean(), r2_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(r2_cv_lgbm_opt), np.std(r2_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_lgbm.joblib']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm_reg, \"OUTPUT/lgbm_reg.joblib\")\n",
    "#joblib.dump(optimized_lgbm, \"OUTPUT/optimized_lgbm.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"OUTPUT/optimizedCV_lgbm.joblib\") \n",
    "#loaded_rf = joblib.load(\"OUTPUT/optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.698209     0.056938\n",
      "1                    TP        19.100000     2.685351\n",
      "2                    TN        98.300000     1.636392\n",
      "3                    FP         2.200000     1.475730\n",
      "4                    FN        14.300000     2.750757\n",
      "5              Accuracy         0.876793     0.025590\n",
      "6             Precision         0.897943     0.067966\n",
      "7           Sensitivity         0.572057     0.080057\n",
      "8           Specificity         0.978120     0.014641\n",
      "9              F1 score         0.696285     0.070523\n",
      "10  F1 score (weighted)         0.866194     0.029326\n",
      "11     F1 score (macro)         0.809472     0.042861\n",
      "12    Balanced Accuracy         0.775088     0.042614\n",
      "13                  MCC         0.650555     0.078023\n",
      "14                  NPV         0.873380     0.021973\n",
      "15              ROC_AUC         0.775088     0.042614\n",
      "CPU times: user 1h 59min 34s, sys: 7.45 s, total: 1h 59min 41s\n",
      "Wall time: 6min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_reg.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores),np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 23:58:02,492] A new study created in memory with name: XGBRegressor\n",
      "[I 2023-12-11 23:58:14,145] Trial 0 finished with value: 0.6843114397040413 and parameters: {'n_estimators': 815, 'eta': 0.02746425648101838, 'max_depth': 7, 'alpha': 0.1037, 'lambda': 32.54414936441472, 'max_bin': 437}. Best is trial 0 with value: 0.6843114397040413.\n",
      "[I 2023-12-11 23:58:22,477] Trial 1 finished with value: 0.4561579252553349 and parameters: {'n_estimators': 293, 'eta': 0.004942320617750206, 'max_depth': 10, 'alpha': 0.7017, 'lambda': 38.33907259426957, 'max_bin': 405}. Best is trial 0 with value: 0.6843114397040413.\n",
      "[I 2023-12-11 23:58:29,909] Trial 2 finished with value: 0.6957625989060257 and parameters: {'n_estimators': 541, 'eta': 0.07254549422322519, 'max_depth': 10, 'alpha': 0.8477, 'lambda': 17.560155865859915, 'max_bin': 447}. Best is trial 2 with value: 0.6957625989060257.\n",
      "[I 2023-12-11 23:58:38,671] Trial 3 finished with value: 0.6925594940900079 and parameters: {'n_estimators': 418, 'eta': 0.07386522202622564, 'max_depth': 11, 'alpha': 0.9478000000000001, 'lambda': 37.57518083898994, 'max_bin': 338}. Best is trial 2 with value: 0.6957625989060257.\n",
      "[I 2023-12-11 23:58:51,642] Trial 4 finished with value: 0.6910339602168982 and parameters: {'n_estimators': 625, 'eta': 0.016599998376120955, 'max_depth': 9, 'alpha': 0.7632, 'lambda': 13.62344549481045, 'max_bin': 269}. Best is trial 2 with value: 0.6957625989060257.\n",
      "[I 2023-12-11 23:59:00,256] Trial 5 finished with value: 0.6856212580422565 and parameters: {'n_estimators': 787, 'eta': 0.05708173872704335, 'max_depth': 7, 'alpha': 0.22390000000000002, 'lambda': 38.76871863457657, 'max_bin': 458}. Best is trial 2 with value: 0.6957625989060257.\n",
      "[I 2023-12-11 23:59:07,711] Trial 6 finished with value: 0.6800320433885839 and parameters: {'n_estimators': 513, 'eta': 0.033672489773143965, 'max_depth': 6, 'alpha': 0.6786, 'lambda': 20.381078038648027, 'max_bin': 376}. Best is trial 2 with value: 0.6957625989060257.\n",
      "[I 2023-12-11 23:59:16,750] Trial 7 finished with value: 0.6982460077371211 and parameters: {'n_estimators': 623, 'eta': 0.053083959581457354, 'max_depth': 11, 'alpha': 0.5955, 'lambda': 15.68554858180244, 'max_bin': 251}. Best is trial 7 with value: 0.6982460077371211.\n",
      "[I 2023-12-11 23:59:21,272] Trial 8 finished with value: 0.6957041857772106 and parameters: {'n_estimators': 297, 'eta': 0.06294353190701903, 'max_depth': 8, 'alpha': 0.397, 'lambda': 3.199962735751592, 'max_bin': 373}. Best is trial 7 with value: 0.6982460077371211.\n",
      "[I 2023-12-11 23:59:26,526] Trial 9 finished with value: 0.18478141871862347 and parameters: {'n_estimators': 235, 'eta': 0.0016895876410465262, 'max_depth': 8, 'alpha': 0.3265, 'lambda': 39.69873156693728, 'max_bin': 271}. Best is trial 7 with value: 0.6982460077371211.\n",
      "[I 2023-12-11 23:59:28,466] Trial 10 finished with value: 0.6567847811794799 and parameters: {'n_estimators': 54, 'eta': 0.09586151174094759, 'max_depth': 12, 'alpha': 0.5503, 'lambda': 29.055877043768014, 'max_bin': 313}. Best is trial 7 with value: 0.6982460077371211.\n",
      "[I 2023-12-11 23:59:39,735] Trial 11 finished with value: 0.6975297069077375 and parameters: {'n_estimators': 653, 'eta': 0.043217051268240665, 'max_depth': 10, 'alpha': 0.9152, 'lambda': 16.958169412151683, 'max_bin': 491}. Best is trial 7 with value: 0.6982460077371211.\n",
      "[I 2023-12-11 23:59:50,204] Trial 12 finished with value: 0.7032256432416475 and parameters: {'n_estimators': 683, 'eta': 0.04390681761394196, 'max_depth': 12, 'alpha': 0.5074000000000001, 'lambda': 11.028360205363022, 'max_bin': 497}. Best is trial 12 with value: 0.7032256432416475.\n",
      "[I 2023-12-11 23:59:59,196] Trial 13 finished with value: 0.7008571834393562 and parameters: {'n_estimators': 704, 'eta': 0.04901870696100378, 'max_depth': 12, 'alpha': 0.56, 'lambda': 9.532080166435133, 'max_bin': 316}. Best is trial 12 with value: 0.7032256432416475.\n",
      "[I 2023-12-12 00:00:09,886] Trial 14 finished with value: 0.7033789018760751 and parameters: {'n_estimators': 884, 'eta': 0.040535372345427735, 'max_depth': 12, 'alpha': 0.43510000000000004, 'lambda': 8.51253423748848, 'max_bin': 314}. Best is trial 14 with value: 0.7033789018760751.\n",
      "[I 2023-12-12 00:00:21,693] Trial 15 finished with value: 0.701710374535866 and parameters: {'n_estimators': 885, 'eta': 0.03504814555775021, 'max_depth': 12, 'alpha': 0.4037, 'lambda': 7.238009063361655, 'max_bin': 492}. Best is trial 14 with value: 0.7033789018760751.\n",
      "[I 2023-12-12 00:00:29,927] Trial 16 finished with value: 0.6887384791898825 and parameters: {'n_estimators': 777, 'eta': 0.02113392061836334, 'max_depth': 5, 'alpha': 0.2233, 'lambda': 2.098418018195069, 'max_bin': 353}. Best is trial 14 with value: 0.7033789018760751.\n",
      "[I 2023-12-12 00:00:39,446] Trial 17 finished with value: 0.7035966189550977 and parameters: {'n_estimators': 894, 'eta': 0.04582972069309016, 'max_depth': 11, 'alpha': 0.0022, 'lambda': 11.098892603499628, 'max_bin': 302}. Best is trial 17 with value: 0.7035966189550977.\n",
      "[I 2023-12-12 00:00:49,379] Trial 18 finished with value: 0.7034204233117227 and parameters: {'n_estimators': 849, 'eta': 0.0378594408787154, 'max_depth': 11, 'alpha': 0.0258, 'lambda': 7.069442084505113, 'max_bin': 304}. Best is trial 17 with value: 0.7035966189550977.\n",
      "[I 2023-12-12 00:00:59,383] Trial 19 finished with value: 0.6980556966612311 and parameters: {'n_estimators': 900, 'eta': 0.02930578382978145, 'max_depth': 9, 'alpha': 0.0067, 'lambda': 5.835659446492828, 'max_bin': 290}. Best is trial 17 with value: 0.7035966189550977.\n",
      "[I 2023-12-12 00:01:10,467] Trial 20 finished with value: 0.700779782515278 and parameters: {'n_estimators': 411, 'eta': 0.013798199943658742, 'max_depth': 11, 'alpha': 0.0397, 'lambda': 1.0403990473277762, 'max_bin': 292}. Best is trial 17 with value: 0.7035966189550977.\n",
      "[I 2023-12-12 00:01:22,964] Trial 21 finished with value: 0.7047860827651752 and parameters: {'n_estimators': 835, 'eta': 0.03929405487919312, 'max_depth': 11, 'alpha': 0.18280000000000002, 'lambda': 11.402157228717085, 'max_bin': 323}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:01:34,584] Trial 22 finished with value: 0.7003789358783099 and parameters: {'n_estimators': 786, 'eta': 0.037198518873697725, 'max_depth': 11, 'alpha': 0.1476, 'lambda': 12.897127989046579, 'max_bin': 349}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:01:46,315] Trial 23 finished with value: 0.6987925855480135 and parameters: {'n_estimators': 741, 'eta': 0.026078450008379786, 'max_depth': 10, 'alpha': 0.131, 'lambda': 6.031728565795909, 'max_bin': 332}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:01:56,359] Trial 24 finished with value: 0.7012676430161414 and parameters: {'n_estimators': 826, 'eta': 0.04911109638181381, 'max_depth': 11, 'alpha': 0.231, 'lambda': 11.208473866252902, 'max_bin': 295}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:02:03,962] Trial 25 finished with value: 0.6976083503676748 and parameters: {'n_estimators': 575, 'eta': 0.03624775539083715, 'max_depth': 9, 'alpha': 0.0027, 'lambda': 5.888224141572413, 'max_bin': 369}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:02:13,341] Trial 26 finished with value: 0.7002101677886299 and parameters: {'n_estimators': 847, 'eta': 0.0457236961579003, 'max_depth': 10, 'alpha': 0.0913, 'lambda': 9.917066433571874, 'max_bin': 270}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:02:21,809] Trial 27 finished with value: 0.7003065105322713 and parameters: {'n_estimators': 732, 'eta': 0.05750697671320528, 'max_depth': 11, 'alpha': 0.3027, 'lambda': 14.041579729817379, 'max_bin': 328}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:02:30,603] Trial 28 finished with value: 0.6980529452534483 and parameters: {'n_estimators': 838, 'eta': 0.030086252917392486, 'max_depth': 9, 'alpha': 0.1632, 'lambda': 4.205463037375899, 'max_bin': 302}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:02:44,296] Trial 29 finished with value: 0.7006616360257865 and parameters: {'n_estimators': 762, 'eta': 0.02632919492392329, 'max_depth': 10, 'alpha': 0.0867, 'lambda': 9.181691247988756, 'max_bin': 397}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:02:55,395] Trial 30 finished with value: 0.6926555289131568 and parameters: {'n_estimators': 833, 'eta': 0.0379042449362939, 'max_depth': 8, 'alpha': 0.0659, 'lambda': 22.876370603758048, 'max_bin': 354}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:03:06,238] Trial 31 finished with value: 0.7036417244221455 and parameters: {'n_estimators': 893, 'eta': 0.041144736489594526, 'max_depth': 12, 'alpha': 0.4112, 'lambda': 7.448159988416033, 'max_bin': 318}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:03:16,640] Trial 32 finished with value: 0.7000849948153032 and parameters: {'n_estimators': 892, 'eta': 0.04314455716658186, 'max_depth': 12, 'alpha': 0.30660000000000004, 'lambda': 7.4108940498687605, 'max_bin': 287}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:03:26,693] Trial 33 finished with value: 0.7000612084551014 and parameters: {'n_estimators': 840, 'eta': 0.03017561284171679, 'max_depth': 11, 'alpha': 0.1865, 'lambda': 4.2552388723613666, 'max_bin': 325}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:03:36,032] Trial 34 finished with value: 0.7005307809045787 and parameters: {'n_estimators': 701, 'eta': 0.04713647023530741, 'max_depth': 11, 'alpha': 0.0878, 'lambda': 11.813606910951155, 'max_bin': 307}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:03:47,262] Trial 35 finished with value: 0.7030422735616512 and parameters: {'n_estimators': 783, 'eta': 0.039553567821222956, 'max_depth': 12, 'alpha': 0.2691, 'lambda': 8.216538517954685, 'max_bin': 280}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:03:56,652] Trial 36 finished with value: 0.6985197549863847 and parameters: {'n_estimators': 856, 'eta': 0.053428802346201724, 'max_depth': 10, 'alpha': 0.3614, 'lambda': 14.64254635986881, 'max_bin': 251}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:04:09,509] Trial 37 finished with value: 0.6775416084649993 and parameters: {'n_estimators': 442, 'eta': 0.009804275934949595, 'max_depth': 11, 'alpha': 0.0526, 'lambda': 11.779009709157613, 'max_bin': 343}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:04:19,726] Trial 38 finished with value: 0.6964053720312469 and parameters: {'n_estimators': 590, 'eta': 0.02333346513031128, 'max_depth': 10, 'alpha': 0.6629, 'lambda': 3.698524599958727, 'max_bin': 418}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:04:29,398] Trial 39 finished with value: 0.6923587319070578 and parameters: {'n_estimators': 810, 'eta': 0.032392408034308116, 'max_depth': 7, 'alpha': 0.17950000000000002, 'lambda': 16.203108158637946, 'max_bin': 392}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:04:39,791] Trial 40 finished with value: 0.6844914115571518 and parameters: {'n_estimators': 340, 'eta': 0.020995111158481027, 'max_depth': 12, 'alpha': 0.1222, 'lambda': 19.26379176886059, 'max_bin': 323}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:04:51,607] Trial 41 finished with value: 0.7041528608358498 and parameters: {'n_estimators': 900, 'eta': 0.040446283705411555, 'max_depth': 12, 'alpha': 0.46630000000000005, 'lambda': 9.023298758254642, 'max_bin': 307}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:05:04,313] Trial 42 finished with value: 0.7009828994740315 and parameters: {'n_estimators': 895, 'eta': 0.041591056442953286, 'max_depth': 11, 'alpha': 0.4418, 'lambda': 13.336586207847866, 'max_bin': 304}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:05:08,960] Trial 43 finished with value: 0.689915746745975 and parameters: {'n_estimators': 160, 'eta': 0.03394746875961245, 'max_depth': 12, 'alpha': 0.7571, 'lambda': 7.040046756265996, 'max_bin': 269}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:05:19,020] Trial 44 finished with value: 0.698693135207087 and parameters: {'n_estimators': 799, 'eta': 0.052503624372966344, 'max_depth': 11, 'alpha': 0.48810000000000003, 'lambda': 9.87643212920279, 'max_bin': 339}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:05:28,098] Trial 45 finished with value: 0.6990456918200583 and parameters: {'n_estimators': 858, 'eta': 0.04648881099239401, 'max_depth': 12, 'alpha': 0.5977, 'lambda': 4.822403809578687, 'max_bin': 282}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:05:39,799] Trial 46 finished with value: 0.7039576307746745 and parameters: {'n_estimators': 744, 'eta': 0.03941057408193803, 'max_depth': 12, 'alpha': 0.48400000000000004, 'lambda': 8.696610336709151, 'max_bin': 364}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:05:48,814] Trial 47 finished with value: 0.7008595330823019 and parameters: {'n_estimators': 747, 'eta': 0.060883917877674075, 'max_depth': 12, 'alpha': 0.4923, 'lambda': 14.8930580383424, 'max_bin': 365}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:05:58,961] Trial 48 finished with value: 0.7028252702916713 and parameters: {'n_estimators': 657, 'eta': 0.04099015870227401, 'max_depth': 12, 'alpha': 0.6214000000000001, 'lambda': 10.384107928701447, 'max_bin': 424}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:06:09,345] Trial 49 finished with value: 0.7026853630928429 and parameters: {'n_estimators': 721, 'eta': 0.050863815537073136, 'max_depth': 12, 'alpha': 0.4504, 'lambda': 12.310383770303735, 'max_bin': 359}. Best is trial 21 with value: 0.7047860827651752.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.7048\n",
      "\tBest params:\n",
      "\t\tn_estimators: 835\n",
      "\t\teta: 0.03929405487919312\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.18280000000000002\n",
      "\t\tlambda: 11.402157228717085\n",
      "\t\tmax_bin: 323\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBRegressor\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.739481\n",
      "1                    TP   39.000000\n",
      "2                    TN  198.000000\n",
      "3                    FP    2.000000\n",
      "4                    FN   29.000000\n",
      "5              Accuracy    0.884328\n",
      "6             Precision    0.951220\n",
      "7           Sensitivity    0.573529\n",
      "8           Specificity    0.990000\n",
      "9              F1 score    0.715596\n",
      "10  F1 score (weighted)    0.873659\n",
      "11     F1 score (macro)    0.821498\n",
      "12    Balanced Accuracy    0.781765\n",
      "13                  MCC    0.681210\n",
      "14                  NPV    0.872200\n",
      "15              ROC_AUC    0.781765\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_xgb_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_xgb_0_cat = np.where(((y_pred_xgb_0 >= 2) | (y_pred_xgb_0 <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:06:20,589] Trial 50 finished with value: 0.6959122273182385 and parameters: {'n_estimators': 511, 'eta': 0.03235130443344665, 'max_depth': 12, 'alpha': 0.3529, 'lambda': 8.333698301372078, 'max_bin': 378}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:06:32,092] Trial 51 finished with value: 0.6967789989206262 and parameters: {'n_estimators': 871, 'eta': 0.03782271868703384, 'max_depth': 11, 'alpha': 0.3864, 'lambda': 6.920119649224081, 'max_bin': 317}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:06:42,546] Trial 52 finished with value: 0.6983772139745538 and parameters: {'n_estimators': 822, 'eta': 0.04469358460162503, 'max_depth': 11, 'alpha': 0.036000000000000004, 'lambda': 8.920224294094828, 'max_bin': 300}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:06:55,737] Trial 53 finished with value: 0.6986514591343371 and parameters: {'n_estimators': 803, 'eta': 0.03492121522701661, 'max_depth': 12, 'alpha': 0.539, 'lambda': 10.785810416870877, 'max_bin': 337}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:07:04,237] Trial 54 finished with value: 0.699072780255414 and parameters: {'n_estimators': 866, 'eta': 0.04129595315966592, 'max_depth': 10, 'alpha': 0.2644, 'lambda': 2.4409407480017355, 'max_bin': 458}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:07:11,569] Trial 55 finished with value: 0.6926505752505612 and parameters: {'n_estimators': 899, 'eta': 0.04627664730806699, 'max_depth': 6, 'alpha': 0.4062, 'lambda': 5.7882534681536315, 'max_bin': 316}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:07:24,826] Trial 56 finished with value: 0.7002119107449472 and parameters: {'n_estimators': 773, 'eta': 0.03881969641820217, 'max_depth': 11, 'alpha': 0.537, 'lambda': 12.970311171897315, 'max_bin': 259}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:07:34,604] Trial 57 finished with value: 0.6983774950544545 and parameters: {'n_estimators': 669, 'eta': 0.04971264554740891, 'max_depth': 12, 'alpha': 0.9988, 'lambda': 8.378381842746894, 'max_bin': 383}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:07:49,363] Trial 58 finished with value: 0.7013089977827127 and parameters: {'n_estimators': 812, 'eta': 0.02782976258185481, 'max_depth': 11, 'alpha': 0.8258000000000001, 'lambda': 10.722825934749128, 'max_bin': 310}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:07:58,152] Trial 59 finished with value: 0.6973418492409236 and parameters: {'n_estimators': 627, 'eta': 0.044127764861730936, 'max_depth': 12, 'alpha': 0.0149, 'lambda': 4.779010881201432, 'max_bin': 279}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:08:10,337] Trial 60 finished with value: 0.7016508787093362 and parameters: {'n_estimators': 861, 'eta': 0.03595187433566732, 'max_depth': 10, 'alpha': 0.46380000000000005, 'lambda': 9.625773629613048, 'max_bin': 348}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:08:20,100] Trial 61 finished with value: 0.6967934421426164 and parameters: {'n_estimators': 873, 'eta': 0.04186544978140754, 'max_depth': 12, 'alpha': 0.4086, 'lambda': 7.010792814636098, 'max_bin': 294}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:08:31,394] Trial 62 finished with value: 0.6950735090558134 and parameters: {'n_estimators': 899, 'eta': 0.03863686514324933, 'max_depth': 12, 'alpha': 0.5664, 'lambda': 8.341212140715976, 'max_bin': 331}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:08:44,293] Trial 63 finished with value: 0.7025314557815154 and parameters: {'n_estimators': 755, 'eta': 0.03170593964570374, 'max_depth': 11, 'alpha': 0.5152, 'lambda': 12.046023452231845, 'max_bin': 320}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:08:51,345] Trial 64 finished with value: 0.698668828430895 and parameters: {'n_estimators': 834, 'eta': 0.04785233462888962, 'max_depth': 11, 'alpha': 0.3547, 'lambda': 2.94402580578549, 'max_bin': 308}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:09:02,266] Trial 65 finished with value: 0.6978247064772212 and parameters: {'n_estimators': 790, 'eta': 0.035301300114884374, 'max_depth': 12, 'alpha': 0.2061, 'lambda': 5.572266197535624, 'max_bin': 295}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:09:10,354] Trial 66 finished with value: 0.6936907792374667 and parameters: {'n_estimators': 871, 'eta': 0.053859313861940344, 'max_depth': 12, 'alpha': 0.4353, 'lambda': 1.1142097481405981, 'max_bin': 334}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:09:22,464] Trial 67 finished with value: 0.6989801613457255 and parameters: {'n_estimators': 837, 'eta': 0.04355501219579745, 'max_depth': 11, 'alpha': 0.124, 'lambda': 9.445125530106768, 'max_bin': 300}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:09:33,730] Trial 68 finished with value: 0.6983104124281182 and parameters: {'n_estimators': 815, 'eta': 0.028994865693997618, 'max_depth': 9, 'alpha': 0.2969, 'lambda': 11.292601702033108, 'max_bin': 325}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:09:45,775] Trial 69 finished with value: 0.7000650818859334 and parameters: {'n_estimators': 705, 'eta': 0.03832275797377396, 'max_depth': 10, 'alpha': 0.6271, 'lambda': 13.652355520673611, 'max_bin': 288}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:09:53,355] Trial 70 finished with value: 0.6973350441455686 and parameters: {'n_estimators': 768, 'eta': 0.04860632660893218, 'max_depth': 8, 'alpha': 0.4778, 'lambda': 7.256439154195142, 'max_bin': 312}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:10:04,205] Trial 71 finished with value: 0.6976789281616165 and parameters: {'n_estimators': 680, 'eta': 0.04418850755454049, 'max_depth': 12, 'alpha': 0.5047, 'lambda': 10.636391783214444, 'max_bin': 500}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:10:15,940] Trial 72 finished with value: 0.6999006878484667 and parameters: {'n_estimators': 858, 'eta': 0.040087305242721444, 'max_depth': 12, 'alpha': 0.42900000000000005, 'lambda': 9.177222594413278, 'max_bin': 446}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:10:29,550] Trial 73 finished with value: 0.6997488213653998 and parameters: {'n_estimators': 729, 'eta': 0.03348237393680423, 'max_depth': 11, 'alpha': 0.06280000000000001, 'lambda': 12.422641272178435, 'max_bin': 465}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:10:38,827] Trial 74 finished with value: 0.698034196706507 and parameters: {'n_estimators': 843, 'eta': 0.04591795892979449, 'max_depth': 12, 'alpha': 0.5216000000000001, 'lambda': 7.613013794004204, 'max_bin': 407}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:10:48,461] Trial 75 finished with value: 0.6962424236329229 and parameters: {'n_estimators': 882, 'eta': 0.042278107990320196, 'max_depth': 11, 'alpha': 0.5821000000000001, 'lambda': 6.125567229120801, 'max_bin': 472}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:10:55,236] Trial 76 finished with value: 0.6824100200062254 and parameters: {'n_estimators': 791, 'eta': 0.037359956263223824, 'max_depth': 5, 'alpha': 0.1058, 'lambda': 11.199946356575994, 'max_bin': 346}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:11:06,928] Trial 77 finished with value: 0.6986303779204766 and parameters: {'n_estimators': 822, 'eta': 0.050707518818645575, 'max_depth': 12, 'alpha': 0.38520000000000004, 'lambda': 14.88925942818306, 'max_bin': 274}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:11:16,666] Trial 78 finished with value: 0.6991503669160297 and parameters: {'n_estimators': 888, 'eta': 0.04082491657839245, 'max_depth': 11, 'alpha': 0.329, 'lambda': 3.685967351633511, 'max_bin': 329}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:11:28,780] Trial 79 finished with value: 0.699575242400545 and parameters: {'n_estimators': 564, 'eta': 0.031147953156751965, 'max_depth': 12, 'alpha': 0.15130000000000002, 'lambda': 9.415417467149094, 'max_bin': 358}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:11:37,771] Trial 80 finished with value: 0.6953780505290803 and parameters: {'n_estimators': 362, 'eta': 0.03464086260651175, 'max_depth': 12, 'alpha': 0.0175, 'lambda': 8.126337356557725, 'max_bin': 303}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:11:50,589] Trial 81 finished with value: 0.6975059791995879 and parameters: {'n_estimators': 784, 'eta': 0.03911572138102146, 'max_depth': 12, 'alpha': 0.2497, 'lambda': 8.765778776510295, 'max_bin': 280}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:12:02,166] Trial 82 finished with value: 0.6999343876382786 and parameters: {'n_estimators': 747, 'eta': 0.04446346614601601, 'max_depth': 12, 'alpha': 0.2746, 'lambda': 10.205565430214747, 'max_bin': 287}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:12:13,929] Trial 83 finished with value: 0.6969816248556151 and parameters: {'n_estimators': 845, 'eta': 0.04139744645454455, 'max_depth': 12, 'alpha': 0.4545, 'lambda': 6.790280005690695, 'max_bin': 265}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:12:25,152] Trial 84 finished with value: 0.6969048653183798 and parameters: {'n_estimators': 875, 'eta': 0.03615003373566159, 'max_depth': 11, 'alpha': 0.4222, 'lambda': 4.790013913941605, 'max_bin': 320}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:12:30,695] Trial 85 finished with value: 0.6905516224756509 and parameters: {'n_estimators': 202, 'eta': 0.046552642501217135, 'max_depth': 12, 'alpha': 0.3377, 'lambda': 8.284940410957503, 'max_bin': 261}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:12:39,566] Trial 86 finished with value: 0.6923768462636947 and parameters: {'n_estimators': 776, 'eta': 0.03322505263958284, 'max_depth': 6, 'alpha': 0.4743, 'lambda': 12.60817602964981, 'max_bin': 297}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:12:50,154] Trial 87 finished with value: 0.6995173399124576 and parameters: {'n_estimators': 803, 'eta': 0.03898327570311801, 'max_depth': 11, 'alpha': 0.37560000000000004, 'lambda': 10.089459771123547, 'max_bin': 313}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:12:52,535] Trial 88 finished with value: 0.6606385199070102 and parameters: {'n_estimators': 70, 'eta': 0.048371244270570754, 'max_depth': 12, 'alpha': 0.171, 'lambda': 6.001237838207716, 'max_bin': 305}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:13:04,280] Trial 89 finished with value: 0.7024953519056251 and parameters: {'n_estimators': 633, 'eta': 0.04364350919382906, 'max_depth': 11, 'alpha': 0.0673, 'lambda': 11.386139679682683, 'max_bin': 482}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:13:15,870] Trial 90 finished with value: 0.697323419794962 and parameters: {'n_estimators': 690, 'eta': 0.03644941905639229, 'max_depth': 10, 'alpha': 0.22160000000000002, 'lambda': 7.6546888625049405, 'max_bin': 275}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:13:27,023] Trial 91 finished with value: 0.6981546056085619 and parameters: {'n_estimators': 727, 'eta': 0.040671417422129735, 'max_depth': 12, 'alpha': 0.6953, 'lambda': 10.110913109896194, 'max_bin': 426}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:13:39,636] Trial 92 finished with value: 0.6960788842911653 and parameters: {'n_estimators': 657, 'eta': 0.04030621287677104, 'max_depth': 12, 'alpha': 0.6109, 'lambda': 13.680941780735495, 'max_bin': 439}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:13:49,995] Trial 93 finished with value: 0.6963493660452003 and parameters: {'n_estimators': 608, 'eta': 0.04263837487134085, 'max_depth': 12, 'alpha': 0.5415, 'lambda': 8.869379164646478, 'max_bin': 425}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:14:04,035] Trial 94 finished with value: 0.6964942084103 and parameters: {'n_estimators': 710, 'eta': 0.030548040095066294, 'max_depth': 12, 'alpha': 0.6249, 'lambda': 10.617581785042322, 'max_bin': 404}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:14:17,318] Trial 95 finished with value: 0.6978483038341678 and parameters: {'n_estimators': 829, 'eta': 0.036387043677056065, 'max_depth': 12, 'alpha': 0.7281000000000001, 'lambda': 11.628978499532973, 'max_bin': 384}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:14:26,025] Trial 96 finished with value: 0.6991099401124641 and parameters: {'n_estimators': 512, 'eta': 0.047102575697038414, 'max_depth': 11, 'alpha': 0.6582, 'lambda': 7.690580552874929, 'max_bin': 342}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:14:39,978] Trial 97 finished with value: 0.6989124879781354 and parameters: {'n_estimators': 898, 'eta': 0.033285489968576054, 'max_depth': 12, 'alpha': 0.034800000000000005, 'lambda': 6.286424486383353, 'max_bin': 366}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:14:52,491] Trial 98 finished with value: 0.7002377989455931 and parameters: {'n_estimators': 854, 'eta': 0.045078152733698584, 'max_depth': 11, 'alpha': 0.5725, 'lambda': 12.771502519032557, 'max_bin': 287}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:15:03,099] Trial 99 finished with value: 0.6990962853186429 and parameters: {'n_estimators': 764, 'eta': 0.039030268835923455, 'max_depth': 12, 'alpha': 0.5037, 'lambda': 5.328613013112278, 'max_bin': 324}. Best is trial 21 with value: 0.7047860827651752.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7048\n",
      "\tBest params:\n",
      "\t\tn_estimators: 835\n",
      "\t\teta: 0.03929405487919312\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.18280000000000002\n",
      "\t\tlambda: 11.402157228717085\n",
      "\t\tmax_bin: 323\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.739481    0.710332\n",
      "1                    TP   39.000000   42.000000\n",
      "2                    TN  198.000000  197.000000\n",
      "3                    FP    2.000000    5.000000\n",
      "4                    FN   29.000000   24.000000\n",
      "5              Accuracy    0.884328    0.891791\n",
      "6             Precision    0.951220    0.893617\n",
      "7           Sensitivity    0.573529    0.636364\n",
      "8           Specificity    0.990000    0.975200\n",
      "9              F1 score    0.715596    0.743363\n",
      "10  F1 score (weighted)    0.873659    0.885124\n",
      "11     F1 score (macro)    0.821498    0.837402\n",
      "12    Balanced Accuracy    0.781765    0.805806\n",
      "13                  MCC    0.681210    0.692912\n",
      "14                  NPV    0.872200    0.891400\n",
      "15              ROC_AUC    0.781765    0.805806\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_xgb_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_1_cat = np.where(((y_pred_xgb_1 >= 2) | (y_pred_xgb_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:15:11,720] Trial 100 finished with value: 0.6997188604064772 and parameters: {'n_estimators': 812, 'eta': 0.05073513491098818, 'max_depth': 7, 'alpha': 0.2049, 'lambda': 6.824320784530791, 'max_bin': 315}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:15:23,412] Trial 101 finished with value: 0.6995892362665534 and parameters: {'n_estimators': 737, 'eta': 0.04305669541883183, 'max_depth': 12, 'alpha': 0.44970000000000004, 'lambda': 9.847918223537599, 'max_bin': 360}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:15:33,012] Trial 102 finished with value: 0.6995090779233109 and parameters: {'n_estimators': 711, 'eta': 0.0512590593282467, 'max_depth': 12, 'alpha': 0.4007, 'lambda': 12.161026574617686, 'max_bin': 292}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:15:42,378] Trial 103 finished with value: 0.6992316498839461 and parameters: {'n_estimators': 657, 'eta': 0.05464741339209051, 'max_depth': 12, 'alpha': 0.47800000000000004, 'lambda': 8.922011185143665, 'max_bin': 379}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:15:53,828] Trial 104 finished with value: 0.7006674799731394 and parameters: {'n_estimators': 879, 'eta': 0.045738457574912295, 'max_depth': 12, 'alpha': 0.4173, 'lambda': 11.17172531880605, 'max_bin': 336}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:16:04,971] Trial 105 finished with value: 0.7021098840526236 and parameters: {'n_estimators': 856, 'eta': 0.04932785086695054, 'max_depth': 12, 'alpha': 0.45580000000000004, 'lambda': 14.237904686487505, 'max_bin': 371}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:16:14,784] Trial 106 finished with value: 0.7038329762859984 and parameters: {'n_estimators': 800, 'eta': 0.04204040358274026, 'max_depth': 11, 'alpha': 0.525, 'lambda': 8.070164292094177, 'max_bin': 353}. Best is trial 21 with value: 0.7047860827651752.\n",
      "[I 2023-12-12 00:16:27,536] Trial 107 finished with value: 0.7050395954895138 and parameters: {'n_estimators': 826, 'eta': 0.03729065338760725, 'max_depth': 11, 'alpha': 0.5487000000000001, 'lambda': 8.087573962009671, 'max_bin': 353}. Best is trial 107 with value: 0.7050395954895138.\n",
      "[I 2023-12-12 00:16:40,532] Trial 108 finished with value: 0.704072951096302 and parameters: {'n_estimators': 829, 'eta': 0.03790665776826645, 'max_depth': 11, 'alpha': 0.5288, 'lambda': 7.704854824968923, 'max_bin': 356}. Best is trial 107 with value: 0.7050395954895138.\n",
      "[I 2023-12-12 00:16:53,572] Trial 109 finished with value: 0.7029879881191125 and parameters: {'n_estimators': 825, 'eta': 0.03696935064106667, 'max_depth': 10, 'alpha': 0.5342, 'lambda': 7.856024500757415, 'max_bin': 352}. Best is trial 107 with value: 0.7050395954895138.\n",
      "[I 2023-12-12 00:17:04,424] Trial 110 finished with value: 0.7058399642279453 and parameters: {'n_estimators': 881, 'eta': 0.0425471946023848, 'max_depth': 11, 'alpha': 0.5921000000000001, 'lambda': 6.4976493189138225, 'max_bin': 345}. Best is trial 110 with value: 0.7058399642279453.\n",
      "[I 2023-12-12 00:17:14,800] Trial 111 finished with value: 0.7010952208490576 and parameters: {'n_estimators': 883, 'eta': 0.0423426269481467, 'max_depth': 11, 'alpha': 0.5546, 'lambda': 6.815498186095971, 'max_bin': 356}. Best is trial 110 with value: 0.7058399642279453.\n",
      "[I 2023-12-12 00:17:28,235] Trial 112 finished with value: 0.7033662331141571 and parameters: {'n_estimators': 842, 'eta': 0.034304733148308494, 'max_depth': 11, 'alpha': 0.5905, 'lambda': 8.971040453581201, 'max_bin': 362}. Best is trial 110 with value: 0.7058399642279453.\n",
      "[I 2023-12-12 00:17:39,571] Trial 113 finished with value: 0.7021201368568957 and parameters: {'n_estimators': 848, 'eta': 0.03403567157578446, 'max_depth': 11, 'alpha': 0.6598, 'lambda': 4.923780966372892, 'max_bin': 361}. Best is trial 110 with value: 0.7058399642279453.\n",
      "[I 2023-12-12 00:17:52,463] Trial 114 finished with value: 0.7015831527230796 and parameters: {'n_estimators': 872, 'eta': 0.034674677223475336, 'max_depth': 11, 'alpha': 0.6403, 'lambda': 9.109855289004395, 'max_bin': 350}. Best is trial 110 with value: 0.7058399642279453.\n",
      "[I 2023-12-12 00:18:03,368] Trial 115 finished with value: 0.7055134978011676 and parameters: {'n_estimators': 800, 'eta': 0.037894506772383055, 'max_depth': 10, 'alpha': 0.5983, 'lambda': 6.333035980581878, 'max_bin': 368}. Best is trial 110 with value: 0.7058399642279453.\n",
      "[I 2023-12-12 00:18:15,046] Trial 116 finished with value: 0.7033521301764363 and parameters: {'n_estimators': 797, 'eta': 0.03794495686613218, 'max_depth': 10, 'alpha': 0.5211, 'lambda': 5.502695218190655, 'max_bin': 354}. Best is trial 110 with value: 0.7058399642279453.\n",
      "[I 2023-12-12 00:18:27,309] Trial 117 finished with value: 0.7065008514788376 and parameters: {'n_estimators': 809, 'eta': 0.03983291017054692, 'max_depth': 11, 'alpha': 0.5553, 'lambda': 6.2342513635974335, 'max_bin': 342}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:18:38,047] Trial 118 finished with value: 0.7004886852808052 and parameters: {'n_estimators': 821, 'eta': 0.040688519305567425, 'max_depth': 11, 'alpha': 0.5607, 'lambda': 4.323136806278876, 'max_bin': 344}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:18:50,790] Trial 119 finished with value: 0.7037490027934413 and parameters: {'n_estimators': 803, 'eta': 0.03176565454953285, 'max_depth': 11, 'alpha': 0.6054, 'lambda': 6.191966276490419, 'max_bin': 366}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:19:02,709] Trial 120 finished with value: 0.6981785404132576 and parameters: {'n_estimators': 804, 'eta': 0.02855363543575207, 'max_depth': 10, 'alpha': 0.6062000000000001, 'lambda': 6.417889857259323, 'max_bin': 370}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:19:14,719] Trial 121 finished with value: 0.7005901036568485 and parameters: {'n_estimators': 861, 'eta': 0.036987646673828284, 'max_depth': 11, 'alpha': 0.5675, 'lambda': 7.4390156437059165, 'max_bin': 340}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:19:23,288] Trial 122 finished with value: 0.7022612129387437 and parameters: {'n_estimators': 762, 'eta': 0.039338474519472984, 'max_depth': 11, 'alpha': 0.5832, 'lambda': 3.774667230815917, 'max_bin': 375}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:19:36,586] Trial 123 finished with value: 0.7023785728179422 and parameters: {'n_estimators': 781, 'eta': 0.030100471725632502, 'max_depth': 11, 'alpha': 0.4948, 'lambda': 6.183775933863661, 'max_bin': 390}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:19:49,317] Trial 124 finished with value: 0.7054731496198061 and parameters: {'n_estimators': 900, 'eta': 0.03137813714601283, 'max_depth': 10, 'alpha': 0.5508000000000001, 'lambda': 5.119196991344819, 'max_bin': 364}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:20:02,291] Trial 125 finished with value: 0.7012246652388924 and parameters: {'n_estimators': 884, 'eta': 0.02682583881135882, 'max_depth': 9, 'alpha': 0.5436, 'lambda': 5.180012280944226, 'max_bin': 367}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:20:13,377] Trial 126 finished with value: 0.703024409084601 and parameters: {'n_estimators': 900, 'eta': 0.03212322907938612, 'max_depth': 10, 'alpha': 0.5233, 'lambda': 2.8850197941121456, 'max_bin': 348}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:20:25,645] Trial 127 finished with value: 0.702433782294324 and parameters: {'n_estimators': 836, 'eta': 0.03589827852978499, 'max_depth': 10, 'alpha': 0.6425000000000001, 'lambda': 8.121927804951458, 'max_bin': 331}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:20:37,138] Trial 128 finished with value: 0.701778253070399 and parameters: {'n_estimators': 865, 'eta': 0.031111537359193687, 'max_depth': 11, 'alpha': 0.5892000000000001, 'lambda': 4.273755779997311, 'max_bin': 357}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:20:49,082] Trial 129 finished with value: 0.7004771997955195 and parameters: {'n_estimators': 816, 'eta': 0.025221659591867007, 'max_depth': 8, 'alpha': 0.49360000000000004, 'lambda': 6.643047716681901, 'max_bin': 380}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:20:56,201] Trial 130 finished with value: 0.700074522615606 and parameters: {'n_estimators': 473, 'eta': 0.04218740493569564, 'max_depth': 9, 'alpha': 0.6785, 'lambda': 5.754643744385158, 'max_bin': 364}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:21:05,136] Trial 131 finished with value: 0.7022028792000385 and parameters: {'n_estimators': 843, 'eta': 0.03808098895058929, 'max_depth': 11, 'alpha': 0.5573, 'lambda': 7.418572187961485, 'max_bin': 371}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:21:12,996] Trial 132 finished with value: 0.7009936388771202 and parameters: {'n_estimators': 800, 'eta': 0.044283106447249795, 'max_depth': 11, 'alpha': 0.0027, 'lambda': 8.139348935417612, 'max_bin': 337}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:21:21,693] Trial 133 finished with value: 0.703123870044373 and parameters: {'n_estimators': 880, 'eta': 0.03998902279457747, 'max_depth': 11, 'alpha': 0.5969, 'lambda': 6.986075777568454, 'max_bin': 352}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:21:31,363] Trial 134 finished with value: 0.7019842008049724 and parameters: {'n_estimators': 857, 'eta': 0.035484260008232765, 'max_depth': 11, 'alpha': 0.5301, 'lambda': 5.399388165192624, 'max_bin': 308}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:21:42,949] Trial 135 finished with value: 0.7047202683089551 and parameters: {'n_estimators': 824, 'eta': 0.03275595576802256, 'max_depth': 10, 'alpha': 0.5119, 'lambda': 9.658603341018738, 'max_bin': 346}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:21:54,770] Trial 136 finished with value: 0.705946168637392 and parameters: {'n_estimators': 900, 'eta': 0.032950019146695916, 'max_depth': 10, 'alpha': 0.47800000000000004, 'lambda': 9.522631574680135, 'max_bin': 346}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:22:05,612] Trial 137 finished with value: 0.7039802718619914 and parameters: {'n_estimators': 828, 'eta': 0.032488805078389096, 'max_depth': 10, 'alpha': 0.47150000000000003, 'lambda': 9.716217214377043, 'max_bin': 344}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:22:16,451] Trial 138 finished with value: 0.7045915382977624 and parameters: {'n_estimators': 827, 'eta': 0.032725164668102665, 'max_depth': 10, 'alpha': 0.5124000000000001, 'lambda': 9.64281844007711, 'max_bin': 343}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:22:27,820] Trial 139 finished with value: 0.7049076635453562 and parameters: {'n_estimators': 829, 'eta': 0.028250935976236194, 'max_depth': 10, 'alpha': 0.5151, 'lambda': 9.387137226600103, 'max_bin': 343}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:22:39,021] Trial 140 finished with value: 0.7042376737855076 and parameters: {'n_estimators': 828, 'eta': 0.029652665759917124, 'max_depth': 10, 'alpha': 0.47290000000000004, 'lambda': 9.436789926514443, 'max_bin': 345}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:22:50,666] Trial 141 finished with value: 0.7045381807378016 and parameters: {'n_estimators': 830, 'eta': 0.028539017327925974, 'max_depth': 10, 'alpha': 0.5017, 'lambda': 9.570956220977383, 'max_bin': 340}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:23:02,662] Trial 142 finished with value: 0.7055753909101544 and parameters: {'n_estimators': 831, 'eta': 0.029123164712027614, 'max_depth': 10, 'alpha': 0.5107, 'lambda': 10.087861055137216, 'max_bin': 344}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:23:15,367] Trial 143 finished with value: 0.7039689857354475 and parameters: {'n_estimators': 839, 'eta': 0.02853203043779002, 'max_depth': 10, 'alpha': 0.5072, 'lambda': 10.491186824929944, 'max_bin': 340}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:23:28,088] Trial 144 finished with value: 0.7014200012197305 and parameters: {'n_estimators': 870, 'eta': 0.025918371930828916, 'max_depth': 10, 'alpha': 0.5052, 'lambda': 9.663671682690659, 'max_bin': 333}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:23:40,842] Trial 145 finished with value: 0.7039314881692323 and parameters: {'n_estimators': 827, 'eta': 0.029449117188740423, 'max_depth': 10, 'alpha': 0.5545, 'lambda': 10.460448702736034, 'max_bin': 349}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:23:53,246] Trial 146 finished with value: 0.7000672116158938 and parameters: {'n_estimators': 781, 'eta': 0.022338855595981262, 'max_depth': 10, 'alpha': 0.48100000000000004, 'lambda': 8.771177597275665, 'max_bin': 327}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:24:04,607] Trial 147 finished with value: 0.7047923303020669 and parameters: {'n_estimators': 852, 'eta': 0.027764865164692894, 'max_depth': 10, 'alpha': 0.5744, 'lambda': 9.690291716982287, 'max_bin': 345}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:24:15,846] Trial 148 finished with value: 0.7007367976717117 and parameters: {'n_estimators': 857, 'eta': 0.027705764771780318, 'max_depth': 10, 'alpha': 0.5712, 'lambda': 11.594827012496612, 'max_bin': 345}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:24:29,192] Trial 149 finished with value: 0.7001496706519347 and parameters: {'n_estimators': 898, 'eta': 0.02560847328753192, 'max_depth': 10, 'alpha': 0.8533000000000001, 'lambda': 10.063034867636098, 'max_bin': 335}. Best is trial 117 with value: 0.7065008514788376.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7065\n",
      "\tBest params:\n",
      "\t\tn_estimators: 809\n",
      "\t\teta: 0.03983291017054692\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.5553\n",
      "\t\tlambda: 6.2342513635974335\n",
      "\t\tmax_bin: 342\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.739481    0.710332    0.690333\n",
      "1                    TP   39.000000   42.000000   37.000000\n",
      "2                    TN  198.000000  197.000000  198.000000\n",
      "3                    FP    2.000000    5.000000    2.000000\n",
      "4                    FN   29.000000   24.000000   31.000000\n",
      "5              Accuracy    0.884328    0.891791    0.876866\n",
      "6             Precision    0.951220    0.893617    0.948718\n",
      "7           Sensitivity    0.573529    0.636364    0.544118\n",
      "8           Specificity    0.990000    0.975200    0.990000\n",
      "9              F1 score    0.715596    0.743363    0.691589\n",
      "10  F1 score (weighted)    0.873659    0.885124    0.864341\n",
      "11     F1 score (macro)    0.821498    0.837402    0.807333\n",
      "12    Balanced Accuracy    0.781765    0.805806    0.767059\n",
      "13                  MCC    0.681210    0.692912    0.659108\n",
      "14                  NPV    0.872200    0.891400    0.864600\n",
      "15              ROC_AUC    0.781765    0.805806    0.767059\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_xgb_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_2_cat = np.where(((y_pred_xgb_2 >= 2) | (y_pred_xgb_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:24:39,698] Trial 150 finished with value: 0.6909768761557428 and parameters: {'n_estimators': 849, 'eta': 0.030161684895254802, 'max_depth': 9, 'alpha': 0.5481, 'lambda': 9.371385313802572, 'max_bin': 341}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:24:52,158] Trial 151 finished with value: 0.6885533222409028 and parameters: {'n_estimators': 818, 'eta': 0.025034661642812588, 'max_depth': 10, 'alpha': 0.5114000000000001, 'lambda': 10.960344866709555, 'max_bin': 347}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:25:03,199] Trial 152 finished with value: 0.6918585988267041 and parameters: {'n_estimators': 868, 'eta': 0.027551077622518398, 'max_depth': 10, 'alpha': 0.538, 'lambda': 8.829320801136323, 'max_bin': 355}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:25:14,749] Trial 153 finished with value: 0.6905402914111208 and parameters: {'n_estimators': 830, 'eta': 0.02360136256524159, 'max_depth': 10, 'alpha': 0.46690000000000004, 'lambda': 7.716833008123723, 'max_bin': 338}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:25:24,275] Trial 154 finished with value: 0.6907107656975706 and parameters: {'n_estimators': 876, 'eta': 0.033092606682516235, 'max_depth': 9, 'alpha': 0.5832, 'lambda': 9.522515605113345, 'max_bin': 329}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:25:34,203] Trial 155 finished with value: 0.6892900111186396 and parameters: {'n_estimators': 846, 'eta': 0.030878427303396953, 'max_depth': 10, 'alpha': 0.43760000000000004, 'lambda': 11.880900293255035, 'max_bin': 359}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:25:45,247] Trial 156 finished with value: 0.6918705371759414 and parameters: {'n_estimators': 815, 'eta': 0.02896327127229013, 'max_depth': 10, 'alpha': 0.5182, 'lambda': 10.519580457387281, 'max_bin': 345}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:25:54,054] Trial 157 finished with value: 0.6898205831493611 and parameters: {'n_estimators': 792, 'eta': 0.03409048207954296, 'max_depth': 10, 'alpha': 0.4983, 'lambda': 8.409923912556499, 'max_bin': 350}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:26:05,802] Trial 158 finished with value: 0.6871947271218397 and parameters: {'n_estimators': 888, 'eta': 0.0205923138833339, 'max_depth': 10, 'alpha': 0.6181, 'lambda': 7.164374014344814, 'max_bin': 341}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:26:15,511] Trial 159 finished with value: 0.6887919965125902 and parameters: {'n_estimators': 837, 'eta': 0.032626870960155235, 'max_depth': 10, 'alpha': 0.5632, 'lambda': 8.685582784500784, 'max_bin': 333}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:26:26,312] Trial 160 finished with value: 0.6899296315373055 and parameters: {'n_estimators': 862, 'eta': 0.027243453966563785, 'max_depth': 10, 'alpha': 0.48460000000000003, 'lambda': 13.200238605340768, 'max_bin': 320}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:26:36,685] Trial 161 finished with value: 0.6891117071175469 and parameters: {'n_estimators': 818, 'eta': 0.03169341148737751, 'max_depth': 10, 'alpha': 0.4682, 'lambda': 9.761786936120092, 'max_bin': 345}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:26:46,871] Trial 162 finished with value: 0.6915148167733742 and parameters: {'n_estimators': 829, 'eta': 0.035343656433139636, 'max_depth': 10, 'alpha': 0.45320000000000005, 'lambda': 10.991558296240768, 'max_bin': 355}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:26:56,027] Trial 163 finished with value: 0.6872571634500968 and parameters: {'n_estimators': 780, 'eta': 0.03320002432497342, 'max_depth': 10, 'alpha': 0.5411, 'lambda': 9.596767055092263, 'max_bin': 343}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:27:06,188] Trial 164 finished with value: 0.6942744741709308 and parameters: {'n_estimators': 853, 'eta': 0.02979122387379442, 'max_depth': 10, 'alpha': 0.49570000000000003, 'lambda': 7.900577383989817, 'max_bin': 350}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:27:14,281] Trial 165 finished with value: 0.6874097026621685 and parameters: {'n_estimators': 807, 'eta': 0.036767697351622275, 'max_depth': 9, 'alpha': 0.4767, 'lambda': 10.223802925676384, 'max_bin': 339}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:27:22,544] Trial 166 finished with value: 0.688863056159129 and parameters: {'n_estimators': 900, 'eta': 0.032224405028719136, 'max_depth': 10, 'alpha': 0.5209, 'lambda': 9.260554922036068, 'max_bin': 362}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:27:30,981] Trial 167 finished with value: 0.6892157471309636 and parameters: {'n_estimators': 834, 'eta': 0.034260678316486236, 'max_depth': 10, 'alpha': 0.5318, 'lambda': 8.509119261358423, 'max_bin': 326}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:27:42,454] Trial 168 finished with value: 0.6913842785499711 and parameters: {'n_estimators': 879, 'eta': 0.02854733113631019, 'max_depth': 10, 'alpha': 0.5781000000000001, 'lambda': 12.155734419025649, 'max_bin': 334}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:27:48,932] Trial 169 finished with value: 0.6882377413926436 and parameters: {'n_estimators': 794, 'eta': 0.03766876925785889, 'max_depth': 10, 'alpha': 0.4297, 'lambda': 6.869962506155348, 'max_bin': 357}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:27:57,514] Trial 170 finished with value: 0.6888603771991171 and parameters: {'n_estimators': 763, 'eta': 0.03038407381775654, 'max_depth': 9, 'alpha': 0.55, 'lambda': 10.915366065991952, 'max_bin': 345}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:28:08,917] Trial 171 finished with value: 0.6913318526302452 and parameters: {'n_estimators': 839, 'eta': 0.027925074172738505, 'max_depth': 10, 'alpha': 0.5087, 'lambda': 10.34223449979543, 'max_bin': 340}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:28:20,108] Trial 172 finished with value: 0.691425848692561 and parameters: {'n_estimators': 852, 'eta': 0.026715542800773828, 'max_depth': 10, 'alpha': 0.49400000000000005, 'lambda': 9.898109425103542, 'max_bin': 350}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:28:26,418] Trial 173 finished with value: 0.6797641118787168 and parameters: {'n_estimators': 280, 'eta': 0.0237295464410476, 'max_depth': 10, 'alpha': 0.462, 'lambda': 9.168739807041538, 'max_bin': 337}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:28:36,161] Trial 174 finished with value: 0.6913271953545758 and parameters: {'n_estimators': 822, 'eta': 0.0319425777618362, 'max_depth': 10, 'alpha': 0.5084000000000001, 'lambda': 7.885704174552785, 'max_bin': 342}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:28:45,825] Trial 175 finished with value: 0.6901031614873891 and parameters: {'n_estimators': 867, 'eta': 0.03512108421560267, 'max_depth': 10, 'alpha': 0.5999, 'lambda': 11.38855067358347, 'max_bin': 353}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:28:56,501] Trial 176 finished with value: 0.6885641340741682 and parameters: {'n_estimators': 839, 'eta': 0.030146379015496803, 'max_depth': 10, 'alpha': 0.6407, 'lambda': 10.4987327443, 'max_bin': 330}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:29:09,208] Trial 177 finished with value: 0.6892249496384819 and parameters: {'n_estimators': 816, 'eta': 0.02911647002673981, 'max_depth': 10, 'alpha': 0.5690000000000001, 'lambda': 17.187001790571312, 'max_bin': 347}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:29:17,301] Trial 178 finished with value: 0.6903380641656236 and parameters: {'n_estimators': 882, 'eta': 0.03584179769008226, 'max_depth': 9, 'alpha': 0.5254, 'lambda': 6.297020902567143, 'max_bin': 361}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:29:25,915] Trial 179 finished with value: 0.6871275351628816 and parameters: {'n_estimators': 863, 'eta': 0.03340020197393002, 'max_depth': 10, 'alpha': 0.554, 'lambda': 7.359154539315234, 'max_bin': 338}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:29:33,606] Trial 180 finished with value: 0.6872953198776792 and parameters: {'n_estimators': 805, 'eta': 0.03866021910253705, 'max_depth': 10, 'alpha': 0.4822, 'lambda': 8.560719951338505, 'max_bin': 355}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:29:41,369] Trial 181 finished with value: 0.6833233805980024 and parameters: {'n_estimators': 833, 'eta': 0.03943936832746976, 'max_depth': 10, 'alpha': 0.509, 'lambda': 9.286836839075214, 'max_bin': 374}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:29:49,625] Trial 182 finished with value: 0.6869471944259302 and parameters: {'n_estimators': 783, 'eta': 0.03772542061403702, 'max_depth': 10, 'alpha': 0.4544, 'lambda': 10.028176760566936, 'max_bin': 365}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:30:00,132] Trial 183 finished with value: 0.6879735246479226 and parameters: {'n_estimators': 848, 'eta': 0.031905640009758006, 'max_depth': 10, 'alpha': 0.4808, 'lambda': 15.88415456481496, 'max_bin': 348}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:30:11,695] Trial 184 finished with value: 0.6929950757958175 and parameters: {'n_estimators': 812, 'eta': 0.0251550299710225, 'max_depth': 10, 'alpha': 0.5368, 'lambda': 8.435049837725067, 'max_bin': 359}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:30:23,100] Trial 185 finished with value: 0.6924491858709934 and parameters: {'n_estimators': 883, 'eta': 0.03657620686588438, 'max_depth': 10, 'alpha': 0.44220000000000004, 'lambda': 21.927112811662226, 'max_bin': 343}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:30:34,650] Trial 186 finished with value: 0.6919365329676718 and parameters: {'n_estimators': 828, 'eta': 0.028008112037035304, 'max_depth': 11, 'alpha': 0.5021, 'lambda': 11.460678443368815, 'max_bin': 333}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:30:43,708] Trial 187 finished with value: 0.6907478219208365 and parameters: {'n_estimators': 794, 'eta': 0.03382771512972176, 'max_depth': 11, 'alpha': 0.5840000000000001, 'lambda': 4.919469286014193, 'max_bin': 368}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:30:51,209] Trial 188 finished with value: 0.6853977694870851 and parameters: {'n_estimators': 865, 'eta': 0.04036678556566371, 'max_depth': 10, 'alpha': 0.521, 'lambda': 9.114223327073855, 'max_bin': 352}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:31:01,363] Trial 189 finished with value: 0.6923528544805693 and parameters: {'n_estimators': 899, 'eta': 0.03091467581858455, 'max_depth': 10, 'alpha': 0.4698, 'lambda': 7.655963907030951, 'max_bin': 323}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:31:11,308] Trial 190 finished with value: 0.6890767730218958 and parameters: {'n_estimators': 752, 'eta': 0.035429755275748015, 'max_depth': 10, 'alpha': 0.5484, 'lambda': 12.739220808846898, 'max_bin': 346}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:31:23,094] Trial 191 finished with value: 0.6911692363582354 and parameters: {'n_estimators': 830, 'eta': 0.028895537242123227, 'max_depth': 10, 'alpha': 0.5528000000000001, 'lambda': 10.72405106108771, 'max_bin': 350}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:31:34,216] Trial 192 finished with value: 0.689481254825828 and parameters: {'n_estimators': 844, 'eta': 0.026479606842389054, 'max_depth': 10, 'alpha': 0.5704, 'lambda': 10.159414303429921, 'max_bin': 342}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:31:45,765] Trial 193 finished with value: 0.6904406677250104 and parameters: {'n_estimators': 822, 'eta': 0.029854803786625903, 'max_depth': 10, 'alpha': 0.5319, 'lambda': 9.641424631227071, 'max_bin': 356}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:31:58,078] Trial 194 finished with value: 0.6894144438773384 and parameters: {'n_estimators': 853, 'eta': 0.03216635593468453, 'max_depth': 10, 'alpha': 0.6065, 'lambda': 10.771454008931597, 'max_bin': 349}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:32:11,713] Trial 195 finished with value: 0.6913361277218882 and parameters: {'n_estimators': 805, 'eta': 0.0239728014897164, 'max_depth': 11, 'alpha': 0.49450000000000005, 'lambda': 8.662849585267441, 'max_bin': 337}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:32:21,718] Trial 196 finished with value: 0.6885228830114318 and parameters: {'n_estimators': 774, 'eta': 0.029071581634747923, 'max_depth': 10, 'alpha': 0.5161, 'lambda': 11.977419830950604, 'max_bin': 359}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:32:32,788] Trial 197 finished with value: 0.690378103107673 and parameters: {'n_estimators': 872, 'eta': 0.026614826074015056, 'max_depth': 10, 'alpha': 0.5555, 'lambda': 6.676192507330567, 'max_bin': 364}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:32:41,735] Trial 198 finished with value: 0.6883043379650963 and parameters: {'n_estimators': 827, 'eta': 0.03839012326593968, 'max_depth': 11, 'alpha': 0.47950000000000004, 'lambda': 7.861057307778428, 'max_bin': 346}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:32:49,281] Trial 199 finished with value: 0.686989565624579 and parameters: {'n_estimators': 845, 'eta': 0.042547478702100794, 'max_depth': 10, 'alpha': 0.5912000000000001, 'lambda': 5.767027860177867, 'max_bin': 338}. Best is trial 117 with value: 0.7065008514788376.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7065\n",
      "\tBest params:\n",
      "\t\tn_estimators: 809\n",
      "\t\teta: 0.03983291017054692\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.5553\n",
      "\t\tlambda: 6.2342513635974335\n",
      "\t\tmax_bin: 342\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.739481    0.710332    0.690333    0.755649\n",
      "1                    TP   39.000000   42.000000   37.000000   36.000000\n",
      "2                    TN  198.000000  197.000000  198.000000  199.000000\n",
      "3                    FP    2.000000    5.000000    2.000000    2.000000\n",
      "4                    FN   29.000000   24.000000   31.000000   31.000000\n",
      "5              Accuracy    0.884328    0.891791    0.876866    0.876866\n",
      "6             Precision    0.951220    0.893617    0.948718    0.947368\n",
      "7           Sensitivity    0.573529    0.636364    0.544118    0.537313\n",
      "8           Specificity    0.990000    0.975200    0.990000    0.990000\n",
      "9              F1 score    0.715596    0.743363    0.691589    0.685714\n",
      "10  F1 score (weighted)    0.873659    0.885124    0.864341    0.864004\n",
      "11     F1 score (macro)    0.821498    0.837402    0.807333    0.804574\n",
      "12    Balanced Accuracy    0.781765    0.805806    0.767059    0.763682\n",
      "13                  MCC    0.681210    0.692912    0.659108    0.654620\n",
      "14                  NPV    0.872200    0.891400    0.864600    0.865200\n",
      "15              ROC_AUC    0.781765    0.805806    0.767059    0.763682\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_xgb_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_3_cat = np.where(((y_pred_xgb_3 >= 2) | (y_pred_xgb_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:33:01,621] Trial 200 finished with value: 0.6966557048711929 and parameters: {'n_estimators': 812, 'eta': 0.021565904896055355, 'max_depth': 10, 'alpha': 0.6223000000000001, 'lambda': 9.714332333067222, 'max_bin': 330}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:33:09,712] Trial 201 finished with value: 0.6943579849555617 and parameters: {'n_estimators': 796, 'eta': 0.041973238779491266, 'max_depth': 11, 'alpha': 0.5288, 'lambda': 8.41250078704469, 'max_bin': 352}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:33:16,191] Trial 202 finished with value: 0.6916843136038863 and parameters: {'n_estimators': 792, 'eta': 0.040756579370986104, 'max_depth': 11, 'alpha': 0.5124000000000001, 'lambda': 7.244939425768737, 'max_bin': 353}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:33:24,917] Trial 203 finished with value: 0.6914544826269473 and parameters: {'n_estimators': 823, 'eta': 0.033549797583413234, 'max_depth': 11, 'alpha': 0.5365, 'lambda': 9.212162017564067, 'max_bin': 342}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:33:32,978] Trial 204 finished with value: 0.6916594287359688 and parameters: {'n_estimators': 860, 'eta': 0.03710903238280451, 'max_depth': 11, 'alpha': 0.5657, 'lambda': 10.402943287100483, 'max_bin': 348}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:33:43,541] Trial 205 finished with value: 0.6918709113147846 and parameters: {'n_estimators': 839, 'eta': 0.03974148464621366, 'max_depth': 11, 'alpha': 0.49400000000000005, 'lambda': 19.251023489219122, 'max_bin': 356}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:33:52,323] Trial 206 finished with value: 0.6910582200136314 and parameters: {'n_estimators': 770, 'eta': 0.03093946812956521, 'max_depth': 10, 'alpha': 0.5435, 'lambda': 8.19357173831419, 'max_bin': 369}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:34:00,232] Trial 207 finished with value: 0.6928073121916367 and parameters: {'n_estimators': 885, 'eta': 0.043198755169593546, 'max_depth': 10, 'alpha': 0.46440000000000003, 'lambda': 11.115281421976785, 'max_bin': 362}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:34:08,602] Trial 208 finished with value: 0.6946580799751343 and parameters: {'n_estimators': 809, 'eta': 0.0354630585755179, 'max_depth': 9, 'alpha': 0.5188, 'lambda': 9.394990793324595, 'max_bin': 344}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:34:18,702] Trial 209 finished with value: 0.6922911313952558 and parameters: {'n_estimators': 850, 'eta': 0.027864067367613588, 'max_depth': 11, 'alpha': 0.49610000000000004, 'lambda': 6.732904469351832, 'max_bin': 334}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:34:22,997] Trial 210 finished with value: 0.6883099471135297 and parameters: {'n_estimators': 549, 'eta': 0.0899959964254724, 'max_depth': 10, 'alpha': 0.5743, 'lambda': 7.799659014285971, 'max_bin': 350}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:34:29,834] Trial 211 finished with value: 0.6896614608459696 and parameters: {'n_estimators': 410, 'eta': 0.03213907647857251, 'max_depth': 11, 'alpha': 0.5920000000000001, 'lambda': 5.9655446563234475, 'max_bin': 365}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:34:37,958] Trial 212 finished with value: 0.6863771675449641 and parameters: {'n_estimators': 803, 'eta': 0.030318608575523133, 'max_depth': 11, 'alpha': 0.6144000000000001, 'lambda': 4.215428783888046, 'max_bin': 357}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:34:45,692] Trial 213 finished with value: 0.6915674150585683 and parameters: {'n_estimators': 827, 'eta': 0.032052321657320686, 'max_depth': 11, 'alpha': 0.5541, 'lambda': 6.120937759531772, 'max_bin': 342}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:34:53,898] Trial 214 finished with value: 0.6913069853175957 and parameters: {'n_estimators': 791, 'eta': 0.03408952280571555, 'max_depth': 11, 'alpha': 0.6415000000000001, 'lambda': 8.72695370653335, 'max_bin': 368}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:34:59,995] Trial 215 finished with value: 0.6907833998799265 and parameters: {'n_estimators': 810, 'eta': 0.03785670174205852, 'max_depth': 10, 'alpha': 0.5236000000000001, 'lambda': 5.0381821896632495, 'max_bin': 378}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:35:08,225] Trial 216 finished with value: 0.6874819536369278 and parameters: {'n_estimators': 870, 'eta': 0.029143719860473514, 'max_depth': 11, 'alpha': 0.48960000000000004, 'lambda': 3.422898238649143, 'max_bin': 352}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:35:21,110] Trial 217 finished with value: 0.6906122901626837 and parameters: {'n_estimators': 835, 'eta': 0.025841709955387468, 'max_depth': 10, 'alpha': 0.44170000000000004, 'lambda': 25.346252026900096, 'max_bin': 338}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:35:27,794] Trial 218 finished with value: 0.6908463477499185 and parameters: {'n_estimators': 819, 'eta': 0.044908197552109064, 'max_depth': 10, 'alpha': 0.5973, 'lambda': 9.814967127096208, 'max_bin': 373}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:35:34,295] Trial 219 finished with value: 0.6886479613789248 and parameters: {'n_estimators': 782, 'eta': 0.035253333182575834, 'max_depth': 10, 'alpha': 0.5685, 'lambda': 7.314608397292439, 'max_bin': 361}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:35:40,409] Trial 220 finished with value: 0.6924724387100287 and parameters: {'n_estimators': 856, 'eta': 0.04075825839590853, 'max_depth': 8, 'alpha': 0.5388000000000001, 'lambda': 10.185887894296853, 'max_bin': 346}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:35:47,415] Trial 221 finished with value: 0.6939655300712954 and parameters: {'n_estimators': 900, 'eta': 0.04147004663558449, 'max_depth': 11, 'alpha': 0.46490000000000004, 'lambda': 6.715782171286045, 'max_bin': 317}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:35:55,271] Trial 222 finished with value: 0.6929570773402884 and parameters: {'n_estimators': 885, 'eta': 0.03675935391817268, 'max_depth': 10, 'alpha': 0.4141, 'lambda': 8.57017725037406, 'max_bin': 310}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:36:03,794] Trial 223 finished with value: 0.6903792948374241 and parameters: {'n_estimators': 886, 'eta': 0.03890463483436571, 'max_depth': 11, 'alpha': 0.5052, 'lambda': 7.741058649749049, 'max_bin': 321}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:36:09,556] Trial 224 finished with value: 0.6932335849962847 and parameters: {'n_estimators': 870, 'eta': 0.04310549869436002, 'max_depth': 10, 'alpha': 0.4737, 'lambda': 6.007091617832808, 'max_bin': 354}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:36:18,006] Trial 225 finished with value: 0.6899509850524896 and parameters: {'n_estimators': 838, 'eta': 0.030886867563367094, 'max_depth': 12, 'alpha': 0.5178, 'lambda': 9.173842828246743, 'max_bin': 341}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:36:25,671] Trial 226 finished with value: 0.6892699729652053 and parameters: {'n_estimators': 806, 'eta': 0.033491178269890204, 'max_depth': 10, 'alpha': 0.5483, 'lambda': 2.139489217179296, 'max_bin': 345}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:36:31,547] Trial 227 finished with value: 0.6896357116212888 and parameters: {'n_estimators': 899, 'eta': 0.04636338550807853, 'max_depth': 11, 'alpha': 0.4853, 'lambda': 8.299004599217284, 'max_bin': 329}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:36:45,181] Trial 228 finished with value: 0.6873580502139203 and parameters: {'n_estimators': 848, 'eta': 0.027990789860972184, 'max_depth': 10, 'alpha': 0.4484, 'lambda': 35.16291435672288, 'max_bin': 349}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:36:54,345] Trial 229 finished with value: 0.6941284913956459 and parameters: {'n_estimators': 827, 'eta': 0.039153974517223133, 'max_depth': 11, 'alpha': 0.5045000000000001, 'lambda': 10.553115281015357, 'max_bin': 335}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:37:01,397] Trial 230 finished with value: 0.6924443177829173 and parameters: {'n_estimators': 866, 'eta': 0.03613762837958641, 'max_depth': 10, 'alpha': 0.5803, 'lambda': 5.211894040664955, 'max_bin': 358}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:37:09,130] Trial 231 finished with value: 0.6915760771875771 and parameters: {'n_estimators': 886, 'eta': 0.04436495964524947, 'max_depth': 11, 'alpha': 0.14, 'lambda': 11.627184724672166, 'max_bin': 312}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:37:17,999] Trial 232 finished with value: 0.6937608350225407 and parameters: {'n_estimators': 876, 'eta': 0.04111401578012322, 'max_depth': 11, 'alpha': 0.5333, 'lambda': 9.687141219143667, 'max_bin': 340}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:37:24,383] Trial 233 finished with value: 0.6931123241358074 and parameters: {'n_estimators': 900, 'eta': 0.048160399236098814, 'max_depth': 11, 'alpha': 0.10350000000000001, 'lambda': 10.86015334317242, 'max_bin': 349}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:37:30,815] Trial 234 finished with value: 0.6937354268797378 and parameters: {'n_estimators': 850, 'eta': 0.0425975502774046, 'max_depth': 11, 'alpha': 0.6156, 'lambda': 7.253214583245752, 'max_bin': 352}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:37:38,341] Trial 235 finished with value: 0.6921774263038442 and parameters: {'n_estimators': 820, 'eta': 0.029316096766830682, 'max_depth': 6, 'alpha': 0.5052, 'lambda': 15.16656391229736, 'max_bin': 303}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:37:46,338] Trial 236 finished with value: 0.6946070273152063 and parameters: {'n_estimators': 867, 'eta': 0.04540181290038712, 'max_depth': 12, 'alpha': 0.5582, 'lambda': 13.424614727062798, 'max_bin': 298}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:37:55,943] Trial 237 finished with value: 0.6932426453846097 and parameters: {'n_estimators': 837, 'eta': 0.031552046621393964, 'max_depth': 10, 'alpha': 0.5284, 'lambda': 9.021328868479033, 'max_bin': 305}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:38:04,011] Trial 238 finished with value: 0.6904339359810627 and parameters: {'n_estimators': 805, 'eta': 0.03991951388995585, 'max_depth': 11, 'alpha': 0.4817, 'lambda': 12.350604325632974, 'max_bin': 364}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:38:14,322] Trial 239 finished with value: 0.6944617243024543 and parameters: {'n_estimators': 858, 'eta': 0.024972171006521653, 'max_depth': 10, 'alpha': 0.191, 'lambda': 10.073325929382305, 'max_bin': 345}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:38:23,659] Trial 240 finished with value: 0.6925471815938643 and parameters: {'n_estimators': 885, 'eta': 0.03372361055115386, 'max_depth': 10, 'alpha': 0.4234, 'lambda': 8.177930835555562, 'max_bin': 338}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:38:31,092] Trial 241 finished with value: 0.6943974646892291 and parameters: {'n_estimators': 835, 'eta': 0.03854113080525714, 'max_depth': 11, 'alpha': 0.28950000000000004, 'lambda': 6.694007539248806, 'max_bin': 302}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:38:38,968] Trial 242 finished with value: 0.6936278792200239 and parameters: {'n_estimators': 823, 'eta': 0.03774469786835012, 'max_depth': 11, 'alpha': 0.2546, 'lambda': 5.534436381411736, 'max_bin': 296}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:38:46,810] Trial 243 finished with value: 0.6936654337237915 and parameters: {'n_estimators': 847, 'eta': 0.0410939808645774, 'max_depth': 11, 'alpha': 0.5792, 'lambda': 9.498209829041953, 'max_bin': 307}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:38:56,258] Trial 244 finished with value: 0.691379860665501 and parameters: {'n_estimators': 804, 'eta': 0.02758866351273606, 'max_depth': 11, 'alpha': 0.5113, 'lambda': 7.377777126037548, 'max_bin': 316}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:39:06,638] Trial 245 finished with value: 0.6921788280317299 and parameters: {'n_estimators': 875, 'eta': 0.035787465874455346, 'max_depth': 12, 'alpha': 0.5404, 'lambda': 11.202360671818795, 'max_bin': 355}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:39:09,478] Trial 246 finished with value: 0.6535986314838361 and parameters: {'n_estimators': 110, 'eta': 0.030879081553475875, 'max_depth': 10, 'alpha': 0.0419, 'lambda': 14.1526381850611, 'max_bin': 345}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:39:17,242] Trial 247 finished with value: 0.692713585736992 and parameters: {'n_estimators': 792, 'eta': 0.04345492143090612, 'max_depth': 11, 'alpha': 0.007, 'lambda': 8.870684840045989, 'max_bin': 370}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:39:25,442] Trial 248 finished with value: 0.6879735104354301 and parameters: {'n_estimators': 852, 'eta': 0.03332023121440171, 'max_depth': 12, 'alpha': 0.0402, 'lambda': 6.536168365036864, 'max_bin': 361}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:39:32,041] Trial 249 finished with value: 0.6889916030377191 and parameters: {'n_estimators': 825, 'eta': 0.03979768170819629, 'max_depth': 11, 'alpha': 0.0217, 'lambda': 4.305696535536738, 'max_bin': 292}. Best is trial 117 with value: 0.7065008514788376.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7065\n",
      "\tBest params:\n",
      "\t\tn_estimators: 809\n",
      "\t\teta: 0.03983291017054692\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.5553\n",
      "\t\tlambda: 6.2342513635974335\n",
      "\t\tmax_bin: 342\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.739481    0.710332    0.690333    0.755649   \n",
      "1                    TP   39.000000   42.000000   37.000000   36.000000   \n",
      "2                    TN  198.000000  197.000000  198.000000  199.000000   \n",
      "3                    FP    2.000000    5.000000    2.000000    2.000000   \n",
      "4                    FN   29.000000   24.000000   31.000000   31.000000   \n",
      "5              Accuracy    0.884328    0.891791    0.876866    0.876866   \n",
      "6             Precision    0.951220    0.893617    0.948718    0.947368   \n",
      "7           Sensitivity    0.573529    0.636364    0.544118    0.537313   \n",
      "8           Specificity    0.990000    0.975200    0.990000    0.990000   \n",
      "9              F1 score    0.715596    0.743363    0.691589    0.685714   \n",
      "10  F1 score (weighted)    0.873659    0.885124    0.864341    0.864004   \n",
      "11     F1 score (macro)    0.821498    0.837402    0.807333    0.804574   \n",
      "12    Balanced Accuracy    0.781765    0.805806    0.767059    0.763682   \n",
      "13                  MCC    0.681210    0.692912    0.659108    0.654620   \n",
      "14                  NPV    0.872200    0.891400    0.864600    0.865200   \n",
      "15              ROC_AUC    0.781765    0.805806    0.767059    0.763682   \n",
      "\n",
      "          Set4  \n",
      "0     0.686552  \n",
      "1    40.000000  \n",
      "2   199.000000  \n",
      "3     2.000000  \n",
      "4    27.000000  \n",
      "5     0.891791  \n",
      "6     0.952381  \n",
      "7     0.597015  \n",
      "8     0.990000  \n",
      "9     0.733945  \n",
      "10    0.882549  \n",
      "11    0.833015  \n",
      "12    0.793532  \n",
      "13    0.699266  \n",
      "14    0.880500  \n",
      "15    0.793532  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_xgb_4)\n",
    "# now convert the resuls to binary with cutoff 6.4\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_4_cat = np.where(((y_pred_xgb_4 >= 2) | (y_pred_xgb_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:39:44,471] Trial 250 finished with value: 0.687903088530347 and parameters: {'n_estimators': 837, 'eta': 0.029139771636301152, 'max_depth': 10, 'alpha': 0.48900000000000005, 'lambda': 8.047287387241695, 'max_bin': 349}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:39:53,615] Trial 251 finished with value: 0.6875221983674387 and parameters: {'n_estimators': 885, 'eta': 0.03733706064112344, 'max_depth': 10, 'alpha': 0.062400000000000004, 'lambda': 10.16478254624756, 'max_bin': 310}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:40:03,775] Trial 252 finished with value: 0.6852534715294792 and parameters: {'n_estimators': 861, 'eta': 0.02702530344641878, 'max_depth': 11, 'alpha': 0.4565, 'lambda': 6.241154624704946, 'max_bin': 340}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:40:10,827] Trial 253 finished with value: 0.6846339156842467 and parameters: {'n_estimators': 815, 'eta': 0.04170316119863271, 'max_depth': 10, 'alpha': 0.022000000000000002, 'lambda': 7.4839141276144865, 'max_bin': 325}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:40:20,561] Trial 254 finished with value: 0.6861238242102639 and parameters: {'n_estimators': 786, 'eta': 0.032023379266865146, 'max_depth': 10, 'alpha': 0.6056, 'lambda': 9.090213938786308, 'max_bin': 334}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:40:30,221] Trial 255 finished with value: 0.6819981517342147 and parameters: {'n_estimators': 899, 'eta': 0.03491889966692638, 'max_depth': 11, 'alpha': 0.558, 'lambda': 10.97846356849474, 'max_bin': 353}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:40:40,130] Trial 256 finished with value: 0.6861690937543082 and parameters: {'n_estimators': 840, 'eta': 0.022740794516002992, 'max_depth': 10, 'alpha': 0.0839, 'lambda': 8.472811701942605, 'max_bin': 343}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:40:46,770] Trial 257 finished with value: 0.683869730632393 and parameters: {'n_estimators': 869, 'eta': 0.06489554752277463, 'max_depth': 11, 'alpha': 0.5221, 'lambda': 9.785762383204293, 'max_bin': 300}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:41:00,244] Trial 258 finished with value: 0.6805899970008304 and parameters: {'n_estimators': 811, 'eta': 0.018802983777535212, 'max_depth': 12, 'alpha': 0.5042, 'lambda': 10.491645463705522, 'max_bin': 358}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:41:14,819] Trial 259 finished with value: 0.6848833690908658 and parameters: {'n_estimators': 855, 'eta': 0.011875547455782003, 'max_depth': 10, 'alpha': 0.23220000000000002, 'lambda': 5.524603636917541, 'max_bin': 348}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:41:25,248] Trial 260 finished with value: 0.6813822569714045 and parameters: {'n_estimators': 826, 'eta': 0.02940525029089472, 'max_depth': 12, 'alpha': 0.7951, 'lambda': 11.688359090801324, 'max_bin': 366}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:41:34,079] Trial 261 finished with value: 0.6870056949604051 and parameters: {'n_estimators': 767, 'eta': 0.025885457335800537, 'max_depth': 9, 'alpha': 0.0004, 'lambda': 6.892498059820114, 'max_bin': 336}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:41:45,398] Trial 262 finished with value: 0.6785387524032334 and parameters: {'n_estimators': 879, 'eta': 0.039004041582375436, 'max_depth': 11, 'alpha': 0.37310000000000004, 'lambda': 16.718788522871304, 'max_bin': 343}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:41:53,284] Trial 263 finished with value: 0.6859642398724539 and parameters: {'n_estimators': 800, 'eta': 0.046807150740600025, 'max_depth': 10, 'alpha': 0.4716, 'lambda': 7.903907075157506, 'max_bin': 354}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:42:00,763] Trial 264 finished with value: 0.684750894055109 and parameters: {'n_estimators': 900, 'eta': 0.044060877320131656, 'max_depth': 10, 'alpha': 0.5686, 'lambda': 4.747237074781985, 'max_bin': 314}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:42:11,525] Trial 265 finished with value: 0.6780650118411347 and parameters: {'n_estimators': 839, 'eta': 0.03462603412627549, 'max_depth': 11, 'alpha': 0.5335, 'lambda': 18.218502094505034, 'max_bin': 330}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:42:21,004] Trial 266 finished with value: 0.684359158984442 and parameters: {'n_estimators': 861, 'eta': 0.03718472396915423, 'max_depth': 10, 'alpha': 0.1588, 'lambda': 9.127225186350113, 'max_bin': 349}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:42:31,698] Trial 267 finished with value: 0.6810056649758887 and parameters: {'n_estimators': 819, 'eta': 0.03148341381895755, 'max_depth': 10, 'alpha': 0.9129, 'lambda': 12.77319526359406, 'max_bin': 373}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:42:39,702] Trial 268 finished with value: 0.6816617864807994 and parameters: {'n_estimators': 880, 'eta': 0.042050091072282955, 'max_depth': 11, 'alpha': 0.5934, 'lambda': 10.240362220188517, 'max_bin': 360}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:42:50,190] Trial 269 finished with value: 0.6853580128157614 and parameters: {'n_estimators': 844, 'eta': 0.023968521447813872, 'max_depth': 10, 'alpha': 0.5488000000000001, 'lambda': 8.419063760738581, 'max_bin': 340}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:43:01,319] Trial 270 finished with value: 0.6802359345164481 and parameters: {'n_estimators': 802, 'eta': 0.029940255692788378, 'max_depth': 12, 'alpha': 0.48850000000000005, 'lambda': 9.391494866759908, 'max_bin': 345}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:43:10,019] Trial 271 finished with value: 0.6829248658085445 and parameters: {'n_estimators': 490, 'eta': 0.03285938534878156, 'max_depth': 11, 'alpha': 0.5118, 'lambda': 6.284673398278865, 'max_bin': 319}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:43:19,039] Trial 272 finished with value: 0.6810882620235372 and parameters: {'n_estimators': 828, 'eta': 0.036312146018344586, 'max_depth': 11, 'alpha': 0.44430000000000003, 'lambda': 7.265539982379719, 'max_bin': 352}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:43:27,289] Trial 273 finished with value: 0.682385597341189 and parameters: {'n_estimators': 786, 'eta': 0.027321203506843007, 'max_depth': 10, 'alpha': 0.3123, 'lambda': 11.004145630666475, 'max_bin': 308}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:43:45,506] Trial 274 finished with value: 0.6774804968546395 and parameters: {'n_estimators': 866, 'eta': 0.007088933787619114, 'max_depth': 10, 'alpha': 0.523, 'lambda': 7.938744785525206, 'max_bin': 366}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:43:52,639] Trial 275 finished with value: 0.6842369140056179 and parameters: {'n_estimators': 815, 'eta': 0.03979819551109012, 'max_depth': 11, 'alpha': 0.6334000000000001, 'lambda': 1.7060331451229933, 'max_bin': 337}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:44:00,133] Trial 276 finished with value: 0.6800403730983111 and parameters: {'n_estimators': 748, 'eta': 0.03408497225625492, 'max_depth': 10, 'alpha': 0.4732, 'lambda': 9.672857006791657, 'max_bin': 347}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:44:07,574] Trial 277 finished with value: 0.6840243985843568 and parameters: {'n_estimators': 851, 'eta': 0.03804248358086746, 'max_depth': 8, 'alpha': 0.5720000000000001, 'lambda': 12.136284911708175, 'max_bin': 357}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:44:15,733] Trial 278 finished with value: 0.6888976024664365 and parameters: {'n_estimators': 884, 'eta': 0.030565275073945856, 'max_depth': 7, 'alpha': 0.5443, 'lambda': 5.925485958149958, 'max_bin': 341}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:44:26,087] Trial 279 finished with value: 0.677299164090097 and parameters: {'n_estimators': 833, 'eta': 0.04124182509442457, 'max_depth': 10, 'alpha': 0.4979, 'lambda': 28.678069361192094, 'max_bin': 333}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:44:33,042] Trial 280 finished with value: 0.6812401670203504 and parameters: {'n_estimators': 797, 'eta': 0.04471203031171812, 'max_depth': 12, 'alpha': 0.5839, 'lambda': 3.757442708819318, 'max_bin': 303}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:44:42,657] Trial 281 finished with value: 0.6824871366762413 and parameters: {'n_estimators': 870, 'eta': 0.02886883798245724, 'max_depth': 11, 'alpha': 0.4632, 'lambda': 8.78214294183515, 'max_bin': 376}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:44:51,550] Trial 282 finished with value: 0.6834888182289416 and parameters: {'n_estimators': 850, 'eta': 0.03246149336748993, 'max_depth': 10, 'alpha': 0.5258, 'lambda': 10.692733559756265, 'max_bin': 326}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:44:59,651] Trial 283 finished with value: 0.6816706667227503 and parameters: {'n_estimators': 816, 'eta': 0.035013685713077544, 'max_depth': 11, 'alpha': 0.5602, 'lambda': 7.023878315787353, 'max_bin': 353}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:45:09,973] Trial 284 finished with value: 0.6770766182500568 and parameters: {'n_estimators': 887, 'eta': 0.027067478309844496, 'max_depth': 10, 'alpha': 0.6147, 'lambda': 14.585051274336282, 'max_bin': 362}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:45:19,048] Trial 285 finished with value: 0.6835154470962419 and parameters: {'n_estimators': 779, 'eta': 0.04303297501300945, 'max_depth': 9, 'alpha': 0.49360000000000004, 'lambda': 15.82002075151776, 'max_bin': 345}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:45:25,821] Trial 286 finished with value: 0.6825788727477372 and parameters: {'n_estimators': 831, 'eta': 0.03954891238337836, 'max_depth': 11, 'alpha': 0.3899, 'lambda': 3.06066054500295, 'max_bin': 297}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:45:34,890] Trial 287 finished with value: 0.6804890370978806 and parameters: {'n_estimators': 899, 'eta': 0.037727085937084806, 'max_depth': 12, 'alpha': 0.537, 'lambda': 9.879805311773803, 'max_bin': 350}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:45:55,489] Trial 288 finished with value: 0.6146658847496749 and parameters: {'n_estimators': 859, 'eta': 0.002663832176139941, 'max_depth': 10, 'alpha': 0.6669, 'lambda': 8.792789863388755, 'max_bin': 356}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:46:00,668] Trial 289 finished with value: 0.6826985237312034 and parameters: {'n_estimators': 337, 'eta': 0.049104772945388585, 'max_depth': 10, 'alpha': 0.5114000000000001, 'lambda': 7.711204546294536, 'max_bin': 340}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:46:10,068] Trial 290 finished with value: 0.6812143937313042 and parameters: {'n_estimators': 840, 'eta': 0.030662656360076798, 'max_depth': 11, 'alpha': 0.43360000000000004, 'lambda': 4.787313887012708, 'max_bin': 371}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:46:19,742] Trial 291 finished with value: 0.6810374242715949 and parameters: {'n_estimators': 812, 'eta': 0.025014663024212028, 'max_depth': 10, 'alpha': 0.5987, 'lambda': 11.626850846345768, 'max_bin': 334}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:46:27,730] Trial 292 finished with value: 0.6837070436730653 and parameters: {'n_estimators': 870, 'eta': 0.03656861397377641, 'max_depth': 11, 'alpha': 0.4798, 'lambda': 5.4690329598310505, 'max_bin': 348}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:46:36,267] Trial 293 finished with value: 0.6830973336969688 and parameters: {'n_estimators': 801, 'eta': 0.032757562082370274, 'max_depth': 10, 'alpha': 0.5494, 'lambda': 10.31295135335665, 'max_bin': 382}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:46:45,446] Trial 294 finished with value: 0.6848528967551769 and parameters: {'n_estimators': 848, 'eta': 0.040223704726396525, 'max_depth': 12, 'alpha': 0.5196000000000001, 'lambda': 9.396778180621336, 'max_bin': 363}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:46:55,609] Trial 295 finished with value: 0.6829826422180509 and parameters: {'n_estimators': 826, 'eta': 0.028432758720580254, 'max_depth': 11, 'alpha': 0.5661, 'lambda': 8.27308926562297, 'max_bin': 341}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:47:02,821] Trial 296 finished with value: 0.6828338071618938 and parameters: {'n_estimators': 877, 'eta': 0.04221446840470351, 'max_depth': 10, 'alpha': 0.4577, 'lambda': 6.360718239629153, 'max_bin': 358}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:47:11,908] Trial 297 finished with value: 0.6864945533507939 and parameters: {'n_estimators': 775, 'eta': 0.03514494539216165, 'max_depth': 9, 'alpha': 0.4944, 'lambda': 7.559419503190702, 'max_bin': 346}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:47:21,261] Trial 298 finished with value: 0.6847973927214397 and parameters: {'n_estimators': 836, 'eta': 0.04653470114431629, 'max_depth': 10, 'alpha': 0.5800000000000001, 'lambda': 18.06666775685199, 'max_bin': 312}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:47:31,131] Trial 299 finished with value: 0.6832973990924063 and parameters: {'n_estimators': 861, 'eta': 0.03103154612034049, 'max_depth': 11, 'alpha': 0.5419, 'lambda': 11.11060859469125, 'max_bin': 352}. Best is trial 117 with value: 0.7065008514788376.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7065\n",
      "\tBest params:\n",
      "\t\tn_estimators: 809\n",
      "\t\teta: 0.03983291017054692\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.5553\n",
      "\t\tlambda: 6.2342513635974335\n",
      "\t\tmax_bin: 342\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.739481    0.710332    0.690333    0.755649   \n",
      "1                    TP   39.000000   42.000000   37.000000   36.000000   \n",
      "2                    TN  198.000000  197.000000  198.000000  199.000000   \n",
      "3                    FP    2.000000    5.000000    2.000000    2.000000   \n",
      "4                    FN   29.000000   24.000000   31.000000   31.000000   \n",
      "5              Accuracy    0.884328    0.891791    0.876866    0.876866   \n",
      "6             Precision    0.951220    0.893617    0.948718    0.947368   \n",
      "7           Sensitivity    0.573529    0.636364    0.544118    0.537313   \n",
      "8           Specificity    0.990000    0.975200    0.990000    0.990000   \n",
      "9              F1 score    0.715596    0.743363    0.691589    0.685714   \n",
      "10  F1 score (weighted)    0.873659    0.885124    0.864341    0.864004   \n",
      "11     F1 score (macro)    0.821498    0.837402    0.807333    0.804574   \n",
      "12    Balanced Accuracy    0.781765    0.805806    0.767059    0.763682   \n",
      "13                  MCC    0.681210    0.692912    0.659108    0.654620   \n",
      "14                  NPV    0.872200    0.891400    0.864600    0.865200   \n",
      "15              ROC_AUC    0.781765    0.805806    0.767059    0.763682   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.686552    0.710546  \n",
      "1    40.000000   44.000000  \n",
      "2   199.000000  197.000000  \n",
      "3     2.000000    3.000000  \n",
      "4    27.000000   24.000000  \n",
      "5     0.891791    0.899254  \n",
      "6     0.952381    0.936170  \n",
      "7     0.597015    0.647059  \n",
      "8     0.990000    0.985000  \n",
      "9     0.733945    0.765217  \n",
      "10    0.882549    0.892568  \n",
      "11    0.833015    0.850542  \n",
      "12    0.793532    0.816029  \n",
      "13    0.699266    0.723239  \n",
      "14    0.880500    0.891400  \n",
      "15    0.793532    0.816029  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_xgb_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_5_cat = np.where(((y_pred_xgb_5 >= 2) | (y_pred_xgb_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:47:42,155] Trial 300 finished with value: 0.6939477145450328 and parameters: {'n_estimators': 814, 'eta': 0.03839842295515375, 'max_depth': 10, 'alpha': 0.5067, 'lambda': 8.601543665768721, 'max_bin': 321}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:47:55,610] Trial 301 finished with value: 0.6965747823305153 and parameters: {'n_estimators': 891, 'eta': 0.028430258393117462, 'max_depth': 11, 'alpha': 0.0302, 'lambda': 13.460739117163476, 'max_bin': 329}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:48:02,391] Trial 302 finished with value: 0.6825599400585343 and parameters: {'n_estimators': 792, 'eta': 0.02618964316329288, 'max_depth': 5, 'alpha': 0.4698, 'lambda': 6.979018625432867, 'max_bin': 368}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:48:13,513] Trial 303 finished with value: 0.6931639930891975 and parameters: {'n_estimators': 846, 'eta': 0.032924455256157344, 'max_depth': 12, 'alpha': 0.5227, 'lambda': 9.850325597623968, 'max_bin': 343}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:48:22,088] Trial 304 finished with value: 0.6953184938870052 and parameters: {'n_estimators': 825, 'eta': 0.044673768620931344, 'max_depth': 10, 'alpha': 0.12010000000000001, 'lambda': 9.194252709052943, 'max_bin': 337}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:48:31,258] Trial 305 finished with value: 0.6874338149585764 and parameters: {'n_estimators': 899, 'eta': 0.05243864770680507, 'max_depth': 11, 'alpha': 0.33630000000000004, 'lambda': 20.868949028851947, 'max_bin': 350}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:48:40,616] Trial 306 finished with value: 0.6932737434884555 and parameters: {'n_estimators': 871, 'eta': 0.040337986046198704, 'max_depth': 10, 'alpha': 0.6022000000000001, 'lambda': 10.489128648677601, 'max_bin': 304}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:48:45,484] Trial 307 finished with value: 0.6846648797625525 and parameters: {'n_estimators': 241, 'eta': 0.03690912165655024, 'max_depth': 10, 'alpha': 0.556, 'lambda': 1.258903233477735, 'max_bin': 356}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:48:55,601] Trial 308 finished with value: 0.6895645317676253 and parameters: {'n_estimators': 758, 'eta': 0.034780302201563626, 'max_depth': 11, 'alpha': 0.6258, 'lambda': 5.714306071068562, 'max_bin': 291}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:49:08,126] Trial 309 finished with value: 0.695370071265455 and parameters: {'n_estimators': 592, 'eta': 0.02318593514822434, 'max_depth': 12, 'alpha': 0.4847, 'lambda': 7.900391798848655, 'max_bin': 345}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:49:20,425] Trial 310 finished with value: 0.6952519353825816 and parameters: {'n_estimators': 807, 'eta': 0.030909002572294415, 'max_depth': 10, 'alpha': 0.41100000000000003, 'lambda': 12.380668673124138, 'max_bin': 362}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:49:27,908] Trial 311 finished with value: 0.6825746231642145 and parameters: {'n_estimators': 859, 'eta': 0.08391725147693799, 'max_depth': 10, 'alpha': 0.5327000000000001, 'lambda': 39.436593964635996, 'max_bin': 337}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:49:35,860] Trial 312 finished with value: 0.6898248903679056 and parameters: {'n_estimators': 833, 'eta': 0.04300883915304035, 'max_depth': 11, 'alpha': 0.5055000000000001, 'lambda': 6.574212500947409, 'max_bin': 332}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:49:52,384] Trial 313 finished with value: 0.6926354605006473 and parameters: {'n_estimators': 882, 'eta': 0.01366935554510007, 'max_depth': 10, 'alpha': 0.4545, 'lambda': 11.488008337775648, 'max_bin': 353}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:50:01,831] Trial 314 finished with value: 0.693240393003925 and parameters: {'n_estimators': 846, 'eta': 0.02930934235088952, 'max_depth': 11, 'alpha': 0.07980000000000001, 'lambda': 4.159075019677943, 'max_bin': 348}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:50:12,052] Trial 315 finished with value: 0.6935489976346134 and parameters: {'n_estimators': 792, 'eta': 0.03367826015704781, 'max_depth': 10, 'alpha': 0.5830000000000001, 'lambda': 9.068535941698121, 'max_bin': 308}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:50:21,890] Trial 316 finished with value: 0.6915578506884014 and parameters: {'n_estimators': 817, 'eta': 0.03893145251427883, 'max_depth': 12, 'alpha': 0.5593, 'lambda': 10.216234778320183, 'max_bin': 315}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:50:30,414] Trial 317 finished with value: 0.6948005064986953 and parameters: {'n_estimators': 857, 'eta': 0.041162985632764305, 'max_depth': 11, 'alpha': 0.4872, 'lambda': 7.970649206032644, 'max_bin': 342}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:50:34,979] Trial 318 finished with value: 0.6925285889021459 and parameters: {'n_estimators': 830, 'eta': 0.056790219580038236, 'max_depth': 10, 'alpha': 0.053500000000000006, 'lambda': 2.294158261680182, 'max_bin': 366}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:50:45,788] Trial 319 finished with value: 0.6968257641893946 and parameters: {'n_estimators': 872, 'eta': 0.026336555557999818, 'max_depth': 10, 'alpha': 0.5312, 'lambda': 7.032760194605969, 'max_bin': 358}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:50:53,685] Trial 320 finished with value: 0.6929001222167109 and parameters: {'n_estimators': 806, 'eta': 0.0481954658352962, 'max_depth': 11, 'alpha': 0.7170000000000001, 'lambda': 9.444226211290761, 'max_bin': 348}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:51:03,919] Trial 321 finished with value: 0.6960738050647458 and parameters: {'n_estimators': 886, 'eta': 0.036619769696213736, 'max_depth': 9, 'alpha': 0.5066, 'lambda': 8.596869928489319, 'max_bin': 300}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:51:12,868] Trial 322 finished with value: 0.6876797573659369 and parameters: {'n_estimators': 844, 'eta': 0.03189031963964231, 'max_depth': 10, 'alpha': 0.5459, 'lambda': 5.061075493243776, 'max_bin': 339}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:51:21,782] Trial 323 finished with value: 0.6917142797867191 and parameters: {'n_estimators': 775, 'eta': 0.04550475320571259, 'max_depth': 12, 'alpha': 0.4681, 'lambda': 10.862589290460575, 'max_bin': 353}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:51:35,388] Trial 324 finished with value: 0.6973371637719625 and parameters: {'n_estimators': 729, 'eta': 0.020929518680180333, 'max_depth': 11, 'alpha': 0.431, 'lambda': 9.972969597768344, 'max_bin': 372}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:51:46,081] Trial 325 finished with value: 0.6947886974804452 and parameters: {'n_estimators': 900, 'eta': 0.028420654054640858, 'max_depth': 10, 'alpha': 0.5912000000000001, 'lambda': 6.2816888286655335, 'max_bin': 342}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:51:57,104] Trial 326 finished with value: 0.6921508102568538 and parameters: {'n_estimators': 815, 'eta': 0.035673238526853564, 'max_depth': 11, 'alpha': 0.5216000000000001, 'lambda': 7.509414033436735, 'max_bin': 324}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:52:07,541] Trial 327 finished with value: 0.6952752520240659 and parameters: {'n_estimators': 859, 'eta': 0.03852263479658561, 'max_depth': 10, 'alpha': 0.5736, 'lambda': 11.580633708856368, 'max_bin': 360}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:52:29,123] Trial 328 finished with value: 0.4061653849408223 and parameters: {'n_estimators': 830, 'eta': 0.0008862133331743877, 'max_depth': 11, 'alpha': 0.6451, 'lambda': 8.605767407379648, 'max_bin': 333}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:52:38,011] Trial 329 finished with value: 0.6761521549114357 and parameters: {'n_estimators': 430, 'eta': 0.01667691859639376, 'max_depth': 10, 'alpha': 0.49000000000000005, 'lambda': 16.882176400299265, 'max_bin': 346}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:52:47,615] Trial 330 finished with value: 0.6915689871603148 and parameters: {'n_estimators': 792, 'eta': 0.041779877840801914, 'max_depth': 12, 'alpha': 0.6073000000000001, 'lambda': 9.504287726569393, 'max_bin': 353}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:53:00,494] Trial 331 finished with value: 0.694443238275047 and parameters: {'n_estimators': 877, 'eta': 0.030101807799112305, 'max_depth': 10, 'alpha': 0.0008, 'lambda': 15.278910429856897, 'max_bin': 376}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:53:12,479] Trial 332 finished with value: 0.6964501691218528 and parameters: {'n_estimators': 843, 'eta': 0.033105257652717324, 'max_depth': 11, 'alpha': 0.4456, 'lambda': 14.358961322764106, 'max_bin': 366}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:53:24,037] Trial 333 finished with value: 0.6957632705872329 and parameters: {'n_estimators': 818, 'eta': 0.024648463316765127, 'max_depth': 10, 'alpha': 0.5453, 'lambda': 5.67500612712709, 'max_bin': 337}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:53:35,237] Trial 334 finished with value: 0.6951590437980534 and parameters: {'n_estimators': 900, 'eta': 0.043368508638941455, 'max_depth': 11, 'alpha': 0.5159, 'lambda': 12.796328040461699, 'max_bin': 344}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:53:47,550] Trial 335 finished with value: 0.6912477473064483 and parameters: {'n_estimators': 864, 'eta': 0.03432380064931102, 'max_depth': 10, 'alpha': 0.5653, 'lambda': 19.430803187964557, 'max_bin': 350}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:53:57,497] Trial 336 finished with value: 0.6952527598291307 and parameters: {'n_estimators': 803, 'eta': 0.04032617928353692, 'max_depth': 11, 'alpha': 0.4721, 'lambda': 10.714204160493866, 'max_bin': 329}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:54:08,727] Trial 337 finished with value: 0.698459026596252 and parameters: {'n_estimators': 832, 'eta': 0.02700600928616141, 'max_depth': 10, 'alpha': 0.4994, 'lambda': 7.019687212590446, 'max_bin': 360}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:54:20,597] Trial 338 finished with value: 0.6939951733294281 and parameters: {'n_estimators': 852, 'eta': 0.03728581276746222, 'max_depth': 12, 'alpha': 0.5411, 'lambda': 8.16333734368103, 'max_bin': 355}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:54:29,871] Trial 339 finished with value: 0.6910392507350592 and parameters: {'n_estimators': 875, 'eta': 0.031866748081992725, 'max_depth': 10, 'alpha': 0.5298, 'lambda': 4.67353208869015, 'max_bin': 341}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:54:35,764] Trial 340 finished with value: 0.68960159678911 and parameters: {'n_estimators': 786, 'eta': 0.06301016400686205, 'max_depth': 9, 'alpha': 0.48090000000000005, 'lambda': 9.112849025017386, 'max_bin': 347}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:54:51,898] Trial 341 finished with value: 0.6853547881415556 and parameters: {'n_estimators': 823, 'eta': 0.0293614063376757, 'max_depth': 11, 'alpha': 0.5105000000000001, 'lambda': 37.49364836351569, 'max_bin': 389}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:55:01,469] Trial 342 finished with value: 0.6897167279795148 and parameters: {'n_estimators': 840, 'eta': 0.03585087218613573, 'max_depth': 10, 'alpha': 0.5780000000000001, 'lambda': 3.2329042299544093, 'max_bin': 308}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:55:09,466] Trial 343 finished with value: 0.6895758984328764 and parameters: {'n_estimators': 762, 'eta': 0.04985579957220572, 'max_depth': 11, 'alpha': 0.5584, 'lambda': 10.18066738285544, 'max_bin': 284}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:55:16,251] Trial 344 finished with value: 0.6934042378298938 and parameters: {'n_estimators': 885, 'eta': 0.07755577964552826, 'max_depth': 12, 'alpha': 0.19010000000000002, 'lambda': 12.069486358901258, 'max_bin': 296}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:55:25,024] Trial 345 finished with value: 0.6918297933168424 and parameters: {'n_estimators': 854, 'eta': 0.039319089951209495, 'max_depth': 11, 'alpha': 0.2808, 'lambda': 7.46002865588248, 'max_bin': 336}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:55:42,625] Trial 346 finished with value: 0.6802326940264632 and parameters: {'n_estimators': 802, 'eta': 0.006607021270035561, 'max_depth': 10, 'alpha': 0.6186, 'lambda': 6.273729453577054, 'max_bin': 317}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:55:50,089] Trial 347 finished with value: 0.6937281537471107 and parameters: {'n_estimators': 817, 'eta': 0.04434590480952542, 'max_depth': 10, 'alpha': 0.462, 'lambda': 8.696101583715949, 'max_bin': 356}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:55:55,394] Trial 348 finished with value: 0.6851922054277804 and parameters: {'n_estimators': 870, 'eta': 0.09604593714237208, 'max_depth': 11, 'alpha': 0.49510000000000004, 'lambda': 9.618540859174377, 'max_bin': 350}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:56:06,762] Trial 349 finished with value: 0.6953086271559089 and parameters: {'n_estimators': 837, 'eta': 0.03009925834849631, 'max_depth': 10, 'alpha': 0.5242, 'lambda': 10.766921349915028, 'max_bin': 362}. Best is trial 117 with value: 0.7065008514788376.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7065\n",
      "\tBest params:\n",
      "\t\tn_estimators: 809\n",
      "\t\teta: 0.03983291017054692\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.5553\n",
      "\t\tlambda: 6.2342513635974335\n",
      "\t\tmax_bin: 342\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.739481    0.710332    0.690333    0.755649   \n",
      "1                    TP   39.000000   42.000000   37.000000   36.000000   \n",
      "2                    TN  198.000000  197.000000  198.000000  199.000000   \n",
      "3                    FP    2.000000    5.000000    2.000000    2.000000   \n",
      "4                    FN   29.000000   24.000000   31.000000   31.000000   \n",
      "5              Accuracy    0.884328    0.891791    0.876866    0.876866   \n",
      "6             Precision    0.951220    0.893617    0.948718    0.947368   \n",
      "7           Sensitivity    0.573529    0.636364    0.544118    0.537313   \n",
      "8           Specificity    0.990000    0.975200    0.990000    0.990000   \n",
      "9              F1 score    0.715596    0.743363    0.691589    0.685714   \n",
      "10  F1 score (weighted)    0.873659    0.885124    0.864341    0.864004   \n",
      "11     F1 score (macro)    0.821498    0.837402    0.807333    0.804574   \n",
      "12    Balanced Accuracy    0.781765    0.805806    0.767059    0.763682   \n",
      "13                  MCC    0.681210    0.692912    0.659108    0.654620   \n",
      "14                  NPV    0.872200    0.891400    0.864600    0.865200   \n",
      "15              ROC_AUC    0.781765    0.805806    0.767059    0.763682   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.686552    0.710546    0.745050  \n",
      "1    40.000000   44.000000   46.000000  \n",
      "2   199.000000  197.000000  198.000000  \n",
      "3     2.000000    3.000000    3.000000  \n",
      "4    27.000000   24.000000   21.000000  \n",
      "5     0.891791    0.899254    0.910448  \n",
      "6     0.952381    0.936170    0.938776  \n",
      "7     0.597015    0.647059    0.686567  \n",
      "8     0.990000    0.985000    0.985100  \n",
      "9     0.733945    0.765217    0.793103  \n",
      "10    0.882549    0.892568    0.905419  \n",
      "11    0.833015    0.850542    0.867980  \n",
      "12    0.793532    0.816029    0.835821  \n",
      "13    0.699266    0.723239    0.752407  \n",
      "14    0.880500    0.891400    0.904100  \n",
      "15    0.793532    0.816029    0.835821  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_xgb_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_6_cat = np.where(((y_pred_xgb_6 >= 2) | (y_pred_xgb_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 00:56:19,140] Trial 350 finished with value: 0.6824793891864475 and parameters: {'n_estimators': 886, 'eta': 0.03317214493651596, 'max_depth': 10, 'alpha': 0.5926, 'lambda': 22.86275781396017, 'max_bin': 343}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:56:26,602] Trial 351 finished with value: 0.6839307989557943 and parameters: {'n_estimators': 854, 'eta': 0.04649565960985118, 'max_depth': 11, 'alpha': 0.35900000000000004, 'lambda': 8.169745911093097, 'max_bin': 327}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:56:35,800] Trial 352 finished with value: 0.6763245230196427 and parameters: {'n_estimators': 822, 'eta': 0.02755720729714938, 'max_depth': 12, 'alpha': 0.4363, 'lambda': 5.309245834906618, 'max_bin': 367}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:56:45,976] Trial 353 finished with value: 0.6829098849083851 and parameters: {'n_estimators': 796, 'eta': 0.024129911553807292, 'max_depth': 10, 'alpha': 0.5377000000000001, 'lambda': 13.517401530046378, 'max_bin': 347}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:56:52,231] Trial 354 finished with value: 0.6763627801467876 and parameters: {'n_estimators': 838, 'eta': 0.04112870192466763, 'max_depth': 11, 'alpha': 0.5014000000000001, 'lambda': 6.680984323506463, 'max_bin': 338}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:57:00,144] Trial 355 finished with value: 0.6805492571659687 and parameters: {'n_estimators': 779, 'eta': 0.03838044949277728, 'max_depth': 9, 'alpha': 0.031100000000000003, 'lambda': 11.454026062382779, 'max_bin': 352}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:57:08,564] Trial 356 finished with value: 0.6790777870279476 and parameters: {'n_estimators': 867, 'eta': 0.031240000938495088, 'max_depth': 10, 'alpha': 0.5693, 'lambda': 9.849965313262722, 'max_bin': 305}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:57:17,761] Trial 357 finished with value: 0.681975040069522 and parameters: {'n_estimators': 806, 'eta': 0.03524106171211537, 'max_depth': 11, 'alpha': 0.5158, 'lambda': 8.973461254618929, 'max_bin': 321}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:57:28,150] Trial 358 finished with value: 0.6745040408019239 and parameters: {'n_estimators': 886, 'eta': 0.04281878869399982, 'max_depth': 12, 'alpha': 0.2232, 'lambda': 32.508980210071215, 'max_bin': 312}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:57:33,085] Trial 359 finished with value: 0.6765927669051633 and parameters: {'n_estimators': 691, 'eta': 0.06579812477221811, 'max_depth': 10, 'alpha': 0.4798, 'lambda': 7.4920235159049575, 'max_bin': 334}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:57:40,152] Trial 360 finished with value: 0.6745727250103314 and parameters: {'n_estimators': 899, 'eta': 0.02594293963771941, 'max_depth': 6, 'alpha': 0.5603, 'lambda': 4.319828919424111, 'max_bin': 371}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:57:46,078] Trial 361 finished with value: 0.6754364679635304 and parameters: {'n_estimators': 824, 'eta': 0.09253109449548703, 'max_depth': 10, 'alpha': 0.5398000000000001, 'lambda': 24.55603642402491, 'max_bin': 357}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:57:54,299] Trial 362 finished with value: 0.6784031452043868 and parameters: {'n_estimators': 855, 'eta': 0.03396650878038024, 'max_depth': 11, 'alpha': 0.9871000000000001, 'lambda': 8.141991883757242, 'max_bin': 342}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:58:02,409] Trial 363 finished with value: 0.6814125741055841 and parameters: {'n_estimators': 842, 'eta': 0.037012984104261834, 'max_depth': 10, 'alpha': 0.4032, 'lambda': 10.936467692119473, 'max_bin': 351}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:58:13,463] Trial 364 finished with value: 0.6828241537287827 and parameters: {'n_estimators': 745, 'eta': 0.02866489374012531, 'max_depth': 11, 'alpha': 0.4529, 'lambda': 16.539519651148183, 'max_bin': 345}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:58:20,342] Trial 365 finished with value: 0.6790562094443692 and parameters: {'n_estimators': 874, 'eta': 0.04039436385912931, 'max_depth': 10, 'alpha': 0.5962000000000001, 'lambda': 5.8980402059659, 'max_bin': 362}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:58:31,481] Trial 366 finished with value: 0.6834713628521968 and parameters: {'n_estimators': 822, 'eta': 0.03178036896981602, 'max_depth': 12, 'alpha': 0.49520000000000003, 'lambda': 16.1437998734413, 'max_bin': 332}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:58:38,780] Trial 367 finished with value: 0.6765273512242933 and parameters: {'n_estimators': 394, 'eta': 0.022218626070972823, 'max_depth': 10, 'alpha': 0.6229, 'lambda': 9.151738183316889, 'max_bin': 347}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:58:46,641] Trial 368 finished with value: 0.6809374263916529 and parameters: {'n_estimators': 809, 'eta': 0.039139120623003835, 'max_depth': 11, 'alpha': 0.0181, 'lambda': 10.219286229882808, 'max_bin': 338}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:58:55,377] Trial 369 finished with value: 0.6789841127421147 and parameters: {'n_estimators': 847, 'eta': 0.030237441629332576, 'max_depth': 10, 'alpha': 0.5134000000000001, 'lambda': 6.907972223686328, 'max_bin': 301}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:59:03,964] Trial 370 finished with value: 0.6780622162709783 and parameters: {'n_estimators': 795, 'eta': 0.04240260504125907, 'max_depth': 11, 'alpha': 0.5544, 'lambda': 17.93923837449722, 'max_bin': 356}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:59:10,764] Trial 371 finished with value: 0.6756613387086109 and parameters: {'n_estimators': 526, 'eta': 0.03628422105403695, 'max_depth': 10, 'alpha': 0.4834, 'lambda': 3.674276337729199, 'max_bin': 342}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:59:16,591] Trial 372 finished with value: 0.6787443925996571 and parameters: {'n_estimators': 865, 'eta': 0.047333733765963816, 'max_depth': 11, 'alpha': 0.6568, 'lambda': 8.63508964734441, 'max_bin': 351}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:59:25,251] Trial 373 finished with value: 0.6795134576747193 and parameters: {'n_estimators': 770, 'eta': 0.027455227308848255, 'max_depth': 10, 'alpha': 0.5255000000000001, 'lambda': 9.868313755947685, 'max_bin': 366}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:59:33,646] Trial 374 finished with value: 0.6809014005887002 and parameters: {'n_estimators': 833, 'eta': 0.03276791211930834, 'max_depth': 12, 'alpha': 0.4615, 'lambda': 7.750471955385953, 'max_bin': 359}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:59:39,096] Trial 375 finished with value: 0.6711898867681422 and parameters: {'n_estimators': 888, 'eta': 0.037924342457523345, 'max_depth': 10, 'alpha': 0.5719000000000001, 'lambda': 1.0444953730863524, 'max_bin': 345}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:59:48,064] Trial 376 finished with value: 0.682495240142768 and parameters: {'n_estimators': 814, 'eta': 0.03460567285824308, 'max_depth': 11, 'alpha': 0.5439, 'lambda': 11.711730980013202, 'max_bin': 491}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 00:59:56,192] Trial 377 finished with value: 0.6837565271389116 and parameters: {'n_estimators': 900, 'eta': 0.04504387479004867, 'max_depth': 11, 'alpha': 0.5032, 'lambda': 15.312617748191075, 'max_bin': 339}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:00:05,671] Trial 378 finished with value: 0.6801228582230529 and parameters: {'n_estimators': 855, 'eta': 0.025293125720132065, 'max_depth': 10, 'alpha': 0.16840000000000002, 'lambda': 9.368191378122969, 'max_bin': 379}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:00:12,226] Trial 379 finished with value: 0.6786210998864787 and parameters: {'n_estimators': 830, 'eta': 0.04102835085652972, 'max_depth': 10, 'alpha': 0.5889, 'lambda': 6.225886074022161, 'max_bin': 326}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:00:18,077] Trial 380 finished with value: 0.6799929110668765 and parameters: {'n_estimators': 874, 'eta': 0.055251967660068145, 'max_depth': 9, 'alpha': 0.5307000000000001, 'lambda': 12.609213489266608, 'max_bin': 293}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:00:24,450] Trial 381 finished with value: 0.6799973443631017 and parameters: {'n_estimators': 786, 'eta': 0.051144958445906274, 'max_depth': 11, 'alpha': 0.47900000000000004, 'lambda': 10.579767563528492, 'max_bin': 350}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:00:33,200] Trial 382 finished with value: 0.6811396200008388 and parameters: {'n_estimators': 845, 'eta': 0.029557371285002763, 'max_depth': 10, 'alpha': 0.1356, 'lambda': 7.307946226806784, 'max_bin': 448}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:00:40,156] Trial 383 finished with value: 0.6744332726783193 and parameters: {'n_estimators': 806, 'eta': 0.03143327273951096, 'max_depth': 12, 'alpha': 0.6107, 'lambda': 2.8857362450276476, 'max_bin': 355}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:00:49,954] Trial 384 finished with value: 0.6800946105815354 and parameters: {'n_estimators': 869, 'eta': 0.01862816328595037, 'max_depth': 10, 'alpha': 0.4325, 'lambda': 4.870077912431703, 'max_bin': 332}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:00:57,481] Trial 385 finished with value: 0.6772261117905025 and parameters: {'n_estimators': 827, 'eta': 0.02781004384459882, 'max_depth': 11, 'alpha': 0.5118, 'lambda': 8.713391155829866, 'max_bin': 346}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:01:04,507] Trial 386 finished with value: 0.6783776220923097 and parameters: {'n_estimators': 882, 'eta': 0.04357311783588062, 'max_depth': 10, 'alpha': 0.5486, 'lambda': 7.922141724913936, 'max_bin': 371}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:01:12,850] Trial 387 finished with value: 0.6801063993084242 and parameters: {'n_estimators': 855, 'eta': 0.03609962820572218, 'max_depth': 11, 'alpha': 0.09920000000000001, 'lambda': 9.834040646419416, 'max_bin': 312}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:01:21,099] Trial 388 finished with value: 0.6828140458312799 and parameters: {'n_estimators': 812, 'eta': 0.039569071857325, 'max_depth': 11, 'alpha': 0.0502, 'lambda': 13.949265296226683, 'max_bin': 362}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:01:26,448] Trial 389 finished with value: 0.676472133282189 and parameters: {'n_estimators': 835, 'eta': 0.07102317736381458, 'max_depth': 8, 'alpha': 0.4727, 'lambda': 19.165009713542364, 'max_bin': 341}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:01:35,093] Trial 390 finished with value: 0.6825099175171127 and parameters: {'n_estimators': 782, 'eta': 0.034337070769835026, 'max_depth': 10, 'alpha': 0.5802, 'lambda': 11.299082230904691, 'max_bin': 335}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:01:40,498] Trial 391 finished with value: 0.6745988152606035 and parameters: {'n_estimators': 640, 'eta': 0.09766211009585606, 'max_depth': 12, 'alpha': 0.4955, 'lambda': 21.19667256321668, 'max_bin': 354}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:01:44,013] Trial 392 finished with value: 0.672309448171576 and parameters: {'n_estimators': 179, 'eta': 0.03278693510417105, 'max_depth': 10, 'alpha': 0.4484, 'lambda': 5.54785679120139, 'max_bin': 300}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:01:50,633] Trial 393 finished with value: 0.6761847256305831 and parameters: {'n_estimators': 799, 'eta': 0.03733926711406843, 'max_depth': 11, 'alpha': 0.5232, 'lambda': 6.700929051590263, 'max_bin': 348}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:01:57,559] Trial 394 finished with value: 0.679056292753685 and parameters: {'n_estimators': 861, 'eta': 0.041886136292316176, 'max_depth': 10, 'alpha': 0.5692, 'lambda': 9.224375022391435, 'max_bin': 340}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:02:07,499] Trial 395 finished with value: 0.6795473792740339 and parameters: {'n_estimators': 884, 'eta': 0.029564070503385963, 'max_depth': 12, 'alpha': 0.24760000000000001, 'lambda': 8.17065481555335, 'max_bin': 317}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:02:16,705] Trial 396 finished with value: 0.6800544697800298 and parameters: {'n_estimators': 822, 'eta': 0.02652185512492553, 'max_depth': 10, 'alpha': 0.6848000000000001, 'lambda': 10.858698118562593, 'max_bin': 305}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:02:25,180] Trial 397 finished with value: 0.6750805642389988 and parameters: {'n_estimators': 840, 'eta': 0.023518193597266725, 'max_depth': 7, 'alpha': 0.6387, 'lambda': 12.169585957366738, 'max_bin': 358}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:02:32,601] Trial 398 finished with value: 0.6815024577901908 and parameters: {'n_estimators': 858, 'eta': 0.03875061587427086, 'max_depth': 10, 'alpha': 0.5474, 'lambda': 10.12678207701553, 'max_bin': 374}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:02:38,699] Trial 399 finished with value: 0.6767611144453138 and parameters: {'n_estimators': 883, 'eta': 0.04527855987177185, 'max_depth': 11, 'alpha': 0.4164, 'lambda': 7.340600625456799, 'max_bin': 351}. Best is trial 117 with value: 0.7065008514788376.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7065\n",
      "\tBest params:\n",
      "\t\tn_estimators: 809\n",
      "\t\teta: 0.03983291017054692\n",
      "\t\tmax_depth: 11\n",
      "\t\talpha: 0.5553\n",
      "\t\tlambda: 6.2342513635974335\n",
      "\t\tmax_bin: 342\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.739481    0.710332    0.690333    0.755649   \n",
      "1                    TP   39.000000   42.000000   37.000000   36.000000   \n",
      "2                    TN  198.000000  197.000000  198.000000  199.000000   \n",
      "3                    FP    2.000000    5.000000    2.000000    2.000000   \n",
      "4                    FN   29.000000   24.000000   31.000000   31.000000   \n",
      "5              Accuracy    0.884328    0.891791    0.876866    0.876866   \n",
      "6             Precision    0.951220    0.893617    0.948718    0.947368   \n",
      "7           Sensitivity    0.573529    0.636364    0.544118    0.537313   \n",
      "8           Specificity    0.990000    0.975200    0.990000    0.990000   \n",
      "9              F1 score    0.715596    0.743363    0.691589    0.685714   \n",
      "10  F1 score (weighted)    0.873659    0.885124    0.864341    0.864004   \n",
      "11     F1 score (macro)    0.821498    0.837402    0.807333    0.804574   \n",
      "12    Balanced Accuracy    0.781765    0.805806    0.767059    0.763682   \n",
      "13                  MCC    0.681210    0.692912    0.659108    0.654620   \n",
      "14                  NPV    0.872200    0.891400    0.864600    0.865200   \n",
      "15              ROC_AUC    0.781765    0.805806    0.767059    0.763682   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.686552    0.710546    0.745050    0.723587  \n",
      "1    40.000000   44.000000   46.000000   47.000000  \n",
      "2   199.000000  197.000000  198.000000  197.000000  \n",
      "3     2.000000    3.000000    3.000000    4.000000  \n",
      "4    27.000000   24.000000   21.000000   20.000000  \n",
      "5     0.891791    0.899254    0.910448    0.910448  \n",
      "6     0.952381    0.936170    0.938776    0.921569  \n",
      "7     0.597015    0.647059    0.686567    0.701493  \n",
      "8     0.990000    0.985000    0.985100    0.980100  \n",
      "9     0.733945    0.765217    0.793103    0.796610  \n",
      "10    0.882549    0.892568    0.905419    0.906090  \n",
      "11    0.833015    0.850542    0.867980    0.869597  \n",
      "12    0.793532    0.816029    0.835821    0.840796  \n",
      "13    0.699266    0.723239    0.752407    0.751874  \n",
      "14    0.880500    0.891400    0.904100    0.907800  \n",
      "15    0.793532    0.816029    0.835821    0.840796  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_xgb_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_7_cat = np.where(((y_pred_xgb_7 >= 2) | (y_pred_xgb_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:02:49,934] Trial 400 finished with value: 0.7021205516364986 and parameters: {'n_estimators': 900, 'eta': 0.03165636393193604, 'max_depth': 10, 'alpha': 0.6039, 'lambda': 8.557006123904767, 'max_bin': 365}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:03:02,780] Trial 401 finished with value: 0.7062396301324766 and parameters: {'n_estimators': 842, 'eta': 0.03441633676535615, 'max_depth': 11, 'alpha': 0.5192, 'lambda': 17.11428836171178, 'max_bin': 344}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:03:17,002] Trial 402 finished with value: 0.7055956492374528 and parameters: {'n_estimators': 764, 'eta': 0.034519264356344836, 'max_depth': 11, 'alpha': 0.5122, 'lambda': 22.116133043252297, 'max_bin': 345}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:03:31,820] Trial 403 finished with value: 0.7026743980534667 and parameters: {'n_estimators': 773, 'eta': 0.033995545282263104, 'max_depth': 11, 'alpha': 0.5114000000000001, 'lambda': 26.188216726468852, 'max_bin': 342}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:03:42,960] Trial 404 finished with value: 0.7057886082821101 and parameters: {'n_estimators': 747, 'eta': 0.03584045737776174, 'max_depth': 10, 'alpha': 0.49450000000000005, 'lambda': 21.926281631750317, 'max_bin': 346}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:03:54,223] Trial 405 finished with value: 0.7046872300652629 and parameters: {'n_estimators': 729, 'eta': 0.03529975002692004, 'max_depth': 10, 'alpha': 0.5227, 'lambda': 20.03976590882113, 'max_bin': 347}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:04:05,363] Trial 406 finished with value: 0.7038198929490345 and parameters: {'n_estimators': 719, 'eta': 0.035552056925812855, 'max_depth': 10, 'alpha': 0.49510000000000004, 'lambda': 21.798675074883178, 'max_bin': 347}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:04:16,956] Trial 407 finished with value: 0.7019141101440941 and parameters: {'n_estimators': 763, 'eta': 0.034809236172180874, 'max_depth': 10, 'alpha': 0.5245000000000001, 'lambda': 22.027072376997744, 'max_bin': 344}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:04:30,035] Trial 408 finished with value: 0.7042895813071578 and parameters: {'n_estimators': 771, 'eta': 0.033313483464637375, 'max_depth': 10, 'alpha': 0.5287000000000001, 'lambda': 23.252338369952003, 'max_bin': 339}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:04:42,902] Trial 409 finished with value: 0.7059747487839345 and parameters: {'n_estimators': 730, 'eta': 0.032949916772149396, 'max_depth': 10, 'alpha': 0.497, 'lambda': 22.722776114751465, 'max_bin': 336}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:04:54,941] Trial 410 finished with value: 0.7039667124732875 and parameters: {'n_estimators': 725, 'eta': 0.03347345671159864, 'max_depth': 10, 'alpha': 0.4894, 'lambda': 23.02773162205797, 'max_bin': 334}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:05:06,290] Trial 411 finished with value: 0.7028797977309926 and parameters: {'n_estimators': 712, 'eta': 0.03340841332485438, 'max_depth': 10, 'alpha': 0.4887, 'lambda': 22.339328124268707, 'max_bin': 332}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:05:18,017] Trial 412 finished with value: 0.7028214481222824 and parameters: {'n_estimators': 696, 'eta': 0.0334234952697679, 'max_depth': 10, 'alpha': 0.8759, 'lambda': 22.80574180827692, 'max_bin': 337}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:05:29,975] Trial 413 finished with value: 0.7043853515401052 and parameters: {'n_estimators': 725, 'eta': 0.03569504672339595, 'max_depth': 10, 'alpha': 0.5046, 'lambda': 23.539371188140777, 'max_bin': 329}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:05:42,056] Trial 414 finished with value: 0.7035535596452175 and parameters: {'n_estimators': 732, 'eta': 0.036124265720229136, 'max_depth': 10, 'alpha': 0.5066, 'lambda': 23.82453160929603, 'max_bin': 328}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:05:53,926] Trial 415 finished with value: 0.7054533225271308 and parameters: {'n_estimators': 741, 'eta': 0.035385374772665595, 'max_depth': 10, 'alpha': 0.5244, 'lambda': 20.16115647255843, 'max_bin': 337}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:06:05,740] Trial 416 finished with value: 0.7044490225157302 and parameters: {'n_estimators': 741, 'eta': 0.03533982584903892, 'max_depth': 10, 'alpha': 0.5289, 'lambda': 20.734928047525806, 'max_bin': 336}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:06:17,712] Trial 417 finished with value: 0.7061982570957558 and parameters: {'n_estimators': 745, 'eta': 0.035865175908031344, 'max_depth': 10, 'alpha': 0.5394, 'lambda': 20.439857407670754, 'max_bin': 329}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:06:29,371] Trial 418 finished with value: 0.7051521728371223 and parameters: {'n_estimators': 729, 'eta': 0.03665790145212061, 'max_depth': 10, 'alpha': 0.5333, 'lambda': 20.62003577036161, 'max_bin': 328}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:06:40,743] Trial 419 finished with value: 0.7047965206309115 and parameters: {'n_estimators': 741, 'eta': 0.03570174503786171, 'max_depth': 10, 'alpha': 0.5315, 'lambda': 20.256881811456967, 'max_bin': 329}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:06:53,162] Trial 420 finished with value: 0.7037715928720036 and parameters: {'n_estimators': 746, 'eta': 0.03587519459096689, 'max_depth': 10, 'alpha': 0.5446, 'lambda': 20.190817330683906, 'max_bin': 326}. Best is trial 117 with value: 0.7065008514788376.\n",
      "[I 2023-12-12 01:07:05,620] Trial 421 finished with value: 0.7069755716672145 and parameters: {'n_estimators': 736, 'eta': 0.03552522512988063, 'max_depth': 10, 'alpha': 0.5346000000000001, 'lambda': 20.382093364944772, 'max_bin': 323}. Best is trial 421 with value: 0.7069755716672145.\n",
      "[I 2023-12-12 01:07:16,542] Trial 422 finished with value: 0.7059179109830753 and parameters: {'n_estimators': 739, 'eta': 0.03627711296578818, 'max_depth': 10, 'alpha': 0.533, 'lambda': 20.44212924857296, 'max_bin': 322}. Best is trial 421 with value: 0.7069755716672145.\n",
      "[I 2023-12-12 01:07:28,208] Trial 423 finished with value: 0.7047807383459227 and parameters: {'n_estimators': 742, 'eta': 0.037446763705386965, 'max_depth': 10, 'alpha': 0.5538000000000001, 'lambda': 20.639884045113043, 'max_bin': 323}. Best is trial 421 with value: 0.7069755716672145.\n",
      "[I 2023-12-12 01:07:38,997] Trial 424 finished with value: 0.7021556971401678 and parameters: {'n_estimators': 718, 'eta': 0.03718623924904333, 'max_depth': 10, 'alpha': 0.5565, 'lambda': 19.88668531651185, 'max_bin': 322}. Best is trial 421 with value: 0.7069755716672145.\n",
      "[I 2023-12-12 01:07:50,084] Trial 425 finished with value: 0.7055619891109192 and parameters: {'n_estimators': 674, 'eta': 0.037229157917054126, 'max_depth': 10, 'alpha': 0.5485, 'lambda': 21.17458167950418, 'max_bin': 325}. Best is trial 421 with value: 0.7069755716672145.\n",
      "[I 2023-12-12 01:08:02,191] Trial 426 finished with value: 0.707589437705248 and parameters: {'n_estimators': 741, 'eta': 0.037064671740369835, 'max_depth': 10, 'alpha': 0.5585, 'lambda': 21.372948829361285, 'max_bin': 322}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:08:13,386] Trial 427 finished with value: 0.7071318341897609 and parameters: {'n_estimators': 707, 'eta': 0.03740878445532374, 'max_depth': 10, 'alpha': 0.5700000000000001, 'lambda': 21.505261558983957, 'max_bin': 322}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:08:24,796] Trial 428 finished with value: 0.7042743008833169 and parameters: {'n_estimators': 676, 'eta': 0.037598042478788965, 'max_depth': 10, 'alpha': 0.5664, 'lambda': 21.173642478945514, 'max_bin': 322}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:08:35,556] Trial 429 finished with value: 0.7062418348873958 and parameters: {'n_estimators': 704, 'eta': 0.037639030441553104, 'max_depth': 10, 'alpha': 0.5630000000000001, 'lambda': 21.471058760520982, 'max_bin': 321}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:08:46,521] Trial 430 finished with value: 0.7055137235774422 and parameters: {'n_estimators': 706, 'eta': 0.03734379228074016, 'max_depth': 10, 'alpha': 0.5741, 'lambda': 21.102637848697967, 'max_bin': 321}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:08:57,151] Trial 431 finished with value: 0.7048897248353355 and parameters: {'n_estimators': 703, 'eta': 0.037721375976492125, 'max_depth': 10, 'alpha': 0.7726000000000001, 'lambda': 21.384952431569584, 'max_bin': 319}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:09:08,236] Trial 432 finished with value: 0.7024837027320826 and parameters: {'n_estimators': 684, 'eta': 0.03749991657890247, 'max_depth': 10, 'alpha': 0.8300000000000001, 'lambda': 20.695146892280295, 'max_bin': 319}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:09:19,736] Trial 433 finished with value: 0.7056479200624374 and parameters: {'n_estimators': 698, 'eta': 0.03824605855026562, 'max_depth': 10, 'alpha': 0.7285, 'lambda': 21.730850613642495, 'max_bin': 323}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:09:30,895] Trial 434 finished with value: 0.703011694179642 and parameters: {'n_estimators': 690, 'eta': 0.03779234320712889, 'max_depth': 10, 'alpha': 0.7487, 'lambda': 21.33409029162482, 'max_bin': 324}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:09:41,782] Trial 435 finished with value: 0.7041478164351608 and parameters: {'n_estimators': 714, 'eta': 0.03863342128105872, 'max_depth': 10, 'alpha': 0.756, 'lambda': 21.716373109891624, 'max_bin': 318}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:09:52,314] Trial 436 finished with value: 0.7026936595186897 and parameters: {'n_estimators': 664, 'eta': 0.03660446014612213, 'max_depth': 10, 'alpha': 0.7803, 'lambda': 21.648502577194453, 'max_bin': 323}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:10:03,518] Trial 437 finished with value: 0.701889743981349 and parameters: {'n_estimators': 705, 'eta': 0.03919248423356723, 'max_depth': 10, 'alpha': 0.7943, 'lambda': 22.086331098781617, 'max_bin': 319}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:10:14,854] Trial 438 finished with value: 0.7030905219000606 and parameters: {'n_estimators': 704, 'eta': 0.036161038493314354, 'max_depth': 10, 'alpha': 0.7241000000000001, 'lambda': 20.37923364235258, 'max_bin': 323}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:10:21,345] Trial 439 finished with value: 0.7039899989118342 and parameters: {'n_estimators': 700, 'eta': 0.05891639334278811, 'max_depth': 9, 'alpha': 0.5798, 'lambda': 21.365294720871386, 'max_bin': 327}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:10:32,703] Trial 440 finished with value: 0.7054178173218043 and parameters: {'n_estimators': 742, 'eta': 0.03513798606309256, 'max_depth': 10, 'alpha': 0.5559000000000001, 'lambda': 19.396091332441273, 'max_bin': 316}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:10:43,192] Trial 441 finished with value: 0.7035590991030815 and parameters: {'n_estimators': 751, 'eta': 0.038777585853909, 'max_depth': 10, 'alpha': 0.7469, 'lambda': 19.05597906622266, 'max_bin': 316}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:10:53,963] Trial 442 finished with value: 0.7034443872111205 and parameters: {'n_estimators': 678, 'eta': 0.03684364709450358, 'max_depth': 10, 'alpha': 0.5567, 'lambda': 22.44221670780567, 'max_bin': 318}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:11:05,614] Trial 443 finished with value: 0.7037954408411933 and parameters: {'n_estimators': 717, 'eta': 0.03493268700264568, 'max_depth': 10, 'alpha': 0.5814, 'lambda': 21.03622274685776, 'max_bin': 314}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:11:15,713] Trial 444 finished with value: 0.7041847398392012 and parameters: {'n_estimators': 735, 'eta': 0.03843717860303583, 'max_depth': 10, 'alpha': 0.5531, 'lambda': 19.753716668030172, 'max_bin': 325}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:11:26,902] Trial 445 finished with value: 0.7057176512617025 and parameters: {'n_estimators': 704, 'eta': 0.03480430036629848, 'max_depth': 10, 'alpha': 0.7859, 'lambda': 19.09238423866982, 'max_bin': 320}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:11:38,853] Trial 446 finished with value: 0.7030874496348744 and parameters: {'n_estimators': 732, 'eta': 0.03515712620299616, 'max_depth': 10, 'alpha': 0.8077000000000001, 'lambda': 19.720211413153052, 'max_bin': 326}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:11:49,325] Trial 447 finished with value: 0.7046320402852975 and parameters: {'n_estimators': 758, 'eta': 0.035004420020958694, 'max_depth': 10, 'alpha': 0.8307, 'lambda': 18.746758518553058, 'max_bin': 320}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:12:01,241] Trial 448 finished with value: 0.7074769538716967 and parameters: {'n_estimators': 708, 'eta': 0.03382527850978557, 'max_depth': 10, 'alpha': 0.5682, 'lambda': 20.45622354132508, 'max_bin': 314}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:12:16,037] Trial 449 finished with value: 0.011140108106409852 and parameters: {'n_estimators': 711, 'eta': 3.8385489060287215e-05, 'max_depth': 9, 'alpha': 0.8844000000000001, 'lambda': 18.66125071537128, 'max_bin': 323}. Best is trial 426 with value: 0.707589437705248.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7076\n",
      "\tBest params:\n",
      "\t\tn_estimators: 741\n",
      "\t\teta: 0.037064671740369835\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.5585\n",
      "\t\tlambda: 21.372948829361285\n",
      "\t\tmax_bin: 322\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.739481    0.710332    0.690333    0.755649   \n",
      "1                    TP   39.000000   42.000000   37.000000   36.000000   \n",
      "2                    TN  198.000000  197.000000  198.000000  199.000000   \n",
      "3                    FP    2.000000    5.000000    2.000000    2.000000   \n",
      "4                    FN   29.000000   24.000000   31.000000   31.000000   \n",
      "5              Accuracy    0.884328    0.891791    0.876866    0.876866   \n",
      "6             Precision    0.951220    0.893617    0.948718    0.947368   \n",
      "7           Sensitivity    0.573529    0.636364    0.544118    0.537313   \n",
      "8           Specificity    0.990000    0.975200    0.990000    0.990000   \n",
      "9              F1 score    0.715596    0.743363    0.691589    0.685714   \n",
      "10  F1 score (weighted)    0.873659    0.885124    0.864341    0.864004   \n",
      "11     F1 score (macro)    0.821498    0.837402    0.807333    0.804574   \n",
      "12    Balanced Accuracy    0.781765    0.805806    0.767059    0.763682   \n",
      "13                  MCC    0.681210    0.692912    0.659108    0.654620   \n",
      "14                  NPV    0.872200    0.891400    0.864600    0.865200   \n",
      "15              ROC_AUC    0.781765    0.805806    0.767059    0.763682   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.686552    0.710546    0.745050    0.723587    0.706756  \n",
      "1    40.000000   44.000000   46.000000   47.000000   36.000000  \n",
      "2   199.000000  197.000000  198.000000  197.000000  199.000000  \n",
      "3     2.000000    3.000000    3.000000    4.000000    3.000000  \n",
      "4    27.000000   24.000000   21.000000   20.000000   30.000000  \n",
      "5     0.891791    0.899254    0.910448    0.910448    0.876866  \n",
      "6     0.952381    0.936170    0.938776    0.921569    0.923077  \n",
      "7     0.597015    0.647059    0.686567    0.701493    0.545455  \n",
      "8     0.990000    0.985000    0.985100    0.980100    0.985100  \n",
      "9     0.733945    0.765217    0.793103    0.796610    0.685714  \n",
      "10    0.882549    0.892568    0.905419    0.906090    0.864891  \n",
      "11    0.833015    0.850542    0.867980    0.869597    0.804574  \n",
      "12    0.793532    0.816029    0.835821    0.840796    0.765302  \n",
      "13    0.699266    0.723239    0.752407    0.751874    0.648287  \n",
      "14    0.880500    0.891400    0.904100    0.907800    0.869000  \n",
      "15    0.793532    0.816029    0.835821    0.840796    0.765302  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_xgb_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_8_cat = np.where(((y_pred_xgb_8 >= 2) | (y_pred_xgb_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:12:26,529] Trial 450 finished with value: 0.6944175904680344 and parameters: {'n_estimators': 667, 'eta': 0.03420680015714146, 'max_depth': 10, 'alpha': 0.5881000000000001, 'lambda': 20.57570000825541, 'max_bin': 315}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:12:35,620] Trial 451 finished with value: 0.6937594116011861 and parameters: {'n_estimators': 694, 'eta': 0.03674867538953142, 'max_depth': 10, 'alpha': 0.5669000000000001, 'lambda': 19.751173769039575, 'max_bin': 313}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:12:45,402] Trial 452 finished with value: 0.6957435785578825 and parameters: {'n_estimators': 725, 'eta': 0.03968615275239206, 'max_depth': 10, 'alpha': 0.6984, 'lambda': 22.545005699555787, 'max_bin': 329}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:12:55,755] Trial 453 finished with value: 0.6945147066055306 and parameters: {'n_estimators': 738, 'eta': 0.03404190734980883, 'max_depth': 10, 'alpha': 0.5706, 'lambda': 20.64521140286199, 'max_bin': 319}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:13:12,213] Trial 454 finished with value: 0.686334377968131 and parameters: {'n_estimators': 752, 'eta': 0.011569000594065362, 'max_depth': 10, 'alpha': 0.5949, 'lambda': 21.9673701102334, 'max_bin': 326}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:13:21,448] Trial 455 finished with value: 0.6950754288586711 and parameters: {'n_estimators': 712, 'eta': 0.03668576098004251, 'max_depth': 10, 'alpha': 0.557, 'lambda': 19.31673455974028, 'max_bin': 312}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:13:29,553] Trial 456 finished with value: 0.6941767561036574 and parameters: {'n_estimators': 682, 'eta': 0.04007187420270191, 'max_depth': 10, 'alpha': 0.5481, 'lambda': 17.71966867522704, 'max_bin': 330}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:13:39,432] Trial 457 finished with value: 0.6946942812880172 and parameters: {'n_estimators': 731, 'eta': 0.03430000066932398, 'max_depth': 10, 'alpha': 0.5406000000000001, 'lambda': 20.2715037990863, 'max_bin': 321}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:13:49,036] Trial 458 finished with value: 0.6955080658624503 and parameters: {'n_estimators': 751, 'eta': 0.037179072908629394, 'max_depth': 10, 'alpha': 0.5752, 'lambda': 21.264717337491174, 'max_bin': 324}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:13:59,216] Trial 459 finished with value: 0.6928090145741248 and parameters: {'n_estimators': 698, 'eta': 0.03304251837962542, 'max_depth': 10, 'alpha': 0.5449, 'lambda': 22.209273161092963, 'max_bin': 317}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:14:08,243] Trial 460 finished with value: 0.6962588811108062 and parameters: {'n_estimators': 716, 'eta': 0.039710676358921226, 'max_depth': 10, 'alpha': 0.5862, 'lambda': 18.550758942025208, 'max_bin': 315}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:14:17,794] Trial 461 finished with value: 0.6942510321822902 and parameters: {'n_estimators': 651, 'eta': 0.03510501732370691, 'max_depth': 10, 'alpha': 0.5707, 'lambda': 19.554957401613695, 'max_bin': 330}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:14:27,590] Trial 462 finished with value: 0.6956314270143995 and parameters: {'n_estimators': 753, 'eta': 0.03765981866275758, 'max_depth': 10, 'alpha': 0.546, 'lambda': 21.158704018862373, 'max_bin': 322}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:14:34,478] Trial 463 finished with value: 0.690916739626908 and parameters: {'n_estimators': 722, 'eta': 0.05176432575876212, 'max_depth': 10, 'alpha': 0.6033000000000001, 'lambda': 20.315897964613907, 'max_bin': 331}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:14:43,875] Trial 464 finished with value: 0.6940728466559124 and parameters: {'n_estimators': 684, 'eta': 0.033055688453494624, 'max_depth': 9, 'alpha': 0.5632, 'lambda': 21.885182115034542, 'max_bin': 312}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:14:54,488] Trial 465 finished with value: 0.6969359445485352 and parameters: {'n_estimators': 734, 'eta': 0.036254563162312986, 'max_depth': 10, 'alpha': 0.5366000000000001, 'lambda': 18.83903181595238, 'max_bin': 326}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:15:03,899] Trial 466 finished with value: 0.6956882547310002 and parameters: {'n_estimators': 703, 'eta': 0.03985630414662504, 'max_depth': 10, 'alpha': 0.5376000000000001, 'lambda': 20.812882764210098, 'max_bin': 320}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:15:14,299] Trial 467 finished with value: 0.6948353062002596 and parameters: {'n_estimators': 745, 'eta': 0.035115962685834966, 'max_depth': 10, 'alpha': 0.5889, 'lambda': 22.53639472751272, 'max_bin': 328}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:15:24,149] Trial 468 finished with value: 0.6968131527670398 and parameters: {'n_estimators': 717, 'eta': 0.03265839035433254, 'max_depth': 10, 'alpha': 0.5629000000000001, 'lambda': 17.566677897010983, 'max_bin': 315}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:15:33,201] Trial 469 finished with value: 0.6943234894761717 and parameters: {'n_estimators': 757, 'eta': 0.03803575396247775, 'max_depth': 10, 'alpha': 0.6209, 'lambda': 19.78732122193868, 'max_bin': 324}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:15:43,019] Trial 470 finished with value: 0.694088639602424 and parameters: {'n_estimators': 696, 'eta': 0.034431259939606766, 'max_depth': 10, 'alpha': 0.8044, 'lambda': 21.550862971414805, 'max_bin': 330}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:15:52,785] Trial 471 finished with value: 0.6955958598632555 and parameters: {'n_estimators': 732, 'eta': 0.04018733158940186, 'max_depth': 10, 'alpha': 0.5361, 'lambda': 23.167349114427054, 'max_bin': 322}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:16:01,492] Trial 472 finished with value: 0.6964079676516 and parameters: {'n_estimators': 676, 'eta': 0.03657288398659855, 'max_depth': 9, 'alpha': 0.561, 'lambda': 20.30933954602239, 'max_bin': 312}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:16:11,624] Trial 473 finished with value: 0.6941328421840594 and parameters: {'n_estimators': 712, 'eta': 0.031585993357598645, 'max_depth': 10, 'alpha': 0.5977, 'lambda': 21.071455397337118, 'max_bin': 317}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:16:21,028] Trial 474 finished with value: 0.6959020891649976 and parameters: {'n_estimators': 740, 'eta': 0.03872943366262484, 'max_depth': 10, 'alpha': 0.8582000000000001, 'lambda': 18.93804661989438, 'max_bin': 332}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:16:31,697] Trial 475 finished with value: 0.694497437885212 and parameters: {'n_estimators': 756, 'eta': 0.03471074844620301, 'max_depth': 10, 'alpha': 0.5276000000000001, 'lambda': 22.212128049956434, 'max_bin': 326}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:16:41,714] Trial 476 finished with value: 0.6946872006666653 and parameters: {'n_estimators': 661, 'eta': 0.03620859637695673, 'max_depth': 10, 'alpha': 0.5674, 'lambda': 18.135995208380923, 'max_bin': 309}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:16:51,846] Trial 477 finished with value: 0.6933644465217145 and parameters: {'n_estimators': 723, 'eta': 0.03248371322894643, 'max_depth': 10, 'alpha': 0.5442, 'lambda': 19.664733114911595, 'max_bin': 333}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:17:01,012] Trial 478 finished with value: 0.6930210438651352 and parameters: {'n_estimators': 693, 'eta': 0.041244090352151604, 'max_depth': 10, 'alpha': 0.5739000000000001, 'lambda': 23.721208093778746, 'max_bin': 484}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:17:10,367] Trial 479 finished with value: 0.6958349546104284 and parameters: {'n_estimators': 761, 'eta': 0.03788809566973137, 'max_depth': 10, 'alpha': 0.524, 'lambda': 21.627258634493927, 'max_bin': 321}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:17:16,179] Trial 480 finished with value: 0.6922348910632413 and parameters: {'n_estimators': 617, 'eta': 0.08640043130616773, 'max_depth': 10, 'alpha': 0.5524, 'lambda': 20.66795142247872, 'max_bin': 327}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:17:35,017] Trial 481 finished with value: 0.6059079604257799 and parameters: {'n_estimators': 740, 'eta': 0.003149841707852992, 'max_depth': 10, 'alpha': 0.5857, 'lambda': 20.21702956866675, 'max_bin': 320}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:17:42,290] Trial 482 finished with value: 0.6963565852257385 and parameters: {'n_estimators': 705, 'eta': 0.05372365103814678, 'max_depth': 10, 'alpha': 0.5196000000000001, 'lambda': 17.43178614428443, 'max_bin': 332}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:17:52,808] Trial 483 finished with value: 0.6956367303580985 and parameters: {'n_estimators': 729, 'eta': 0.03421481292420429, 'max_depth': 10, 'alpha': 0.6069, 'lambda': 19.307028000698672, 'max_bin': 316}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:18:02,255] Trial 484 finished with value: 0.6926901735657809 and parameters: {'n_estimators': 712, 'eta': 0.036536815828561095, 'max_depth': 10, 'alpha': 0.737, 'lambda': 22.91005805516043, 'max_bin': 327}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:18:12,467] Trial 485 finished with value: 0.6925743845409541 and parameters: {'n_estimators': 680, 'eta': 0.03211230762872314, 'max_depth': 10, 'alpha': 0.5322, 'lambda': 21.645804051524834, 'max_bin': 333}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:18:21,626] Trial 486 finished with value: 0.6944640069883851 and parameters: {'n_estimators': 762, 'eta': 0.038905615376481795, 'max_depth': 10, 'alpha': 0.5545, 'lambda': 21.018805121270823, 'max_bin': 322}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:18:29,876] Trial 487 finished with value: 0.6903972085172375 and parameters: {'n_estimators': 746, 'eta': 0.05961645284874768, 'max_depth': 10, 'alpha': 0.5786, 'lambda': 22.541715720688984, 'max_bin': 311}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:18:39,248] Trial 488 finished with value: 0.695809645843198 and parameters: {'n_estimators': 722, 'eta': 0.04107022080598016, 'max_depth': 9, 'alpha': 0.7176, 'lambda': 18.618440067864068, 'max_bin': 325}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:18:45,716] Trial 489 finished with value: 0.6917103356029342 and parameters: {'n_estimators': 693, 'eta': 0.06229942513909312, 'max_depth': 10, 'alpha': 0.543, 'lambda': 19.70155546485129, 'max_bin': 317}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:18:51,324] Trial 490 finished with value: 0.6957925454954933 and parameters: {'n_estimators': 733, 'eta': 0.07666351568743701, 'max_depth': 10, 'alpha': 0.9335, 'lambda': 20.6020068280312, 'max_bin': 334}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:19:00,599] Trial 491 finished with value: 0.691958562172597 and parameters: {'n_estimators': 701, 'eta': 0.0341712102454424, 'max_depth': 10, 'alpha': 0.5124000000000001, 'lambda': 22.229948729660148, 'max_bin': 328}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:19:07,103] Trial 492 finished with value: 0.695803915760218 and parameters: {'n_estimators': 765, 'eta': 0.06875829316233498, 'max_depth': 10, 'alpha': 0.6692, 'lambda': 21.534356014996952, 'max_bin': 316}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:19:16,104] Trial 493 finished with value: 0.6973568878543885 and parameters: {'n_estimators': 746, 'eta': 0.036508423133432416, 'max_depth': 10, 'alpha': 0.5659000000000001, 'lambda': 16.130416180352924, 'max_bin': 322}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:19:26,620] Trial 494 finished with value: 0.691421638942612 and parameters: {'n_estimators': 720, 'eta': 0.03833370206603878, 'max_depth': 10, 'alpha': 0.7753, 'lambda': 24.157416229252657, 'max_bin': 336}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:19:32,391] Trial 495 finished with value: 0.6940650716328902 and parameters: {'n_estimators': 673, 'eta': 0.08121197900567682, 'max_depth': 10, 'alpha': 0.5328, 'lambda': 22.944148140964938, 'max_bin': 309}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:19:43,156] Trial 496 finished with value: 0.6942880550943284 and parameters: {'n_estimators': 710, 'eta': 0.03136198567734533, 'max_depth': 10, 'alpha': 0.5901000000000001, 'lambda': 20.0739691845134, 'max_bin': 330}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:19:53,364] Trial 497 finished with value: 0.6937177930156142 and parameters: {'n_estimators': 736, 'eta': 0.0354059382942882, 'max_depth': 10, 'alpha': 0.6277, 'lambda': 20.946640695268407, 'max_bin': 325}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:20:02,274] Trial 498 finished with value: 0.6949890584659648 and parameters: {'n_estimators': 761, 'eta': 0.04055632293045303, 'max_depth': 10, 'alpha': 0.557, 'lambda': 17.274584159189775, 'max_bin': 319}. Best is trial 426 with value: 0.707589437705248.\n",
      "[I 2023-12-12 01:20:11,656] Trial 499 finished with value: 0.6967086321049221 and parameters: {'n_estimators': 696, 'eta': 0.03342321948057184, 'max_depth': 10, 'alpha': 0.5104000000000001, 'lambda': 18.23334595189295, 'max_bin': 335}. Best is trial 426 with value: 0.707589437705248.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7076\n",
      "\tBest params:\n",
      "\t\tn_estimators: 741\n",
      "\t\teta: 0.037064671740369835\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.5585\n",
      "\t\tlambda: 21.372948829361285\n",
      "\t\tmax_bin: 322\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.739481    0.710332    0.690333    0.755649   \n",
      "1                    TP   39.000000   42.000000   37.000000   36.000000   \n",
      "2                    TN  198.000000  197.000000  198.000000  199.000000   \n",
      "3                    FP    2.000000    5.000000    2.000000    2.000000   \n",
      "4                    FN   29.000000   24.000000   31.000000   31.000000   \n",
      "5              Accuracy    0.884328    0.891791    0.876866    0.876866   \n",
      "6             Precision    0.951220    0.893617    0.948718    0.947368   \n",
      "7           Sensitivity    0.573529    0.636364    0.544118    0.537313   \n",
      "8           Specificity    0.990000    0.975200    0.990000    0.990000   \n",
      "9              F1 score    0.715596    0.743363    0.691589    0.685714   \n",
      "10  F1 score (weighted)    0.873659    0.885124    0.864341    0.864004   \n",
      "11     F1 score (macro)    0.821498    0.837402    0.807333    0.804574   \n",
      "12    Balanced Accuracy    0.781765    0.805806    0.767059    0.763682   \n",
      "13                  MCC    0.681210    0.692912    0.659108    0.654620   \n",
      "14                  NPV    0.872200    0.891400    0.864600    0.865200   \n",
      "15              ROC_AUC    0.781765    0.805806    0.767059    0.763682   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.686552    0.710546    0.745050    0.723587    0.706756    0.688991  \n",
      "1    40.000000   44.000000   46.000000   47.000000   36.000000   39.000000  \n",
      "2   199.000000  197.000000  198.000000  197.000000  199.000000  199.000000  \n",
      "3     2.000000    3.000000    3.000000    4.000000    3.000000    3.000000  \n",
      "4    27.000000   24.000000   21.000000   20.000000   30.000000   27.000000  \n",
      "5     0.891791    0.899254    0.910448    0.910448    0.876866    0.888060  \n",
      "6     0.952381    0.936170    0.938776    0.921569    0.923077    0.928571  \n",
      "7     0.597015    0.647059    0.686567    0.701493    0.545455    0.590909  \n",
      "8     0.990000    0.985000    0.985100    0.980100    0.985100    0.985100  \n",
      "9     0.733945    0.765217    0.793103    0.796610    0.685714    0.722222  \n",
      "10    0.882549    0.892568    0.905419    0.906090    0.864891    0.878760  \n",
      "11    0.833015    0.850542    0.867980    0.869597    0.804574    0.826064  \n",
      "12    0.793532    0.816029    0.835821    0.840796    0.765302    0.788029  \n",
      "13    0.699266    0.723239    0.752407    0.751874    0.648287    0.682708  \n",
      "14    0.880500    0.891400    0.904100    0.907800    0.869000    0.880500  \n",
      "15    0.793532    0.816029    0.835821    0.840796    0.765302    0.788029  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_xgb_9)\n",
    "# now convert the resuls to binary with cutoff 6.9\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_9_cat = np.where(((y_pred_xgb_9 >= 2) | (y_pred_xgb_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.722228</td>\n",
       "      <td>0.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>3.025815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>98.500000</td>\n",
       "      <td>1.509231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.563472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2.943920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.880507</td>\n",
       "      <td>0.022528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.909793</td>\n",
       "      <td>0.061936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.580717</td>\n",
       "      <td>0.089108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.980140</td>\n",
       "      <td>0.015465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.704608</td>\n",
       "      <td>0.069960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.870115</td>\n",
       "      <td>0.026749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.814805</td>\n",
       "      <td>0.041363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.780428</td>\n",
       "      <td>0.043975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.662498</td>\n",
       "      <td>0.070517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.876120</td>\n",
       "      <td>0.022732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.780428</td>\n",
       "      <td>0.043975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.722228     0.050300\n",
       "1                    TP        19.400000     3.025815\n",
       "2                    TN        98.500000     1.509231\n",
       "3                    FP         2.000000     1.563472\n",
       "4                    FN        14.000000     2.943920\n",
       "5              Accuracy         0.880507     0.022528\n",
       "6             Precision         0.909793     0.061936\n",
       "7           Sensitivity         0.580717     0.089108\n",
       "8           Specificity         0.980140     0.015465\n",
       "9              F1 score         0.704608     0.069960\n",
       "10  F1 score (weighted)         0.870115     0.026749\n",
       "11     F1 score (macro)         0.814805     0.041363\n",
       "12    Balanced Accuracy         0.780428     0.043975\n",
       "13                  MCC         0.662498     0.070517\n",
       "14                  NPV         0.876120     0.022732\n",
       "15              ROC_AUC         0.780428     0.043975"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.739481</td>\n",
       "      <td>0.710332</td>\n",
       "      <td>0.690333</td>\n",
       "      <td>0.755649</td>\n",
       "      <td>0.686552</td>\n",
       "      <td>0.710546</td>\n",
       "      <td>0.745050</td>\n",
       "      <td>0.723587</td>\n",
       "      <td>0.706756</td>\n",
       "      <td>0.688991</td>\n",
       "      <td>0.715728</td>\n",
       "      <td>0.024556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>40.600000</td>\n",
       "      <td>4.005552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>198.100000</td>\n",
       "      <td>0.875595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.994429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>4.005552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.890672</td>\n",
       "      <td>0.012812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.934147</td>\n",
       "      <td>0.018230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>0.537313</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.605982</td>\n",
       "      <td>0.059436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.975200</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.980100</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.985560</td>\n",
       "      <td>0.004910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.715596</td>\n",
       "      <td>0.743363</td>\n",
       "      <td>0.691589</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.733945</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.733307</td>\n",
       "      <td>0.041379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.873659</td>\n",
       "      <td>0.885124</td>\n",
       "      <td>0.864341</td>\n",
       "      <td>0.864004</td>\n",
       "      <td>0.882549</td>\n",
       "      <td>0.892568</td>\n",
       "      <td>0.905419</td>\n",
       "      <td>0.906090</td>\n",
       "      <td>0.864891</td>\n",
       "      <td>0.878760</td>\n",
       "      <td>0.881741</td>\n",
       "      <td>0.015841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.821498</td>\n",
       "      <td>0.837402</td>\n",
       "      <td>0.807333</td>\n",
       "      <td>0.804574</td>\n",
       "      <td>0.833015</td>\n",
       "      <td>0.850542</td>\n",
       "      <td>0.867980</td>\n",
       "      <td>0.869597</td>\n",
       "      <td>0.804574</td>\n",
       "      <td>0.826064</td>\n",
       "      <td>0.832258</td>\n",
       "      <td>0.024358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.781765</td>\n",
       "      <td>0.805806</td>\n",
       "      <td>0.767059</td>\n",
       "      <td>0.763682</td>\n",
       "      <td>0.793532</td>\n",
       "      <td>0.816029</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.765302</td>\n",
       "      <td>0.788029</td>\n",
       "      <td>0.795782</td>\n",
       "      <td>0.028257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.681210</td>\n",
       "      <td>0.692912</td>\n",
       "      <td>0.659108</td>\n",
       "      <td>0.654620</td>\n",
       "      <td>0.699266</td>\n",
       "      <td>0.723239</td>\n",
       "      <td>0.752407</td>\n",
       "      <td>0.751874</td>\n",
       "      <td>0.648287</td>\n",
       "      <td>0.682708</td>\n",
       "      <td>0.694563</td>\n",
       "      <td>0.037698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.872200</td>\n",
       "      <td>0.891400</td>\n",
       "      <td>0.864600</td>\n",
       "      <td>0.865200</td>\n",
       "      <td>0.880500</td>\n",
       "      <td>0.891400</td>\n",
       "      <td>0.904100</td>\n",
       "      <td>0.907800</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.880500</td>\n",
       "      <td>0.882670</td>\n",
       "      <td>0.015566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.781765</td>\n",
       "      <td>0.805806</td>\n",
       "      <td>0.767059</td>\n",
       "      <td>0.763682</td>\n",
       "      <td>0.793532</td>\n",
       "      <td>0.816029</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.765302</td>\n",
       "      <td>0.788029</td>\n",
       "      <td>0.795782</td>\n",
       "      <td>0.028257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.739481    0.710332    0.690333    0.755649   \n",
       "1                    TP   39.000000   42.000000   37.000000   36.000000   \n",
       "2                    TN  198.000000  197.000000  198.000000  199.000000   \n",
       "3                    FP    2.000000    5.000000    2.000000    2.000000   \n",
       "4                    FN   29.000000   24.000000   31.000000   31.000000   \n",
       "5              Accuracy    0.884328    0.891791    0.876866    0.876866   \n",
       "6             Precision    0.951220    0.893617    0.948718    0.947368   \n",
       "7           Sensitivity    0.573529    0.636364    0.544118    0.537313   \n",
       "8           Specificity    0.990000    0.975200    0.990000    0.990000   \n",
       "9              F1 score    0.715596    0.743363    0.691589    0.685714   \n",
       "10  F1 score (weighted)    0.873659    0.885124    0.864341    0.864004   \n",
       "11     F1 score (macro)    0.821498    0.837402    0.807333    0.804574   \n",
       "12    Balanced Accuracy    0.781765    0.805806    0.767059    0.763682   \n",
       "13                  MCC    0.681210    0.692912    0.659108    0.654620   \n",
       "14                  NPV    0.872200    0.891400    0.864600    0.865200   \n",
       "15              ROC_AUC    0.781765    0.805806    0.767059    0.763682   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.686552    0.710546    0.745050    0.723587    0.706756    0.688991   \n",
       "1    40.000000   44.000000   46.000000   47.000000   36.000000   39.000000   \n",
       "2   199.000000  197.000000  198.000000  197.000000  199.000000  199.000000   \n",
       "3     2.000000    3.000000    3.000000    4.000000    3.000000    3.000000   \n",
       "4    27.000000   24.000000   21.000000   20.000000   30.000000   27.000000   \n",
       "5     0.891791    0.899254    0.910448    0.910448    0.876866    0.888060   \n",
       "6     0.952381    0.936170    0.938776    0.921569    0.923077    0.928571   \n",
       "7     0.597015    0.647059    0.686567    0.701493    0.545455    0.590909   \n",
       "8     0.990000    0.985000    0.985100    0.980100    0.985100    0.985100   \n",
       "9     0.733945    0.765217    0.793103    0.796610    0.685714    0.722222   \n",
       "10    0.882549    0.892568    0.905419    0.906090    0.864891    0.878760   \n",
       "11    0.833015    0.850542    0.867980    0.869597    0.804574    0.826064   \n",
       "12    0.793532    0.816029    0.835821    0.840796    0.765302    0.788029   \n",
       "13    0.699266    0.723239    0.752407    0.751874    0.648287    0.682708   \n",
       "14    0.880500    0.891400    0.904100    0.907800    0.869000    0.880500   \n",
       "15    0.793532    0.816029    0.835821    0.840796    0.765302    0.788029   \n",
       "\n",
       "           ave       std  \n",
       "0     0.715728  0.024556  \n",
       "1    40.600000  4.005552  \n",
       "2   198.100000  0.875595  \n",
       "3     2.900000  0.994429  \n",
       "4    26.400000  4.005552  \n",
       "5     0.890672  0.012812  \n",
       "6     0.934147  0.018230  \n",
       "7     0.605982  0.059436  \n",
       "8     0.985560  0.004910  \n",
       "9     0.733307  0.041379  \n",
       "10    0.881741  0.015841  \n",
       "11    0.832258  0.024358  \n",
       "12    0.795782  0.028257  \n",
       "13    0.694563  0.037698  \n",
       "14    0.882670  0.015566  \n",
       "15    0.795782  0.028257  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test['std'] = mat_met_xgb_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.716604</td>\n",
       "      <td>0.058164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.882895</td>\n",
       "      <td>0.020725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.916214</td>\n",
       "      <td>0.051781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.586053</td>\n",
       "      <td>0.074981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.981686</td>\n",
       "      <td>0.012304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.711813</td>\n",
       "      <td>0.060321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.872853</td>\n",
       "      <td>0.024696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.819133</td>\n",
       "      <td>0.036204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.783869</td>\n",
       "      <td>0.037314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.670274</td>\n",
       "      <td>0.062327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.877412</td>\n",
       "      <td>0.020506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.783869</td>\n",
       "      <td>0.037314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.716604     0.058164\n",
       "1              Accuracy         0.882895     0.020725\n",
       "2             Precision         0.916214     0.051781\n",
       "3           Sensitivity         0.586053     0.074981\n",
       "4           Specificity         0.981686     0.012304\n",
       "5              F1 score         0.711813     0.060321\n",
       "6   F1 score (weighted)         0.872853     0.024696\n",
       "7      F1 score (macro)         0.819133     0.036204\n",
       "8     Balanced Accuracy         0.783869     0.037314\n",
       "9                   MCC         0.670274     0.062327\n",
       "10                  NPV         0.877412     0.020506\n",
       "11              ROC_AUC         0.783869     0.037314"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"rmse\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_xgb_cat = np.where(((y_pred_optimized_xgb >= 2) | (y_pred_optimized_xgb <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_xgb_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_xgb))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "xgb_run0 = data_xgb[['y_test_idx0', 'y_test0', 'y_pred_xgb0']]\n",
    "xgb_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "xgb_run0.reset_index(inplace=True, drop=True)\n",
    "xgb_run1 = data_xgb[['y_test_idx1', 'y_test1', 'y_pred_xgb1']]\n",
    "xgb_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "xgb_run1.reset_index(inplace=True, drop=True)\n",
    "xgb_run2 = data_xgb[['y_test_idx2', 'y_test2', 'y_pred_xgb2']]\n",
    "xgb_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "xgb_run2.reset_index(inplace=True, drop=True)\n",
    "xgb_run3 = data_xgb[['y_test_idx3', 'y_test3', 'y_pred_xgb3']]\n",
    "xgb_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "xgb_run3.reset_index(inplace=True, drop=True)\n",
    "xgb_run4 = data_xgb[['y_test_idx4', 'y_test4', 'y_pred_xgb4']]\n",
    "xgb_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "xgb_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "xgb_5preds = pd.concat([chembl_id, xgb_run0, xgb_run1, xgb_run2, xgb_run3, xgb_run4], axis=1)\n",
    "xgb_5preds = xgb_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_xgb0', 'y_pred_xgb1', 'y_pred_xgb2', 'y_pred_xgb3', 'y_pred_xgb4']]\n",
    "xgb_5preds['y_pred_xgb_ave'] = xgb_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "xgb_5preds['y_pred_xgb_std'] = xgb_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "\n",
    "mat_met_optimized_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "02aaad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b19aca7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt  \n",
    "# retrieve performance metrics\n",
    "results = optimized_xgb_0.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "    \n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "pyplot.show()\n",
    "\n",
    " # plot classification error\n",
    "#fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "#ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "#ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "#ax.legend()\n",
    "    \n",
    "#pyplot.ylabel('Classification Error')\n",
    "#pyplot.title('XGBoost Classification Error')\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost baseline model r2_score 0.6939 with a standard deviation of 0.0575\n",
      "XGBoost optimized model r2_score 0.7308 with a standard deviation of 0.0444\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_reg, X, Y, cv=10, scoring=\"r2\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_xgb.joblib']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_reg, \"OUTPUT/xgb_reg.joblib\")\n",
    "#joblib.dump(optimized_xgb, \"OUTPUT/optimized_xgb.joblib\")\n",
    "joblib.dump(optimizedCV_xgb, \"OUTPUT/optimizedCV_xgb.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.686656     0.087485\n",
      "1                    TP        22.000000     2.449490\n",
      "2                    TN        98.000000     1.943651\n",
      "3                    FP         2.500000     1.354006\n",
      "4                    FN        11.400000     2.875181\n",
      "5              Accuracy         0.896218     0.025640\n",
      "6             Precision         0.899599     0.050026\n",
      "7           Sensitivity         0.659701     0.081077\n",
      "8           Specificity         0.975070     0.013645\n",
      "9              F1 score         0.758859     0.062419\n",
      "10  F1 score (weighted)         0.890028     0.028307\n",
      "11     F1 score (macro)         0.846340     0.039100\n",
      "12    Balanced Accuracy         0.817384     0.042281\n",
      "13                  MCC         0.710045     0.071196\n",
      "14                  NPV         0.896210     0.024361\n",
      "15              ROC_AUC         0.817384     0.042281\n",
      "CPU times: user 881 ms, sys: 2.67 s, total: 3.55 s\n",
      "Wall time: 151 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    \n",
    "    knn_reg.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:22:30,936] A new study created in memory with name: KNNregressor\n",
      "[I 2023-12-12 01:22:31,112] Trial 0 finished with value: 0.6568463466565081 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 91}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:31,265] Trial 1 finished with value: 0.5162528613089643 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 65}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:31,413] Trial 2 finished with value: 0.45039185829222994 and parameters: {'n_neighbors': 29, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:31,564] Trial 3 finished with value: 0.545566610687367 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 90}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:31,716] Trial 4 finished with value: 0.5389704690952233 and parameters: {'n_neighbors': 16, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:31,860] Trial 5 finished with value: 0.563797882158541 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 84}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:32,004] Trial 6 finished with value: 0.5998841590430037 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:32,149] Trial 7 finished with value: 0.5895543229914691 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:32,293] Trial 8 finished with value: 0.6246620918136917 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:32,441] Trial 9 finished with value: 0.5584555803761921 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:32,594] Trial 10 finished with value: 0.6520240531019507 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:32,748] Trial 11 finished with value: 0.6568463466565081 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 68}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:32,907] Trial 12 finished with value: 0.6090040120186521 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 78}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:33,065] Trial 13 finished with value: 0.6568463466565081 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:33,223] Trial 14 finished with value: 0.5886282214316244 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 55}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:33,375] Trial 15 finished with value: 0.6568463466565081 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:33,528] Trial 16 finished with value: 0.5993264253661069 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:33,687] Trial 17 finished with value: 0.5133820896576845 and parameters: {'n_neighbors': 19, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 57}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:33,845] Trial 18 finished with value: 0.4929952549933553 and parameters: {'n_neighbors': 23, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 78}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:33,998] Trial 19 finished with value: 0.6246620918136917 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:34,152] Trial 20 finished with value: 0.5886282214316244 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 52}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:34,312] Trial 21 finished with value: 0.6568463466565081 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:34,468] Trial 22 finished with value: 0.6341250469781872 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:34,618] Trial 23 finished with value: 0.6568463466565081 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 81}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:34,761] Trial 24 finished with value: 0.5724959472692931 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 92}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:34,911] Trial 25 finished with value: 0.6341250469781872 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 67}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:35,063] Trial 26 finished with value: 0.6423111839221977 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:35,214] Trial 27 finished with value: 0.6341250469781872 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 90}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:35,366] Trial 28 finished with value: 0.5321501240742308 and parameters: {'n_neighbors': 17, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 48}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:35,517] Trial 29 finished with value: 0.5691407494713142 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:35,664] Trial 30 finished with value: 0.5993264253661069 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 86}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:35,812] Trial 31 finished with value: 0.6568463466565081 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:35,954] Trial 32 finished with value: 0.6568463466565081 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:36,098] Trial 33 finished with value: 0.44528757800013785 and parameters: {'n_neighbors': 30, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:36,241] Trial 34 finished with value: 0.6520240531019507 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:36,383] Trial 35 finished with value: 0.6246620918136917 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 0 with value: 0.6568463466565081.\n",
      "[I 2023-12-12 01:22:36,526] Trial 36 finished with value: 0.6850730262822318 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:36,672] Trial 37 finished with value: 0.607309390589654 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 82}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:36,823] Trial 38 finished with value: 0.6624087508750722 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:36,972] Trial 39 finished with value: 0.6622553993776713 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:37,115] Trial 40 finished with value: 0.6622553993776713 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:37,258] Trial 41 finished with value: 0.6622553993776713 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:37,401] Trial 42 finished with value: 0.6574696239651824 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:37,545] Trial 43 finished with value: 0.6316075235172177 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:37,688] Trial 44 finished with value: 0.6405399821265656 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:37,832] Trial 45 finished with value: 0.6622553993776713 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:37,975] Trial 46 finished with value: 0.6768101062884997 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:38,118] Trial 47 finished with value: 0.6768101062884997 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:38,290] Trial 48 finished with value: 0.6768101062884997 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:38,455] Trial 49 finished with value: 0.6768101062884997 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 36 with value: 0.6850730262822318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6851\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 82\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNregressor\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.749303\n",
      "1                    TP   43.000000\n",
      "2                    TN  196.000000\n",
      "3                    FP    4.000000\n",
      "4                    FN   25.000000\n",
      "5              Accuracy    0.891791\n",
      "6             Precision    0.914894\n",
      "7           Sensitivity    0.632353\n",
      "8           Specificity    0.980000\n",
      "9              F1 score    0.747826\n",
      "10  F1 score (weighted)    0.884610\n",
      "11     F1 score (macro)    0.839471\n",
      "12    Balanced Accuracy    0.806176\n",
      "13                  MCC    0.700690\n",
      "14                  NPV    0.886900\n",
      "15              ROC_AUC    0.806176\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_knn_0)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_knn_0_cat = np.where(((y_pred_knn_0 >= 2) | (y_pred_knn_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_knn_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:22:38,691] Trial 50 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:38,839] Trial 51 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:38,986] Trial 52 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:39,134] Trial 53 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:39,281] Trial 54 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:39,428] Trial 55 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:39,575] Trial 56 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:39,724] Trial 57 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:39,872] Trial 58 finished with value: 0.669767241001642 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:40,020] Trial 59 finished with value: 0.5765431790749832 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:40,168] Trial 60 finished with value: 0.5542004276947122 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:40,318] Trial 61 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:40,469] Trial 62 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:40,623] Trial 63 finished with value: 0.6782123881560695 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:40,778] Trial 64 finished with value: 0.669767241001642 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:40,927] Trial 65 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:41,075] Trial 66 finished with value: 0.6524291233693813 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:41,229] Trial 67 finished with value: 0.6528132347252841 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:41,381] Trial 68 finished with value: 0.6782123881560695 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:41,549] Trial 69 finished with value: 0.6584239240470058 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:41,698] Trial 70 finished with value: 0.6782123881560695 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:41,850] Trial 71 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:42,005] Trial 72 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:42,163] Trial 73 finished with value: 0.669767241001642 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:42,318] Trial 74 finished with value: 0.6782123881560695 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:42,474] Trial 75 finished with value: 0.6751672386353414 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:42,642] Trial 76 finished with value: 0.6528132347252841 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 21}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:42,802] Trial 77 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:42,955] Trial 78 finished with value: 0.590250861855141 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:43,108] Trial 79 finished with value: 0.6751672386353414 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:43,261] Trial 80 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:43,414] Trial 81 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:43,567] Trial 82 finished with value: 0.6782123881560695 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:43,721] Trial 83 finished with value: 0.6178853055542743 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:43,874] Trial 84 finished with value: 0.6751672386353414 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:44,027] Trial 85 finished with value: 0.6607899354865604 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:44,200] Trial 86 finished with value: 0.669767241001642 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:44,380] Trial 87 finished with value: 0.6782123881560695 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:44,535] Trial 88 finished with value: 0.6705216368704507 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 59}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:44,696] Trial 89 finished with value: 0.6751672386353414 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:44,852] Trial 90 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:45,006] Trial 91 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:45,167] Trial 92 finished with value: 0.6782123881560695 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:45,326] Trial 93 finished with value: 0.6751672386353414 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:45,476] Trial 94 finished with value: 0.669767241001642 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:45,625] Trial 95 finished with value: 0.6607899354865604 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:45,773] Trial 96 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:45,922] Trial 97 finished with value: 0.6782123881560695 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:46,071] Trial 98 finished with value: 0.681797438816964 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:46,220] Trial 99 finished with value: 0.6751672386353414 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 36 with value: 0.6850730262822318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6851\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 82\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.749303    0.704667\n",
      "1                    TP   43.000000   49.000000\n",
      "2                    TN  196.000000  196.000000\n",
      "3                    FP    4.000000    6.000000\n",
      "4                    FN   25.000000   17.000000\n",
      "5              Accuracy    0.891791    0.914179\n",
      "6             Precision    0.914894    0.890909\n",
      "7           Sensitivity    0.632353    0.742424\n",
      "8           Specificity    0.980000    0.970300\n",
      "9              F1 score    0.747826    0.809917\n",
      "10  F1 score (weighted)    0.884610    0.911416\n",
      "11     F1 score (macro)    0.839471    0.877248\n",
      "12    Balanced Accuracy    0.806176    0.856361\n",
      "13                  MCC    0.700690    0.760320\n",
      "14                  NPV    0.886900    0.920200\n",
      "15              ROC_AUC    0.806176    0.856361\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_knn_1)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet1_cat = np.where(((Y_testSet1>=2) | (Y_testSet1<=-2)), 1, 0) \n",
    "y_pred_knn_1_cat = np.where(((y_pred_knn_1 >= 2) | (y_pred_knn_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_knn_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:22:46,411] Trial 100 finished with value: 0.6783095124339762 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 36 with value: 0.6850730262822318.\n",
      "[I 2023-12-12 01:22:46,557] Trial 101 finished with value: 0.6932077046134063 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 101 with value: 0.6932077046134063.\n",
      "[I 2023-12-12 01:22:46,702] Trial 102 finished with value: 0.6932077046134063 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 101 with value: 0.6932077046134063.\n",
      "[I 2023-12-12 01:22:46,847] Trial 103 finished with value: 0.6981056883532422 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:46,992] Trial 104 finished with value: 0.6981056883532422 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:47,137] Trial 105 finished with value: 0.6981056883532422 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:47,285] Trial 106 finished with value: 0.6981056883532422 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:47,432] Trial 107 finished with value: 0.6981056883532422 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:47,584] Trial 108 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:47,739] Trial 109 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:47,897] Trial 110 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:48,055] Trial 111 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:48,214] Trial 112 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:48,374] Trial 113 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:48,533] Trial 114 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:48,694] Trial 115 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:48,860] Trial 116 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:49,026] Trial 117 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:49,191] Trial 118 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:49,355] Trial 119 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 20}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:49,520] Trial 120 finished with value: 0.5167760826506014 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:49,683] Trial 121 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:49,845] Trial 122 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:50,006] Trial 123 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:50,155] Trial 124 finished with value: 0.6697120682737682 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:50,308] Trial 125 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:50,464] Trial 126 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:50,613] Trial 127 finished with value: 0.6697120682737682 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:50,764] Trial 128 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:50,915] Trial 129 finished with value: 0.6829109230777798 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:51,070] Trial 130 finished with value: 0.6697120682737682 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:51,228] Trial 131 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:51,384] Trial 132 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:51,540] Trial 133 finished with value: 0.6829109230777798 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:51,694] Trial 134 finished with value: 0.6829109230777798 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 20}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:51,851] Trial 135 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:52,010] Trial 136 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:52,167] Trial 137 finished with value: 0.6697120682737682 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:52,323] Trial 138 finished with value: 0.6829109230777798 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:52,480] Trial 139 finished with value: 0.4945334116730673 and parameters: {'n_neighbors': 23, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 34}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:52,636] Trial 140 finished with value: 0.6829109230777798 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:52,792] Trial 141 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:52,950] Trial 142 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:53,106] Trial 143 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:53,264] Trial 144 finished with value: 0.6829109230777798 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:53,420] Trial 145 finished with value: 0.6829109230777798 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 21}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:53,578] Trial 146 finished with value: 0.5871549969287736 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:53,735] Trial 147 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:53,891] Trial 148 finished with value: 0.6697120682737682 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:54,048] Trial 149 finished with value: 0.689244053863962 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 103 with value: 0.6981056883532422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6981\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 27\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.749303    0.704667    0.662835\n",
      "1                    TP   43.000000   49.000000   41.000000\n",
      "2                    TN  196.000000  196.000000  197.000000\n",
      "3                    FP    4.000000    6.000000    3.000000\n",
      "4                    FN   25.000000   17.000000   27.000000\n",
      "5              Accuracy    0.891791    0.914179    0.888060\n",
      "6             Precision    0.914894    0.890909    0.931818\n",
      "7           Sensitivity    0.632353    0.742424    0.602941\n",
      "8           Specificity    0.980000    0.970300    0.985000\n",
      "9              F1 score    0.747826    0.809917    0.732143\n",
      "10  F1 score (weighted)    0.884610    0.911416    0.879234\n",
      "11     F1 score (macro)    0.839471    0.877248    0.830694\n",
      "12    Balanced Accuracy    0.806176    0.856361    0.793971\n",
      "13                  MCC    0.700690    0.760320    0.690642\n",
      "14                  NPV    0.886900    0.920200    0.879500\n",
      "15              ROC_AUC    0.806176    0.856361    0.793971\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_knn_2)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet2_cat = np.where(((Y_testSet2>=2) | (Y_testSet2<=-2)), 1, 0) \n",
    "y_pred_knn_2_cat = np.where(((y_pred_knn_2 >= 2) | (y_pred_knn_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_knn_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:22:54,266] Trial 150 finished with value: 0.644794579466623 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:54,423] Trial 151 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:54,580] Trial 152 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:54,738] Trial 153 finished with value: 0.644794579466623 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:54,895] Trial 154 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:55,053] Trial 155 finished with value: 0.644794579466623 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:55,209] Trial 156 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:55,366] Trial 157 finished with value: 0.644794579466623 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:55,524] Trial 158 finished with value: 0.6380403594083159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:55,683] Trial 159 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 20}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:55,850] Trial 160 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:56,007] Trial 161 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:56,164] Trial 162 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:56,321] Trial 163 finished with value: 0.644794579466623 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:56,478] Trial 164 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:56,634] Trial 165 finished with value: 0.644794579466623 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:56,791] Trial 166 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:56,948] Trial 167 finished with value: 0.644794579466623 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:57,102] Trial 168 finished with value: 0.5492214696203326 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:57,252] Trial 169 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:57,409] Trial 170 finished with value: 0.6380403594083159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:57,566] Trial 171 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:57,722] Trial 172 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:57,880] Trial 173 finished with value: 0.644794579466623 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:58,038] Trial 174 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:58,196] Trial 175 finished with value: 0.644794579466623 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 37}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:58,351] Trial 176 finished with value: 0.6594439071264165 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:58,504] Trial 177 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:58,662] Trial 178 finished with value: 0.6533526694332967 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:58,815] Trial 179 finished with value: 0.512188639398091 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 21}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:58,965] Trial 180 finished with value: 0.6594439071264165 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:59,116] Trial 181 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:59,274] Trial 182 finished with value: 0.644794579466623 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 20}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:59,432] Trial 183 finished with value: 0.644794579466623 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 20}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:59,590] Trial 184 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:59,748] Trial 185 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:22:59,906] Trial 186 finished with value: 0.6380403594083159 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:00,062] Trial 187 finished with value: 0.6594439071264165 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:00,212] Trial 188 finished with value: 0.644794579466623 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:00,370] Trial 189 finished with value: 0.644794579466623 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:00,524] Trial 190 finished with value: 0.6359358749835937 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:00,676] Trial 191 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:00,834] Trial 192 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:00,992] Trial 193 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:01,150] Trial 194 finished with value: 0.644794579466623 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:01,308] Trial 195 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:01,468] Trial 196 finished with value: 0.6054051585895653 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:01,642] Trial 197 finished with value: 0.6594439071264165 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:01,803] Trial 198 finished with value: 0.644794579466623 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 21}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:01,960] Trial 199 finished with value: 0.6528912206628014 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6981\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 27\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.749303    0.704667    0.662835    0.767822\n",
      "1                    TP   43.000000   49.000000   41.000000   40.000000\n",
      "2                    TN  196.000000  196.000000  197.000000  198.000000\n",
      "3                    FP    4.000000    6.000000    3.000000    3.000000\n",
      "4                    FN   25.000000   17.000000   27.000000   27.000000\n",
      "5              Accuracy    0.891791    0.914179    0.888060    0.888060\n",
      "6             Precision    0.914894    0.890909    0.931818    0.930233\n",
      "7           Sensitivity    0.632353    0.742424    0.602941    0.597015\n",
      "8           Specificity    0.980000    0.970300    0.985000    0.985100\n",
      "9              F1 score    0.747826    0.809917    0.732143    0.727273\n",
      "10  F1 score (weighted)    0.884610    0.911416    0.879234    0.879001\n",
      "11     F1 score (macro)    0.839471    0.877248    0.830694    0.828425\n",
      "12    Balanced Accuracy    0.806176    0.856361    0.793971    0.791045\n",
      "13                  MCC    0.700690    0.760320    0.690642    0.686752\n",
      "14                  NPV    0.886900    0.920200    0.879500    0.880000\n",
      "15              ROC_AUC    0.806176    0.856361    0.793971    0.791045\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_knn_3)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet3_cat = np.where(((Y_testSet3>=2) | (Y_testSet3<=-2)), 1, 0) \n",
    "y_pred_knn_3_cat = np.where(((y_pred_knn_3 >= 2) | (y_pred_knn_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_knn_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:23:02,169] Trial 200 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:02,317] Trial 201 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:02,467] Trial 202 finished with value: 0.6885431853480317 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:02,616] Trial 203 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:02,766] Trial 204 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:02,925] Trial 205 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:03,074] Trial 206 finished with value: 0.6885431853480317 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:03,225] Trial 207 finished with value: 0.6885431853480317 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:03,375] Trial 208 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:03,526] Trial 209 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:03,675] Trial 210 finished with value: 0.6885431853480317 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:03,826] Trial 211 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:03,976] Trial 212 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:04,127] Trial 213 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:04,277] Trial 214 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:04,427] Trial 215 finished with value: 0.6885431853480317 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:04,578] Trial 216 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:04,728] Trial 217 finished with value: 0.6828968030328034 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:04,879] Trial 218 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:05,029] Trial 219 finished with value: 0.6885431853480317 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:05,180] Trial 220 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:05,330] Trial 221 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:05,481] Trial 222 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:05,632] Trial 223 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:05,782] Trial 224 finished with value: 0.6885431853480317 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:05,932] Trial 225 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:06,083] Trial 226 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:06,233] Trial 227 finished with value: 0.6885431853480317 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:06,384] Trial 228 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:06,535] Trial 229 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:06,685] Trial 230 finished with value: 0.6828968030328034 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:06,836] Trial 231 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:06,987] Trial 232 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:07,137] Trial 233 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:07,316] Trial 234 finished with value: 0.6885431853480317 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:07,494] Trial 235 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:07,706] Trial 236 finished with value: 0.6885431853480317 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:07,856] Trial 237 finished with value: 0.6645655214623757 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:08,006] Trial 238 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:08,156] Trial 239 finished with value: 0.6885431853480317 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:08,307] Trial 240 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:08,458] Trial 241 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:08,609] Trial 242 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:08,760] Trial 243 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:08,911] Trial 244 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:09,063] Trial 245 finished with value: 0.6885431853480317 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:09,214] Trial 246 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:09,365] Trial 247 finished with value: 0.6885431853480317 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:09,517] Trial 248 finished with value: 0.6916292058762472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:09,670] Trial 249 finished with value: 0.561700606509412 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6981\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 27\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.749303    0.704667    0.662835    0.767822   \n",
      "1                    TP   43.000000   49.000000   41.000000   40.000000   \n",
      "2                    TN  196.000000  196.000000  197.000000  198.000000   \n",
      "3                    FP    4.000000    6.000000    3.000000    3.000000   \n",
      "4                    FN   25.000000   17.000000   27.000000   27.000000   \n",
      "5              Accuracy    0.891791    0.914179    0.888060    0.888060   \n",
      "6             Precision    0.914894    0.890909    0.931818    0.930233   \n",
      "7           Sensitivity    0.632353    0.742424    0.602941    0.597015   \n",
      "8           Specificity    0.980000    0.970300    0.985000    0.985100   \n",
      "9              F1 score    0.747826    0.809917    0.732143    0.727273   \n",
      "10  F1 score (weighted)    0.884610    0.911416    0.879234    0.879001   \n",
      "11     F1 score (macro)    0.839471    0.877248    0.830694    0.828425   \n",
      "12    Balanced Accuracy    0.806176    0.856361    0.793971    0.791045   \n",
      "13                  MCC    0.700690    0.760320    0.690642    0.686752   \n",
      "14                  NPV    0.886900    0.920200    0.879500    0.880000   \n",
      "15              ROC_AUC    0.806176    0.856361    0.793971    0.791045   \n",
      "\n",
      "          Set4  \n",
      "0     0.575407  \n",
      "1    44.000000  \n",
      "2   198.000000  \n",
      "3     3.000000  \n",
      "4    23.000000  \n",
      "5     0.902985  \n",
      "6     0.936170  \n",
      "7     0.656716  \n",
      "8     0.985100  \n",
      "9     0.771930  \n",
      "10    0.896774  \n",
      "11    0.855159  \n",
      "12    0.820896  \n",
      "13    0.730776  \n",
      "14    0.895900  \n",
      "15    0.820896  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_knn_4)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet4_cat = np.where(((Y_testSet4>=2) | (Y_testSet4<=-2)), 1, 0) \n",
    "y_pred_knn_4_cat = np.where(((y_pred_knn_4 >= 2) | (y_pred_knn_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_knn_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:23:09,874] Trial 250 finished with value: 0.6637230721223357 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:10,027] Trial 251 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:10,179] Trial 252 finished with value: 0.6637230721223357 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:10,330] Trial 253 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:10,483] Trial 254 finished with value: 0.5855432934040777 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:10,635] Trial 255 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:10,788] Trial 256 finished with value: 0.6637230721223357 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:10,940] Trial 257 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:11,093] Trial 258 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:11,246] Trial 259 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:11,397] Trial 260 finished with value: 0.6637230721223357 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:11,551] Trial 261 finished with value: 0.6333657793614114 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:11,703] Trial 262 finished with value: 0.6604179049962157 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:11,856] Trial 263 finished with value: 0.6637230721223357 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:12,008] Trial 264 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:12,160] Trial 265 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:12,311] Trial 266 finished with value: 0.6637230721223357 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:12,462] Trial 267 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:12,613] Trial 268 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:12,765] Trial 269 finished with value: 0.6637230721223357 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:12,918] Trial 270 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:13,069] Trial 271 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:13,244] Trial 272 finished with value: 0.6604179049962157 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:13,414] Trial 273 finished with value: 0.6760807890885361 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:13,570] Trial 274 finished with value: 0.6271065386158876 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:13,729] Trial 275 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:13,888] Trial 276 finished with value: 0.6637230721223357 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:14,046] Trial 277 finished with value: 0.6024977289711708 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:14,202] Trial 278 finished with value: 0.6760807890885361 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:14,364] Trial 279 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:14,536] Trial 280 finished with value: 0.6206185686816325 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:14,709] Trial 281 finished with value: 0.6760807890885361 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:14,869] Trial 282 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:15,027] Trial 283 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:15,190] Trial 284 finished with value: 0.6677485434143706 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:15,359] Trial 285 finished with value: 0.6637230721223357 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:15,517] Trial 286 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:15,678] Trial 287 finished with value: 0.6760807890885361 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:15,830] Trial 288 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:15,983] Trial 289 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:16,135] Trial 290 finished with value: 0.6637230721223357 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:16,288] Trial 291 finished with value: 0.6237689257059695 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:16,440] Trial 292 finished with value: 0.6760807890885361 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:16,594] Trial 293 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:16,747] Trial 294 finished with value: 0.6637230721223357 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:16,902] Trial 295 finished with value: 0.6760807890885361 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:17,055] Trial 296 finished with value: 0.6760807890885361 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:17,208] Trial 297 finished with value: 0.6727671275740245 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:17,361] Trial 298 finished with value: 0.6333657793614114 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:17,514] Trial 299 finished with value: 0.6604179049962157 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6981\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 27\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.749303    0.704667    0.662835    0.767822   \n",
      "1                    TP   43.000000   49.000000   41.000000   40.000000   \n",
      "2                    TN  196.000000  196.000000  197.000000  198.000000   \n",
      "3                    FP    4.000000    6.000000    3.000000    3.000000   \n",
      "4                    FN   25.000000   17.000000   27.000000   27.000000   \n",
      "5              Accuracy    0.891791    0.914179    0.888060    0.888060   \n",
      "6             Precision    0.914894    0.890909    0.931818    0.930233   \n",
      "7           Sensitivity    0.632353    0.742424    0.602941    0.597015   \n",
      "8           Specificity    0.980000    0.970300    0.985000    0.985100   \n",
      "9              F1 score    0.747826    0.809917    0.732143    0.727273   \n",
      "10  F1 score (weighted)    0.884610    0.911416    0.879234    0.879001   \n",
      "11     F1 score (macro)    0.839471    0.877248    0.830694    0.828425   \n",
      "12    Balanced Accuracy    0.806176    0.856361    0.793971    0.791045   \n",
      "13                  MCC    0.700690    0.760320    0.690642    0.686752   \n",
      "14                  NPV    0.886900    0.920200    0.879500    0.880000   \n",
      "15              ROC_AUC    0.806176    0.856361    0.793971    0.791045   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.575407    0.691381  \n",
      "1    44.000000   49.000000  \n",
      "2   198.000000  195.000000  \n",
      "3     3.000000    5.000000  \n",
      "4    23.000000   19.000000  \n",
      "5     0.902985    0.910448  \n",
      "6     0.936170    0.907407  \n",
      "7     0.656716    0.720588  \n",
      "8     0.985100    0.975000  \n",
      "9     0.771930    0.803279  \n",
      "10    0.896774    0.906824  \n",
      "11    0.855159    0.872654  \n",
      "12    0.820896    0.847794  \n",
      "13    0.730776    0.754602  \n",
      "14    0.895900    0.911200  \n",
      "15    0.820896    0.847794  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_knn_5)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet5_cat = np.where(((Y_testSet5>=2) | (Y_testSet5<=-2)), 1, 0) \n",
    "y_pred_knn_5_cat = np.where(((y_pred_knn_5 >= 2) | (y_pred_knn_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_knn_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:23:17,721] Trial 300 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:17,872] Trial 301 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:18,023] Trial 302 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:18,175] Trial 303 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:18,340] Trial 304 finished with value: 0.6857663952959702 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:18,508] Trial 305 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:18,669] Trial 306 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:18,822] Trial 307 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:19,003] Trial 308 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:19,191] Trial 309 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:19,355] Trial 310 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:19,509] Trial 311 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:19,661] Trial 312 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:19,813] Trial 313 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:19,966] Trial 314 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:20,119] Trial 315 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:20,272] Trial 316 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:20,425] Trial 317 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:20,577] Trial 318 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:20,731] Trial 319 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:20,884] Trial 320 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:21,037] Trial 321 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:21,190] Trial 322 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:21,343] Trial 323 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:21,496] Trial 324 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:21,648] Trial 325 finished with value: 0.663550687012248 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:21,800] Trial 326 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:21,952] Trial 327 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:22,104] Trial 328 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:22,268] Trial 329 finished with value: 0.6857663952959702 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:22,434] Trial 330 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:22,590] Trial 331 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:22,746] Trial 332 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:22,904] Trial 333 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:23,064] Trial 334 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:23,222] Trial 335 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:23,378] Trial 336 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:23,541] Trial 337 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:23,706] Trial 338 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:23,869] Trial 339 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:24,023] Trial 340 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:24,178] Trial 341 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:24,333] Trial 342 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:24,488] Trial 343 finished with value: 0.663550687012248 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:24,642] Trial 344 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:24,795] Trial 345 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:24,980] Trial 346 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:25,172] Trial 347 finished with value: 0.5633952778871117 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:25,344] Trial 348 finished with value: 0.5373650925298349 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:25,497] Trial 349 finished with value: 0.694865068487362 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6981\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 27\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.749303    0.704667    0.662835    0.767822   \n",
      "1                    TP   43.000000   49.000000   41.000000   40.000000   \n",
      "2                    TN  196.000000  196.000000  197.000000  198.000000   \n",
      "3                    FP    4.000000    6.000000    3.000000    3.000000   \n",
      "4                    FN   25.000000   17.000000   27.000000   27.000000   \n",
      "5              Accuracy    0.891791    0.914179    0.888060    0.888060   \n",
      "6             Precision    0.914894    0.890909    0.931818    0.930233   \n",
      "7           Sensitivity    0.632353    0.742424    0.602941    0.597015   \n",
      "8           Specificity    0.980000    0.970300    0.985000    0.985100   \n",
      "9              F1 score    0.747826    0.809917    0.732143    0.727273   \n",
      "10  F1 score (weighted)    0.884610    0.911416    0.879234    0.879001   \n",
      "11     F1 score (macro)    0.839471    0.877248    0.830694    0.828425   \n",
      "12    Balanced Accuracy    0.806176    0.856361    0.793971    0.791045   \n",
      "13                  MCC    0.700690    0.760320    0.690642    0.686752   \n",
      "14                  NPV    0.886900    0.920200    0.879500    0.880000   \n",
      "15              ROC_AUC    0.806176    0.856361    0.793971    0.791045   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.575407    0.691381    0.755554  \n",
      "1    44.000000   49.000000   45.000000  \n",
      "2   198.000000  195.000000  200.000000  \n",
      "3     3.000000    5.000000    1.000000  \n",
      "4    23.000000   19.000000   22.000000  \n",
      "5     0.902985    0.910448    0.914179  \n",
      "6     0.936170    0.907407    0.978261  \n",
      "7     0.656716    0.720588    0.671642  \n",
      "8     0.985100    0.975000    0.995000  \n",
      "9     0.771930    0.803279    0.796460  \n",
      "10    0.896774    0.906824    0.908335  \n",
      "11    0.855159    0.872654    0.871043  \n",
      "12    0.820896    0.847794    0.833333  \n",
      "13    0.730776    0.754602    0.765577  \n",
      "14    0.895900    0.911200    0.900900  \n",
      "15    0.820896    0.847794    0.833333  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_knn_6)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet6_cat = np.where(((Y_testSet6>=2) | (Y_testSet6<=-2)), 1, 0) \n",
    "y_pred_knn_6_cat = np.where(((y_pred_knn_6 >= 2) | (y_pred_knn_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_knn_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:23:25,711] Trial 350 finished with value: 0.6442732588219342 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:25,866] Trial 351 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:26,020] Trial 352 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:26,173] Trial 353 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:26,326] Trial 354 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:26,479] Trial 355 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:26,632] Trial 356 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:26,786] Trial 357 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:26,940] Trial 358 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:27,093] Trial 359 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:27,247] Trial 360 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:27,402] Trial 361 finished with value: 0.6022749005570592 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:27,556] Trial 362 finished with value: 0.6328629302132418 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:27,710] Trial 363 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:27,862] Trial 364 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:28,015] Trial 365 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:28,169] Trial 366 finished with value: 0.5712710614725504 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:28,322] Trial 367 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:28,476] Trial 368 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:28,629] Trial 369 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:28,783] Trial 370 finished with value: 0.5957104229162681 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:28,941] Trial 371 finished with value: 0.6442732588219342 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:29,099] Trial 372 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:29,263] Trial 373 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:29,418] Trial 374 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:29,575] Trial 375 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:29,730] Trial 376 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:29,883] Trial 377 finished with value: 0.6265366748584086 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:30,036] Trial 378 finished with value: 0.621705516062977 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:30,190] Trial 379 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:30,388] Trial 380 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:30,555] Trial 381 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:30,714] Trial 382 finished with value: 0.6328629302132418 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:30,870] Trial 383 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:31,026] Trial 384 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:31,184] Trial 385 finished with value: 0.5342621963190901 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:31,348] Trial 386 finished with value: 0.5774143929680946 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:31,514] Trial 387 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:31,679] Trial 388 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:31,833] Trial 389 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:31,987] Trial 390 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:32,142] Trial 391 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:32,298] Trial 392 finished with value: 0.6442732588219342 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:32,465] Trial 393 finished with value: 0.6511725567199128 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:32,620] Trial 394 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:32,777] Trial 395 finished with value: 0.6511725567199128 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:32,946] Trial 396 finished with value: 0.6511725567199128 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:33,102] Trial 397 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:33,269] Trial 398 finished with value: 0.6511725567199128 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:33,422] Trial 399 finished with value: 0.6488751166677715 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6981\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 27\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.749303    0.704667    0.662835    0.767822   \n",
      "1                    TP   43.000000   49.000000   41.000000   40.000000   \n",
      "2                    TN  196.000000  196.000000  197.000000  198.000000   \n",
      "3                    FP    4.000000    6.000000    3.000000    3.000000   \n",
      "4                    FN   25.000000   17.000000   27.000000   27.000000   \n",
      "5              Accuracy    0.891791    0.914179    0.888060    0.888060   \n",
      "6             Precision    0.914894    0.890909    0.931818    0.930233   \n",
      "7           Sensitivity    0.632353    0.742424    0.602941    0.597015   \n",
      "8           Specificity    0.980000    0.970300    0.985000    0.985100   \n",
      "9              F1 score    0.747826    0.809917    0.732143    0.727273   \n",
      "10  F1 score (weighted)    0.884610    0.911416    0.879234    0.879001   \n",
      "11     F1 score (macro)    0.839471    0.877248    0.830694    0.828425   \n",
      "12    Balanced Accuracy    0.806176    0.856361    0.793971    0.791045   \n",
      "13                  MCC    0.700690    0.760320    0.690642    0.686752   \n",
      "14                  NPV    0.886900    0.920200    0.879500    0.880000   \n",
      "15              ROC_AUC    0.806176    0.856361    0.793971    0.791045   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.575407    0.691381    0.755554    0.739568  \n",
      "1    44.000000   49.000000   45.000000   49.000000  \n",
      "2   198.000000  195.000000  200.000000  199.000000  \n",
      "3     3.000000    5.000000    1.000000    2.000000  \n",
      "4    23.000000   19.000000   22.000000   18.000000  \n",
      "5     0.902985    0.910448    0.914179    0.925373  \n",
      "6     0.936170    0.907407    0.978261    0.960784  \n",
      "7     0.656716    0.720588    0.671642    0.731343  \n",
      "8     0.985100    0.975000    0.995000    0.990000  \n",
      "9     0.771930    0.803279    0.796460    0.830508  \n",
      "10    0.896774    0.906824    0.908335    0.921742  \n",
      "11    0.855159    0.872654    0.871043    0.891331  \n",
      "12    0.820896    0.847794    0.833333    0.860697  \n",
      "13    0.730776    0.754602    0.765577    0.795779  \n",
      "14    0.895900    0.911200    0.900900    0.917100  \n",
      "15    0.820896    0.847794    0.833333    0.860697  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_knn_7)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet7_cat = np.where(((Y_testSet7>=2) | (Y_testSet7<=-2)), 1, 0) \n",
    "y_pred_knn_7_cat = np.where(((y_pred_knn_7 >= 2) | (y_pred_knn_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_knn_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:23:33,634] Trial 400 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:33,788] Trial 401 finished with value: 0.6468672505434452 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:33,943] Trial 402 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:34,096] Trial 403 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:34,251] Trial 404 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:34,406] Trial 405 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:34,561] Trial 406 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:34,715] Trial 407 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:34,869] Trial 408 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:35,024] Trial 409 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:35,179] Trial 410 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:35,333] Trial 411 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:35,489] Trial 412 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:35,652] Trial 413 finished with value: 0.667021091097715 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:35,817] Trial 414 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:35,972] Trial 415 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:36,128] Trial 416 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:36,283] Trial 417 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:36,438] Trial 418 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:36,593] Trial 419 finished with value: 0.6468672505434452 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:36,749] Trial 420 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:36,905] Trial 421 finished with value: 0.6013126759591966 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:37,060] Trial 422 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:37,215] Trial 423 finished with value: 0.6524952272021743 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:37,371] Trial 424 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:37,526] Trial 425 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:37,681] Trial 426 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:37,838] Trial 427 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:37,993] Trial 428 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:38,151] Trial 429 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:38,310] Trial 430 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:38,469] Trial 431 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:38,631] Trial 432 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:38,789] Trial 433 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:38,947] Trial 434 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:39,109] Trial 435 finished with value: 0.6663773077129582 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 76}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:39,283] Trial 436 finished with value: 0.6569490974832919 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:39,462] Trial 437 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:39,628] Trial 438 finished with value: 0.6468672505434452 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:39,788] Trial 439 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:39,950] Trial 440 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:40,109] Trial 441 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:40,268] Trial 442 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:40,428] Trial 443 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:40,584] Trial 444 finished with value: 0.6282836842322843 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:40,741] Trial 445 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:40,917] Trial 446 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:41,125] Trial 447 finished with value: 0.6734659607642817 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:41,295] Trial 448 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:41,450] Trial 449 finished with value: 0.676530315547472 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6981\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 27\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.749303    0.704667    0.662835    0.767822   \n",
      "1                    TP   43.000000   49.000000   41.000000   40.000000   \n",
      "2                    TN  196.000000  196.000000  197.000000  198.000000   \n",
      "3                    FP    4.000000    6.000000    3.000000    3.000000   \n",
      "4                    FN   25.000000   17.000000   27.000000   27.000000   \n",
      "5              Accuracy    0.891791    0.914179    0.888060    0.888060   \n",
      "6             Precision    0.914894    0.890909    0.931818    0.930233   \n",
      "7           Sensitivity    0.632353    0.742424    0.602941    0.597015   \n",
      "8           Specificity    0.980000    0.970300    0.985000    0.985100   \n",
      "9              F1 score    0.747826    0.809917    0.732143    0.727273   \n",
      "10  F1 score (weighted)    0.884610    0.911416    0.879234    0.879001   \n",
      "11     F1 score (macro)    0.839471    0.877248    0.830694    0.828425   \n",
      "12    Balanced Accuracy    0.806176    0.856361    0.793971    0.791045   \n",
      "13                  MCC    0.700690    0.760320    0.690642    0.686752   \n",
      "14                  NPV    0.886900    0.920200    0.879500    0.880000   \n",
      "15              ROC_AUC    0.806176    0.856361    0.793971    0.791045   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.575407    0.691381    0.755554    0.739568    0.695934  \n",
      "1    44.000000   49.000000   45.000000   49.000000   39.000000  \n",
      "2   198.000000  195.000000  200.000000  199.000000  196.000000  \n",
      "3     3.000000    5.000000    1.000000    2.000000    6.000000  \n",
      "4    23.000000   19.000000   22.000000   18.000000   27.000000  \n",
      "5     0.902985    0.910448    0.914179    0.925373    0.876866  \n",
      "6     0.936170    0.907407    0.978261    0.960784    0.866667  \n",
      "7     0.656716    0.720588    0.671642    0.731343    0.590909  \n",
      "8     0.985100    0.975000    0.995000    0.990000    0.970300  \n",
      "9     0.771930    0.803279    0.796460    0.830508    0.702703  \n",
      "10    0.896774    0.906824    0.908335    0.921742    0.868260  \n",
      "11    0.855159    0.872654    0.871043    0.891331    0.812528  \n",
      "12    0.820896    0.847794    0.833333    0.860697    0.780603  \n",
      "13    0.730776    0.754602    0.765577    0.795779    0.646862  \n",
      "14    0.895900    0.911200    0.900900    0.917100    0.878900  \n",
      "15    0.820896    0.847794    0.833333    0.860697    0.780603  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_knn_8)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet8_cat = np.where(((Y_testSet8>=2) | (Y_testSet8<=-2)), 1, 0) \n",
    "y_pred_knn_8_cat = np.where(((y_pred_knn_8 >= 2) | (y_pred_knn_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_knn_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:23:41,668] Trial 450 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:41,824] Trial 451 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:41,980] Trial 452 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:42,135] Trial 453 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:42,291] Trial 454 finished with value: 0.6753261525413712 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:42,451] Trial 455 finished with value: 0.6643657578157696 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:42,614] Trial 456 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:42,770] Trial 457 finished with value: 0.6471277351236393 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:42,925] Trial 458 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:43,080] Trial 459 finished with value: 0.6753261525413712 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:43,236] Trial 460 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:43,391] Trial 461 finished with value: 0.6753261525413712 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:43,547] Trial 462 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:43,701] Trial 463 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:43,858] Trial 464 finished with value: 0.6753261525413712 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:44,013] Trial 465 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:44,168] Trial 466 finished with value: 0.6753261525413712 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:44,324] Trial 467 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:44,479] Trial 468 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:44,635] Trial 469 finished with value: 0.6753261525413712 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:44,791] Trial 470 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:44,946] Trial 471 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:45,102] Trial 472 finished with value: 0.6753261525413712 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:45,257] Trial 473 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:45,413] Trial 474 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:45,569] Trial 475 finished with value: 0.6753261525413712 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:45,726] Trial 476 finished with value: 0.6389552905606422 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:45,883] Trial 477 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:46,039] Trial 478 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:46,219] Trial 479 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:46,413] Trial 480 finished with value: 0.6753261525413712 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:46,596] Trial 481 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:46,752] Trial 482 finished with value: 0.6753261525413712 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:46,908] Trial 483 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:47,064] Trial 484 finished with value: 0.6753261525413712 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:47,221] Trial 485 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:47,376] Trial 486 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:47,532] Trial 487 finished with value: 0.6753261525413712 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:47,688] Trial 488 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:47,844] Trial 489 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:48,117] Trial 490 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:48,276] Trial 491 finished with value: 0.5699308072581495 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:48,432] Trial 492 finished with value: 0.629357966931795 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:48,590] Trial 493 finished with value: 0.6753261525413712 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:48,747] Trial 494 finished with value: 0.6753261525413712 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:48,904] Trial 495 finished with value: 0.6471277351236393 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:49,062] Trial 496 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:49,223] Trial 497 finished with value: 0.6687717358383356 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:49,379] Trial 498 finished with value: 0.6753261525413712 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 103 with value: 0.6981056883532422.\n",
      "[I 2023-12-12 01:23:49,536] Trial 499 finished with value: 0.6766281044161389 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 103 with value: 0.6981056883532422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6981\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 27\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.749303    0.704667    0.662835    0.767822   \n",
      "1                    TP   43.000000   49.000000   41.000000   40.000000   \n",
      "2                    TN  196.000000  196.000000  197.000000  198.000000   \n",
      "3                    FP    4.000000    6.000000    3.000000    3.000000   \n",
      "4                    FN   25.000000   17.000000   27.000000   27.000000   \n",
      "5              Accuracy    0.891791    0.914179    0.888060    0.888060   \n",
      "6             Precision    0.914894    0.890909    0.931818    0.930233   \n",
      "7           Sensitivity    0.632353    0.742424    0.602941    0.597015   \n",
      "8           Specificity    0.980000    0.970300    0.985000    0.985100   \n",
      "9              F1 score    0.747826    0.809917    0.732143    0.727273   \n",
      "10  F1 score (weighted)    0.884610    0.911416    0.879234    0.879001   \n",
      "11     F1 score (macro)    0.839471    0.877248    0.830694    0.828425   \n",
      "12    Balanced Accuracy    0.806176    0.856361    0.793971    0.791045   \n",
      "13                  MCC    0.700690    0.760320    0.690642    0.686752   \n",
      "14                  NPV    0.886900    0.920200    0.879500    0.880000   \n",
      "15              ROC_AUC    0.806176    0.856361    0.793971    0.791045   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.575407    0.691381    0.755554    0.739568    0.695934    0.660671  \n",
      "1    44.000000   49.000000   45.000000   49.000000   39.000000   48.000000  \n",
      "2   198.000000  195.000000  200.000000  199.000000  196.000000  200.000000  \n",
      "3     3.000000    5.000000    1.000000    2.000000    6.000000    2.000000  \n",
      "4    23.000000   19.000000   22.000000   18.000000   27.000000   18.000000  \n",
      "5     0.902985    0.910448    0.914179    0.925373    0.876866    0.925373  \n",
      "6     0.936170    0.907407    0.978261    0.960784    0.866667    0.960000  \n",
      "7     0.656716    0.720588    0.671642    0.731343    0.590909    0.727273  \n",
      "8     0.985100    0.975000    0.995000    0.990000    0.970300    0.990100  \n",
      "9     0.771930    0.803279    0.796460    0.830508    0.702703    0.827586  \n",
      "10    0.896774    0.906824    0.908335    0.921742    0.868260    0.921648  \n",
      "11    0.855159    0.872654    0.871043    0.891331    0.812528    0.889984  \n",
      "12    0.820896    0.847794    0.833333    0.860697    0.780603    0.858686  \n",
      "13    0.730776    0.754602    0.765577    0.795779    0.646862    0.793375  \n",
      "14    0.895900    0.911200    0.900900    0.917100    0.878900    0.917400  \n",
      "15    0.820896    0.847794    0.833333    0.860697    0.780603    0.858686  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_knn_9)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet9_cat = np.where(((Y_testSet9>=2) | (Y_testSet9<=-2)), 1, 0) \n",
    "y_pred_knn_9_cat = np.where(((y_pred_knn_9 >= 2) | (y_pred_knn_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_knn_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.709044</td>\n",
       "      <td>0.077166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>2.183270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>97.900000</td>\n",
       "      <td>1.911951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1.349897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>2.668749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.903681</td>\n",
       "      <td>0.026134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.899445</td>\n",
       "      <td>0.049748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.692688</td>\n",
       "      <td>0.074422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.974080</td>\n",
       "      <td>0.013590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.781291</td>\n",
       "      <td>0.059316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.898913</td>\n",
       "      <td>0.028063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.859751</td>\n",
       "      <td>0.037960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.833383</td>\n",
       "      <td>0.040673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.732054</td>\n",
       "      <td>0.072166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.905090</td>\n",
       "      <td>0.023644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.833383</td>\n",
       "      <td>0.040673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.709044     0.077166\n",
       "1                    TP        23.100000     2.183270\n",
       "2                    TN        97.900000     1.911951\n",
       "3                    FP         2.600000     1.349897\n",
       "4                    FN        10.300000     2.668749\n",
       "5              Accuracy         0.903681     0.026134\n",
       "6             Precision         0.899445     0.049748\n",
       "7           Sensitivity         0.692688     0.074422\n",
       "8           Specificity         0.974080     0.013590\n",
       "9              F1 score         0.781291     0.059316\n",
       "10  F1 score (weighted)         0.898913     0.028063\n",
       "11     F1 score (macro)         0.859751     0.037960\n",
       "12    Balanced Accuracy         0.833383     0.040673\n",
       "13                  MCC         0.732054     0.072166\n",
       "14                  NPV         0.905090     0.023644\n",
       "15              ROC_AUC         0.833383     0.040673"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.749303</td>\n",
       "      <td>0.704667</td>\n",
       "      <td>0.662835</td>\n",
       "      <td>0.767822</td>\n",
       "      <td>0.575407</td>\n",
       "      <td>0.691381</td>\n",
       "      <td>0.755554</td>\n",
       "      <td>0.739568</td>\n",
       "      <td>0.695934</td>\n",
       "      <td>0.660671</td>\n",
       "      <td>0.700314</td>\n",
       "      <td>0.057971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>44.700000</td>\n",
       "      <td>3.917199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>197.500000</td>\n",
       "      <td>1.779513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.715938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>22.300000</td>\n",
       "      <td>4.083844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.914179</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>0.914179</td>\n",
       "      <td>0.925373</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.925373</td>\n",
       "      <td>0.903731</td>\n",
       "      <td>0.016853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.927714</td>\n",
       "      <td>0.034065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.632353</td>\n",
       "      <td>0.742424</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.667320</td>\n",
       "      <td>0.060048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.970300</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.970300</td>\n",
       "      <td>0.990100</td>\n",
       "      <td>0.982590</td>\n",
       "      <td>0.008499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>0.809917</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.796460</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.774963</td>\n",
       "      <td>0.045234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.884610</td>\n",
       "      <td>0.911416</td>\n",
       "      <td>0.879234</td>\n",
       "      <td>0.879001</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.906824</td>\n",
       "      <td>0.908335</td>\n",
       "      <td>0.921742</td>\n",
       "      <td>0.868260</td>\n",
       "      <td>0.921648</td>\n",
       "      <td>0.897784</td>\n",
       "      <td>0.019048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.839471</td>\n",
       "      <td>0.877248</td>\n",
       "      <td>0.830694</td>\n",
       "      <td>0.828425</td>\n",
       "      <td>0.855159</td>\n",
       "      <td>0.872654</td>\n",
       "      <td>0.871043</td>\n",
       "      <td>0.891331</td>\n",
       "      <td>0.812528</td>\n",
       "      <td>0.889984</td>\n",
       "      <td>0.856854</td>\n",
       "      <td>0.027720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.806176</td>\n",
       "      <td>0.856361</td>\n",
       "      <td>0.793971</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.847794</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.860697</td>\n",
       "      <td>0.780603</td>\n",
       "      <td>0.858686</td>\n",
       "      <td>0.824956</td>\n",
       "      <td>0.030622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.700690</td>\n",
       "      <td>0.760320</td>\n",
       "      <td>0.690642</td>\n",
       "      <td>0.686752</td>\n",
       "      <td>0.730776</td>\n",
       "      <td>0.754602</td>\n",
       "      <td>0.765577</td>\n",
       "      <td>0.795779</td>\n",
       "      <td>0.646862</td>\n",
       "      <td>0.793375</td>\n",
       "      <td>0.732537</td>\n",
       "      <td>0.049733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.886900</td>\n",
       "      <td>0.920200</td>\n",
       "      <td>0.879500</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.895900</td>\n",
       "      <td>0.911200</td>\n",
       "      <td>0.900900</td>\n",
       "      <td>0.917100</td>\n",
       "      <td>0.878900</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>0.898800</td>\n",
       "      <td>0.016895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.806176</td>\n",
       "      <td>0.856361</td>\n",
       "      <td>0.793971</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.847794</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.860697</td>\n",
       "      <td>0.780603</td>\n",
       "      <td>0.858686</td>\n",
       "      <td>0.824956</td>\n",
       "      <td>0.030622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.749303    0.704667    0.662835    0.767822   \n",
       "1                    TP   43.000000   49.000000   41.000000   40.000000   \n",
       "2                    TN  196.000000  196.000000  197.000000  198.000000   \n",
       "3                    FP    4.000000    6.000000    3.000000    3.000000   \n",
       "4                    FN   25.000000   17.000000   27.000000   27.000000   \n",
       "5              Accuracy    0.891791    0.914179    0.888060    0.888060   \n",
       "6             Precision    0.914894    0.890909    0.931818    0.930233   \n",
       "7           Sensitivity    0.632353    0.742424    0.602941    0.597015   \n",
       "8           Specificity    0.980000    0.970300    0.985000    0.985100   \n",
       "9              F1 score    0.747826    0.809917    0.732143    0.727273   \n",
       "10  F1 score (weighted)    0.884610    0.911416    0.879234    0.879001   \n",
       "11     F1 score (macro)    0.839471    0.877248    0.830694    0.828425   \n",
       "12    Balanced Accuracy    0.806176    0.856361    0.793971    0.791045   \n",
       "13                  MCC    0.700690    0.760320    0.690642    0.686752   \n",
       "14                  NPV    0.886900    0.920200    0.879500    0.880000   \n",
       "15              ROC_AUC    0.806176    0.856361    0.793971    0.791045   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.575407    0.691381    0.755554    0.739568    0.695934    0.660671   \n",
       "1    44.000000   49.000000   45.000000   49.000000   39.000000   48.000000   \n",
       "2   198.000000  195.000000  200.000000  199.000000  196.000000  200.000000   \n",
       "3     3.000000    5.000000    1.000000    2.000000    6.000000    2.000000   \n",
       "4    23.000000   19.000000   22.000000   18.000000   27.000000   18.000000   \n",
       "5     0.902985    0.910448    0.914179    0.925373    0.876866    0.925373   \n",
       "6     0.936170    0.907407    0.978261    0.960784    0.866667    0.960000   \n",
       "7     0.656716    0.720588    0.671642    0.731343    0.590909    0.727273   \n",
       "8     0.985100    0.975000    0.995000    0.990000    0.970300    0.990100   \n",
       "9     0.771930    0.803279    0.796460    0.830508    0.702703    0.827586   \n",
       "10    0.896774    0.906824    0.908335    0.921742    0.868260    0.921648   \n",
       "11    0.855159    0.872654    0.871043    0.891331    0.812528    0.889984   \n",
       "12    0.820896    0.847794    0.833333    0.860697    0.780603    0.858686   \n",
       "13    0.730776    0.754602    0.765577    0.795779    0.646862    0.793375   \n",
       "14    0.895900    0.911200    0.900900    0.917100    0.878900    0.917400   \n",
       "15    0.820896    0.847794    0.833333    0.860697    0.780603    0.858686   \n",
       "\n",
       "           ave       std  \n",
       "0     0.700314  0.057971  \n",
       "1    44.700000  3.917199  \n",
       "2   197.500000  1.779513  \n",
       "3     3.500000  1.715938  \n",
       "4    22.300000  4.083844  \n",
       "5     0.903731  0.016853  \n",
       "6     0.927714  0.034065  \n",
       "7     0.667320  0.060048  \n",
       "8     0.982590  0.008499  \n",
       "9     0.774963  0.045234  \n",
       "10    0.897784  0.019048  \n",
       "11    0.856854  0.027720  \n",
       "12    0.824956  0.030622  \n",
       "13    0.732537  0.049733  \n",
       "14    0.898800  0.016895  \n",
       "15    0.824956  0.030622  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test['std'] = mat_met_knn_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.708188</td>\n",
       "      <td>0.067109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.901868</td>\n",
       "      <td>0.022717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.908031</td>\n",
       "      <td>0.050496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.676418</td>\n",
       "      <td>0.075737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.976904</td>\n",
       "      <td>0.012812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.773038</td>\n",
       "      <td>0.057648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.896321</td>\n",
       "      <td>0.025215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.855196</td>\n",
       "      <td>0.035775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.826660</td>\n",
       "      <td>0.039127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.726442</td>\n",
       "      <td>0.065945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.901160</td>\n",
       "      <td>0.021956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.826660</td>\n",
       "      <td>0.039127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.708188     0.067109\n",
       "1              Accuracy         0.901868     0.022717\n",
       "2             Precision         0.908031     0.050496\n",
       "3           Sensitivity         0.676418     0.075737\n",
       "4           Specificity         0.976904     0.012812\n",
       "5              F1 score         0.773038     0.057648\n",
       "6   F1 score (weighted)         0.896321     0.025215\n",
       "7      F1 score (macro)         0.855196     0.035775\n",
       "8     Balanced Accuracy         0.826660     0.039127\n",
       "9                   MCC         0.726442     0.065945\n",
       "10                  NPV         0.901160     0.021956\n",
       "11              ROC_AUC         0.826660     0.039127"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_knn_cat = np.where(((y_pred_optimized_knn >= 2) | (y_pred_optimized_knn <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_knn_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_knn))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "knn_run0 = data_knn[['y_test_idx0', 'y_test0', 'y_pred_knn0']]\n",
    "knn_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "knn_run0.reset_index(inplace=True, drop=True)\n",
    "knn_run1 = data_knn[['y_test_idx1', 'y_test1', 'y_pred_knn1']]\n",
    "knn_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "knn_run1.reset_index(inplace=True, drop=True)\n",
    "knn_run2 = data_knn[['y_test_idx2', 'y_test2', 'y_pred_knn2']]\n",
    "knn_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "knn_run2.reset_index(inplace=True, drop=True)\n",
    "knn_run3 = data_knn[['y_test_idx3', 'y_test3', 'y_pred_knn3']]\n",
    "knn_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "knn_run3.reset_index(inplace=True, drop=True)\n",
    "knn_run4 = data_knn[['y_test_idx4', 'y_test4', 'y_pred_knn4']]\n",
    "knn_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "knn_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "knn_5preds = pd.concat([chembl_id, knn_run0, knn_run1, knn_run2, knn_run3, knn_run4], axis=1)\n",
    "knn_5preds = knn_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_knn0', 'y_pred_knn1', 'y_pred_knn2', 'y_pred_knn3', 'y_pred_knn4']]\n",
    "knn_5preds['y_pred_knn_ave'] = knn_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "knn_5preds['y_pred_knn_std'] = knn_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "\n",
    "mat_met_optimized_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0bc43db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model r2_score 0.6831 with a standard deviation of 0.0377\n",
      "KNN optimized model r2_score 0.7138 with a standard deviation of 0.0325\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"KNN baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_knn.joblib']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_reg, \"OUTPUT/knn_reg.joblib\")\n",
    "#joblib.dump(optimized_knn, \"OUTPUT/optimized_knn.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"OUTPUT/optimizedCV_knn.joblib\")\n",
    "#loaded_rf = joblib.load(\"OUTPUT/optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.690516     0.052980\n",
      "1                    TP        15.600000     3.747592\n",
      "2                    TN        99.500000     1.178511\n",
      "3                    FP         1.000000     0.942809\n",
      "4                    FN        17.800000     3.852849\n",
      "5              Accuracy         0.859623     0.031129\n",
      "6             Precision         0.937868     0.058484\n",
      "7           Sensitivity         0.467381     0.114074\n",
      "8           Specificity         0.990060     0.009334\n",
      "9              F1 score         0.617460     0.108265\n",
      "10  F1 score (weighted)         0.839969     0.040218\n",
      "11     F1 score (macro)         0.765687     0.062884\n",
      "12    Balanced Accuracy         0.728720     0.058450\n",
      "13                  MCC         0.597282     0.099813\n",
      "14                  NPV         0.849010     0.028586\n",
      "15              ROC_AUC         0.728720     0.058450\n",
      "CPU times: user 2.08 s, sys: 24 ms, total: 2.11 s\n",
      "Wall time: 2.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    \n",
    "    svm_reg.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:23:55,534] A new study created in memory with name: SVM_regressor_CV\n",
      "[I 2023-12-12 01:23:56,975] Trial 0 finished with value: 0.018229293643771592 and parameters: {'C': 2.0, 'gamma': 1.0}. Best is trial 0 with value: 0.018229293643771592.\n",
      "[I 2023-12-12 01:23:58,350] Trial 1 finished with value: 0.631921428622543 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 1 with value: 0.631921428622543.\n",
      "[I 2023-12-12 01:23:59,666] Trial 2 finished with value: 0.03429677894844814 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 1 with value: 0.631921428622543.\n",
      "[I 2023-12-12 01:24:00,977] Trial 3 finished with value: 0.12786919918860296 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 1 with value: 0.631921428622543.\n",
      "[I 2023-12-12 01:24:02,269] Trial 4 finished with value: 0.22049208773329682 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 1 with value: 0.631921428622543.\n",
      "[I 2023-12-12 01:24:03,592] Trial 5 finished with value: 0.5487055835937189 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 1 with value: 0.631921428622543.\n",
      "[I 2023-12-12 01:24:04,927] Trial 6 finished with value: 0.6751100548086757 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:06,283] Trial 7 finished with value: 0.11937049966567645 and parameters: {'C': 0.125, 'gamma': 0.0009765625}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:07,653] Trial 8 finished with value: 0.4537469059223346 and parameters: {'C': 2.0, 'gamma': 0.00048828125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:09,094] Trial 9 finished with value: 0.01558403497560873 and parameters: {'C': 64.0, 'gamma': 2.0}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:10,401] Trial 10 finished with value: 0.635384132057841 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:11,701] Trial 11 finished with value: 0.635384132057841 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:12,996] Trial 12 finished with value: 0.635384132057841 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:14,294] Trial 13 finished with value: 0.4915384150067091 and parameters: {'C': 0.25, 'gamma': 0.0078125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:15,581] Trial 14 finished with value: 0.12594049372344995 and parameters: {'C': 0.5, 'gamma': 0.000244140625}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:16,909] Trial 15 finished with value: 0.6750344738801994 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:18,466] Trial 16 finished with value: 0.6243215210135202 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:19,796] Trial 17 finished with value: -0.018365719768247257 and parameters: {'C': 0.03125, 'gamma': 0.5}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:21,203] Trial 18 finished with value: 0.24795133214113868 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:22,746] Trial 19 finished with value: -0.01992225148757356 and parameters: {'C': 0.015625, 'gamma': 8.0}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:24,168] Trial 20 finished with value: 0.058907610670447065 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:25,449] Trial 21 finished with value: 0.2049916443476501 and parameters: {'C': 0.0625, 'gamma': 0.03125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:26,786] Trial 22 finished with value: 0.6750344738801994 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:28,155] Trial 23 finished with value: 0.6202049912073382 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:29,602] Trial 24 finished with value: 0.014308990438363377 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:30,944] Trial 25 finished with value: 0.6751100548086757 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:32,267] Trial 26 finished with value: 0.632508917031663 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:33,600] Trial 27 finished with value: 0.6751100548086757 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:34,937] Trial 28 finished with value: 0.6751100548086757 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:36,373] Trial 29 finished with value: 0.019269313805493314 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:37,805] Trial 30 finished with value: 0.05888193941521676 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:39,135] Trial 31 finished with value: 0.6751100548086757 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:40,466] Trial 32 finished with value: 0.6751100548086757 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.6751100548086757.\n",
      "[I 2023-12-12 01:24:41,807] Trial 33 finished with value: 0.6751120253308635 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 33 with value: 0.6751120253308635.\n",
      "[I 2023-12-12 01:24:43,161] Trial 34 finished with value: 0.6946682377239121 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:24:44,523] Trial 35 finished with value: 0.4564269893588575 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:24:45,878] Trial 36 finished with value: 0.6946682377239121 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:24:47,243] Trial 37 finished with value: 0.6946682377239121 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:24:48,608] Trial 38 finished with value: 0.6946682377239121 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:24:50,027] Trial 39 finished with value: 0.6946682377239121 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:24:51,458] Trial 40 finished with value: 0.6946682377239121 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:24:52,898] Trial 41 finished with value: 0.6946682377239121 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:24:54,334] Trial 42 finished with value: 0.6946682377239121 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:24:55,721] Trial 43 finished with value: 0.6946682377239121 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:24:57,017] Trial 44 finished with value: 0.03429677894844814 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:24:58,363] Trial 45 finished with value: 0.6946682377239121 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:24:59,693] Trial 46 finished with value: 0.22081870438878243 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:25:00,973] Trial 47 finished with value: 0.13575768182018125 and parameters: {'C': 0.125, 'gamma': 0.0625}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:25:02,277] Trial 48 finished with value: 0.6853922710512971 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:25:03,579] Trial 49 finished with value: 0.6244788773409362 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 34 with value: 0.6946682377239121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6947\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_regressor_CV\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.739025\n",
      "1                    TP   32.000000\n",
      "2                    TN  199.000000\n",
      "3                    FP    1.000000\n",
      "4                    FN   36.000000\n",
      "5              Accuracy    0.861940\n",
      "6             Precision    0.969697\n",
      "7           Sensitivity    0.470588\n",
      "8           Specificity    0.995000\n",
      "9              F1 score    0.633663\n",
      "10  F1 score (weighted)    0.843573\n",
      "11     F1 score (macro)    0.774303\n",
      "12    Balanced Accuracy    0.732794\n",
      "13                  MCC    0.616567\n",
      "14                  NPV    0.846800\n",
      "15              ROC_AUC    0.732794\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_svm_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_svm_0_cat = np.where(((y_pred_svm_0 >= 2) | (y_pred_svm_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_svm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:25:05,474] Trial 50 finished with value: 0.5971261367481693 and parameters: {'C': 64.0, 'gamma': 0.0009765625}. Best is trial 34 with value: 0.6946682377239121.\n",
      "[I 2023-12-12 01:25:06,861] Trial 51 finished with value: 0.7057463863524159 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.7057463863524159.\n",
      "[I 2023-12-12 01:25:08,256] Trial 52 finished with value: 0.7057463863524159 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.7057463863524159.\n",
      "[I 2023-12-12 01:25:09,663] Trial 53 finished with value: -0.01933361586784351 and parameters: {'C': 0.25, 'gamma': 2.0}. Best is trial 51 with value: 0.7057463863524159.\n",
      "[I 2023-12-12 01:25:10,977] Trial 54 finished with value: 0.6034876732906495 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 51 with value: 0.7057463863524159.\n",
      "[I 2023-12-12 01:25:12,386] Trial 55 finished with value: 0.6890822829862647 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 51 with value: 0.7057463863524159.\n",
      "[I 2023-12-12 01:25:13,738] Trial 56 finished with value: 0.14795877616867337 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 51 with value: 0.7057463863524159.\n",
      "[I 2023-12-12 01:25:15,098] Trial 57 finished with value: 0.03797104034319874 and parameters: {'C': 0.015625, 'gamma': 0.00390625}. Best is trial 51 with value: 0.7057463863524159.\n",
      "[I 2023-12-12 01:25:16,394] Trial 58 finished with value: -0.005134133785391159 and parameters: {'C': 0.0625, 'gamma': 0.000244140625}. Best is trial 51 with value: 0.7057463863524159.\n",
      "[I 2023-12-12 01:25:17,872] Trial 59 finished with value: 0.014407236547419466 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 51 with value: 0.7057463863524159.\n",
      "[I 2023-12-12 01:25:19,296] Trial 60 finished with value: 0.22673775933553406 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 51 with value: 0.7057463863524159.\n",
      "[I 2023-12-12 01:25:20,676] Trial 61 finished with value: 0.7057463863524159 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.7057463863524159.\n",
      "[I 2023-12-12 01:25:22,070] Trial 62 finished with value: 0.7057463863524159 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.7057463863524159.\n",
      "[I 2023-12-12 01:25:23,816] Trial 63 finished with value: 0.0034032914700235393 and parameters: {'C': 16.0, 'gamma': 8.0}. Best is trial 51 with value: 0.7057463863524159.\n",
      "[I 2023-12-12 01:25:25,135] Trial 64 finished with value: 0.5254348309953859 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 51 with value: 0.7057463863524159.\n",
      "[I 2023-12-12 01:25:26,631] Trial 65 finished with value: 7.878176406540538e-05 and parameters: {'C': 1.0, 'gamma': 4.0}. Best is trial 51 with value: 0.7057463863524159.\n",
      "[I 2023-12-12 01:25:28,033] Trial 66 finished with value: 0.7059028047334621 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:29,420] Trial 67 finished with value: 0.6448558333261413 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:30,931] Trial 68 finished with value: 0.006751210314499845 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:32,311] Trial 69 finished with value: 0.02793791928780819 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:33,653] Trial 70 finished with value: 0.692968435296231 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:35,041] Trial 71 finished with value: 0.7057463863524159 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:36,388] Trial 72 finished with value: 0.3894150369560878 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:37,768] Trial 73 finished with value: 0.7058014601894163 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:39,101] Trial 74 finished with value: 0.5856171744100525 and parameters: {'C': 64.0, 'gamma': 6.103515625e-05}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:40,384] Trial 75 finished with value: 0.5255612085960937 and parameters: {'C': 64.0, 'gamma': 3.0517578125e-05}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:41,769] Trial 76 finished with value: 0.7058014601894163 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:43,273] Trial 77 finished with value: 0.6138784468323584 and parameters: {'C': 64.0, 'gamma': 0.00048828125}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:44,608] Trial 78 finished with value: 0.542030995544219 and parameters: {'C': 64.0, 'gamma': 0.0625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:45,965] Trial 79 finished with value: 0.0031565616892597847 and parameters: {'C': 0.5, 'gamma': 0.25}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:47,345] Trial 80 finished with value: 0.7059028047334621 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:48,727] Trial 81 finished with value: 0.7059028047334621 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:50,124] Trial 82 finished with value: 0.7059028047334621 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:51,519] Trial 83 finished with value: 0.7059028047334621 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:53,024] Trial 84 finished with value: 0.00412263810559198 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:54,320] Trial 85 finished with value: 0.623321928091406 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:55,702] Trial 86 finished with value: 0.7059028047334621 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:56,992] Trial 87 finished with value: 0.5252315974126686 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:58,391] Trial 88 finished with value: 0.7059028047334621 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:25:59,780] Trial 89 finished with value: 0.6644550775542539 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:01,140] Trial 90 finished with value: 0.690155180155572 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:02,542] Trial 91 finished with value: 0.7059028047334621 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:03,944] Trial 92 finished with value: 0.7059028047334621 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:05,334] Trial 93 finished with value: 0.7059028047334621 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:06,714] Trial 94 finished with value: 0.7059028047334621 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:08,196] Trial 95 finished with value: 0.014407236547419466 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:09,627] Trial 96 finished with value: 0.22673775933553406 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:11,363] Trial 97 finished with value: 0.0034031067287293103 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:12,740] Trial 98 finished with value: 0.7059028047334621 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:14,085] Trial 99 finished with value: 0.44994587926481505 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 66 with value: 0.7059028047334621.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7059\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.739025    0.714064\n",
      "1                    TP   32.000000   40.000000\n",
      "2                    TN  199.000000  198.000000\n",
      "3                    FP    1.000000    4.000000\n",
      "4                    FN   36.000000   26.000000\n",
      "5              Accuracy    0.861940    0.888060\n",
      "6             Precision    0.969697    0.909091\n",
      "7           Sensitivity    0.470588    0.606061\n",
      "8           Specificity    0.995000    0.980200\n",
      "9              F1 score    0.633663    0.727273\n",
      "10  F1 score (weighted)    0.843573    0.879756\n",
      "11     F1 score (macro)    0.774303    0.828425\n",
      "12    Balanced Accuracy    0.732794    0.793129\n",
      "13                  MCC    0.616567    0.681846\n",
      "14                  NPV    0.846800    0.883900\n",
      "15              ROC_AUC    0.732794    0.793129\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_svm_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet1_cat = np.where(((Y_testSet1>=2) | (Y_testSet1<=-2)), 1, 0) \n",
    "y_pred_svm_1_cat = np.where(((y_pred_svm_1 >= 2) | (y_pred_svm_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_svm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:26:15,708] Trial 100 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:17,087] Trial 101 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:18,458] Trial 102 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:19,807] Trial 103 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:21,198] Trial 104 finished with value: 0.6508301850458377 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:22,555] Trial 105 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:24,010] Trial 106 finished with value: -0.01155666618203569 and parameters: {'C': 0.25, 'gamma': 4.0}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:25,391] Trial 107 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:26,831] Trial 108 finished with value: -0.023125105092202947 and parameters: {'C': 0.03125, 'gamma': 1.0}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:28,240] Trial 109 finished with value: 0.08514500352707947 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:29,624] Trial 110 finished with value: 0.35147142841029877 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:30,976] Trial 111 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:32,301] Trial 112 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:33,586] Trial 113 finished with value: 0.2757395581238683 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:34,920] Trial 114 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:36,283] Trial 115 finished with value: 0.7042735651167791 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:37,619] Trial 116 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:38,998] Trial 117 finished with value: 0.22895627201838117 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:40,331] Trial 118 finished with value: 0.5061821814064145 and parameters: {'C': 1.0, 'gamma': 0.0625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:41,750] Trial 119 finished with value: 0.06569637298991039 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:43,064] Trial 120 finished with value: 0.5962977441192129 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:44,443] Trial 121 finished with value: 0.034562990865961166 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:45,785] Trial 122 finished with value: 0.704150269094692 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:47,117] Trial 123 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:48,476] Trial 124 finished with value: 0.41490529671598536 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:49,799] Trial 125 finished with value: 0.633743199095349 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:51,113] Trial 126 finished with value: 0.6967359527303479 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:52,552] Trial 127 finished with value: 0.01818544113786804 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:53,875] Trial 128 finished with value: 0.1324241487991304 and parameters: {'C': 0.5, 'gamma': 0.000244140625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:55,273] Trial 129 finished with value: 0.6905521967454182 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:56,610] Trial 130 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:57,967] Trial 131 finished with value: 0.7041484278415914 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:26:59,289] Trial 132 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:00,867] Trial 133 finished with value: 0.6453584708761582 and parameters: {'C': 64.0, 'gamma': 0.00390625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:02,257] Trial 134 finished with value: 0.7041484278415914 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:03,739] Trial 135 finished with value: 0.02952056143271322 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:05,123] Trial 136 finished with value: 0.5265308093972555 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:06,540] Trial 137 finished with value: -0.015281291457352807 and parameters: {'C': 0.03125, 'gamma': 0.125}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:07,898] Trial 138 finished with value: 0.08514500352707947 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:09,557] Trial 139 finished with value: 0.016544095722119467 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:10,887] Trial 140 finished with value: -0.012709185570917925 and parameters: {'C': 0.0625, 'gamma': 0.0001220703125}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:12,227] Trial 141 finished with value: 0.7041484278415914 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:13,546] Trial 142 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:14,870] Trial 143 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:16,325] Trial 144 finished with value: 0.016757501371405602 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:17,637] Trial 145 finished with value: 0.7040561125722359 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:18,993] Trial 146 finished with value: 0.6508301850458377 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:20,339] Trial 147 finished with value: 0.034562990865961166 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:21,775] Trial 148 finished with value: 0.016739359893302653 and parameters: {'C': 1.0, 'gamma': 1.0}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:23,121] Trial 149 finished with value: 0.7041484278415914 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7059\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.739025    0.714064    0.677058\n",
      "1                    TP   32.000000   40.000000   31.000000\n",
      "2                    TN  199.000000  198.000000  199.000000\n",
      "3                    FP    1.000000    4.000000    1.000000\n",
      "4                    FN   36.000000   26.000000   37.000000\n",
      "5              Accuracy    0.861940    0.888060    0.858209\n",
      "6             Precision    0.969697    0.909091    0.968750\n",
      "7           Sensitivity    0.470588    0.606061    0.455882\n",
      "8           Specificity    0.995000    0.980200    0.995000\n",
      "9              F1 score    0.633663    0.727273    0.620000\n",
      "10  F1 score (weighted)    0.843573    0.879756    0.838540\n",
      "11     F1 score (macro)    0.774303    0.828425    0.766422\n",
      "12    Balanced Accuracy    0.732794    0.793129    0.725441\n",
      "13                  MCC    0.616567    0.681846    0.605065\n",
      "14                  NPV    0.846800    0.883900    0.843200\n",
      "15              ROC_AUC    0.732794    0.793129    0.725441\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_svm_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet2_cat = np.where(((Y_testSet2>=2) | (Y_testSet2<=-2)), 1, 0) \n",
    "y_pred_svm_2_cat = np.where(((y_pred_svm_2 >= 2) | (y_pred_svm_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_svm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:27:24,641] Trial 150 finished with value: 0.6842883502130508 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:25,940] Trial 151 finished with value: 0.6832414040785334 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:27,250] Trial 152 finished with value: 0.6678062369590754 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:28,531] Trial 153 finished with value: 0.6832414040785334 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:29,779] Trial 154 finished with value: -0.017094518643401235 and parameters: {'C': 0.125, 'gamma': 6.103515625e-05}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:31,059] Trial 155 finished with value: 0.6832414040785334 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:32,350] Trial 156 finished with value: 0.6794574954265682 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:33,594] Trial 157 finished with value: -0.004412240864039896 and parameters: {'C': 0.5, 'gamma': 3.0517578125e-05}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:34,882] Trial 158 finished with value: 0.6832414040785334 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:36,166] Trial 159 finished with value: 0.5732236570424882 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:37,440] Trial 160 finished with value: -0.012200946618962927 and parameters: {'C': 0.25, 'gamma': 0.25}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:38,725] Trial 161 finished with value: 0.6836383837595527 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:40,012] Trial 162 finished with value: 0.6836383837595527 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:41,300] Trial 163 finished with value: 0.6836383837595527 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:42,587] Trial 164 finished with value: 0.6836383837595527 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:43,850] Trial 165 finished with value: 0.68429301064301 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:45,166] Trial 166 finished with value: 0.5372069878152926 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:46,456] Trial 167 finished with value: 0.6022049399758367 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:47,737] Trial 168 finished with value: 0.15356288822522202 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:49,087] Trial 169 finished with value: 0.007712214482264679 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:50,362] Trial 170 finished with value: 0.24037676994482657 and parameters: {'C': 0.0625, 'gamma': 0.0078125}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:51,650] Trial 171 finished with value: 0.6836383837595527 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:52,942] Trial 172 finished with value: 0.6832414040785334 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:54,228] Trial 173 finished with value: 0.6836383837595527 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:55,526] Trial 174 finished with value: 0.6836383837595527 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:56,794] Trial 175 finished with value: -0.023617582999084185 and parameters: {'C': 0.015625, 'gamma': 0.000244140625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:58,272] Trial 176 finished with value: 0.635662689821249 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:27:59,569] Trial 177 finished with value: 0.6832414040785334 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:00,959] Trial 178 finished with value: 0.014035345034299674 and parameters: {'C': 64.0, 'gamma': 0.5}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:02,239] Trial 179 finished with value: 0.6832414040785334 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:03,789] Trial 180 finished with value: 0.007270804583905166 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:05,070] Trial 181 finished with value: 0.6836383837595527 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:06,351] Trial 182 finished with value: 0.6836383837595527 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:07,694] Trial 183 finished with value: 0.23207821158395586 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:08,985] Trial 184 finished with value: 0.6836383837595527 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:10,274] Trial 185 finished with value: 0.6832414040785334 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:11,526] Trial 186 finished with value: 0.009797862029994275 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:12,801] Trial 187 finished with value: 0.12452661210291831 and parameters: {'C': 1.0, 'gamma': 0.0001220703125}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:14,090] Trial 188 finished with value: 0.6832414040785334 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:15,382] Trial 189 finished with value: 0.6836383837595527 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:16,744] Trial 190 finished with value: 0.007316025134596749 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:18,091] Trial 191 finished with value: 0.6840117030070136 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:19,369] Trial 192 finished with value: 0.6842883502130508 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:20,718] Trial 193 finished with value: 0.6840117030070136 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:22,054] Trial 194 finished with value: 0.6840117030070136 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:23,374] Trial 195 finished with value: 0.40271256763559854 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:24,645] Trial 196 finished with value: 0.68429301064301 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:25,947] Trial 197 finished with value: 0.6678062369590754 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:27,284] Trial 198 finished with value: 0.6840117030070136 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 66 with value: 0.7059028047334621.\n",
      "[I 2023-12-12 01:28:28,623] Trial 199 finished with value: 0.6159393038677251 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 66 with value: 0.7059028047334621.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7059\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.739025    0.714064    0.677058    0.749786\n",
      "1                    TP   32.000000   40.000000   31.000000   33.000000\n",
      "2                    TN  199.000000  198.000000  199.000000  199.000000\n",
      "3                    FP    1.000000    4.000000    1.000000    2.000000\n",
      "4                    FN   36.000000   26.000000   37.000000   34.000000\n",
      "5              Accuracy    0.861940    0.888060    0.858209    0.865672\n",
      "6             Precision    0.969697    0.909091    0.968750    0.942857\n",
      "7           Sensitivity    0.470588    0.606061    0.455882    0.492537\n",
      "8           Specificity    0.995000    0.980200    0.995000    0.990000\n",
      "9              F1 score    0.633663    0.727273    0.620000    0.647059\n",
      "10  F1 score (weighted)    0.843573    0.879756    0.838540    0.849553\n",
      "11     F1 score (macro)    0.774303    0.828425    0.766422    0.782055\n",
      "12    Balanced Accuracy    0.732794    0.793129    0.725441    0.741294\n",
      "13                  MCC    0.616567    0.681846    0.605065    0.620153\n",
      "14                  NPV    0.846800    0.883900    0.843200    0.854100\n",
      "15              ROC_AUC    0.732794    0.793129    0.725441    0.741294\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_svm_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet3_cat = np.where(((Y_testSet3>=2) | (Y_testSet3<=-2)), 1, 0) \n",
    "y_pred_svm_3_cat = np.where(((y_pred_svm_3 >= 2) | (y_pred_svm_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_svm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:28:30,279] Trial 200 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:31,799] Trial 201 finished with value: 0.022881181667879568 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:33,198] Trial 202 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:34,601] Trial 203 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:36,005] Trial 204 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:37,408] Trial 205 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:38,723] Trial 206 finished with value: 0.3613352647409625 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:40,121] Trial 207 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:41,517] Trial 208 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:42,917] Trial 209 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:44,306] Trial 210 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:45,709] Trial 211 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:47,114] Trial 212 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:48,519] Trial 213 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:49,918] Trial 214 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:51,309] Trial 215 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:52,700] Trial 216 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:54,091] Trial 217 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:55,489] Trial 218 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:56,890] Trial 219 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:58,289] Trial 220 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:28:59,684] Trial 221 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:01,084] Trial 222 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:02,485] Trial 223 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:03,877] Trial 224 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:05,286] Trial 225 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:06,603] Trial 226 finished with value: 0.23532162276856874 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:08,008] Trial 227 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:09,366] Trial 228 finished with value: 0.6083501702373119 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:10,777] Trial 229 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:12,184] Trial 230 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:13,580] Trial 231 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:14,981] Trial 232 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:16,391] Trial 233 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:17,853] Trial 234 finished with value: 0.050365150883648235 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:19,257] Trial 235 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:20,642] Trial 236 finished with value: 0.5520603101595626 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:22,056] Trial 237 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:23,458] Trial 238 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:24,870] Trial 239 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:26,339] Trial 240 finished with value: 0.020292066781247054 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:27,745] Trial 241 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:29,156] Trial 242 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:30,581] Trial 243 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:31,975] Trial 244 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:33,336] Trial 245 finished with value: 0.6460582903501122 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:34,744] Trial 246 finished with value: 0.701656089481405 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:36,146] Trial 247 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:37,570] Trial 248 finished with value: 0.7061176157661752 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:38,904] Trial 249 finished with value: 0.5538212325511316 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 200 with value: 0.7061176157661752.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7061\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.739025    0.714064    0.677058    0.749786   \n",
      "1                    TP   32.000000   40.000000   31.000000   33.000000   \n",
      "2                    TN  199.000000  198.000000  199.000000  199.000000   \n",
      "3                    FP    1.000000    4.000000    1.000000    2.000000   \n",
      "4                    FN   36.000000   26.000000   37.000000   34.000000   \n",
      "5              Accuracy    0.861940    0.888060    0.858209    0.865672   \n",
      "6             Precision    0.969697    0.909091    0.968750    0.942857   \n",
      "7           Sensitivity    0.470588    0.606061    0.455882    0.492537   \n",
      "8           Specificity    0.995000    0.980200    0.995000    0.990000   \n",
      "9              F1 score    0.633663    0.727273    0.620000    0.647059   \n",
      "10  F1 score (weighted)    0.843573    0.879756    0.838540    0.849553   \n",
      "11     F1 score (macro)    0.774303    0.828425    0.766422    0.782055   \n",
      "12    Balanced Accuracy    0.732794    0.793129    0.725441    0.741294   \n",
      "13                  MCC    0.616567    0.681846    0.605065    0.620153   \n",
      "14                  NPV    0.846800    0.883900    0.843200    0.854100   \n",
      "15              ROC_AUC    0.732794    0.793129    0.725441    0.741294   \n",
      "\n",
      "          Set4  \n",
      "0     0.653836  \n",
      "1    38.000000  \n",
      "2   201.000000  \n",
      "3     0.000000  \n",
      "4    29.000000  \n",
      "5     0.891791  \n",
      "6     1.000000  \n",
      "7     0.567164  \n",
      "8     1.000000  \n",
      "9     0.723810  \n",
      "10    0.880488  \n",
      "11    0.828262  \n",
      "12    0.783582  \n",
      "13    0.704026  \n",
      "14    0.873900  \n",
      "15    0.783582  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_svm_4)\n",
    "# now convert the resuls to binary with cutoff 6.4\n",
    "Y_testSet4_cat = np.where(((Y_testSet4>=2) | (Y_testSet4<=-2)), 1, 0) \n",
    "y_pred_svm_4_cat = np.where(((y_pred_svm_4 >= 2) | (y_pred_svm_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_svm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:29:40,449] Trial 250 finished with value: 0.6911199499612399 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:41,734] Trial 251 finished with value: 0.6703559797885463 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:43,039] Trial 252 finished with value: 0.6600498636800178 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:44,307] Trial 253 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:45,649] Trial 254 finished with value: 0.2396889706637285 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:47,041] Trial 255 finished with value: 0.022957627621125364 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:48,307] Trial 256 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:49,591] Trial 257 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:50,873] Trial 258 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:52,357] Trial 259 finished with value: -0.014597697710318925 and parameters: {'C': 0.25, 'gamma': 8.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:53,594] Trial 260 finished with value: 0.6028248963934139 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:54,844] Trial 261 finished with value: 0.45444183843184327 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:56,121] Trial 262 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:57,482] Trial 263 finished with value: 0.010887321723986997 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:29:58,741] Trial 264 finished with value: 0.08134030413936802 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:00,065] Trial 265 finished with value: 0.2645003654954073 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:01,343] Trial 266 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:02,655] Trial 267 finished with value: 0.05345967893977579 and parameters: {'C': 0.03125, 'gamma': 0.001953125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:04,025] Trial 268 finished with value: 0.015819067767997075 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:05,318] Trial 269 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:06,606] Trial 270 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:07,840] Trial 271 finished with value: 0.3402739902397831 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:09,109] Trial 272 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:10,382] Trial 273 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:11,640] Trial 274 finished with value: 0.03356438711097322 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:12,913] Trial 275 finished with value: 0.022828798167309615 and parameters: {'C': 1.0, 'gamma': 3.0517578125e-05}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:14,183] Trial 276 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:15,488] Trial 277 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:16,761] Trial 278 finished with value: 0.5915330535861804 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:18,144] Trial 279 finished with value: 0.05289747810634747 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:19,419] Trial 280 finished with value: 0.6911483807571155 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:20,689] Trial 281 finished with value: 0.534226045748313 and parameters: {'C': 2.0, 'gamma': 0.0625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:21,957] Trial 282 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:23,198] Trial 283 finished with value: 0.396665819837638 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:24,478] Trial 284 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:25,753] Trial 285 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:27,102] Trial 286 finished with value: 0.012351491809266313 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:28,358] Trial 287 finished with value: 0.6285688607467907 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:29,627] Trial 288 finished with value: 0.6911199499612399 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:30,980] Trial 289 finished with value: 0.6747179889147481 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:32,272] Trial 290 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:33,555] Trial 291 finished with value: 0.1274410587439257 and parameters: {'C': 0.5, 'gamma': 0.000244140625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:34,836] Trial 292 finished with value: 0.6703559797885463 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:36,112] Trial 293 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:37,388] Trial 294 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:38,673] Trial 295 finished with value: 0.41839478161487964 and parameters: {'C': 0.25, 'gamma': 0.00390625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:40,077] Trial 296 finished with value: 0.022957627621125364 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:41,374] Trial 297 finished with value: 0.6913999001135905 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:42,745] Trial 298 finished with value: 0.2396889706637285 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:44,043] Trial 299 finished with value: 0.15602822259919188 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7061\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.739025    0.714064    0.677058    0.749786   \n",
      "1                    TP   32.000000   40.000000   31.000000   33.000000   \n",
      "2                    TN  199.000000  198.000000  199.000000  199.000000   \n",
      "3                    FP    1.000000    4.000000    1.000000    2.000000   \n",
      "4                    FN   36.000000   26.000000   37.000000   34.000000   \n",
      "5              Accuracy    0.861940    0.888060    0.858209    0.865672   \n",
      "6             Precision    0.969697    0.909091    0.968750    0.942857   \n",
      "7           Sensitivity    0.470588    0.606061    0.455882    0.492537   \n",
      "8           Specificity    0.995000    0.980200    0.995000    0.990000   \n",
      "9              F1 score    0.633663    0.727273    0.620000    0.647059   \n",
      "10  F1 score (weighted)    0.843573    0.879756    0.838540    0.849553   \n",
      "11     F1 score (macro)    0.774303    0.828425    0.766422    0.782055   \n",
      "12    Balanced Accuracy    0.732794    0.793129    0.725441    0.741294   \n",
      "13                  MCC    0.616567    0.681846    0.605065    0.620153   \n",
      "14                  NPV    0.846800    0.883900    0.843200    0.854100   \n",
      "15              ROC_AUC    0.732794    0.793129    0.725441    0.741294   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.653836    0.719615  \n",
      "1    38.000000   39.000000  \n",
      "2   201.000000  200.000000  \n",
      "3     0.000000    0.000000  \n",
      "4    29.000000   29.000000  \n",
      "5     0.891791    0.891791  \n",
      "6     1.000000    1.000000  \n",
      "7     0.567164    0.573529  \n",
      "8     1.000000    1.000000  \n",
      "9     0.723810    0.728972  \n",
      "10    0.880488    0.880785  \n",
      "11    0.828262    0.830686  \n",
      "12    0.783582    0.786765  \n",
      "13    0.704026    0.707742  \n",
      "14    0.873900    0.873400  \n",
      "15    0.783582    0.786765  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_svm_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "Y_testSet5_cat = np.where(((Y_testSet5>=2) | (Y_testSet5<=-2)), 1, 0) \n",
    "y_pred_svm_5_cat = np.where(((y_pred_svm_5 >= 2) | (y_pred_svm_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_svm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:30:45,635] Trial 300 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:46,948] Trial 301 finished with value: 0.07617968896223544 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:48,512] Trial 302 finished with value: -0.03207471688901272 and parameters: {'C': 0.0625, 'gamma': 8.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:49,816] Trial 303 finished with value: 0.4689411029224796 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:51,162] Trial 304 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:52,505] Trial 305 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:53,975] Trial 306 finished with value: 0.004663990353119307 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:55,316] Trial 307 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:56,662] Trial 308 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:58,005] Trial 309 finished with value: 0.5319950020651348 and parameters: {'C': 1.0, 'gamma': 0.001953125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:30:59,353] Trial 310 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:00,674] Trial 311 finished with value: 0.02629129199178485 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:02,144] Trial 312 finished with value: 0.010015762795971906 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:03,487] Trial 313 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:04,828] Trial 314 finished with value: 0.34938037948325995 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:06,173] Trial 315 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:07,477] Trial 316 finished with value: 0.6763943425404354 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:08,819] Trial 317 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:10,171] Trial 318 finished with value: 0.411147835037294 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:11,505] Trial 319 finished with value: 0.2228638124732595 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:12,825] Trial 320 finished with value: 0.5859505341903881 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:14,307] Trial 321 finished with value: 0.051663002801818694 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:15,651] Trial 322 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:16,982] Trial 323 finished with value: 0.684049156076391 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:18,339] Trial 324 finished with value: 0.5475998603122181 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:19,681] Trial 325 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:20,995] Trial 326 finished with value: 0.6011769132125444 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:22,339] Trial 327 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:23,676] Trial 328 finished with value: 0.6265425655244412 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:25,140] Trial 329 finished with value: 0.00585366155428958 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:26,486] Trial 330 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:27,820] Trial 331 finished with value: 0.5204531460048997 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:29,212] Trial 332 finished with value: 0.6713211476252849 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:30,549] Trial 333 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:31,886] Trial 334 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:33,229] Trial 335 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:34,567] Trial 336 finished with value: -0.030611639328437256 and parameters: {'C': 0.0625, 'gamma': 0.5}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:35,929] Trial 337 finished with value: 0.6667374729426488 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:37,270] Trial 338 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:38,599] Trial 339 finished with value: -0.021773081476512135 and parameters: {'C': 0.03125, 'gamma': 0.000244140625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:40,054] Trial 340 finished with value: 0.24541853427550916 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:41,377] Trial 341 finished with value: 0.07617968896223544 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:43,047] Trial 342 finished with value: 0.004497859761731893 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:44,379] Trial 343 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:45,640] Trial 344 finished with value: 0.6560133558969157 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:47,029] Trial 345 finished with value: 0.6577545627165619 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:48,329] Trial 346 finished with value: 0.4689411029224796 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:49,671] Trial 347 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:50,987] Trial 348 finished with value: 0.02629129199178485 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:52,336] Trial 349 finished with value: 0.6880110805065571 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7061\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.739025    0.714064    0.677058    0.749786   \n",
      "1                    TP   32.000000   40.000000   31.000000   33.000000   \n",
      "2                    TN  199.000000  198.000000  199.000000  199.000000   \n",
      "3                    FP    1.000000    4.000000    1.000000    2.000000   \n",
      "4                    FN   36.000000   26.000000   37.000000   34.000000   \n",
      "5              Accuracy    0.861940    0.888060    0.858209    0.865672   \n",
      "6             Precision    0.969697    0.909091    0.968750    0.942857   \n",
      "7           Sensitivity    0.470588    0.606061    0.455882    0.492537   \n",
      "8           Specificity    0.995000    0.980200    0.995000    0.990000   \n",
      "9              F1 score    0.633663    0.727273    0.620000    0.647059   \n",
      "10  F1 score (weighted)    0.843573    0.879756    0.838540    0.849553   \n",
      "11     F1 score (macro)    0.774303    0.828425    0.766422    0.782055   \n",
      "12    Balanced Accuracy    0.732794    0.793129    0.725441    0.741294   \n",
      "13                  MCC    0.616567    0.681846    0.605065    0.620153   \n",
      "14                  NPV    0.846800    0.883900    0.843200    0.854100   \n",
      "15              ROC_AUC    0.732794    0.793129    0.725441    0.741294   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.653836    0.719615    0.728529  \n",
      "1    38.000000   39.000000   38.000000  \n",
      "2   201.000000  200.000000  200.000000  \n",
      "3     0.000000    0.000000    1.000000  \n",
      "4    29.000000   29.000000   29.000000  \n",
      "5     0.891791    0.891791    0.888060  \n",
      "6     1.000000    1.000000    0.974359  \n",
      "7     0.567164    0.573529    0.567164  \n",
      "8     1.000000    1.000000    0.995000  \n",
      "9     0.723810    0.728972    0.716981  \n",
      "10    0.880488    0.880785    0.876920  \n",
      "11    0.828262    0.830686    0.823607  \n",
      "12    0.783582    0.786765    0.781095  \n",
      "13    0.704026    0.707742    0.690348  \n",
      "14    0.873900    0.873400    0.873400  \n",
      "15    0.783582    0.786765    0.781095  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_svm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "Y_testSet6_cat = np.where(((Y_testSet6>=2) | (Y_testSet6<=-2)), 1, 0) \n",
    "y_pred_svm_6_cat = np.where(((y_pred_svm_6 >= 2) | (y_pred_svm_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_svm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:31:54,323] Trial 350 finished with value: 0.5509206387353583 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:55,730] Trial 351 finished with value: 0.011797380419827563 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:56,973] Trial 352 finished with value: 0.6560240672443602 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:58,207] Trial 353 finished with value: 0.40492478043901137 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:31:59,627] Trial 354 finished with value: 0.016234494127823785 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:00,945] Trial 355 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:02,261] Trial 356 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:03,592] Trial 357 finished with value: 0.6642035618316968 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:04,905] Trial 358 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:06,276] Trial 359 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:07,557] Trial 360 finished with value: 0.22100165565228988 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:08,819] Trial 361 finished with value: 0.3416030985078633 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:10,088] Trial 362 finished with value: 0.5883542792046134 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:11,408] Trial 363 finished with value: 0.5625778194051494 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:12,755] Trial 364 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:14,183] Trial 365 finished with value: 0.05748620800871988 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:15,437] Trial 366 finished with value: 0.24587874054051673 and parameters: {'C': 0.25, 'gamma': 0.0625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:16,807] Trial 367 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:18,163] Trial 368 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:19,510] Trial 369 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:20,862] Trial 370 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:22,206] Trial 371 finished with value: 0.08259812146210198 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:23,542] Trial 372 finished with value: 0.6614174275234501 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:24,782] Trial 373 finished with value: 0.0197235000019478 and parameters: {'C': 0.03125, 'gamma': 0.0009765625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:26,087] Trial 374 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:27,393] Trial 375 finished with value: 0.6442980587480311 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:28,763] Trial 376 finished with value: 0.012694950119745519 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:30,009] Trial 377 finished with value: 0.27054503042081635 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:31,341] Trial 378 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:32,714] Trial 379 finished with value: 0.6233986494172384 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:33,981] Trial 380 finished with value: 0.6504677161003659 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:35,316] Trial 381 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:36,693] Trial 382 finished with value: -0.022617755895898005 and parameters: {'C': 0.0078125, 'gamma': 0.000244140625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:38,156] Trial 383 finished with value: 0.025374394540000823 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:39,423] Trial 384 finished with value: 0.6409642536622668 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:41,074] Trial 385 finished with value: 0.011672531792261354 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:42,428] Trial 386 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:43,783] Trial 387 finished with value: 0.6614106572814901 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:45,080] Trial 388 finished with value: 0.4528044636289549 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:46,346] Trial 389 finished with value: 0.40492478043901137 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:47,719] Trial 390 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:49,172] Trial 391 finished with value: 0.011981272633120732 and parameters: {'C': 2.0, 'gamma': 4.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:50,541] Trial 392 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:51,963] Trial 393 finished with value: 0.24576124753742104 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:53,300] Trial 394 finished with value: 0.6028506664061595 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:54,657] Trial 395 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:56,015] Trial 396 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:57,365] Trial 397 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:32:58,837] Trial 398 finished with value: 0.016234494127823785 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:00,165] Trial 399 finished with value: 0.6626398491067232 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7061\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.739025    0.714064    0.677058    0.749786   \n",
      "1                    TP   32.000000   40.000000   31.000000   33.000000   \n",
      "2                    TN  199.000000  198.000000  199.000000  199.000000   \n",
      "3                    FP    1.000000    4.000000    1.000000    2.000000   \n",
      "4                    FN   36.000000   26.000000   37.000000   34.000000   \n",
      "5              Accuracy    0.861940    0.888060    0.858209    0.865672   \n",
      "6             Precision    0.969697    0.909091    0.968750    0.942857   \n",
      "7           Sensitivity    0.470588    0.606061    0.455882    0.492537   \n",
      "8           Specificity    0.995000    0.980200    0.995000    0.990000   \n",
      "9              F1 score    0.633663    0.727273    0.620000    0.647059   \n",
      "10  F1 score (weighted)    0.843573    0.879756    0.838540    0.849553   \n",
      "11     F1 score (macro)    0.774303    0.828425    0.766422    0.782055   \n",
      "12    Balanced Accuracy    0.732794    0.793129    0.725441    0.741294   \n",
      "13                  MCC    0.616567    0.681846    0.605065    0.620153   \n",
      "14                  NPV    0.846800    0.883900    0.843200    0.854100   \n",
      "15              ROC_AUC    0.732794    0.793129    0.725441    0.741294   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.653836    0.719615    0.728529    0.750170  \n",
      "1    38.000000   39.000000   38.000000   36.000000  \n",
      "2   201.000000  200.000000  200.000000  199.000000  \n",
      "3     0.000000    0.000000    1.000000    2.000000  \n",
      "4    29.000000   29.000000   29.000000   31.000000  \n",
      "5     0.891791    0.891791    0.888060    0.876866  \n",
      "6     1.000000    1.000000    0.974359    0.947368  \n",
      "7     0.567164    0.573529    0.567164    0.537313  \n",
      "8     1.000000    1.000000    0.995000    0.990000  \n",
      "9     0.723810    0.728972    0.716981    0.685714  \n",
      "10    0.880488    0.880785    0.876920    0.864004  \n",
      "11    0.828262    0.830686    0.823607    0.804574  \n",
      "12    0.783582    0.786765    0.781095    0.763682  \n",
      "13    0.704026    0.707742    0.690348    0.654620  \n",
      "14    0.873900    0.873400    0.873400    0.865200  \n",
      "15    0.783582    0.786765    0.781095    0.763682  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_svm_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "Y_testSet7_cat = np.where(((Y_testSet7>=2) | (Y_testSet7<=-2)), 1, 0) \n",
    "y_pred_svm_7_cat = np.where(((y_pred_svm_7 >= 2) | (y_pred_svm_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_svm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:33:01,765] Trial 400 finished with value: 0.3353038228345806 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:03,117] Trial 401 finished with value: 0.6034043317312471 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:04,469] Trial 402 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:05,826] Trial 403 finished with value: -0.012832417806502506 and parameters: {'C': 0.25, 'gamma': 3.0517578125e-05}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:07,188] Trial 404 finished with value: 0.702657193139119 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:08,538] Trial 405 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:09,976] Trial 406 finished with value: 0.052865576053749155 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:11,339] Trial 407 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:12,705] Trial 408 finished with value: 0.5853201055051909 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:14,076] Trial 409 finished with value: 0.06122084777888233 and parameters: {'C': 0.0625, 'gamma': 0.0625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:15,410] Trial 410 finished with value: 0.15241867067134035 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:16,768] Trial 411 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:18,123] Trial 412 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:19,480] Trial 413 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:20,836] Trial 414 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:22,177] Trial 415 finished with value: 0.07777331833217013 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:23,542] Trial 416 finished with value: 0.7025474767061312 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:24,873] Trial 417 finished with value: 0.6212868669218489 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:26,324] Trial 418 finished with value: 0.011493012709461781 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:27,632] Trial 419 finished with value: 0.5276111515909406 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:28,952] Trial 420 finished with value: 0.02558769684908042 and parameters: {'C': 0.0078125, 'gamma': 0.0078125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:30,306] Trial 421 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:31,655] Trial 422 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:33,002] Trial 423 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:34,281] Trial 424 finished with value: 0.5849057510633439 and parameters: {'C': 1.0, 'gamma': 0.00390625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:35,637] Trial 425 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:37,029] Trial 426 finished with value: 0.2519703038521276 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:38,365] Trial 427 finished with value: 0.6712758564915207 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:39,687] Trial 428 finished with value: -0.017305528417184267 and parameters: {'C': 0.125, 'gamma': 0.5}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:41,351] Trial 429 finished with value: 0.010634424952597999 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:42,706] Trial 430 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:44,058] Trial 431 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:45,409] Trial 432 finished with value: 0.6999477636301377 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:46,711] Trial 433 finished with value: 0.4529863916906517 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:48,076] Trial 434 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:49,546] Trial 435 finished with value: 0.010742171235466525 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:50,897] Trial 436 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:52,276] Trial 437 finished with value: 0.6428445625629352 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:53,635] Trial 438 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:54,992] Trial 439 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:56,315] Trial 440 finished with value: 0.6034043317312471 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:57,628] Trial 441 finished with value: 0.5169206114157435 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:33:59,009] Trial 442 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:00,387] Trial 443 finished with value: 0.702657193139119 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:01,864] Trial 444 finished with value: 0.01420745253584461 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:03,219] Trial 445 finished with value: 0.7019355107168612 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:04,541] Trial 446 finished with value: 0.21272517791722206 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:05,868] Trial 447 finished with value: 0.07777331833217013 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:07,226] Trial 448 finished with value: 0.5853201055051909 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:08,578] Trial 449 finished with value: -0.021448861969428534 and parameters: {'C': 0.03125, 'gamma': 0.25}. Best is trial 200 with value: 0.7061176157661752.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7061\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.739025    0.714064    0.677058    0.749786   \n",
      "1                    TP   32.000000   40.000000   31.000000   33.000000   \n",
      "2                    TN  199.000000  198.000000  199.000000  199.000000   \n",
      "3                    FP    1.000000    4.000000    1.000000    2.000000   \n",
      "4                    FN   36.000000   26.000000   37.000000   34.000000   \n",
      "5              Accuracy    0.861940    0.888060    0.858209    0.865672   \n",
      "6             Precision    0.969697    0.909091    0.968750    0.942857   \n",
      "7           Sensitivity    0.470588    0.606061    0.455882    0.492537   \n",
      "8           Specificity    0.995000    0.980200    0.995000    0.990000   \n",
      "9              F1 score    0.633663    0.727273    0.620000    0.647059   \n",
      "10  F1 score (weighted)    0.843573    0.879756    0.838540    0.849553   \n",
      "11     F1 score (macro)    0.774303    0.828425    0.766422    0.782055   \n",
      "12    Balanced Accuracy    0.732794    0.793129    0.725441    0.741294   \n",
      "13                  MCC    0.616567    0.681846    0.605065    0.620153   \n",
      "14                  NPV    0.846800    0.883900    0.843200    0.854100   \n",
      "15              ROC_AUC    0.732794    0.793129    0.725441    0.741294   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.653836    0.719615    0.728529    0.750170    0.680643  \n",
      "1    38.000000   39.000000   38.000000   36.000000   33.000000  \n",
      "2   201.000000  200.000000  200.000000  199.000000  198.000000  \n",
      "3     0.000000    0.000000    1.000000    2.000000    4.000000  \n",
      "4    29.000000   29.000000   29.000000   31.000000   33.000000  \n",
      "5     0.891791    0.891791    0.888060    0.876866    0.861940  \n",
      "6     1.000000    1.000000    0.974359    0.947368    0.891892  \n",
      "7     0.567164    0.573529    0.567164    0.537313    0.500000  \n",
      "8     1.000000    1.000000    0.995000    0.990000    0.980200  \n",
      "9     0.723810    0.728972    0.716981    0.685714    0.640777  \n",
      "10    0.880488    0.880785    0.876920    0.864004    0.847128  \n",
      "11    0.828262    0.830686    0.823607    0.804574    0.777663  \n",
      "12    0.783582    0.786765    0.781095    0.763682    0.740099  \n",
      "13    0.704026    0.707742    0.690348    0.654620    0.599737  \n",
      "14    0.873900    0.873400    0.873400    0.865200    0.857100  \n",
      "15    0.783582    0.786765    0.781095    0.763682    0.740099  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_svm_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "Y_testSet8_cat = np.where(((Y_testSet8>=2) | (Y_testSet8<=-2)), 1, 0) \n",
    "y_pred_svm_8_cat = np.where(((y_pred_svm_8 >= 2) | (y_pred_svm_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_svm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-12 01:34:10,119] Trial 450 finished with value: -0.02066317947198635 and parameters: {'C': 0.0625, 'gamma': 6.103515625e-05}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:11,445] Trial 451 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:12,769] Trial 452 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:14,056] Trial 453 finished with value: 0.552347057695892 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:15,384] Trial 454 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:16,686] Trial 455 finished with value: 0.6924387121129181 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:17,942] Trial 456 finished with value: 0.660251460924825 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:19,259] Trial 457 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:20,499] Trial 458 finished with value: 0.6249432817881287 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:21,822] Trial 459 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:23,168] Trial 460 finished with value: 0.014376793137585008 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:24,520] Trial 461 finished with value: 0.6828178002885453 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:25,832] Trial 462 finished with value: 0.03302144485141077 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:27,351] Trial 463 finished with value: 0.6079071577983587 and parameters: {'C': 128.0, 'gamma': 0.000244140625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:28,662] Trial 464 finished with value: 0.687330940340177 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:29,940] Trial 465 finished with value: 0.33919157844358605 and parameters: {'C': 0.125, 'gamma': 0.03125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:31,274] Trial 466 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:32,659] Trial 467 finished with value: 0.6637739239944752 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:34,064] Trial 468 finished with value: 0.029732450450979987 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:35,342] Trial 469 finished with value: 0.6947590567597036 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:36,659] Trial 470 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:37,979] Trial 471 finished with value: 0.24983193964417513 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:39,297] Trial 472 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:40,621] Trial 473 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:42,242] Trial 474 finished with value: 0.012616574468278352 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:43,572] Trial 475 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:44,850] Trial 476 finished with value: 0.47782907412643166 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:46,173] Trial 477 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:47,455] Trial 478 finished with value: -0.014923154847649379 and parameters: {'C': 0.25, 'gamma': 4.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:48,777] Trial 479 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:50,038] Trial 480 finished with value: 0.46204951187099946 and parameters: {'C': 0.5, 'gamma': 0.001953125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:51,364] Trial 481 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:52,689] Trial 482 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:53,947] Trial 483 finished with value: 0.16047631939323073 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:55,326] Trial 484 finished with value: 0.01906243408382946 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:56,598] Trial 485 finished with value: 0.27329653991733116 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:57,920] Trial 486 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:34:59,189] Trial 487 finished with value: 0.35646678621400674 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:35:00,485] Trial 488 finished with value: 0.08218412911084336 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:35:01,725] Trial 489 finished with value: 0.2292021892288631 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:35:03,053] Trial 490 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:35:04,361] Trial 491 finished with value: 0.6924387121129181 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:35:05,592] Trial 492 finished with value: 0.5925283480730915 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:35:06,917] Trial 493 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:35:08,312] Trial 494 finished with value: 0.06428103820253075 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:35:09,609] Trial 495 finished with value: 0.552347057695892 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:35:10,872] Trial 496 finished with value: 0.660251460924825 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:35:12,203] Trial 497 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:35:13,536] Trial 498 finished with value: 0.692499743989348 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n",
      "[I 2023-12-12 01:35:14,849] Trial 499 finished with value: 0.6924375104105264 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.7061176157661752.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7061\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.739025    0.714064    0.677058    0.749786   \n",
      "1                    TP   32.000000   40.000000   31.000000   33.000000   \n",
      "2                    TN  199.000000  198.000000  199.000000  199.000000   \n",
      "3                    FP    1.000000    4.000000    1.000000    2.000000   \n",
      "4                    FN   36.000000   26.000000   37.000000   34.000000   \n",
      "5              Accuracy    0.861940    0.888060    0.858209    0.865672   \n",
      "6             Precision    0.969697    0.909091    0.968750    0.942857   \n",
      "7           Sensitivity    0.470588    0.606061    0.455882    0.492537   \n",
      "8           Specificity    0.995000    0.980200    0.995000    0.990000   \n",
      "9              F1 score    0.633663    0.727273    0.620000    0.647059   \n",
      "10  F1 score (weighted)    0.843573    0.879756    0.838540    0.849553   \n",
      "11     F1 score (macro)    0.774303    0.828425    0.766422    0.782055   \n",
      "12    Balanced Accuracy    0.732794    0.793129    0.725441    0.741294   \n",
      "13                  MCC    0.616567    0.681846    0.605065    0.620153   \n",
      "14                  NPV    0.846800    0.883900    0.843200    0.854100   \n",
      "15              ROC_AUC    0.732794    0.793129    0.725441    0.741294   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.653836    0.719615    0.728529    0.750170    0.680643    0.698922  \n",
      "1    38.000000   39.000000   38.000000   36.000000   33.000000   31.000000  \n",
      "2   201.000000  200.000000  200.000000  199.000000  198.000000  200.000000  \n",
      "3     0.000000    0.000000    1.000000    2.000000    4.000000    2.000000  \n",
      "4    29.000000   29.000000   29.000000   31.000000   33.000000   35.000000  \n",
      "5     0.891791    0.891791    0.888060    0.876866    0.861940    0.861940  \n",
      "6     1.000000    1.000000    0.974359    0.947368    0.891892    0.939394  \n",
      "7     0.567164    0.573529    0.567164    0.537313    0.500000    0.469697  \n",
      "8     1.000000    1.000000    0.995000    0.990000    0.980200    0.990100  \n",
      "9     0.723810    0.728972    0.716981    0.685714    0.640777    0.626263  \n",
      "10    0.880488    0.880785    0.876920    0.864004    0.847128    0.844143  \n",
      "11    0.828262    0.830686    0.823607    0.804574    0.777663    0.770797  \n",
      "12    0.783582    0.786765    0.781095    0.763682    0.740099    0.729898  \n",
      "13    0.704026    0.707742    0.690348    0.654620    0.599737    0.602868  \n",
      "14    0.873900    0.873400    0.873400    0.865200    0.857100    0.851100  \n",
      "15    0.783582    0.786765    0.781095    0.763682    0.740099    0.729898  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_svm_9)\n",
    "# now convert the resuls to binary with cutoff 6.9\n",
    "Y_testSet9_cat = np.where(((Y_testSet9>=2) | (Y_testSet9<=-2)), 1, 0) \n",
    "y_pred_svm_9_cat = np.where(((y_pred_svm_9 >= 2) | (y_pred_svm_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_svm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7b0e56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7061\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "#plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.722274</td>\n",
       "      <td>0.040017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>3.446415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>99.300000</td>\n",
       "      <td>1.251666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.316561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>3.622461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.876793</td>\n",
       "      <td>0.029833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.938109</td>\n",
       "      <td>0.061222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.542447</td>\n",
       "      <td>0.105925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.988100</td>\n",
       "      <td>0.013031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.682484</td>\n",
       "      <td>0.090414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.863340</td>\n",
       "      <td>0.035781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.802985</td>\n",
       "      <td>0.053917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.765273</td>\n",
       "      <td>0.054485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.651908</td>\n",
       "      <td>0.090626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.867210</td>\n",
       "      <td>0.028268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.765273</td>\n",
       "      <td>0.054485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.722274     0.040017\n",
       "1                    TP        18.100000     3.446415\n",
       "2                    TN        99.300000     1.251666\n",
       "3                    FP         1.200000     1.316561\n",
       "4                    FN        15.300000     3.622461\n",
       "5              Accuracy         0.876793     0.029833\n",
       "6             Precision         0.938109     0.061222\n",
       "7           Sensitivity         0.542447     0.105925\n",
       "8           Specificity         0.988100     0.013031\n",
       "9              F1 score         0.682484     0.090414\n",
       "10  F1 score (weighted)         0.863340     0.035781\n",
       "11     F1 score (macro)         0.802985     0.053917\n",
       "12    Balanced Accuracy         0.765273     0.054485\n",
       "13                  MCC         0.651908     0.090626\n",
       "14                  NPV         0.867210     0.028268\n",
       "15              ROC_AUC         0.765273     0.054485"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.739025</td>\n",
       "      <td>0.714064</td>\n",
       "      <td>0.677058</td>\n",
       "      <td>0.749786</td>\n",
       "      <td>0.653836</td>\n",
       "      <td>0.719615</td>\n",
       "      <td>0.728529</td>\n",
       "      <td>0.750170</td>\n",
       "      <td>0.680643</td>\n",
       "      <td>0.698922</td>\n",
       "      <td>0.711165</td>\n",
       "      <td>0.032833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>35.100000</td>\n",
       "      <td>3.478505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>199.300000</td>\n",
       "      <td>0.948683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1.418136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>31.900000</td>\n",
       "      <td>3.634709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.861940</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.861940</td>\n",
       "      <td>0.861940</td>\n",
       "      <td>0.874627</td>\n",
       "      <td>0.014094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.954341</td>\n",
       "      <td>0.035536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.492537</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.537313</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.469697</td>\n",
       "      <td>0.523994</td>\n",
       "      <td>0.052808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>0.990100</td>\n",
       "      <td>0.991550</td>\n",
       "      <td>0.007016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.633663</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.723810</td>\n",
       "      <td>0.728972</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.640777</td>\n",
       "      <td>0.626263</td>\n",
       "      <td>0.675051</td>\n",
       "      <td>0.045906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.843573</td>\n",
       "      <td>0.879756</td>\n",
       "      <td>0.838540</td>\n",
       "      <td>0.849553</td>\n",
       "      <td>0.880488</td>\n",
       "      <td>0.880785</td>\n",
       "      <td>0.876920</td>\n",
       "      <td>0.864004</td>\n",
       "      <td>0.847128</td>\n",
       "      <td>0.844143</td>\n",
       "      <td>0.860489</td>\n",
       "      <td>0.017634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.774303</td>\n",
       "      <td>0.828425</td>\n",
       "      <td>0.766422</td>\n",
       "      <td>0.782055</td>\n",
       "      <td>0.828262</td>\n",
       "      <td>0.830686</td>\n",
       "      <td>0.823607</td>\n",
       "      <td>0.804574</td>\n",
       "      <td>0.777663</td>\n",
       "      <td>0.770797</td>\n",
       "      <td>0.798679</td>\n",
       "      <td>0.027020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.732794</td>\n",
       "      <td>0.793129</td>\n",
       "      <td>0.725441</td>\n",
       "      <td>0.741294</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.786765</td>\n",
       "      <td>0.781095</td>\n",
       "      <td>0.763682</td>\n",
       "      <td>0.740099</td>\n",
       "      <td>0.729898</td>\n",
       "      <td>0.757778</td>\n",
       "      <td>0.026595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.616567</td>\n",
       "      <td>0.681846</td>\n",
       "      <td>0.605065</td>\n",
       "      <td>0.620153</td>\n",
       "      <td>0.704026</td>\n",
       "      <td>0.707742</td>\n",
       "      <td>0.690348</td>\n",
       "      <td>0.654620</td>\n",
       "      <td>0.599737</td>\n",
       "      <td>0.602868</td>\n",
       "      <td>0.648297</td>\n",
       "      <td>0.044301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.846800</td>\n",
       "      <td>0.883900</td>\n",
       "      <td>0.843200</td>\n",
       "      <td>0.854100</td>\n",
       "      <td>0.873900</td>\n",
       "      <td>0.873400</td>\n",
       "      <td>0.873400</td>\n",
       "      <td>0.865200</td>\n",
       "      <td>0.857100</td>\n",
       "      <td>0.851100</td>\n",
       "      <td>0.862210</td>\n",
       "      <td>0.013665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.732794</td>\n",
       "      <td>0.793129</td>\n",
       "      <td>0.725441</td>\n",
       "      <td>0.741294</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.786765</td>\n",
       "      <td>0.781095</td>\n",
       "      <td>0.763682</td>\n",
       "      <td>0.740099</td>\n",
       "      <td>0.729898</td>\n",
       "      <td>0.757778</td>\n",
       "      <td>0.026595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.739025    0.714064    0.677058    0.749786   \n",
       "1                    TP   32.000000   40.000000   31.000000   33.000000   \n",
       "2                    TN  199.000000  198.000000  199.000000  199.000000   \n",
       "3                    FP    1.000000    4.000000    1.000000    2.000000   \n",
       "4                    FN   36.000000   26.000000   37.000000   34.000000   \n",
       "5              Accuracy    0.861940    0.888060    0.858209    0.865672   \n",
       "6             Precision    0.969697    0.909091    0.968750    0.942857   \n",
       "7           Sensitivity    0.470588    0.606061    0.455882    0.492537   \n",
       "8           Specificity    0.995000    0.980200    0.995000    0.990000   \n",
       "9              F1 score    0.633663    0.727273    0.620000    0.647059   \n",
       "10  F1 score (weighted)    0.843573    0.879756    0.838540    0.849553   \n",
       "11     F1 score (macro)    0.774303    0.828425    0.766422    0.782055   \n",
       "12    Balanced Accuracy    0.732794    0.793129    0.725441    0.741294   \n",
       "13                  MCC    0.616567    0.681846    0.605065    0.620153   \n",
       "14                  NPV    0.846800    0.883900    0.843200    0.854100   \n",
       "15              ROC_AUC    0.732794    0.793129    0.725441    0.741294   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.653836    0.719615    0.728529    0.750170    0.680643    0.698922   \n",
       "1    38.000000   39.000000   38.000000   36.000000   33.000000   31.000000   \n",
       "2   201.000000  200.000000  200.000000  199.000000  198.000000  200.000000   \n",
       "3     0.000000    0.000000    1.000000    2.000000    4.000000    2.000000   \n",
       "4    29.000000   29.000000   29.000000   31.000000   33.000000   35.000000   \n",
       "5     0.891791    0.891791    0.888060    0.876866    0.861940    0.861940   \n",
       "6     1.000000    1.000000    0.974359    0.947368    0.891892    0.939394   \n",
       "7     0.567164    0.573529    0.567164    0.537313    0.500000    0.469697   \n",
       "8     1.000000    1.000000    0.995000    0.990000    0.980200    0.990100   \n",
       "9     0.723810    0.728972    0.716981    0.685714    0.640777    0.626263   \n",
       "10    0.880488    0.880785    0.876920    0.864004    0.847128    0.844143   \n",
       "11    0.828262    0.830686    0.823607    0.804574    0.777663    0.770797   \n",
       "12    0.783582    0.786765    0.781095    0.763682    0.740099    0.729898   \n",
       "13    0.704026    0.707742    0.690348    0.654620    0.599737    0.602868   \n",
       "14    0.873900    0.873400    0.873400    0.865200    0.857100    0.851100   \n",
       "15    0.783582    0.786765    0.781095    0.763682    0.740099    0.729898   \n",
       "\n",
       "           ave       std  \n",
       "0     0.711165  0.032833  \n",
       "1    35.100000  3.478505  \n",
       "2   199.300000  0.948683  \n",
       "3     1.700000  1.418136  \n",
       "4    31.900000  3.634709  \n",
       "5     0.874627  0.014094  \n",
       "6     0.954341  0.035536  \n",
       "7     0.523994  0.052808  \n",
       "8     0.991550  0.007016  \n",
       "9     0.675051  0.045906  \n",
       "10    0.860489  0.017634  \n",
       "11    0.798679  0.027020  \n",
       "12    0.757778  0.026595  \n",
       "13    0.648297  0.044301  \n",
       "14    0.862210  0.013665  \n",
       "15    0.757778  0.026595  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test['std'] = mat_met_svm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.717705</td>\n",
       "      <td>0.057572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.876629</td>\n",
       "      <td>0.021532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.946100</td>\n",
       "      <td>0.050692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.539148</td>\n",
       "      <td>0.089646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.988850</td>\n",
       "      <td>0.011142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.681495</td>\n",
       "      <td>0.071337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.863017</td>\n",
       "      <td>0.027395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.802444</td>\n",
       "      <td>0.041708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.763999</td>\n",
       "      <td>0.043104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.653066</td>\n",
       "      <td>0.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.866472</td>\n",
       "      <td>0.023063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.763999</td>\n",
       "      <td>0.043104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.717705     0.057572\n",
       "1              Accuracy         0.876629     0.021532\n",
       "2             Precision         0.946100     0.050692\n",
       "3           Sensitivity         0.539148     0.089646\n",
       "4           Specificity         0.988850     0.011142\n",
       "5              F1 score         0.681495     0.071337\n",
       "6   F1 score (weighted)         0.863017     0.027395\n",
       "7      F1 score (macro)         0.802444     0.041708\n",
       "8     Balanced Accuracy         0.763999     0.043104\n",
       "9                   MCC         0.653066     0.063900\n",
       "10                  NPV         0.866472     0.023063\n",
       "11              ROC_AUC         0.763999     0.043104"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_svm_cat = np.where(((y_pred_optimized_svm >= 2) | (y_pred_optimized_svm <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_svm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_svm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "svm_run0 = data_svm[['y_test_idx0', 'y_test0', 'y_pred_svm0']]\n",
    "svm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "svm_run0.reset_index(inplace=True, drop=True)\n",
    "svm_run1 = data_svm[['y_test_idx1', 'y_test1', 'y_pred_svm1']]\n",
    "svm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "svm_run1.reset_index(inplace=True, drop=True)\n",
    "svm_run2 = data_svm[['y_test_idx2', 'y_test2', 'y_pred_svm2']]\n",
    "svm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "svm_run2.reset_index(inplace=True, drop=True)\n",
    "svm_run3 = data_svm[['y_test_idx3', 'y_test3', 'y_pred_svm3']]\n",
    "svm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "svm_run3.reset_index(inplace=True, drop=True)\n",
    "svm_run4 = data_svm[['y_test_idx4', 'y_test4', 'y_pred_svm4']]\n",
    "svm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "svm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "svm_5preds = pd.concat([chembl_id,svm_run0, svm_run1, svm_run2, svm_run3, svm_run4], axis=1)\n",
    "svm_5preds = svm_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_svm0', 'y_pred_svm1', 'y_pred_svm2', 'y_pred_svm3', 'y_pred_svm4']]\n",
    "svm_5preds['y_pred_svm_ave'] = svm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "svm_5preds['y_pred_svm_std'] = svm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "mat_met_optimized_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2869d8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model r2_score 0.6892 with a standard deviation of 0.0421\n",
      "SVM optimized model r2_score 0.7260 with a standard deviation of 0.0386\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVR \n",
    "svm_baseline_CVscore = cross_val_score(svm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"SVM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_svm.joblib']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_reg, \"OUTPUT/svm_reg.joblib\")\n",
    "#joblib.dump(optimized_svm, \"OUTPUT/optimized_svm.joblib\")\n",
    "joblib.dump(optimizedCV_svm, \"OUTPUT/optimizedCV_svm.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "16bdada3-5a01-40a7-acca-1e1263dbaab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/TestSet_EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_rf_test.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_lgbm_test.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_xgb_test.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_knn_test.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_svm_test.to_excel(writer, sheet_name=\"SVM\", )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "80a6dc72-fa55-4a7e-b3de-2730b7621f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_optimized_rf.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_optimized_lgbm.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_optimized_xgb.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_optimized_knn.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_optimized_svm.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b9aa5f0b-2654-4980-be38-d6c0b7c3167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/PredResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    rf_5preds.to_excel(writer, sheet_name=\"RF\", )\n",
    "    lgbm_5preds.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    xgb_5preds.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    knn_5preds.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    svm_5preds.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
