{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/anaconda3/envs/teachopencadd/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "HDAC1and6 = Path(HERE).resolve().parents[1]/'input'\n",
    "output = HERE/'OUTPUT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>molecular_weight</th>\n",
       "      <th>n_rot</th>\n",
       "      <th>n_heavy</th>\n",
       "      <th>n_hba</th>\n",
       "      <th>n_hbd</th>\n",
       "      <th>logp</th>\n",
       "      <th>num_ar</th>\n",
       "      <th>num_sa</th>\n",
       "      <th>num_alip</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4635479</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[3356504, 10511, 2043088, 13972425, 4030911, 8...</td>\n",
       "      <td>424.233522</td>\n",
       "      <td>10.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.58480</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4299417</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[100053, 10511, 1733887, 904525, 692898, 28201...</td>\n",
       "      <td>1034.651502</td>\n",
       "      <td>17.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.18550</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4225331</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5976924, 3913535, 8033062, 6790323, 12480751,...</td>\n",
       "      <td>375.119461</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.04462</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL1094710</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[6411541, 3821889, 137380, 8040222, 7332227, 1...</td>\n",
       "      <td>413.242690</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.09950</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3287256</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[6220534, 11837907, 15961454, 218190, 16674131...</td>\n",
       "      <td>408.128983</td>\n",
       "      <td>8.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.71980</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4635479  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL4299417  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL4225331  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL1094710  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL3287256  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "4  [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  molecular_weight  n_rot  \\\n",
       "0  [3356504, 10511, 2043088, 13972425, 4030911, 8...        424.233522   10.0   \n",
       "1  [100053, 10511, 1733887, 904525, 692898, 28201...       1034.651502   17.0   \n",
       "2  [5976924, 3913535, 8033062, 6790323, 12480751,...        375.119461    4.0   \n",
       "3  [6411541, 3821889, 137380, 8040222, 7332227, 1...        413.242690   10.0   \n",
       "4  [6220534, 11837907, 15961454, 218190, 16674131...        408.128983    8.0   \n",
       "\n",
       "   n_heavy  n_hba  n_hbd     logp  num_ar  num_sa  num_alip  SelectivityWindow  \n",
       "0     31.0    9.0    4.0  2.58480     3.0     0.0       0.0               0.42  \n",
       "1     73.0   19.0    7.0  4.18550     2.0     3.0       3.0               0.98  \n",
       "2     27.0    4.0    2.0  4.04462     3.0     0.0       0.0               0.84  \n",
       "3     30.0    6.0    3.0  4.09950     2.0     1.0       1.0               0.45  \n",
       "4     27.0    7.0    3.0  3.71980     2.0     1.0       1.0              -0.33  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC1and6/\"HDAC1and6_1024B.csv.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>type_HDAC6</th>\n",
       "      <th>pchembl_HDAC6</th>\n",
       "      <th>standard_value_HDAC6</th>\n",
       "      <th>type_HDAC1</th>\n",
       "      <th>pchembl_HDAC1</th>\n",
       "      <th>standard_value_HDAC1</th>\n",
       "      <th>SelectivityRatio</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>sel6</th>\n",
       "      <th>sel1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4098975</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>9.85</td>\n",
       "      <td>0.14</td>\n",
       "      <td>IC50</td>\n",
       "      <td>6.96</td>\n",
       "      <td>110.0</td>\n",
       "      <td>785.71</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.89</td>\n",
       "      <td>-2.89</td>\n",
       "      <td>hDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3912061</td>\n",
       "      <td>CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12</td>\n",
       "      <td>IC50</td>\n",
       "      <td>9.77</td>\n",
       "      <td>0.17</td>\n",
       "      <td>IC50</td>\n",
       "      <td>6.21</td>\n",
       "      <td>623.0</td>\n",
       "      <td>3664.71</td>\n",
       "      <td>3.56</td>\n",
       "      <td>3.56</td>\n",
       "      <td>-3.56</td>\n",
       "      <td>hDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4243347</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>9.70</td>\n",
       "      <td>0.20</td>\n",
       "      <td>IC50</td>\n",
       "      <td>8.70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4247128</td>\n",
       "      <td>C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>9.60</td>\n",
       "      <td>0.25</td>\n",
       "      <td>IC50</td>\n",
       "      <td>7.08</td>\n",
       "      <td>83.0</td>\n",
       "      <td>332.00</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.52</td>\n",
       "      <td>-2.52</td>\n",
       "      <td>hDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4126811</td>\n",
       "      <td>CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>9.48</td>\n",
       "      <td>0.33</td>\n",
       "      <td>IC50</td>\n",
       "      <td>6.36</td>\n",
       "      <td>436.0</td>\n",
       "      <td>1321.21</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3.12</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>hDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>CHEMBL4278591</td>\n",
       "      <td>CC(=O)Nc1ccc(-c2ccnc(Nc3cccc(OCCCCCCCC(=O)NO)c...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>8.17</td>\n",
       "      <td>6.70</td>\n",
       "      <td>IC50</td>\n",
       "      <td>5.98</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>155.67</td>\n",
       "      <td>2.19</td>\n",
       "      <td>2.19</td>\n",
       "      <td>-2.19</td>\n",
       "      <td>hDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>CHEMBL4649511</td>\n",
       "      <td>O=C(CCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO</td>\n",
       "      <td>IC50</td>\n",
       "      <td>7.89</td>\n",
       "      <td>13.00</td>\n",
       "      <td>IC50</td>\n",
       "      <td>8.48</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>0.59</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>CHEMBL4291781</td>\n",
       "      <td>CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(OCCCCCCCC(=O)NO)cc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>7.85</td>\n",
       "      <td>14.00</td>\n",
       "      <td>IC50</td>\n",
       "      <td>5.77</td>\n",
       "      <td>1701.0</td>\n",
       "      <td>121.50</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-2.08</td>\n",
       "      <td>hDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>CHEMBL3215861</td>\n",
       "      <td>CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC</td>\n",
       "      <td>Ki</td>\n",
       "      <td>6.61</td>\n",
       "      <td>247.00</td>\n",
       "      <td>Ki</td>\n",
       "      <td>7.55</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>CHEMBL3233708</td>\n",
       "      <td>O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>6.53</td>\n",
       "      <td>293.60</td>\n",
       "      <td>IC50</td>\n",
       "      <td>6.45</td>\n",
       "      <td>355.4</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>Non-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1339 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                             smiles  \\\n",
       "0         CHEMBL4098975  O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...   \n",
       "1         CHEMBL3912061     CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12   \n",
       "2         CHEMBL4243347  O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...   \n",
       "3         CHEMBL4247128  C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...   \n",
       "4         CHEMBL4126811  CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...   \n",
       "...                 ...                                                ...   \n",
       "1901      CHEMBL4278591  CC(=O)Nc1ccc(-c2ccnc(Nc3cccc(OCCCCCCCC(=O)NO)c...   \n",
       "1902      CHEMBL4649511        O=C(CCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO   \n",
       "1903      CHEMBL4291781  CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(OCCCCCCCC(=O)NO)cc...   \n",
       "1908      CHEMBL3215861           CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC   \n",
       "1909      CHEMBL3233708  O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...   \n",
       "\n",
       "     type_HDAC6  pchembl_HDAC6  standard_value_HDAC6 type_HDAC1  \\\n",
       "0          IC50           9.85                  0.14       IC50   \n",
       "1          IC50           9.77                  0.17       IC50   \n",
       "2          IC50           9.70                  0.20       IC50   \n",
       "3          IC50           9.60                  0.25       IC50   \n",
       "4          IC50           9.48                  0.33       IC50   \n",
       "...         ...            ...                   ...        ...   \n",
       "1901       IC50           8.17                  6.70       IC50   \n",
       "1902       IC50           7.89                 13.00       IC50   \n",
       "1903       IC50           7.85                 14.00       IC50   \n",
       "1908         Ki           6.61                247.00         Ki   \n",
       "1909       IC50           6.53                293.60       IC50   \n",
       "\n",
       "      pchembl_HDAC1  standard_value_HDAC1  SelectivityRatio  \\\n",
       "0              6.96                 110.0            785.71   \n",
       "1              6.21                 623.0           3664.71   \n",
       "2              8.70                   2.0             10.00   \n",
       "3              7.08                  83.0            332.00   \n",
       "4              6.36                 436.0           1321.21   \n",
       "...             ...                   ...               ...   \n",
       "1901           5.98                1043.0            155.67   \n",
       "1902           8.48                   3.3              0.25   \n",
       "1903           5.77                1701.0            121.50   \n",
       "1908           7.55                  28.0              0.11   \n",
       "1909           6.45                 355.4              1.21   \n",
       "\n",
       "      SelectivityWindow  sel6  sel1            label  \n",
       "0                  2.89  2.89 -2.89  hDAC6-selective  \n",
       "1                  3.56  3.56 -3.56  hDAC6-selective  \n",
       "2                  1.00  1.00 -1.00      Dual-binder  \n",
       "3                  2.52  2.52 -2.52  hDAC6-selective  \n",
       "4                  3.12  3.12 -3.12  hDAC6-selective  \n",
       "...                 ...   ...   ...              ...  \n",
       "1901               2.19  2.19 -2.19  hDAC6-selective  \n",
       "1902              -0.59 -0.59  0.59      Dual-binder  \n",
       "1903               2.08  2.08 -2.08  hDAC6-selective  \n",
       "1908              -0.94 -0.94  0.94      Dual-binder  \n",
       "1909               0.08  0.08 -0.08       Non-binder  \n",
       "\n",
       "[1339 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC1and6/\"HDAC1and6_SemiSel_dataset.csv\", index_col=0)\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca3dbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label'] == 'Dual-binder']['SelectivityWindow'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>molecular_weight</th>\n",
       "      <th>n_rot</th>\n",
       "      <th>n_heavy</th>\n",
       "      <th>n_hba</th>\n",
       "      <th>n_hbd</th>\n",
       "      <th>logp</th>\n",
       "      <th>num_ar</th>\n",
       "      <th>num_sa</th>\n",
       "      <th>num_alip</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4635479</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[3356504, 10511, 2043088, 13972425, 4030911, 8...</td>\n",
       "      <td>424.233522</td>\n",
       "      <td>10.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.58480</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4299417</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[100053, 10511, 1733887, 904525, 692898, 28201...</td>\n",
       "      <td>1034.651502</td>\n",
       "      <td>17.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.18550</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4225331</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5976924, 3913535, 8033062, 6790323, 12480751,...</td>\n",
       "      <td>375.119461</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.04462</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL1094710</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[6411541, 3821889, 137380, 8040222, 7332227, 1...</td>\n",
       "      <td>413.242690</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.09950</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4635479  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL4299417  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL4225331  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL1094710  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  molecular_weight  n_rot  \\\n",
       "0  [3356504, 10511, 2043088, 13972425, 4030911, 8...        424.233522   10.0   \n",
       "1  [100053, 10511, 1733887, 904525, 692898, 28201...       1034.651502   17.0   \n",
       "2  [5976924, 3913535, 8033062, 6790323, 12480751,...        375.119461    4.0   \n",
       "3  [6411541, 3821889, 137380, 8040222, 7332227, 1...        413.242690   10.0   \n",
       "\n",
       "   n_heavy  n_hba  n_hbd     logp  num_ar  num_sa  num_alip  \\\n",
       "0     31.0    9.0    4.0  2.58480     3.0     0.0       0.0   \n",
       "1     73.0   19.0    7.0  4.18550     2.0     3.0       3.0   \n",
       "2     27.0    4.0    2.0  4.04462     3.0     0.0       0.0   \n",
       "3     30.0    6.0    3.0  4.09950     2.0     1.0       1.0   \n",
       "\n",
       "   SelectivityWindow        label  Class  \n",
       "0               0.42  Dual-binder    3.0  \n",
       "1               0.98  Dual-binder    3.0  \n",
       "2               0.84  Dual-binder    3.0  \n",
       "3               0.45  Dual-binder    3.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'hDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'hDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'hDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for selectivity\n",
    "df[\"selectivity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as selective if SelectivityWindow is >=2 or >=-2, 0 otherwise\n",
    "df.loc[df[df.SelectivityWindow >= 2.0].index, \"selectivity\"] = 1.0\n",
    "df.loc[df[df.SelectivityWindow <= -2.0].index, \"selectivity\"] = 1.0\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "indices =  np.array(df.index)\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y = df[\"SelectivityWindow\"].values\n",
    "Y_cat =  df[\"selectivity\"].values\n",
    "Y_class = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['SelectivityWindow'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_cat\"] = np.array(list(df.iloc[index_tr_all[i]]['selectivity'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['SelectivityWindow'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_cat\"] = np.array(list(df.iloc[index_te_all[i]]['selectivity'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.708094     0.051837\n",
      "1                    TP        18.400000     3.405877\n",
      "2                    TN        98.900000     1.449138\n",
      "3                    FP         1.600000     1.349897\n",
      "4                    FN        15.000000     3.464102\n",
      "5              Accuracy         0.876041     0.033175\n",
      "6             Precision         0.916718     0.075089\n",
      "7           Sensitivity         0.551086     0.101888\n",
      "8           Specificity         0.984080     0.013424\n",
      "9              F1 score         0.685613     0.093830\n",
      "10  F1 score (weighted)         0.863578     0.038586\n",
      "11     F1 score (macro)         0.804184     0.056888\n",
      "12    Balanced Accuracy         0.767583     0.055533\n",
      "13                  MCC         0.647457     0.103381\n",
      "14                  NPV         0.868870     0.027879\n",
      "15              ROC_AUC         0.767583     0.055533\n",
      "CPU times: user 1min 29s, sys: 374 ms, total: 1min 29s\n",
      "Wall time: 7.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_reg =  RandomForestRegressor(random_state=1121218, max_features = None, n_jobs=24,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_reg.fit(x_train, y_train)\n",
    "        y_pred = rf_reg.predict(x_test)  \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=24, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=24, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 15:02:04,546]\u001b[0m A new study created in memory with name: RFRegressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:02:46,858]\u001b[0m Trial 0 finished with value: 0.6829030767338571 and parameters: {'n_estimators': 718}. Best is trial 0 with value: 0.6829030767338571.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:03:14,733]\u001b[0m Trial 1 finished with value: 0.6835958704586108 and parameters: {'n_estimators': 469}. Best is trial 1 with value: 0.6835958704586108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:03:40,299]\u001b[0m Trial 2 finished with value: 0.6834980909741641 and parameters: {'n_estimators': 431}. Best is trial 1 with value: 0.6835958704586108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:04:05,398]\u001b[0m Trial 3 finished with value: 0.6832480394557217 and parameters: {'n_estimators': 421}. Best is trial 1 with value: 0.6835958704586108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:04:20,068]\u001b[0m Trial 4 finished with value: 0.680381488100111 and parameters: {'n_estimators': 248}. Best is trial 1 with value: 0.6835958704586108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:04:43,663]\u001b[0m Trial 5 finished with value: 0.6831957040207175 and parameters: {'n_estimators': 408}. Best is trial 1 with value: 0.6835958704586108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:05:32,830]\u001b[0m Trial 6 finished with value: 0.6829977992629417 and parameters: {'n_estimators': 855}. Best is trial 1 with value: 0.6835958704586108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:06:27,136]\u001b[0m Trial 7 finished with value: 0.6831257955210676 and parameters: {'n_estimators': 932}. Best is trial 1 with value: 0.6835958704586108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:07:11,817]\u001b[0m Trial 8 finished with value: 0.6829518402250808 and parameters: {'n_estimators': 747}. Best is trial 1 with value: 0.6835958704586108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:07:20,582]\u001b[0m Trial 9 finished with value: 0.6796060009767382 and parameters: {'n_estimators': 145}. Best is trial 1 with value: 0.6835958704586108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:07:54,940]\u001b[0m Trial 10 finished with value: 0.6833435145420015 and parameters: {'n_estimators': 592}. Best is trial 1 with value: 0.6835958704586108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:08:21,611]\u001b[0m Trial 11 finished with value: 0.683060386690054 and parameters: {'n_estimators': 446}. Best is trial 1 with value: 0.6835958704586108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:08:38,938]\u001b[0m Trial 12 finished with value: 0.6815814253513246 and parameters: {'n_estimators': 294}. Best is trial 1 with value: 0.6835958704586108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:09:13,942]\u001b[0m Trial 13 finished with value: 0.6833880268891984 and parameters: {'n_estimators': 590}. Best is trial 1 with value: 0.6835958704586108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:09:44,280]\u001b[0m Trial 14 finished with value: 0.6836740291160776 and parameters: {'n_estimators': 525}. Best is trial 14 with value: 0.6836740291160776.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:10:16,885]\u001b[0m Trial 15 finished with value: 0.6833200671749976 and parameters: {'n_estimators': 549}. Best is trial 14 with value: 0.6836740291160776.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:11:00,807]\u001b[0m Trial 16 finished with value: 0.6829030767338571 and parameters: {'n_estimators': 718}. Best is trial 14 with value: 0.6836740291160776.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:11:18,858]\u001b[0m Trial 17 finished with value: 0.6814881318239707 and parameters: {'n_estimators': 295}. Best is trial 14 with value: 0.6836740291160776.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:11:50,363]\u001b[0m Trial 18 finished with value: 0.6833235215897292 and parameters: {'n_estimators': 545}. Best is trial 14 with value: 0.6836740291160776.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:11:57,893]\u001b[0m Trial 19 finished with value: 0.6790926371452992 and parameters: {'n_estimators': 117}. Best is trial 14 with value: 0.6836740291160776.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:12:35,028]\u001b[0m Trial 20 finished with value: 0.682917844058781 and parameters: {'n_estimators': 640}. Best is trial 14 with value: 0.6836740291160776.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:13:03,260]\u001b[0m Trial 21 finished with value: 0.6839644228553751 and parameters: {'n_estimators': 478}. Best is trial 21 with value: 0.6839644228553751.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:13:32,338]\u001b[0m Trial 22 finished with value: 0.6839797099071248 and parameters: {'n_estimators': 492}. Best is trial 22 with value: 0.6839797099071248.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:13:51,819]\u001b[0m Trial 23 finished with value: 0.6820973530150565 and parameters: {'n_estimators': 326}. Best is trial 22 with value: 0.6839797099071248.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:14:21,306]\u001b[0m Trial 24 finished with value: 0.6840945652904953 and parameters: {'n_estimators': 495}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:14:43,159]\u001b[0m Trial 25 finished with value: 0.6819714173698808 and parameters: {'n_estimators': 351}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:14:55,713]\u001b[0m Trial 26 finished with value: 0.6804231921034712 and parameters: {'n_estimators': 210}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:15:33,478]\u001b[0m Trial 27 finished with value: 0.683008115584219 and parameters: {'n_estimators': 666}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:16:03,945]\u001b[0m Trial 28 finished with value: 0.6839580633634973 and parameters: {'n_estimators': 508}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:16:26,616]\u001b[0m Trial 29 finished with value: 0.6824844403059925 and parameters: {'n_estimators': 367}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:17:13,470]\u001b[0m Trial 30 finished with value: 0.682811255836216 and parameters: {'n_estimators': 772}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:17:42,338]\u001b[0m Trial 31 finished with value: 0.6839578676302862 and parameters: {'n_estimators': 493}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:18:10,505]\u001b[0m Trial 32 finished with value: 0.6839577733401284 and parameters: {'n_estimators': 485}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:18:45,661]\u001b[0m Trial 33 finished with value: 0.6834523227010127 and parameters: {'n_estimators': 611}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:19:08,490]\u001b[0m Trial 34 finished with value: 0.6823729662805459 and parameters: {'n_estimators': 389}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:19:35,107]\u001b[0m Trial 35 finished with value: 0.6833063366745857 and parameters: {'n_estimators': 456}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:20:04,101]\u001b[0m Trial 36 finished with value: 0.6840204342469727 and parameters: {'n_estimators': 503}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:20:43,761]\u001b[0m Trial 37 finished with value: 0.6827860296101005 and parameters: {'n_estimators': 669}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:21:08,564]\u001b[0m Trial 38 finished with value: 0.6830843895170546 and parameters: {'n_estimators': 416}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:21:53,705]\u001b[0m Trial 39 finished with value: 0.6834126248921812 and parameters: {'n_estimators': 589}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:22:30,580]\u001b[0m Trial 40 finished with value: 0.6832601546880046 and parameters: {'n_estimators': 451}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:23:12,208]\u001b[0m Trial 41 finished with value: 0.6837425614075247 and parameters: {'n_estimators': 521}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:23:50,087]\u001b[0m Trial 42 finished with value: 0.683964422855375 and parameters: {'n_estimators': 478}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:24:23,547]\u001b[0m Trial 43 finished with value: 0.6832147738038187 and parameters: {'n_estimators': 409}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:25:47,145]\u001b[0m Trial 44 finished with value: 0.6832900594225153 and parameters: {'n_estimators': 996}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:26:27,664]\u001b[0m Trial 45 finished with value: 0.6839052171698226 and parameters: {'n_estimators': 474}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 15:27:13,553]\u001b[0m Trial 46 finished with value: 0.6833671871829269 and parameters: {'n_estimators': 569}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:27:50,097]\u001b[0m Trial 47 finished with value: 0.683284393247759 and parameters: {'n_estimators': 435}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:28:12,193]\u001b[0m Trial 48 finished with value: 0.680525413706718 and parameters: {'n_estimators': 250}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:28:58,632]\u001b[0m Trial 49 finished with value: 0.6833402362548066 and parameters: {'n_estimators': 540}. Best is trial 24 with value: 0.6840945652904953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6841\n",
      "\tBest params:\n",
      "\t\tn_estimators: 495\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFRegressor\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.664931\n",
      "1                    TP   39.000000\n",
      "2                    TN  199.000000\n",
      "3                    FP    2.000000\n",
      "4                    FN   28.000000\n",
      "5              Accuracy    0.888060\n",
      "6             Precision    0.951220\n",
      "7           Sensitivity    0.582090\n",
      "8           Specificity    0.990000\n",
      "9              F1 score    0.722222\n",
      "10  F1 score (weighted)    0.877985\n",
      "11     F1 score (macro)    0.826064\n",
      "12    Balanced Accuracy    0.786070\n",
      "13                  MCC    0.688228\n",
      "14                  NPV    0.876700\n",
      "15              ROC_AUC    0.786070\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=24, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_rf_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet0 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_0_cat = np.where(((y_pred_rf_0 >= 2) | (y_pred_rf_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_rf_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 15:29:35,478]\u001b[0m Trial 50 finished with value: 0.6939540717274404 and parameters: {'n_estimators': 364}. Best is trial 50 with value: 0.6939540717274404.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:30:07,170]\u001b[0m Trial 51 finished with value: 0.6942753284249883 and parameters: {'n_estimators': 373}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:30:32,616]\u001b[0m Trial 52 finished with value: 0.6931619651665895 and parameters: {'n_estimators': 293}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:30:53,893]\u001b[0m Trial 53 finished with value: 0.6918624589427222 and parameters: {'n_estimators': 251}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:31:16,661]\u001b[0m Trial 54 finished with value: 0.6922772894025225 and parameters: {'n_estimators': 260}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:31:34,824]\u001b[0m Trial 55 finished with value: 0.6908759728366561 and parameters: {'n_estimators': 208}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:31:51,519]\u001b[0m Trial 56 finished with value: 0.6914306464353948 and parameters: {'n_estimators': 198}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:32:06,529]\u001b[0m Trial 57 finished with value: 0.6925379800587331 and parameters: {'n_estimators': 172}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:32:29,464]\u001b[0m Trial 58 finished with value: 0.6919940364496251 and parameters: {'n_estimators': 264}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:32:55,546]\u001b[0m Trial 59 finished with value: 0.6931988045188182 and parameters: {'n_estimators': 295}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:33:09,392]\u001b[0m Trial 60 finished with value: 0.6925126508570316 and parameters: {'n_estimators': 162}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:33:23,118]\u001b[0m Trial 61 finished with value: 0.6918565008783212 and parameters: {'n_estimators': 157}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:33:49,707]\u001b[0m Trial 62 finished with value: 0.6935176835091679 and parameters: {'n_estimators': 306}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:34:16,724]\u001b[0m Trial 63 finished with value: 0.6935502669590645 and parameters: {'n_estimators': 309}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:34:45,741]\u001b[0m Trial 64 finished with value: 0.6940127088743763 and parameters: {'n_estimators': 329}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:35:12,504]\u001b[0m Trial 65 finished with value: 0.6935472727904817 and parameters: {'n_estimators': 311}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:35:41,429]\u001b[0m Trial 66 finished with value: 0.69371500195296 and parameters: {'n_estimators': 325}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:36:14,020]\u001b[0m Trial 67 finished with value: 0.6939526339617877 and parameters: {'n_estimators': 362}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:36:44,632]\u001b[0m Trial 68 finished with value: 0.6939493430370026 and parameters: {'n_estimators': 342}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:37:14,306]\u001b[0m Trial 69 finished with value: 0.6939493430370026 and parameters: {'n_estimators': 342}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:37:45,161]\u001b[0m Trial 70 finished with value: 0.6937859689251828 and parameters: {'n_estimators': 344}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:38:15,818]\u001b[0m Trial 71 finished with value: 0.694134580827101 and parameters: {'n_estimators': 350}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:38:47,870]\u001b[0m Trial 72 finished with value: 0.6939589248612054 and parameters: {'n_estimators': 359}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:39:20,771]\u001b[0m Trial 73 finished with value: 0.6941600773573435 and parameters: {'n_estimators': 371}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:39:54,217]\u001b[0m Trial 74 finished with value: 0.6940896859508834 and parameters: {'n_estimators': 370}. Best is trial 51 with value: 0.6942753284249883.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:40:28,486]\u001b[0m Trial 75 finished with value: 0.6942874069570665 and parameters: {'n_estimators': 379}. Best is trial 75 with value: 0.6942874069570665.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:41:03,570]\u001b[0m Trial 76 finished with value: 0.6942832136973326 and parameters: {'n_estimators': 382}. Best is trial 75 with value: 0.6942874069570665.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:41:38,579]\u001b[0m Trial 77 finished with value: 0.6945026285610231 and parameters: {'n_estimators': 390}. Best is trial 77 with value: 0.6945026285610231.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:42:13,133]\u001b[0m Trial 78 finished with value: 0.694680208598865 and parameters: {'n_estimators': 389}. Best is trial 78 with value: 0.694680208598865.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:42:48,052]\u001b[0m Trial 79 finished with value: 0.6946388721428697 and parameters: {'n_estimators': 392}. Best is trial 78 with value: 0.694680208598865.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:43:23,171]\u001b[0m Trial 80 finished with value: 0.6946388721428697 and parameters: {'n_estimators': 392}. Best is trial 78 with value: 0.694680208598865.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:43:57,147]\u001b[0m Trial 81 finished with value: 0.6943024114423926 and parameters: {'n_estimators': 381}. Best is trial 78 with value: 0.694680208598865.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:44:31,419]\u001b[0m Trial 82 finished with value: 0.694397450978121 and parameters: {'n_estimators': 387}. Best is trial 78 with value: 0.694680208598865.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:45:07,065]\u001b[0m Trial 83 finished with value: 0.6946080791152531 and parameters: {'n_estimators': 393}. Best is trial 78 with value: 0.694680208598865.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:45:43,446]\u001b[0m Trial 84 finished with value: 0.6945131061805798 and parameters: {'n_estimators': 403}. Best is trial 78 with value: 0.694680208598865.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:46:20,405]\u001b[0m Trial 85 finished with value: 0.6945739258818568 and parameters: {'n_estimators': 399}. Best is trial 78 with value: 0.694680208598865.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:46:56,345]\u001b[0m Trial 86 finished with value: 0.6945254809749299 and parameters: {'n_estimators': 402}. Best is trial 78 with value: 0.694680208598865.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:47:33,474]\u001b[0m Trial 87 finished with value: 0.6938542984549226 and parameters: {'n_estimators': 426}. Best is trial 78 with value: 0.694680208598865.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:48:08,808]\u001b[0m Trial 88 finished with value: 0.6945297752654548 and parameters: {'n_estimators': 404}. Best is trial 78 with value: 0.694680208598865.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:48:50,079]\u001b[0m Trial 89 finished with value: 0.6947617592741981 and parameters: {'n_estimators': 456}. Best is trial 89 with value: 0.6947617592741981.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:49:30,665]\u001b[0m Trial 90 finished with value: 0.6944930638046569 and parameters: {'n_estimators': 451}. Best is trial 89 with value: 0.6947617592741981.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:50:05,501]\u001b[0m Trial 91 finished with value: 0.6944175988318357 and parameters: {'n_estimators': 405}. Best is trial 89 with value: 0.6947617592741981.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:50:42,689]\u001b[0m Trial 92 finished with value: 0.6939842382886887 and parameters: {'n_estimators': 412}. Best is trial 89 with value: 0.6947617592741981.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:51:22,111]\u001b[0m Trial 93 finished with value: 0.6945378182377729 and parameters: {'n_estimators': 438}. Best is trial 89 with value: 0.6947617592741981.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:51:59,924]\u001b[0m Trial 94 finished with value: 0.6940902336756862 and parameters: {'n_estimators': 430}. Best is trial 89 with value: 0.6947617592741981.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:52:38,979]\u001b[0m Trial 95 finished with value: 0.6945153580782184 and parameters: {'n_estimators': 445}. Best is trial 89 with value: 0.6947617592741981.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:53:17,872]\u001b[0m Trial 96 finished with value: 0.6944930638046569 and parameters: {'n_estimators': 451}. Best is trial 89 with value: 0.6947617592741981.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 15:54:00,289]\u001b[0m Trial 97 finished with value: 0.6947079755167711 and parameters: {'n_estimators': 466}. Best is trial 89 with value: 0.6947617592741981.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:54:42,495]\u001b[0m Trial 98 finished with value: 0.6947266779434186 and parameters: {'n_estimators': 469}. Best is trial 89 with value: 0.6947617592741981.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:55:28,569]\u001b[0m Trial 99 finished with value: 0.6948885362639611 and parameters: {'n_estimators': 518}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6949\n",
      "\tBest params:\n",
      "\t\tn_estimators: 518\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.664931    0.654276\n",
      "1                    TP   39.000000   42.000000\n",
      "2                    TN  199.000000  198.000000\n",
      "3                    FP    2.000000    3.000000\n",
      "4                    FN   28.000000   25.000000\n",
      "5              Accuracy    0.888060    0.895522\n",
      "6             Precision    0.951220    0.933333\n",
      "7           Sensitivity    0.582090    0.626866\n",
      "8           Specificity    0.990000    0.985100\n",
      "9              F1 score    0.722222    0.750000\n",
      "10  F1 score (weighted)    0.877985    0.887972\n",
      "11     F1 score (macro)    0.826064    0.841981\n",
      "12    Balanced Accuracy    0.786070    0.805970\n",
      "13                  MCC    0.688228    0.708901\n",
      "14                  NPV    0.876700    0.887900\n",
      "15              ROC_AUC    0.786070    0.805970\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=24, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_rf_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_1_cat = np.where(((y_pred_rf_1 >= 2) | (y_pred_rf_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_rf_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 15:56:21,222]\u001b[0m Trial 100 finished with value: 0.654762209800966 and parameters: {'n_estimators': 523}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:57:06,367]\u001b[0m Trial 101 finished with value: 0.6545131644818118 and parameters: {'n_estimators': 503}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:57:48,649]\u001b[0m Trial 102 finished with value: 0.6545949093612524 and parameters: {'n_estimators': 468}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:58:30,417]\u001b[0m Trial 103 finished with value: 0.654450218429861 and parameters: {'n_estimators': 471}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:59:19,288]\u001b[0m Trial 104 finished with value: 0.6543073133383192 and parameters: {'n_estimators': 542}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 15:59:58,302]\u001b[0m Trial 105 finished with value: 0.6548244288460602 and parameters: {'n_estimators': 435}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:00:36,715]\u001b[0m Trial 106 finished with value: 0.6546599950941034 and parameters: {'n_estimators': 423}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:01:27,865]\u001b[0m Trial 107 finished with value: 0.654442496695834 and parameters: {'n_estimators': 560}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:02:13,449]\u001b[0m Trial 108 finished with value: 0.6547982191770815 and parameters: {'n_estimators': 515}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:02:57,668]\u001b[0m Trial 109 finished with value: 0.6542819728479538 and parameters: {'n_estimators': 487}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:03:38,308]\u001b[0m Trial 110 finished with value: 0.6546733889457921 and parameters: {'n_estimators': 465}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:04:15,228]\u001b[0m Trial 111 finished with value: 0.6553046810453358 and parameters: {'n_estimators': 405}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:04:55,483]\u001b[0m Trial 112 finished with value: 0.6548619355696 and parameters: {'n_estimators': 442}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:05:31,723]\u001b[0m Trial 113 finished with value: 0.6554474597877122 and parameters: {'n_estimators': 398}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:06:09,351]\u001b[0m Trial 114 finished with value: 0.6547075719151596 and parameters: {'n_estimators': 421}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:06:52,620]\u001b[0m Trial 115 finished with value: 0.6543566088259942 and parameters: {'n_estimators': 477}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:07:18,635]\u001b[0m Trial 116 finished with value: 0.6573128095042841 and parameters: {'n_estimators': 278}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:08:04,873]\u001b[0m Trial 117 finished with value: 0.6544248531950762 and parameters: {'n_estimators': 498}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:08:47,258]\u001b[0m Trial 118 finished with value: 0.6547745428963341 and parameters: {'n_estimators': 461}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:09:59,728]\u001b[0m Trial 119 finished with value: 0.6545420300765935 and parameters: {'n_estimators': 808}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:10:37,605]\u001b[0m Trial 120 finished with value: 0.6548005850815377 and parameters: {'n_estimators': 418}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:11:16,779]\u001b[0m Trial 121 finished with value: 0.6548241078428745 and parameters: {'n_estimators': 432}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:11:57,330]\u001b[0m Trial 122 finished with value: 0.6547461066890556 and parameters: {'n_estimators': 449}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:12:34,152]\u001b[0m Trial 123 finished with value: 0.6554474597877122 and parameters: {'n_estimators': 398}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:13:04,533]\u001b[0m Trial 124 finished with value: 0.6570697263885522 and parameters: {'n_estimators': 331}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:13:44,259]\u001b[0m Trial 125 finished with value: 0.6547403159998464 and parameters: {'n_estimators': 438}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:14:28,368]\u001b[0m Trial 126 finished with value: 0.6542955402404101 and parameters: {'n_estimators': 482}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:15:11,012]\u001b[0m Trial 127 finished with value: 0.6548333035563955 and parameters: {'n_estimators': 459}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:15:44,362]\u001b[0m Trial 128 finished with value: 0.6567038989094681 and parameters: {'n_estimators': 358}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:16:35,158]\u001b[0m Trial 129 finished with value: 0.6543916955283783 and parameters: {'n_estimators': 571}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:17:23,705]\u001b[0m Trial 130 finished with value: 0.6543069352967203 and parameters: {'n_estimators': 534}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:17:59,376]\u001b[0m Trial 131 finished with value: 0.6554114861380448 and parameters: {'n_estimators': 399}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:18:37,105]\u001b[0m Trial 132 finished with value: 0.6550627385714648 and parameters: {'n_estimators': 410}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:19:12,659]\u001b[0m Trial 133 finished with value: 0.6558751210674583 and parameters: {'n_estimators': 391}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:19:58,798]\u001b[0m Trial 134 finished with value: 0.6546477619997451 and parameters: {'n_estimators': 510}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:20:38,631]\u001b[0m Trial 135 finished with value: 0.6548386502291247 and parameters: {'n_estimators': 436}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:21:13,078]\u001b[0m Trial 136 finished with value: 0.6563286252150513 and parameters: {'n_estimators': 373}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:21:50,926]\u001b[0m Trial 137 finished with value: 0.654954400893839 and parameters: {'n_estimators': 417}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:22:32,230]\u001b[0m Trial 138 finished with value: 0.6547461066890556 and parameters: {'n_estimators': 449}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:23:16,201]\u001b[0m Trial 139 finished with value: 0.6544280031980308 and parameters: {'n_estimators': 491}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:23:51,061]\u001b[0m Trial 140 finished with value: 0.6560996950677586 and parameters: {'n_estimators': 384}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:24:26,389]\u001b[0m Trial 141 finished with value: 0.6558040844745234 and parameters: {'n_estimators': 394}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:24:58,630]\u001b[0m Trial 142 finished with value: 0.65695547136257 and parameters: {'n_estimators': 350}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:25:35,385]\u001b[0m Trial 143 finished with value: 0.6549822281956137 and parameters: {'n_estimators': 409}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:26:14,143]\u001b[0m Trial 144 finished with value: 0.6548019242535423 and parameters: {'n_estimators': 430}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:26:47,306]\u001b[0m Trial 145 finished with value: 0.6563370425904154 and parameters: {'n_estimators': 369}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 16:27:16,808]\u001b[0m Trial 146 finished with value: 0.656844232047588 and parameters: {'n_estimators': 326}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:27:58,854]\u001b[0m Trial 147 finished with value: 0.6546733889457921 and parameters: {'n_estimators': 465}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:28:34,825]\u001b[0m Trial 148 finished with value: 0.6558751210674582 and parameters: {'n_estimators': 391}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:29:12,427]\u001b[0m Trial 149 finished with value: 0.6547519390461872 and parameters: {'n_estimators': 419}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6949\n",
      "\tBest params:\n",
      "\t\tn_estimators: 518\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.664931    0.654276    0.707219\n",
      "1                    TP   39.000000   42.000000   37.000000\n",
      "2                    TN  199.000000  198.000000  198.000000\n",
      "3                    FP    2.000000    3.000000    3.000000\n",
      "4                    FN   28.000000   25.000000   30.000000\n",
      "5              Accuracy    0.888060    0.895522    0.876866\n",
      "6             Precision    0.951220    0.933333    0.925000\n",
      "7           Sensitivity    0.582090    0.626866    0.552239\n",
      "8           Specificity    0.990000    0.985100    0.985100\n",
      "9              F1 score    0.722222    0.750000    0.691589\n",
      "10  F1 score (weighted)    0.877985    0.887972    0.865205\n",
      "11     F1 score (macro)    0.826064    0.841981    0.807333\n",
      "12    Balanced Accuracy    0.786070    0.805970    0.768657\n",
      "13                  MCC    0.688228    0.708901    0.652929\n",
      "14                  NPV    0.876700    0.887900    0.868400\n",
      "15              ROC_AUC    0.786070    0.805970    0.768657\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=24, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_rf_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_2_cat = np.where(((y_pred_rf_2 >= 2) | (y_pred_rf_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_rf_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 16:29:56,505]\u001b[0m Trial 150 finished with value: 0.6839809594012264 and parameters: {'n_estimators': 444}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:30:36,252]\u001b[0m Trial 151 finished with value: 0.6838622133638654 and parameters: {'n_estimators': 461}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:31:16,337]\u001b[0m Trial 152 finished with value: 0.6840735061420957 and parameters: {'n_estimators': 450}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:31:52,498]\u001b[0m Trial 153 finished with value: 0.6840920732135141 and parameters: {'n_estimators': 404}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:32:34,632]\u001b[0m Trial 154 finished with value: 0.6839411204266812 and parameters: {'n_estimators': 482}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:33:55,586]\u001b[0m Trial 155 finished with value: 0.6830783889650489 and parameters: {'n_estimators': 905}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:34:28,793]\u001b[0m Trial 156 finished with value: 0.6834748876272952 and parameters: {'n_estimators': 380}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:35:07,221]\u001b[0m Trial 157 finished with value: 0.6842372168394968 and parameters: {'n_estimators': 425}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:35:55,002]\u001b[0m Trial 158 finished with value: 0.6842181415560588 and parameters: {'n_estimators': 518}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:36:27,276]\u001b[0m Trial 159 finished with value: 0.6833106144164872 and parameters: {'n_estimators': 355}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:37:12,056]\u001b[0m Trial 160 finished with value: 0.6842562929330083 and parameters: {'n_estimators': 500}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:37:52,398]\u001b[0m Trial 161 finished with value: 0.6841559954030737 and parameters: {'n_estimators': 447}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:38:33,639]\u001b[0m Trial 162 finished with value: 0.6837818529173278 and parameters: {'n_estimators': 470}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:39:13,261]\u001b[0m Trial 163 finished with value: 0.6840503189990839 and parameters: {'n_estimators': 440}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:39:50,344]\u001b[0m Trial 164 finished with value: 0.683825584689035 and parameters: {'n_estimators': 412}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:40:25,601]\u001b[0m Trial 165 finished with value: 0.6837742858828892 and parameters: {'n_estimators': 390}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:41:05,973]\u001b[0m Trial 166 finished with value: 0.6839577074686611 and parameters: {'n_estimators': 459}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:41:43,967]\u001b[0m Trial 167 finished with value: 0.6842227148598365 and parameters: {'n_estimators': 429}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:42:17,111]\u001b[0m Trial 168 finished with value: 0.6834260546206659 and parameters: {'n_estimators': 367}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:43:00,313]\u001b[0m Trial 169 finished with value: 0.6839963509961767 and parameters: {'n_estimators': 481}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:43:37,074]\u001b[0m Trial 170 finished with value: 0.683975451797447 and parameters: {'n_estimators': 408}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:44:12,612]\u001b[0m Trial 171 finished with value: 0.6842131996043156 and parameters: {'n_estimators': 403}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:44:47,400]\u001b[0m Trial 172 finished with value: 0.6836086993492053 and parameters: {'n_estimators': 385}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:45:25,339]\u001b[0m Trial 173 finished with value: 0.6842489922590713 and parameters: {'n_estimators': 426}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:46:06,741]\u001b[0m Trial 174 finished with value: 0.6839276131248246 and parameters: {'n_estimators': 456}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:46:37,322]\u001b[0m Trial 175 finished with value: 0.6833687641372602 and parameters: {'n_estimators': 345}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:47:15,903]\u001b[0m Trial 176 finished with value: 0.6841396857936921 and parameters: {'n_estimators': 442}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:47:51,800]\u001b[0m Trial 177 finished with value: 0.683975451797447 and parameters: {'n_estimators': 408}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:48:26,070]\u001b[0m Trial 178 finished with value: 0.6834650908798238 and parameters: {'n_estimators': 379}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:49:03,054]\u001b[0m Trial 179 finished with value: 0.6839210080458801 and parameters: {'n_estimators': 421}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:49:37,791]\u001b[0m Trial 180 finished with value: 0.6837763856991431 and parameters: {'n_estimators': 393}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:50:10,402]\u001b[0m Trial 181 finished with value: 0.6834442292442834 and parameters: {'n_estimators': 362}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:50:46,325]\u001b[0m Trial 182 finished with value: 0.6841635267044472 and parameters: {'n_estimators': 398}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:51:18,548]\u001b[0m Trial 183 finished with value: 0.6834983394016002 and parameters: {'n_estimators': 377}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:51:57,496]\u001b[0m Trial 184 finished with value: 0.6842526669654372 and parameters: {'n_estimators': 433}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:52:33,777]\u001b[0m Trial 185 finished with value: 0.683934218670613 and parameters: {'n_estimators': 415}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:53:13,299]\u001b[0m Trial 186 finished with value: 0.6841149838591212 and parameters: {'n_estimators': 452}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:53:57,204]\u001b[0m Trial 187 finished with value: 0.6842611799778245 and parameters: {'n_estimators': 493}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:54:32,995]\u001b[0m Trial 188 finished with value: 0.6837742858828892 and parameters: {'n_estimators': 390}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:55:05,281]\u001b[0m Trial 189 finished with value: 0.6834168422265556 and parameters: {'n_estimators': 363}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:55:47,749]\u001b[0m Trial 190 finished with value: 0.6838171046249742 and parameters: {'n_estimators': 472}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:56:17,127]\u001b[0m Trial 191 finished with value: 0.6831507138632661 and parameters: {'n_estimators': 340}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:56:51,866]\u001b[0m Trial 192 finished with value: 0.6834748876272952 and parameters: {'n_estimators': 380}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:57:26,538]\u001b[0m Trial 193 finished with value: 0.6841152794465803 and parameters: {'n_estimators': 397}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:58:03,559]\u001b[0m Trial 194 finished with value: 0.6838887443175775 and parameters: {'n_estimators': 420}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:58:42,834]\u001b[0m Trial 195 finished with value: 0.6840503189990839 and parameters: {'n_estimators': 440}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 16:59:19,072]\u001b[0m Trial 196 finished with value: 0.6840223118844484 and parameters: {'n_estimators': 407}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 16:59:52,013]\u001b[0m Trial 197 finished with value: 0.6834650908798238 and parameters: {'n_estimators': 379}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:00:23,601]\u001b[0m Trial 198 finished with value: 0.6834284740913417 and parameters: {'n_estimators': 359}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:01:02,260]\u001b[0m Trial 199 finished with value: 0.6840158473382858 and parameters: {'n_estimators': 438}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6949\n",
      "\tBest params:\n",
      "\t\tn_estimators: 518\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.664931    0.654276    0.707219    0.683312\n",
      "1                    TP   39.000000   42.000000   37.000000   31.000000\n",
      "2                    TN  199.000000  198.000000  198.000000  198.000000\n",
      "3                    FP    2.000000    3.000000    3.000000    2.000000\n",
      "4                    FN   28.000000   25.000000   30.000000   37.000000\n",
      "5              Accuracy    0.888060    0.895522    0.876866    0.854478\n",
      "6             Precision    0.951220    0.933333    0.925000    0.939394\n",
      "7           Sensitivity    0.582090    0.626866    0.552239    0.455882\n",
      "8           Specificity    0.990000    0.985100    0.985100    0.990000\n",
      "9              F1 score    0.722222    0.750000    0.691589    0.613861\n",
      "10  F1 score (weighted)    0.877985    0.887972    0.865205    0.835118\n",
      "11     F1 score (macro)    0.826064    0.841981    0.807333    0.762103\n",
      "12    Balanced Accuracy    0.786070    0.805970    0.768657    0.722941\n",
      "13                  MCC    0.688228    0.708901    0.652929    0.590471\n",
      "14                  NPV    0.876700    0.887900    0.868400    0.842600\n",
      "15              ROC_AUC    0.786070    0.805970    0.768657    0.722941\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=24, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_rf_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_3_cat = np.where(((y_pred_rf_3 >= 2) | (y_pred_rf_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_rf_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 17:01:49,049]\u001b[0m Trial 200 finished with value: 0.6729420843354985 and parameters: {'n_estimators': 462}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:02:23,750]\u001b[0m Trial 201 finished with value: 0.6725244636346839 and parameters: {'n_estimators': 383}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:02:51,974]\u001b[0m Trial 202 finished with value: 0.6711874624662357 and parameters: {'n_estimators': 319}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:03:28,199]\u001b[0m Trial 203 finished with value: 0.672656884486646 and parameters: {'n_estimators': 405}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:04:06,026]\u001b[0m Trial 204 finished with value: 0.6725430901211531 and parameters: {'n_estimators': 420}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:04:40,441]\u001b[0m Trial 205 finished with value: 0.672247317985671 and parameters: {'n_estimators': 374}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:05:12,224]\u001b[0m Trial 206 finished with value: 0.6718858441100478 and parameters: {'n_estimators': 350}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:05:47,494]\u001b[0m Trial 207 finished with value: 0.6728232363744333 and parameters: {'n_estimators': 399}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:06:25,624]\u001b[0m Trial 208 finished with value: 0.6726089506436164 and parameters: {'n_estimators': 421}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:07:05,509]\u001b[0m Trial 209 finished with value: 0.6728634910499529 and parameters: {'n_estimators': 444}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:07:39,883]\u001b[0m Trial 210 finished with value: 0.6726249501971238 and parameters: {'n_estimators': 387}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:08:13,031]\u001b[0m Trial 211 finished with value: 0.6721411615053317 and parameters: {'n_estimators': 370}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:08:48,772]\u001b[0m Trial 212 finished with value: 0.6727118223581512 and parameters: {'n_estimators': 392}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:09:25,913]\u001b[0m Trial 213 finished with value: 0.6727487784628207 and parameters: {'n_estimators': 412}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:09:56,203]\u001b[0m Trial 214 finished with value: 0.6715484645445853 and parameters: {'n_estimators': 337}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:10:28,629]\u001b[0m Trial 215 finished with value: 0.6720255815839711 and parameters: {'n_estimators': 373}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:11:07,701]\u001b[0m Trial 216 finished with value: 0.6728720559184243 and parameters: {'n_estimators': 433}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:11:50,993]\u001b[0m Trial 217 finished with value: 0.6728862378614389 and parameters: {'n_estimators': 474}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:12:27,355]\u001b[0m Trial 218 finished with value: 0.6727914398460849 and parameters: {'n_estimators': 400}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:12:58,902]\u001b[0m Trial 219 finished with value: 0.6716766463410969 and parameters: {'n_estimators': 356}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:13:39,032]\u001b[0m Trial 220 finished with value: 0.6728476889777648 and parameters: {'n_estimators': 452}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:14:13,655]\u001b[0m Trial 221 finished with value: 0.6723112419518592 and parameters: {'n_estimators': 381}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:14:46,273]\u001b[0m Trial 222 finished with value: 0.6721025866350433 and parameters: {'n_estimators': 367}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:15:23,240]\u001b[0m Trial 223 finished with value: 0.6727299788187687 and parameters: {'n_estimators': 413}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:15:57,835]\u001b[0m Trial 224 finished with value: 0.6724905442637802 and parameters: {'n_estimators': 389}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:16:52,410]\u001b[0m Trial 225 finished with value: 0.6716898971362981 and parameters: {'n_estimators': 614}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:17:30,541]\u001b[0m Trial 226 finished with value: 0.6728682133353274 and parameters: {'n_estimators': 425}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:18:06,725]\u001b[0m Trial 227 finished with value: 0.6728735345898677 and parameters: {'n_estimators': 402}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:18:39,496]\u001b[0m Trial 228 finished with value: 0.6718496098258763 and parameters: {'n_estimators': 354}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:19:13,762]\u001b[0m Trial 229 finished with value: 0.6724315111506355 and parameters: {'n_estimators': 382}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:19:59,623]\u001b[0m Trial 230 finished with value: 0.6727072469422313 and parameters: {'n_estimators': 508}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:20:33,417]\u001b[0m Trial 231 finished with value: 0.6720424650197728 and parameters: {'n_estimators': 369}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:21:09,539]\u001b[0m Trial 232 finished with value: 0.6727451948089607 and parameters: {'n_estimators': 401}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:21:48,687]\u001b[0m Trial 233 finished with value: 0.6729522778552901 and parameters: {'n_estimators': 430}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:22:19,432]\u001b[0m Trial 234 finished with value: 0.6714746626741235 and parameters: {'n_estimators': 340}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:22:54,793]\u001b[0m Trial 235 finished with value: 0.6724315111506355 and parameters: {'n_estimators': 382}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:23:36,376]\u001b[0m Trial 236 finished with value: 0.6727283494689763 and parameters: {'n_estimators': 456}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:24:10,056]\u001b[0m Trial 237 finished with value: 0.6721622610161447 and parameters: {'n_estimators': 365}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:24:47,460]\u001b[0m Trial 238 finished with value: 0.6726576910792431 and parameters: {'n_estimators': 414}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:25:21,680]\u001b[0m Trial 239 finished with value: 0.6727118223581512 and parameters: {'n_estimators': 392}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:26:00,564]\u001b[0m Trial 240 finished with value: 0.672822163360171 and parameters: {'n_estimators': 428}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:26:31,991]\u001b[0m Trial 241 finished with value: 0.6717052001295362 and parameters: {'n_estimators': 346}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:27:05,132]\u001b[0m Trial 242 finished with value: 0.6720424650197727 and parameters: {'n_estimators': 369}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:27:39,993]\u001b[0m Trial 243 finished with value: 0.6725853685902186 and parameters: {'n_estimators': 390}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:28:08,838]\u001b[0m Trial 244 finished with value: 0.6718822663326601 and parameters: {'n_estimators': 328}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:28:40,005]\u001b[0m Trial 245 finished with value: 0.6719229814045145 and parameters: {'n_estimators': 352}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 17:29:16,878]\u001b[0m Trial 246 finished with value: 0.6726670378993282 and parameters: {'n_estimators': 406}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:29:50,557]\u001b[0m Trial 247 finished with value: 0.6722193147195421 and parameters: {'n_estimators': 375}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:30:29,890]\u001b[0m Trial 248 finished with value: 0.6728634910499529 and parameters: {'n_estimators': 444}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:31:08,346]\u001b[0m Trial 249 finished with value: 0.6727299788187687 and parameters: {'n_estimators': 413}. Best is trial 99 with value: 0.6948885362639611.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6949\n",
      "\tBest params:\n",
      "\t\tn_estimators: 518\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77894dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.664931    0.654276    0.707219    0.683312   \n",
      "1                    TP   39.000000   42.000000   37.000000   31.000000   \n",
      "2                    TN  199.000000  198.000000  198.000000  198.000000   \n",
      "3                    FP    2.000000    3.000000    3.000000    2.000000   \n",
      "4                    FN   28.000000   25.000000   30.000000   37.000000   \n",
      "5              Accuracy    0.888060    0.895522    0.876866    0.854478   \n",
      "6             Precision    0.951220    0.933333    0.925000    0.939394   \n",
      "7           Sensitivity    0.582090    0.626866    0.552239    0.455882   \n",
      "8           Specificity    0.990000    0.985100    0.985100    0.990000   \n",
      "9              F1 score    0.722222    0.750000    0.691589    0.613861   \n",
      "10  F1 score (weighted)    0.877985    0.887972    0.865205    0.835118   \n",
      "11     F1 score (macro)    0.826064    0.841981    0.807333    0.762103   \n",
      "12    Balanced Accuracy    0.786070    0.805970    0.768657    0.722941   \n",
      "13                  MCC    0.688228    0.708901    0.652929    0.590471   \n",
      "14                  NPV    0.876700    0.887900    0.868400    0.842600   \n",
      "15              ROC_AUC    0.786070    0.805970    0.768657    0.722941   \n",
      "\n",
      "          Set4  \n",
      "0     0.689703  \n",
      "1    29.000000  \n",
      "2   198.000000  \n",
      "3     5.000000  \n",
      "4    36.000000  \n",
      "5     0.847015  \n",
      "6     0.852941  \n",
      "7     0.446154  \n",
      "8     0.975400  \n",
      "9     0.585859  \n",
      "10    0.828489  \n",
      "11    0.746019  \n",
      "12    0.710762  \n",
      "13    0.542849  \n",
      "14    0.846200  \n",
      "15    0.710762  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=24, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_rf_4)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_4_cat = np.where(((y_pred_rf_4 >= 2) | (y_pred_rf_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_rf_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 17:31:56,398]\u001b[0m Trial 250 finished with value: 0.6950839699890223 and parameters: {'n_estimators': 479}. Best is trial 250 with value: 0.6950839699890223.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:32:40,906]\u001b[0m Trial 251 finished with value: 0.695086759084108 and parameters: {'n_estimators': 486}. Best is trial 251 with value: 0.695086759084108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:33:28,160]\u001b[0m Trial 252 finished with value: 0.694603663039066 and parameters: {'n_estimators': 522}. Best is trial 251 with value: 0.695086759084108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:34:16,499]\u001b[0m Trial 253 finished with value: 0.6946991741486925 and parameters: {'n_estimators': 528}. Best is trial 251 with value: 0.695086759084108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:35:04,374]\u001b[0m Trial 254 finished with value: 0.6950012706707919 and parameters: {'n_estimators': 535}. Best is trial 251 with value: 0.695086759084108.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:35:54,298]\u001b[0m Trial 255 finished with value: 0.6953123830920422 and parameters: {'n_estimators': 555}. Best is trial 255 with value: 0.6953123830920422.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:36:44,953]\u001b[0m Trial 256 finished with value: 0.6954058869041898 and parameters: {'n_estimators': 552}. Best is trial 256 with value: 0.6954058869041898.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:37:33,661]\u001b[0m Trial 257 finished with value: 0.6953123830920422 and parameters: {'n_estimators': 555}. Best is trial 256 with value: 0.6954058869041898.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:38:24,486]\u001b[0m Trial 258 finished with value: 0.6953064850618246 and parameters: {'n_estimators': 550}. Best is trial 256 with value: 0.6954058869041898.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:39:13,495]\u001b[0m Trial 259 finished with value: 0.6953014609834696 and parameters: {'n_estimators': 549}. Best is trial 256 with value: 0.6954058869041898.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:40:03,446]\u001b[0m Trial 260 finished with value: 0.6953064850618246 and parameters: {'n_estimators': 550}. Best is trial 256 with value: 0.6954058869041898.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:40:54,763]\u001b[0m Trial 261 finished with value: 0.6955698001591298 and parameters: {'n_estimators': 568}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:41:46,960]\u001b[0m Trial 262 finished with value: 0.6953068651554348 and parameters: {'n_estimators': 558}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:42:36,379]\u001b[0m Trial 263 finished with value: 0.6953883425290416 and parameters: {'n_estimators': 556}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:43:26,176]\u001b[0m Trial 264 finished with value: 0.6953883425290416 and parameters: {'n_estimators': 556}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:44:18,950]\u001b[0m Trial 265 finished with value: 0.6953748180884493 and parameters: {'n_estimators': 581}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:45:11,135]\u001b[0m Trial 266 finished with value: 0.6954433892579821 and parameters: {'n_estimators': 571}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:46:03,081]\u001b[0m Trial 267 finished with value: 0.6954686671628612 and parameters: {'n_estimators': 583}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:46:55,664]\u001b[0m Trial 268 finished with value: 0.6954686671628612 and parameters: {'n_estimators': 583}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:47:47,713]\u001b[0m Trial 269 finished with value: 0.6953596717778743 and parameters: {'n_estimators': 576}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:48:41,242]\u001b[0m Trial 270 finished with value: 0.6953720007532044 and parameters: {'n_estimators': 578}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:49:34,259]\u001b[0m Trial 271 finished with value: 0.6954686671628612 and parameters: {'n_estimators': 583}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:50:26,603]\u001b[0m Trial 272 finished with value: 0.6953354141278736 and parameters: {'n_estimators': 587}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:51:20,575]\u001b[0m Trial 273 finished with value: 0.6950470337636517 and parameters: {'n_estimators': 597}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:52:15,057]\u001b[0m Trial 274 finished with value: 0.6953742875741213 and parameters: {'n_estimators': 585}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:53:08,370]\u001b[0m Trial 275 finished with value: 0.6953742875741213 and parameters: {'n_estimators': 585}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:54:02,670]\u001b[0m Trial 276 finished with value: 0.6954686671628612 and parameters: {'n_estimators': 583}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:54:56,118]\u001b[0m Trial 277 finished with value: 0.6954686671628612 and parameters: {'n_estimators': 583}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:55:48,813]\u001b[0m Trial 278 finished with value: 0.6953467383929904 and parameters: {'n_estimators': 586}. Best is trial 261 with value: 0.6955698001591298.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:56:40,454]\u001b[0m Trial 279 finished with value: 0.6956158239305252 and parameters: {'n_estimators': 566}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:57:33,193]\u001b[0m Trial 280 finished with value: 0.6953795918614558 and parameters: {'n_estimators': 573}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:58:25,483]\u001b[0m Trial 281 finished with value: 0.6955339690827546 and parameters: {'n_estimators': 570}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 17:59:15,631]\u001b[0m Trial 282 finished with value: 0.6953604552836694 and parameters: {'n_estimators': 575}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:00:08,397]\u001b[0m Trial 283 finished with value: 0.6953604552836694 and parameters: {'n_estimators': 575}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:01:00,019]\u001b[0m Trial 284 finished with value: 0.6953984949133208 and parameters: {'n_estimators': 577}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:01:52,037]\u001b[0m Trial 285 finished with value: 0.6953984949133208 and parameters: {'n_estimators': 577}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:02:44,927]\u001b[0m Trial 286 finished with value: 0.6954062171794843 and parameters: {'n_estimators': 582}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:03:36,049]\u001b[0m Trial 287 finished with value: 0.6953720007532044 and parameters: {'n_estimators': 578}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:04:32,420]\u001b[0m Trial 288 finished with value: 0.6949604326414318 and parameters: {'n_estimators': 627}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:05:26,504]\u001b[0m Trial 289 finished with value: 0.6954021590301792 and parameters: {'n_estimators': 579}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:06:19,843]\u001b[0m Trial 290 finished with value: 0.6953748180884493 and parameters: {'n_estimators': 581}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:07:14,797]\u001b[0m Trial 291 finished with value: 0.6950931709660109 and parameters: {'n_estimators': 602}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:08:08,421]\u001b[0m Trial 292 finished with value: 0.6953604552836695 and parameters: {'n_estimators': 575}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:09:05,221]\u001b[0m Trial 293 finished with value: 0.6949928514606007 and parameters: {'n_estimators': 628}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:10:01,100]\u001b[0m Trial 294 finished with value: 0.6947870637217293 and parameters: {'n_estimators': 609}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:10:52,973]\u001b[0m Trial 295 finished with value: 0.6953596717778743 and parameters: {'n_estimators': 576}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 18:11:46,246]\u001b[0m Trial 296 finished with value: 0.6952341726943796 and parameters: {'n_estimators': 589}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:12:38,562]\u001b[0m Trial 297 finished with value: 0.6954156076280376 and parameters: {'n_estimators': 574}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:13:33,584]\u001b[0m Trial 298 finished with value: 0.6947870637217293 and parameters: {'n_estimators': 609}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:14:32,125]\u001b[0m Trial 299 finished with value: 0.6951814300642398 and parameters: {'n_estimators': 642}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6956\n",
      "\tBest params:\n",
      "\t\tn_estimators: 566\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.664931    0.654276    0.707219    0.683312   \n",
      "1                    TP   39.000000   42.000000   37.000000   31.000000   \n",
      "2                    TN  199.000000  198.000000  198.000000  198.000000   \n",
      "3                    FP    2.000000    3.000000    3.000000    2.000000   \n",
      "4                    FN   28.000000   25.000000   30.000000   37.000000   \n",
      "5              Accuracy    0.888060    0.895522    0.876866    0.854478   \n",
      "6             Precision    0.951220    0.933333    0.925000    0.939394   \n",
      "7           Sensitivity    0.582090    0.626866    0.552239    0.455882   \n",
      "8           Specificity    0.990000    0.985100    0.985100    0.990000   \n",
      "9              F1 score    0.722222    0.750000    0.691589    0.613861   \n",
      "10  F1 score (weighted)    0.877985    0.887972    0.865205    0.835118   \n",
      "11     F1 score (macro)    0.826064    0.841981    0.807333    0.762103   \n",
      "12    Balanced Accuracy    0.786070    0.805970    0.768657    0.722941   \n",
      "13                  MCC    0.688228    0.708901    0.652929    0.590471   \n",
      "14                  NPV    0.876700    0.887900    0.868400    0.842600   \n",
      "15              ROC_AUC    0.786070    0.805970    0.768657    0.722941   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.689703    0.667321  \n",
      "1    29.000000   30.000000  \n",
      "2   198.000000  200.000000  \n",
      "3     5.000000    1.000000  \n",
      "4    36.000000   37.000000  \n",
      "5     0.847015    0.858209  \n",
      "6     0.852941    0.967742  \n",
      "7     0.446154    0.447761  \n",
      "8     0.975400    0.995000  \n",
      "9     0.585859    0.612245  \n",
      "10    0.828489    0.837993  \n",
      "11    0.746019    0.762743  \n",
      "12    0.710762    0.721393  \n",
      "13    0.542849    0.599480  \n",
      "14    0.846200    0.843900  \n",
      "15    0.710762    0.721393  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=24, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_rf_5)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_5_cat = np.where(((y_pred_rf_5 >= 2) | (y_pred_rf_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_rf_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 18:15:29,438]\u001b[0m Trial 300 finished with value: 0.6700383979917128 and parameters: {'n_estimators': 563}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:16:23,699]\u001b[0m Trial 301 finished with value: 0.6699549982932622 and parameters: {'n_estimators': 591}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:17:17,727]\u001b[0m Trial 302 finished with value: 0.6698996482463544 and parameters: {'n_estimators': 573}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:18:10,603]\u001b[0m Trial 303 finished with value: 0.6700142850720143 and parameters: {'n_estimators': 596}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:19:02,000]\u001b[0m Trial 304 finished with value: 0.6699338526155916 and parameters: {'n_estimators': 567}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:20:01,308]\u001b[0m Trial 305 finished with value: 0.6698307550827463 and parameters: {'n_estimators': 654}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:20:57,333]\u001b[0m Trial 306 finished with value: 0.6697012758135098 and parameters: {'n_estimators': 612}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:21:51,844]\u001b[0m Trial 307 finished with value: 0.6700574824895352 and parameters: {'n_estimators': 586}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:22:45,255]\u001b[0m Trial 308 finished with value: 0.6700418853373721 and parameters: {'n_estimators': 566}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:23:41,350]\u001b[0m Trial 309 finished with value: 0.669778517233883 and parameters: {'n_estimators': 625}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:24:31,853]\u001b[0m Trial 310 finished with value: 0.6698069634387639 and parameters: {'n_estimators': 544}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:25:25,794]\u001b[0m Trial 311 finished with value: 0.6699030027298688 and parameters: {'n_estimators': 600}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:26:20,341]\u001b[0m Trial 312 finished with value: 0.6700438419649535 and parameters: {'n_estimators': 580}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:27:11,023]\u001b[0m Trial 313 finished with value: 0.6697746134714515 and parameters: {'n_estimators': 540}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:28:01,294]\u001b[0m Trial 314 finished with value: 0.6700383979917128 and parameters: {'n_estimators': 563}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:28:54,986]\u001b[0m Trial 315 finished with value: 0.6700036549723967 and parameters: {'n_estimators': 597}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:29:47,466]\u001b[0m Trial 316 finished with value: 0.6700418853373721 and parameters: {'n_estimators': 566}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:30:44,098]\u001b[0m Trial 317 finished with value: 0.6697963107588902 and parameters: {'n_estimators': 618}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:31:39,741]\u001b[0m Trial 318 finished with value: 0.6699992172639502 and parameters: {'n_estimators': 585}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:32:29,489]\u001b[0m Trial 319 finished with value: 0.669790908127216 and parameters: {'n_estimators': 543}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:33:22,511]\u001b[0m Trial 320 finished with value: 0.6700576292554632 and parameters: {'n_estimators': 576}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:34:17,303]\u001b[0m Trial 321 finished with value: 0.6699032292191361 and parameters: {'n_estimators': 602}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:35:08,799]\u001b[0m Trial 322 finished with value: 0.6701870357049643 and parameters: {'n_estimators': 559}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:35:57,866]\u001b[0m Trial 323 finished with value: 0.6697378369845073 and parameters: {'n_estimators': 534}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:36:51,250]\u001b[0m Trial 324 finished with value: 0.6700574824895353 and parameters: {'n_estimators': 586}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:37:41,320]\u001b[0m Trial 325 finished with value: 0.6700418853373721 and parameters: {'n_estimators': 566}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:38:38,939]\u001b[0m Trial 326 finished with value: 0.6700784683515716 and parameters: {'n_estimators': 635}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:39:33,405]\u001b[0m Trial 327 finished with value: 0.6697971327256859 and parameters: {'n_estimators': 607}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:40:24,192]\u001b[0m Trial 328 finished with value: 0.6696711956468613 and parameters: {'n_estimators': 549}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:41:17,999]\u001b[0m Trial 329 finished with value: 0.6699436582419127 and parameters: {'n_estimators': 582}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:42:10,305]\u001b[0m Trial 330 finished with value: 0.6700474745744277 and parameters: {'n_estimators': 562}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:43:04,753]\u001b[0m Trial 331 finished with value: 0.6699030027298688 and parameters: {'n_estimators': 600}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:43:56,774]\u001b[0m Trial 332 finished with value: 0.6700574824895353 and parameters: {'n_estimators': 586}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:44:47,418]\u001b[0m Trial 333 finished with value: 0.669886177431982 and parameters: {'n_estimators': 538}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:45:43,983]\u001b[0m Trial 334 finished with value: 0.6696042712264196 and parameters: {'n_estimators': 615}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:46:32,908]\u001b[0m Trial 335 finished with value: 0.669943257818613 and parameters: {'n_estimators': 569}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:47:21,507]\u001b[0m Trial 336 finished with value: 0.6697800029076683 and parameters: {'n_estimators': 551}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:48:14,010]\u001b[0m Trial 337 finished with value: 0.66992447152934 and parameters: {'n_estimators': 593}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:49:04,635]\u001b[0m Trial 338 finished with value: 0.6699971481148921 and parameters: {'n_estimators': 577}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:49:52,400]\u001b[0m Trial 339 finished with value: 0.6697484952937309 and parameters: {'n_estimators': 533}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:50:41,074]\u001b[0m Trial 340 finished with value: 0.670068653277475 and parameters: {'n_estimators': 561}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:51:36,817]\u001b[0m Trial 341 finished with value: 0.669957624867636 and parameters: {'n_estimators': 644}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:52:29,643]\u001b[0m Trial 342 finished with value: 0.6697699765077111 and parameters: {'n_estimators': 617}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:53:23,353]\u001b[0m Trial 343 finished with value: 0.6699711968666426 and parameters: {'n_estimators': 595}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:54:23,896]\u001b[0m Trial 344 finished with value: 0.6696187841669772 and parameters: {'n_estimators': 686}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:55:14,944]\u001b[0m Trial 345 finished with value: 0.6699971481148921 and parameters: {'n_estimators': 577}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 18:56:03,680]\u001b[0m Trial 346 finished with value: 0.6699199740344012 and parameters: {'n_estimators': 554}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:56:49,237]\u001b[0m Trial 347 finished with value: 0.6696392869379574 and parameters: {'n_estimators': 521}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:57:40,403]\u001b[0m Trial 348 finished with value: 0.6699971481148921 and parameters: {'n_estimators': 577}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 18:58:33,700]\u001b[0m Trial 349 finished with value: 0.6698047025879068 and parameters: {'n_estimators': 605}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6956\n",
      "\tBest params:\n",
      "\t\tn_estimators: 566\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.664931    0.654276    0.707219    0.683312   \n",
      "1                    TP   39.000000   42.000000   37.000000   31.000000   \n",
      "2                    TN  199.000000  198.000000  198.000000  198.000000   \n",
      "3                    FP    2.000000    3.000000    3.000000    2.000000   \n",
      "4                    FN   28.000000   25.000000   30.000000   37.000000   \n",
      "5              Accuracy    0.888060    0.895522    0.876866    0.854478   \n",
      "6             Precision    0.951220    0.933333    0.925000    0.939394   \n",
      "7           Sensitivity    0.582090    0.626866    0.552239    0.455882   \n",
      "8           Specificity    0.990000    0.985100    0.985100    0.990000   \n",
      "9              F1 score    0.722222    0.750000    0.691589    0.613861   \n",
      "10  F1 score (weighted)    0.877985    0.887972    0.865205    0.835118   \n",
      "11     F1 score (macro)    0.826064    0.841981    0.807333    0.762103   \n",
      "12    Balanced Accuracy    0.786070    0.805970    0.768657    0.722941   \n",
      "13                  MCC    0.688228    0.708901    0.652929    0.590471   \n",
      "14                  NPV    0.876700    0.887900    0.868400    0.842600   \n",
      "15              ROC_AUC    0.786070    0.805970    0.768657    0.722941   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.689703    0.667321    0.720339  \n",
      "1    29.000000   30.000000   36.000000  \n",
      "2   198.000000  200.000000  201.000000  \n",
      "3     5.000000    1.000000    1.000000  \n",
      "4    36.000000   37.000000   30.000000  \n",
      "5     0.847015    0.858209    0.884328  \n",
      "6     0.852941    0.967742    0.972973  \n",
      "7     0.446154    0.447761    0.545455  \n",
      "8     0.975400    0.995000    0.995000  \n",
      "9     0.585859    0.612245    0.699029  \n",
      "10    0.828489    0.837993    0.871918  \n",
      "11    0.746019    0.762743    0.813718  \n",
      "12    0.710762    0.721393    0.770252  \n",
      "13    0.542849    0.599480    0.675056  \n",
      "14    0.846200    0.843900    0.870100  \n",
      "15    0.710762    0.721393    0.770252  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=24, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_rf_6)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_6_cat = np.where(((y_pred_rf_6 >= 2) | (y_pred_rf_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_rf_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 18:59:27,010]\u001b[0m Trial 350 finished with value: 0.6736960367402046 and parameters: {'n_estimators': 547}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:00:18,711]\u001b[0m Trial 351 finished with value: 0.6731351309668563 and parameters: {'n_estimators': 590}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:00:28,106]\u001b[0m Trial 352 finished with value: 0.6693265311118878 and parameters: {'n_estimators': 106}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:01:17,048]\u001b[0m Trial 353 finished with value: 0.673656635724108 and parameters: {'n_estimators': 565}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:02:11,632]\u001b[0m Trial 354 finished with value: 0.6727031833830935 and parameters: {'n_estimators': 624}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:03:03,526]\u001b[0m Trial 355 finished with value: 0.6730081557925035 and parameters: {'n_estimators': 598}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:03:49,833]\u001b[0m Trial 356 finished with value: 0.6739275815714332 and parameters: {'n_estimators': 531}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:04:39,948]\u001b[0m Trial 357 finished with value: 0.6733450538706236 and parameters: {'n_estimators': 573}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:05:28,861]\u001b[0m Trial 358 finished with value: 0.6736913569538766 and parameters: {'n_estimators': 553}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:06:20,651]\u001b[0m Trial 359 finished with value: 0.6733309087053333 and parameters: {'n_estimators': 585}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:07:15,435]\u001b[0m Trial 360 finished with value: 0.6727499022927697 and parameters: {'n_estimators': 611}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:08:05,430]\u001b[0m Trial 361 finished with value: 0.6737002210380778 and parameters: {'n_estimators': 566}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:08:52,511]\u001b[0m Trial 362 finished with value: 0.6737895853582121 and parameters: {'n_estimators': 542}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:09:44,173]\u001b[0m Trial 363 finished with value: 0.6733419557423473 and parameters: {'n_estimators': 587}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:10:35,183]\u001b[0m Trial 364 finished with value: 0.673656635724108 and parameters: {'n_estimators': 565}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:11:27,289]\u001b[0m Trial 365 finished with value: 0.673070464164969 and parameters: {'n_estimators': 601}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:12:17,210]\u001b[0m Trial 366 finished with value: 0.673311039946707 and parameters: {'n_estimators': 580}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:13:11,183]\u001b[0m Trial 367 finished with value: 0.6726552530926418 and parameters: {'n_estimators': 621}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:13:59,582]\u001b[0m Trial 368 finished with value: 0.673727461683967 and parameters: {'n_estimators': 554}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:14:45,987]\u001b[0m Trial 369 finished with value: 0.673952848636938 and parameters: {'n_estimators': 525}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:15:39,459]\u001b[0m Trial 370 finished with value: 0.6730068074321738 and parameters: {'n_estimators': 597}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:16:30,049]\u001b[0m Trial 371 finished with value: 0.6732305002002498 and parameters: {'n_estimators': 576}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:17:19,164]\u001b[0m Trial 372 finished with value: 0.6736960367402046 and parameters: {'n_estimators': 547}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:18:08,580]\u001b[0m Trial 373 finished with value: 0.6737002210380778 and parameters: {'n_estimators': 566}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:19:01,243]\u001b[0m Trial 374 finished with value: 0.6728628113282368 and parameters: {'n_estimators': 608}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:19:56,568]\u001b[0m Trial 375 finished with value: 0.6727093489376071 and parameters: {'n_estimators': 635}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:20:47,691]\u001b[0m Trial 376 finished with value: 0.6733309087053333 and parameters: {'n_estimators': 585}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:21:35,048]\u001b[0m Trial 377 finished with value: 0.6739811194531027 and parameters: {'n_estimators': 532}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:22:25,609]\u001b[0m Trial 378 finished with value: 0.673736995109683 and parameters: {'n_estimators': 559}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:23:18,205]\u001b[0m Trial 379 finished with value: 0.6731053064121058 and parameters: {'n_estimators': 592}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:24:08,461]\u001b[0m Trial 380 finished with value: 0.6733728998435329 and parameters: {'n_estimators': 571}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:24:54,680]\u001b[0m Trial 381 finished with value: 0.6737645135463831 and parameters: {'n_estimators': 540}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:25:48,037]\u001b[0m Trial 382 finished with value: 0.6728148571877952 and parameters: {'n_estimators': 609}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:26:32,612]\u001b[0m Trial 383 finished with value: 0.6738536463442488 and parameters: {'n_estimators': 510}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:27:21,356]\u001b[0m Trial 384 finished with value: 0.6737250239714364 and parameters: {'n_estimators': 556}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:28:11,331]\u001b[0m Trial 385 finished with value: 0.6733083746003482 and parameters: {'n_estimators': 583}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:29:05,873]\u001b[0m Trial 386 finished with value: 0.6726083737554801 and parameters: {'n_estimators': 623}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:29:55,537]\u001b[0m Trial 387 finished with value: 0.6733728998435329 and parameters: {'n_estimators': 571}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:30:48,210]\u001b[0m Trial 388 finished with value: 0.6730068074321738 and parameters: {'n_estimators': 597}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:31:36,382]\u001b[0m Trial 389 finished with value: 0.6737164549948964 and parameters: {'n_estimators': 548}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:32:25,603]\u001b[0m Trial 390 finished with value: 0.6733668260497705 and parameters: {'n_estimators': 579}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:33:18,999]\u001b[0m Trial 391 finished with value: 0.673018666073453 and parameters: {'n_estimators': 602}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:34:07,702]\u001b[0m Trial 392 finished with value: 0.6736341433807101 and parameters: {'n_estimators': 561}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:34:52,815]\u001b[0m Trial 393 finished with value: 0.6738725317908221 and parameters: {'n_estimators': 523}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:35:50,487]\u001b[0m Trial 394 finished with value: 0.6727639689897497 and parameters: {'n_estimators': 664}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:36:42,154]\u001b[0m Trial 395 finished with value: 0.6733485732442054 and parameters: {'n_estimators': 588}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 19:38:09,500]\u001b[0m Trial 396 finished with value: 0.6724327500777278 and parameters: {'n_estimators': 981}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:38:56,892]\u001b[0m Trial 397 finished with value: 0.6737542864312832 and parameters: {'n_estimators': 541}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:39:50,267]\u001b[0m Trial 398 finished with value: 0.6727031833830935 and parameters: {'n_estimators': 624}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:40:39,300]\u001b[0m Trial 399 finished with value: 0.6734024949457362 and parameters: {'n_estimators': 570}. Best is trial 279 with value: 0.6956158239305252.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6956\n",
      "\tBest params:\n",
      "\t\tn_estimators: 566\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.664931    0.654276    0.707219    0.683312   \n",
      "1                    TP   39.000000   42.000000   37.000000   31.000000   \n",
      "2                    TN  199.000000  198.000000  198.000000  198.000000   \n",
      "3                    FP    2.000000    3.000000    3.000000    2.000000   \n",
      "4                    FN   28.000000   25.000000   30.000000   37.000000   \n",
      "5              Accuracy    0.888060    0.895522    0.876866    0.854478   \n",
      "6             Precision    0.951220    0.933333    0.925000    0.939394   \n",
      "7           Sensitivity    0.582090    0.626866    0.552239    0.455882   \n",
      "8           Specificity    0.990000    0.985100    0.985100    0.990000   \n",
      "9              F1 score    0.722222    0.750000    0.691589    0.613861   \n",
      "10  F1 score (weighted)    0.877985    0.887972    0.865205    0.835118   \n",
      "11     F1 score (macro)    0.826064    0.841981    0.807333    0.762103   \n",
      "12    Balanced Accuracy    0.786070    0.805970    0.768657    0.722941   \n",
      "13                  MCC    0.688228    0.708901    0.652929    0.590471   \n",
      "14                  NPV    0.876700    0.887900    0.868400    0.842600   \n",
      "15              ROC_AUC    0.786070    0.805970    0.768657    0.722941   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.689703    0.667321    0.720339    0.703685  \n",
      "1    29.000000   30.000000   36.000000   34.000000  \n",
      "2   198.000000  200.000000  201.000000  202.000000  \n",
      "3     5.000000    1.000000    1.000000    1.000000  \n",
      "4    36.000000   37.000000   30.000000   31.000000  \n",
      "5     0.847015    0.858209    0.884328    0.880597  \n",
      "6     0.852941    0.967742    0.972973    0.971429  \n",
      "7     0.446154    0.447761    0.545455    0.523077  \n",
      "8     0.975400    0.995000    0.995000    0.995100  \n",
      "9     0.585859    0.612245    0.699029    0.680000  \n",
      "10    0.828489    0.837993    0.871918    0.866794  \n",
      "11    0.746019    0.762743    0.813718    0.803303  \n",
      "12    0.710762    0.721393    0.770252    0.759075  \n",
      "13    0.542849    0.599480    0.675056    0.659096  \n",
      "14    0.846200    0.843900    0.870100    0.867000  \n",
      "15    0.710762    0.721393    0.770252    0.759075  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=24, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_rf_7)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_7_cat = np.where(((y_pred_rf_7 >= 2) | (y_pred_rf_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_rf_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 19:41:34,920]\u001b[0m Trial 400 finished with value: 0.7021456655522872 and parameters: {'n_estimators': 607}. Best is trial 400 with value: 0.7021456655522872.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:42:30,452]\u001b[0m Trial 401 finished with value: 0.7023193133646356 and parameters: {'n_estimators': 651}. Best is trial 401 with value: 0.7023193133646356.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:43:28,414]\u001b[0m Trial 402 finished with value: 0.7020562274873252 and parameters: {'n_estimators': 682}. Best is trial 401 with value: 0.7023193133646356.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:44:24,188]\u001b[0m Trial 403 finished with value: 0.7022793266498633 and parameters: {'n_estimators': 647}. Best is trial 401 with value: 0.7023193133646356.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:45:22,395]\u001b[0m Trial 404 finished with value: 0.7022253639292433 and parameters: {'n_estimators': 675}. Best is trial 401 with value: 0.7023193133646356.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:46:22,768]\u001b[0m Trial 405 finished with value: 0.7021003152862122 and parameters: {'n_estimators': 699}. Best is trial 401 with value: 0.7023193133646356.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:47:21,845]\u001b[0m Trial 406 finished with value: 0.7021147801427866 and parameters: {'n_estimators': 685}. Best is trial 401 with value: 0.7023193133646356.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:48:21,222]\u001b[0m Trial 407 finished with value: 0.7020140073460134 and parameters: {'n_estimators': 695}. Best is trial 401 with value: 0.7023193133646356.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:49:21,263]\u001b[0m Trial 408 finished with value: 0.7020562274873252 and parameters: {'n_estimators': 682}. Best is trial 401 with value: 0.7023193133646356.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:50:22,027]\u001b[0m Trial 409 finished with value: 0.7020540797460553 and parameters: {'n_estimators': 700}. Best is trial 401 with value: 0.7023193133646356.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:51:25,074]\u001b[0m Trial 410 finished with value: 0.7020896999419901 and parameters: {'n_estimators': 715}. Best is trial 401 with value: 0.7023193133646356.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:52:26,270]\u001b[0m Trial 411 finished with value: 0.7018420104901788 and parameters: {'n_estimators': 709}. Best is trial 401 with value: 0.7023193133646356.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:53:28,645]\u001b[0m Trial 412 finished with value: 0.7023748109989213 and parameters: {'n_estimators': 724}. Best is trial 412 with value: 0.7023748109989213.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:54:29,057]\u001b[0m Trial 413 finished with value: 0.7020236595908892 and parameters: {'n_estimators': 701}. Best is trial 412 with value: 0.7023748109989213.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:55:29,822]\u001b[0m Trial 414 finished with value: 0.7020540797460552 and parameters: {'n_estimators': 700}. Best is trial 412 with value: 0.7023748109989213.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:56:30,258]\u001b[0m Trial 415 finished with value: 0.7020519210488823 and parameters: {'n_estimators': 714}. Best is trial 412 with value: 0.7023748109989213.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:57:33,929]\u001b[0m Trial 416 finished with value: 0.7023502002001707 and parameters: {'n_estimators': 723}. Best is trial 412 with value: 0.7023748109989213.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:58:36,367]\u001b[0m Trial 417 finished with value: 0.7022280291053608 and parameters: {'n_estimators': 720}. Best is trial 412 with value: 0.7023748109989213.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 19:59:38,590]\u001b[0m Trial 418 finished with value: 0.7023521000588413 and parameters: {'n_estimators': 725}. Best is trial 412 with value: 0.7023748109989213.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:00:41,176]\u001b[0m Trial 419 finished with value: 0.7023748109989213 and parameters: {'n_estimators': 724}. Best is trial 412 with value: 0.7023748109989213.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:01:44,035]\u001b[0m Trial 420 finished with value: 0.7024272708173924 and parameters: {'n_estimators': 731}. Best is trial 420 with value: 0.7024272708173924.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:02:46,318]\u001b[0m Trial 421 finished with value: 0.7023232203278764 and parameters: {'n_estimators': 728}. Best is trial 420 with value: 0.7024272708173924.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:03:49,856]\u001b[0m Trial 422 finished with value: 0.7025044145214077 and parameters: {'n_estimators': 733}. Best is trial 422 with value: 0.7025044145214077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:04:54,484]\u001b[0m Trial 423 finished with value: 0.7024678708820201 and parameters: {'n_estimators': 732}. Best is trial 422 with value: 0.7025044145214077.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:05:58,310]\u001b[0m Trial 424 finished with value: 0.702589913275511 and parameters: {'n_estimators': 738}. Best is trial 424 with value: 0.702589913275511.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:07:02,174]\u001b[0m Trial 425 finished with value: 0.7025771061580596 and parameters: {'n_estimators': 741}. Best is trial 424 with value: 0.702589913275511.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:08:05,816]\u001b[0m Trial 426 finished with value: 0.7027905371426894 and parameters: {'n_estimators': 756}. Best is trial 426 with value: 0.7027905371426894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:09:11,297]\u001b[0m Trial 427 finished with value: 0.7027854231748093 and parameters: {'n_estimators': 749}. Best is trial 426 with value: 0.7027905371426894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:10:16,779]\u001b[0m Trial 428 finished with value: 0.7027600737085817 and parameters: {'n_estimators': 744}. Best is trial 426 with value: 0.7027905371426894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:11:22,278]\u001b[0m Trial 429 finished with value: 0.7027600737085817 and parameters: {'n_estimators': 744}. Best is trial 426 with value: 0.7027905371426894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:12:27,949]\u001b[0m Trial 430 finished with value: 0.7027600737085817 and parameters: {'n_estimators': 744}. Best is trial 426 with value: 0.7027905371426894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:13:32,938]\u001b[0m Trial 431 finished with value: 0.7027600737085817 and parameters: {'n_estimators': 744}. Best is trial 426 with value: 0.7027905371426894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:14:36,937]\u001b[0m Trial 432 finished with value: 0.7027600737085817 and parameters: {'n_estimators': 744}. Best is trial 426 with value: 0.7027905371426894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:15:41,169]\u001b[0m Trial 433 finished with value: 0.7027556487247221 and parameters: {'n_estimators': 754}. Best is trial 426 with value: 0.7027905371426894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:16:44,059]\u001b[0m Trial 434 finished with value: 0.7027672081404662 and parameters: {'n_estimators': 746}. Best is trial 426 with value: 0.7027905371426894.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:17:47,162]\u001b[0m Trial 435 finished with value: 0.7028326820283688 and parameters: {'n_estimators': 747}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:18:51,494]\u001b[0m Trial 436 finished with value: 0.7027556487247221 and parameters: {'n_estimators': 754}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:19:55,887]\u001b[0m Trial 437 finished with value: 0.702754841985463 and parameters: {'n_estimators': 753}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:21:00,436]\u001b[0m Trial 438 finished with value: 0.7027825804030765 and parameters: {'n_estimators': 748}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:22:06,042]\u001b[0m Trial 439 finished with value: 0.7027633294603666 and parameters: {'n_estimators': 768}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:23:12,279]\u001b[0m Trial 440 finished with value: 0.7026318356231508 and parameters: {'n_estimators': 770}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:24:17,390]\u001b[0m Trial 441 finished with value: 0.7027066149392505 and parameters: {'n_estimators': 771}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:25:23,524]\u001b[0m Trial 442 finished with value: 0.7026318356231508 and parameters: {'n_estimators': 770}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:26:29,015]\u001b[0m Trial 443 finished with value: 0.7027393840165381 and parameters: {'n_estimators': 776}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:27:34,359]\u001b[0m Trial 444 finished with value: 0.7026604523885178 and parameters: {'n_estimators': 775}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:28:40,265]\u001b[0m Trial 445 finished with value: 0.7027171293522267 and parameters: {'n_estimators': 774}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 20:29:44,961]\u001b[0m Trial 446 finished with value: 0.7027393840165382 and parameters: {'n_estimators': 776}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:30:36,490]\u001b[0m Trial 447 finished with value: 0.7027073535591278 and parameters: {'n_estimators': 773}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:31:41,956]\u001b[0m Trial 448 finished with value: 0.7027171293522267 and parameters: {'n_estimators': 774}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:32:48,275]\u001b[0m Trial 449 finished with value: 0.7027393840165382 and parameters: {'n_estimators': 776}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7028\n",
      "\tBest params:\n",
      "\t\tn_estimators: 747\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.664931    0.654276    0.707219    0.683312   \n",
      "1                    TP   39.000000   42.000000   37.000000   31.000000   \n",
      "2                    TN  199.000000  198.000000  198.000000  198.000000   \n",
      "3                    FP    2.000000    3.000000    3.000000    2.000000   \n",
      "4                    FN   28.000000   25.000000   30.000000   37.000000   \n",
      "5              Accuracy    0.888060    0.895522    0.876866    0.854478   \n",
      "6             Precision    0.951220    0.933333    0.925000    0.939394   \n",
      "7           Sensitivity    0.582090    0.626866    0.552239    0.455882   \n",
      "8           Specificity    0.990000    0.985100    0.985100    0.990000   \n",
      "9              F1 score    0.722222    0.750000    0.691589    0.613861   \n",
      "10  F1 score (weighted)    0.877985    0.887972    0.865205    0.835118   \n",
      "11     F1 score (macro)    0.826064    0.841981    0.807333    0.762103   \n",
      "12    Balanced Accuracy    0.786070    0.805970    0.768657    0.722941   \n",
      "13                  MCC    0.688228    0.708901    0.652929    0.590471   \n",
      "14                  NPV    0.876700    0.887900    0.868400    0.842600   \n",
      "15              ROC_AUC    0.786070    0.805970    0.768657    0.722941   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.689703    0.667321    0.720339    0.703685    0.538120  \n",
      "1    29.000000   30.000000   36.000000   34.000000   31.000000  \n",
      "2   198.000000  200.000000  201.000000  202.000000  199.000000  \n",
      "3     5.000000    1.000000    1.000000    1.000000    3.000000  \n",
      "4    36.000000   37.000000   30.000000   31.000000   35.000000  \n",
      "5     0.847015    0.858209    0.884328    0.880597    0.858209  \n",
      "6     0.852941    0.967742    0.972973    0.971429    0.911765  \n",
      "7     0.446154    0.447761    0.545455    0.523077    0.469697  \n",
      "8     0.975400    0.995000    0.995000    0.995100    0.985100  \n",
      "9     0.585859    0.612245    0.699029    0.680000    0.620000  \n",
      "10    0.828489    0.837993    0.871918    0.866794    0.840726  \n",
      "11    0.746019    0.762743    0.813718    0.803303    0.766422  \n",
      "12    0.710762    0.721393    0.770252    0.759075    0.727423  \n",
      "13    0.542849    0.599480    0.675056    0.659096    0.588795  \n",
      "14    0.846200    0.843900    0.870100    0.867000    0.850400  \n",
      "15    0.710762    0.721393    0.770252    0.759075    0.727423  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=24, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_rf_8)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_8_cat = np.where(((y_pred_rf_8 >= 2) | (y_pred_rf_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_rf_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 20:34:00,376]\u001b[0m Trial 450 finished with value: 0.6811702481050721 and parameters: {'n_estimators': 772}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:35:05,761]\u001b[0m Trial 451 finished with value: 0.6810245127795131 and parameters: {'n_estimators': 768}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:36:14,335]\u001b[0m Trial 452 finished with value: 0.6812857723319949 and parameters: {'n_estimators': 811}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:37:22,555]\u001b[0m Trial 453 finished with value: 0.6811506365944963 and parameters: {'n_estimators': 800}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:38:27,280]\u001b[0m Trial 454 finished with value: 0.6808376807550021 and parameters: {'n_estimators': 762}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:39:34,940]\u001b[0m Trial 455 finished with value: 0.6812189465360688 and parameters: {'n_estimators': 795}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:40:41,785]\u001b[0m Trial 456 finished with value: 0.6811830033456177 and parameters: {'n_estimators': 782}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:41:46,349]\u001b[0m Trial 457 finished with value: 0.6805907387753667 and parameters: {'n_estimators': 756}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:42:56,164]\u001b[0m Trial 458 finished with value: 0.6811418164106546 and parameters: {'n_estimators': 818}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:44:00,273]\u001b[0m Trial 459 finished with value: 0.6805358892195044 and parameters: {'n_estimators': 750}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:45:07,459]\u001b[0m Trial 460 finished with value: 0.6811415497900192 and parameters: {'n_estimators': 783}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:46:19,137]\u001b[0m Trial 461 finished with value: 0.6809283004974598 and parameters: {'n_estimators': 837}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:47:22,854]\u001b[0m Trial 462 finished with value: 0.680579838174224 and parameters: {'n_estimators': 754}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:48:29,283]\u001b[0m Trial 463 finished with value: 0.6811415497900193 and parameters: {'n_estimators': 783}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:49:34,391]\u001b[0m Trial 464 finished with value: 0.6805907387753667 and parameters: {'n_estimators': 756}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:50:40,745]\u001b[0m Trial 465 finished with value: 0.6812032114193955 and parameters: {'n_estimators': 773}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:51:44,854]\u001b[0m Trial 466 finished with value: 0.6805634440878418 and parameters: {'n_estimators': 748}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:52:51,832]\u001b[0m Trial 467 finished with value: 0.6813562926775555 and parameters: {'n_estimators': 792}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:53:56,537]\u001b[0m Trial 468 finished with value: 0.68104403270855 and parameters: {'n_estimators': 769}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:55:00,768]\u001b[0m Trial 469 finished with value: 0.6805575909861795 and parameters: {'n_estimators': 747}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:56:06,898]\u001b[0m Trial 470 finished with value: 0.6809558765034316 and parameters: {'n_estimators': 766}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:57:18,359]\u001b[0m Trial 471 finished with value: 0.6810068372564295 and parameters: {'n_estimators': 829}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:58:27,714]\u001b[0m Trial 472 finished with value: 0.6812042814334249 and parameters: {'n_estimators': 799}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 20:59:30,089]\u001b[0m Trial 473 finished with value: 0.6805239080056966 and parameters: {'n_estimators': 746}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:00:36,560]\u001b[0m Trial 474 finished with value: 0.681204380603383 and parameters: {'n_estimators': 779}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:01:41,434]\u001b[0m Trial 475 finished with value: 0.6809044620901463 and parameters: {'n_estimators': 764}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:02:44,649]\u001b[0m Trial 476 finished with value: 0.6806223450755422 and parameters: {'n_estimators': 742}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:03:52,044]\u001b[0m Trial 477 finished with value: 0.6812253459580749 and parameters: {'n_estimators': 784}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:04:56,276]\u001b[0m Trial 478 finished with value: 0.6806821608399984 and parameters: {'n_estimators': 759}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:06:04,912]\u001b[0m Trial 479 finished with value: 0.6813125619617714 and parameters: {'n_estimators': 810}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:07:12,271]\u001b[0m Trial 480 finished with value: 0.6813087310560444 and parameters: {'n_estimators': 790}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:08:14,953]\u001b[0m Trial 481 finished with value: 0.6806708975254352 and parameters: {'n_estimators': 740}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:09:20,340]\u001b[0m Trial 482 finished with value: 0.6811690220975228 and parameters: {'n_estimators': 771}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:10:24,156]\u001b[0m Trial 483 finished with value: 0.6805720895369058 and parameters: {'n_estimators': 755}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:11:29,866]\u001b[0m Trial 484 finished with value: 0.6812112233377041 and parameters: {'n_estimators': 777}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:12:32,774]\u001b[0m Trial 485 finished with value: 0.6805253503137705 and parameters: {'n_estimators': 745}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:13:39,482]\u001b[0m Trial 486 finished with value: 0.6812122388361932 and parameters: {'n_estimators': 798}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:14:44,671]\u001b[0m Trial 487 finished with value: 0.6808040458969418 and parameters: {'n_estimators': 761}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:15:46,997]\u001b[0m Trial 488 finished with value: 0.680722177673145 and parameters: {'n_estimators': 737}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:16:58,020]\u001b[0m Trial 489 finished with value: 0.6809522145159763 and parameters: {'n_estimators': 833}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:18:12,549]\u001b[0m Trial 490 finished with value: 0.6812747472052163 and parameters: {'n_estimators': 876}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:19:18,390]\u001b[0m Trial 491 finished with value: 0.6811415497900191 and parameters: {'n_estimators': 783}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:20:22,473]\u001b[0m Trial 492 finished with value: 0.6808376807550021 and parameters: {'n_estimators': 762}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:21:32,407]\u001b[0m Trial 493 finished with value: 0.6811628211456154 and parameters: {'n_estimators': 817}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:22:14,771]\u001b[0m Trial 494 finished with value: 0.6806708975254352 and parameters: {'n_estimators': 740}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:23:19,597]\u001b[0m Trial 495 finished with value: 0.6812151599655064 and parameters: {'n_estimators': 794}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 21:24:04,607]\u001b[0m Trial 496 finished with value: 0.6810245127795131 and parameters: {'n_estimators': 768}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:24:49,660]\u001b[0m Trial 497 finished with value: 0.6805831938455257 and parameters: {'n_estimators': 753}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:25:33,033]\u001b[0m Trial 498 finished with value: 0.6812314457802358 and parameters: {'n_estimators': 778}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:26:15,367]\u001b[0m Trial 499 finished with value: 0.6806931881879486 and parameters: {'n_estimators': 739}. Best is trial 435 with value: 0.7028326820283688.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7028\n",
      "\tBest params:\n",
      "\t\tn_estimators: 747\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.664931    0.654276    0.707219    0.683312   \n",
      "1                    TP   39.000000   42.000000   37.000000   31.000000   \n",
      "2                    TN  199.000000  198.000000  198.000000  198.000000   \n",
      "3                    FP    2.000000    3.000000    3.000000    2.000000   \n",
      "4                    FN   28.000000   25.000000   30.000000   37.000000   \n",
      "5              Accuracy    0.888060    0.895522    0.876866    0.854478   \n",
      "6             Precision    0.951220    0.933333    0.925000    0.939394   \n",
      "7           Sensitivity    0.582090    0.626866    0.552239    0.455882   \n",
      "8           Specificity    0.990000    0.985100    0.985100    0.990000   \n",
      "9              F1 score    0.722222    0.750000    0.691589    0.613861   \n",
      "10  F1 score (weighted)    0.877985    0.887972    0.865205    0.835118   \n",
      "11     F1 score (macro)    0.826064    0.841981    0.807333    0.762103   \n",
      "12    Balanced Accuracy    0.786070    0.805970    0.768657    0.722941   \n",
      "13                  MCC    0.688228    0.708901    0.652929    0.590471   \n",
      "14                  NPV    0.876700    0.887900    0.868400    0.842600   \n",
      "15              ROC_AUC    0.786070    0.805970    0.768657    0.722941   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.689703    0.667321    0.720339    0.703685    0.538120    0.751779  \n",
      "1    29.000000   30.000000   36.000000   34.000000   31.000000   38.000000  \n",
      "2   198.000000  200.000000  201.000000  202.000000  199.000000  200.000000  \n",
      "3     5.000000    1.000000    1.000000    1.000000    3.000000    2.000000  \n",
      "4    36.000000   37.000000   30.000000   31.000000   35.000000   28.000000  \n",
      "5     0.847015    0.858209    0.884328    0.880597    0.858209    0.888060  \n",
      "6     0.852941    0.967742    0.972973    0.971429    0.911765    0.950000  \n",
      "7     0.446154    0.447761    0.545455    0.523077    0.469697    0.575758  \n",
      "8     0.975400    0.995000    0.995000    0.995100    0.985100    0.990100  \n",
      "9     0.585859    0.612245    0.699029    0.680000    0.620000    0.716981  \n",
      "10    0.828489    0.837993    0.871918    0.866794    0.840726    0.877715  \n",
      "11    0.746019    0.762743    0.813718    0.803303    0.766422    0.823607  \n",
      "12    0.710762    0.721393    0.770252    0.759075    0.727423    0.782928  \n",
      "13    0.542849    0.599480    0.675056    0.659096    0.588795    0.684158  \n",
      "14    0.846200    0.843900    0.870100    0.867000    0.850400    0.877200  \n",
      "15    0.710762    0.721393    0.770252    0.759075    0.727423    0.782928  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=24, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_rf_9)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_9_cat = np.where(((y_pred_rf_9 >= 2) | (y_pred_rf_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_rf_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7028\n",
      "\tBest params:\n",
      "\t\tn_estimators: 747\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.708635</td>\n",
       "      <td>0.047445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>2.875181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>98.900000</td>\n",
       "      <td>1.286684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.264911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3.018462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.876046</td>\n",
       "      <td>0.029402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.917263</td>\n",
       "      <td>0.068111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.551270</td>\n",
       "      <td>0.087999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.984090</td>\n",
       "      <td>0.012556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.686719</td>\n",
       "      <td>0.083188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.863802</td>\n",
       "      <td>0.034223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.804712</td>\n",
       "      <td>0.050439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.767680</td>\n",
       "      <td>0.048416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.648020</td>\n",
       "      <td>0.092097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.868710</td>\n",
       "      <td>0.024228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.767680</td>\n",
       "      <td>0.048416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.708635     0.047445\n",
       "1                    TP        18.400000     2.875181\n",
       "2                    TN        98.900000     1.286684\n",
       "3                    FP         1.600000     1.264911\n",
       "4                    FN        15.000000     3.018462\n",
       "5              Accuracy         0.876046     0.029402\n",
       "6             Precision         0.917263     0.068111\n",
       "7           Sensitivity         0.551270     0.087999\n",
       "8           Specificity         0.984090     0.012556\n",
       "9              F1 score         0.686719     0.083188\n",
       "10  F1 score (weighted)         0.863802     0.034223\n",
       "11     F1 score (macro)         0.804712     0.050439\n",
       "12    Balanced Accuracy         0.767680     0.048416\n",
       "13                  MCC         0.648020     0.092097\n",
       "14                  NPV         0.868710     0.024228\n",
       "15              ROC_AUC         0.767680     0.048416"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.664931</td>\n",
       "      <td>0.654276</td>\n",
       "      <td>0.707219</td>\n",
       "      <td>0.683312</td>\n",
       "      <td>0.689703</td>\n",
       "      <td>0.667321</td>\n",
       "      <td>0.720339</td>\n",
       "      <td>0.703685</td>\n",
       "      <td>0.538120</td>\n",
       "      <td>0.751779</td>\n",
       "      <td>0.678069</td>\n",
       "      <td>0.057085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>34.700000</td>\n",
       "      <td>4.372896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>199.300000</td>\n",
       "      <td>1.418136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1.251666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>31.700000</td>\n",
       "      <td>4.270051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.854478</td>\n",
       "      <td>0.847015</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.017054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.937580</td>\n",
       "      <td>0.035991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.446154</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.469697</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.522498</td>\n",
       "      <td>0.064405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.975400</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.995100</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.990100</td>\n",
       "      <td>0.988590</td>\n",
       "      <td>0.006158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.691589</td>\n",
       "      <td>0.613861</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.699029</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.669179</td>\n",
       "      <td>0.056572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.877985</td>\n",
       "      <td>0.887972</td>\n",
       "      <td>0.865205</td>\n",
       "      <td>0.835118</td>\n",
       "      <td>0.828489</td>\n",
       "      <td>0.837993</td>\n",
       "      <td>0.871918</td>\n",
       "      <td>0.866794</td>\n",
       "      <td>0.840726</td>\n",
       "      <td>0.877715</td>\n",
       "      <td>0.858992</td>\n",
       "      <td>0.021325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.826064</td>\n",
       "      <td>0.841981</td>\n",
       "      <td>0.807333</td>\n",
       "      <td>0.762103</td>\n",
       "      <td>0.746019</td>\n",
       "      <td>0.762743</td>\n",
       "      <td>0.813718</td>\n",
       "      <td>0.803303</td>\n",
       "      <td>0.766422</td>\n",
       "      <td>0.823607</td>\n",
       "      <td>0.795329</td>\n",
       "      <td>0.033165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.786070</td>\n",
       "      <td>0.805970</td>\n",
       "      <td>0.768657</td>\n",
       "      <td>0.722941</td>\n",
       "      <td>0.710762</td>\n",
       "      <td>0.721393</td>\n",
       "      <td>0.770252</td>\n",
       "      <td>0.759075</td>\n",
       "      <td>0.727423</td>\n",
       "      <td>0.782928</td>\n",
       "      <td>0.755547</td>\n",
       "      <td>0.032746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.688228</td>\n",
       "      <td>0.708901</td>\n",
       "      <td>0.652929</td>\n",
       "      <td>0.590471</td>\n",
       "      <td>0.542849</td>\n",
       "      <td>0.599480</td>\n",
       "      <td>0.675056</td>\n",
       "      <td>0.659096</td>\n",
       "      <td>0.588795</td>\n",
       "      <td>0.684158</td>\n",
       "      <td>0.638996</td>\n",
       "      <td>0.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.876700</td>\n",
       "      <td>0.887900</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>0.842600</td>\n",
       "      <td>0.846200</td>\n",
       "      <td>0.843900</td>\n",
       "      <td>0.870100</td>\n",
       "      <td>0.867000</td>\n",
       "      <td>0.850400</td>\n",
       "      <td>0.877200</td>\n",
       "      <td>0.863040</td>\n",
       "      <td>0.016079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.786070</td>\n",
       "      <td>0.805970</td>\n",
       "      <td>0.768657</td>\n",
       "      <td>0.722941</td>\n",
       "      <td>0.710762</td>\n",
       "      <td>0.721393</td>\n",
       "      <td>0.770252</td>\n",
       "      <td>0.759075</td>\n",
       "      <td>0.727423</td>\n",
       "      <td>0.782928</td>\n",
       "      <td>0.755547</td>\n",
       "      <td>0.032746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.664931    0.654276    0.707219    0.683312   \n",
       "1                    TP   39.000000   42.000000   37.000000   31.000000   \n",
       "2                    TN  199.000000  198.000000  198.000000  198.000000   \n",
       "3                    FP    2.000000    3.000000    3.000000    2.000000   \n",
       "4                    FN   28.000000   25.000000   30.000000   37.000000   \n",
       "5              Accuracy    0.888060    0.895522    0.876866    0.854478   \n",
       "6             Precision    0.951220    0.933333    0.925000    0.939394   \n",
       "7           Sensitivity    0.582090    0.626866    0.552239    0.455882   \n",
       "8           Specificity    0.990000    0.985100    0.985100    0.990000   \n",
       "9              F1 score    0.722222    0.750000    0.691589    0.613861   \n",
       "10  F1 score (weighted)    0.877985    0.887972    0.865205    0.835118   \n",
       "11     F1 score (macro)    0.826064    0.841981    0.807333    0.762103   \n",
       "12    Balanced Accuracy    0.786070    0.805970    0.768657    0.722941   \n",
       "13                  MCC    0.688228    0.708901    0.652929    0.590471   \n",
       "14                  NPV    0.876700    0.887900    0.868400    0.842600   \n",
       "15              ROC_AUC    0.786070    0.805970    0.768657    0.722941   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.689703    0.667321    0.720339    0.703685    0.538120    0.751779   \n",
       "1    29.000000   30.000000   36.000000   34.000000   31.000000   38.000000   \n",
       "2   198.000000  200.000000  201.000000  202.000000  199.000000  200.000000   \n",
       "3     5.000000    1.000000    1.000000    1.000000    3.000000    2.000000   \n",
       "4    36.000000   37.000000   30.000000   31.000000   35.000000   28.000000   \n",
       "5     0.847015    0.858209    0.884328    0.880597    0.858209    0.888060   \n",
       "6     0.852941    0.967742    0.972973    0.971429    0.911765    0.950000   \n",
       "7     0.446154    0.447761    0.545455    0.523077    0.469697    0.575758   \n",
       "8     0.975400    0.995000    0.995000    0.995100    0.985100    0.990100   \n",
       "9     0.585859    0.612245    0.699029    0.680000    0.620000    0.716981   \n",
       "10    0.828489    0.837993    0.871918    0.866794    0.840726    0.877715   \n",
       "11    0.746019    0.762743    0.813718    0.803303    0.766422    0.823607   \n",
       "12    0.710762    0.721393    0.770252    0.759075    0.727423    0.782928   \n",
       "13    0.542849    0.599480    0.675056    0.659096    0.588795    0.684158   \n",
       "14    0.846200    0.843900    0.870100    0.867000    0.850400    0.877200   \n",
       "15    0.710762    0.721393    0.770252    0.759075    0.727423    0.782928   \n",
       "\n",
       "           ave       std  \n",
       "0     0.678069  0.057085  \n",
       "1    34.700000  4.372896  \n",
       "2   199.300000  1.418136  \n",
       "3     2.300000  1.251666  \n",
       "4    31.700000  4.270051  \n",
       "5     0.873134  0.017054  \n",
       "6     0.937580  0.035991  \n",
       "7     0.522498  0.064405  \n",
       "8     0.988590  0.006158  \n",
       "9     0.669179  0.056572  \n",
       "10    0.858992  0.021325  \n",
       "11    0.795329  0.033165  \n",
       "12    0.755547  0.032746  \n",
       "13    0.638996  0.054700  \n",
       "14    0.863040  0.016079  \n",
       "15    0.755547  0.032746  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test['std'] = mat_met_rf_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_rf0</th>\n",
       "      <th>y_pred_rf1</th>\n",
       "      <th>y_pred_rf2</th>\n",
       "      <th>y_pred_rf3</th>\n",
       "      <th>y_pred_rf4</th>\n",
       "      <th>y_pred_rf_ave</th>\n",
       "      <th>y_pred_rf_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4635479</td>\n",
       "      <td>0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.628772</td>\n",
       "      <td>0.690295</td>\n",
       "      <td>0.696064</td>\n",
       "      <td>0.655359</td>\n",
       "      <td>0.329563</td>\n",
       "      <td>0.570009</td>\n",
       "      <td>0.142252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4299417</td>\n",
       "      <td>1</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.712597</td>\n",
       "      <td>0.800683</td>\n",
       "      <td>0.823726</td>\n",
       "      <td>0.830335</td>\n",
       "      <td>0.845462</td>\n",
       "      <td>0.832134</td>\n",
       "      <td>0.078945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4225331</td>\n",
       "      <td>2</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.357470</td>\n",
       "      <td>1.424734</td>\n",
       "      <td>1.239197</td>\n",
       "      <td>1.730763</td>\n",
       "      <td>1.214605</td>\n",
       "      <td>1.301128</td>\n",
       "      <td>0.266771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL1094710</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.303046</td>\n",
       "      <td>0.310684</td>\n",
       "      <td>0.283235</td>\n",
       "      <td>0.284170</td>\n",
       "      <td>0.216488</td>\n",
       "      <td>0.307937</td>\n",
       "      <td>0.070417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3287256</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.004404</td>\n",
       "      <td>-0.000254</td>\n",
       "      <td>0.047349</td>\n",
       "      <td>-0.045783</td>\n",
       "      <td>0.028086</td>\n",
       "      <td>-0.049366</td>\n",
       "      <td>0.128727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>CHEMBL3769491</td>\n",
       "      <td>1334</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.371218</td>\n",
       "      <td>-0.343873</td>\n",
       "      <td>-0.436314</td>\n",
       "      <td>-0.340303</td>\n",
       "      <td>-0.465999</td>\n",
       "      <td>-0.211285</td>\n",
       "      <td>0.405717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>CHEMBL482095</td>\n",
       "      <td>1335</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.800808</td>\n",
       "      <td>0.678701</td>\n",
       "      <td>0.860607</td>\n",
       "      <td>0.733083</td>\n",
       "      <td>0.731552</td>\n",
       "      <td>0.707459</td>\n",
       "      <td>0.132770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>CHEMBL4095596</td>\n",
       "      <td>1336</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>0.676354</td>\n",
       "      <td>0.501348</td>\n",
       "      <td>0.998049</td>\n",
       "      <td>1.025174</td>\n",
       "      <td>0.823321</td>\n",
       "      <td>0.190099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>CHEMBL4072925</td>\n",
       "      <td>1337</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.784996</td>\n",
       "      <td>0.984292</td>\n",
       "      <td>0.768792</td>\n",
       "      <td>0.606066</td>\n",
       "      <td>0.751627</td>\n",
       "      <td>0.752629</td>\n",
       "      <td>0.125199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>CHEMBL3774993</td>\n",
       "      <td>1338</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.623989</td>\n",
       "      <td>0.476171</td>\n",
       "      <td>0.608286</td>\n",
       "      <td>0.593936</td>\n",
       "      <td>0.595863</td>\n",
       "      <td>0.644708</td>\n",
       "      <td>0.153264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1339 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_rf0  y_pred_rf1  \\\n",
       "0         CHEMBL4635479            0     0.42    0.628772    0.690295   \n",
       "1         CHEMBL4299417            1     0.98    0.712597    0.800683   \n",
       "2         CHEMBL4225331            2     0.84    1.357470    1.424734   \n",
       "3         CHEMBL1094710            3     0.45    0.303046    0.310684   \n",
       "4         CHEMBL3287256            4    -0.33    0.004404   -0.000254   \n",
       "...                 ...          ...      ...         ...         ...   \n",
       "1334      CHEMBL3769491         1334     0.69   -0.371218   -0.343873   \n",
       "1335       CHEMBL482095         1335     0.44    0.800808    0.678701   \n",
       "1336      CHEMBL4095596         1336     0.78    0.959000    0.676354   \n",
       "1337      CHEMBL4072925         1337     0.62    0.784996    0.984292   \n",
       "1338      CHEMBL3774993         1338     0.97    0.623989    0.476171   \n",
       "\n",
       "      y_pred_rf2  y_pred_rf3  y_pred_rf4  y_pred_rf_ave  y_pred_rf_std  \n",
       "0       0.696064    0.655359    0.329563       0.570009       0.142252  \n",
       "1       0.823726    0.830335    0.845462       0.832134       0.078945  \n",
       "2       1.239197    1.730763    1.214605       1.301128       0.266771  \n",
       "3       0.283235    0.284170    0.216488       0.307937       0.070417  \n",
       "4       0.047349   -0.045783    0.028086      -0.049366       0.128727  \n",
       "...          ...         ...         ...            ...            ...  \n",
       "1334   -0.436314   -0.340303   -0.465999      -0.211285       0.405717  \n",
       "1335    0.860607    0.733083    0.731552       0.707459       0.132770  \n",
       "1336    0.501348    0.998049    1.025174       0.823321       0.190099  \n",
       "1337    0.768792    0.606066    0.751627       0.752629       0.125199  \n",
       "1338    0.608286    0.593936    0.595863       0.644708       0.153264  \n",
       "\n",
       "[1339 rows x 10 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=24, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_rf_cat = np.where(((y_pred_optimized_rf >= 2) | (y_pred_optimized_rf <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_rf_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_rf))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "rf_run0 = data_rf[['y_test_idx0', 'y_test0', 'y_pred_rf0']]\n",
    "rf_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "rf_run0.reset_index(inplace=True, drop=True)\n",
    "rf_run1 = data_rf[['y_test_idx1', 'y_test1', 'y_pred_rf1']]\n",
    "rf_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "rf_run1.reset_index(inplace=True, drop=True)\n",
    "rf_run2 = data_rf[['y_test_idx2', 'y_test2', 'y_pred_rf2']]\n",
    "rf_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "rf_run2.reset_index(inplace=True, drop=True)\n",
    "rf_run3 = data_rf[['y_test_idx3', 'y_test3', 'y_pred_rf3']]\n",
    "rf_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "rf_run3.reset_index(inplace=True, drop=True)\n",
    "rf_run4 = data_rf[['y_test_idx4', 'y_test4', 'y_pred_rf4']]\n",
    "rf_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "rf_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "rf_5preds = pd.concat([chembl_id, rf_run0, rf_run1, rf_run2, rf_run3, rf_run4], axis=1)\n",
    "rf_5preds = rf_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_rf0', 'y_pred_rf1', 'y_pred_rf2', 'y_pred_rf3', 'y_pred_rf4']]\n",
    "rf_5preds['y_pred_rf_ave'] = rf_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "rf_5preds['y_pred_rf_std'] = rf_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "rf_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47203ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.702696</td>\n",
       "      <td>0.058795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.872295</td>\n",
       "      <td>0.024554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.925559</td>\n",
       "      <td>0.050158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.530566</td>\n",
       "      <td>0.089777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.985866</td>\n",
       "      <td>0.010089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.670588</td>\n",
       "      <td>0.079204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.858332</td>\n",
       "      <td>0.030358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.795643</td>\n",
       "      <td>0.046590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.758215</td>\n",
       "      <td>0.045759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.637104</td>\n",
       "      <td>0.077303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.863902</td>\n",
       "      <td>0.022977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.758215</td>\n",
       "      <td>0.045759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.702696     0.058795\n",
       "1              Accuracy         0.872295     0.024554\n",
       "2             Precision         0.925559     0.050158\n",
       "3           Sensitivity         0.530566     0.089777\n",
       "4           Specificity         0.985866     0.010089\n",
       "5              F1 score         0.670588     0.079204\n",
       "6   F1 score (weighted)         0.858332     0.030358\n",
       "7      F1 score (macro)         0.795643     0.046590\n",
       "8     Balanced Accuracy         0.758215     0.045759\n",
       "9                   MCC         0.637104     0.077303\n",
       "10                  NPV         0.863902     0.022977\n",
       "11              ROC_AUC         0.758215     0.045759"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_optimized_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d030d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_5preds.to_csv(output/'rf_5test_CV_result.csv')\n",
    "mat_met_optimized_rf.to_csv(output/'mat_met_rf_opt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfc78124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (RF)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model r2_score 0.6905 with a standard deviation of 0.0610\n",
      "RF optimized model r2_score 0.6941 with a standard deviation of 0.0577\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"RF baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_rf_noSemiSel.joblib']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf_reg, output/\"rf_reg.joblib\")\n",
    "#joblib.dump(optimized_rf, output/\"optimized_rf.joblib\") # fitted to whole training set with last random_state selected\n",
    "joblib.dump(optimizedCV_rf, output/\"optimizedCV_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.699810     0.055438\n",
      "1                    TP        19.000000     3.265986\n",
      "2                    TN        98.600000     1.429841\n",
      "3                    FP         1.900000     1.449138\n",
      "4                    FN        14.400000     3.272783\n",
      "5              Accuracy         0.878274     0.029624\n",
      "6             Precision         0.908215     0.072478\n",
      "7           Sensitivity         0.568912     0.096940\n",
      "8           Specificity         0.981110     0.014399\n",
      "9              F1 score         0.696489     0.084504\n",
      "10  F1 score (weighted)         0.867119     0.034401\n",
      "11     F1 score (macro)         0.810150     0.051097\n",
      "12    Balanced Accuracy         0.775010     0.051353\n",
      "13                  MCC         0.654569     0.093029\n",
      "14                  NPV         0.873130     0.026212\n",
      "15              ROC_AUC         0.775010     0.051353\n",
      "CPU times: user 3min 8s, sys: 141 ms, total: 3min 8s\n",
      "Wall time: 9.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_reg = lgbm.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=24,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_reg.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"rmse\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_reg.predict(X_test) \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=24,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=24,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "         # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 21:32:19,628]\u001b[0m A new study created in memory with name: lgbmRegressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:32:25,217]\u001b[0m Trial 0 finished with value: 0.655678962422538 and parameters: {'n_estimators': 295, 'learning_rate': 0.1438258242479013, 'max_depth': 9, 'max_bin': 273, 'num_leaves': 497}. Best is trial 0 with value: 0.655678962422538.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:32:31,493]\u001b[0m Trial 1 finished with value: 0.6488532665754951 and parameters: {'n_estimators': 423, 'learning_rate': 0.17801848518046437, 'max_depth': 8, 'max_bin': 240, 'num_leaves': 643}. Best is trial 0 with value: 0.655678962422538.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:32:40,645]\u001b[0m Trial 2 finished with value: 0.6546774641544368 and parameters: {'n_estimators': 825, 'learning_rate': 0.09965433396080157, 'max_depth': 6, 'max_bin': 216, 'num_leaves': 620}. Best is trial 0 with value: 0.655678962422538.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:32:49,295]\u001b[0m Trial 3 finished with value: 0.6586694733670738 and parameters: {'n_estimators': 678, 'learning_rate': 0.1999705798781728, 'max_depth': 10, 'max_bin': 234, 'num_leaves': 335}. Best is trial 3 with value: 0.6586694733670738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:32:58,766]\u001b[0m Trial 4 finished with value: 0.653542208915358 and parameters: {'n_estimators': 829, 'learning_rate': 0.04965134160280446, 'max_depth': 5, 'max_bin': 285, 'num_leaves': 319}. Best is trial 3 with value: 0.6586694733670738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:33:07,141]\u001b[0m Trial 5 finished with value: 0.6534662920792561 and parameters: {'n_estimators': 200, 'learning_rate': 0.1002983628242034, 'max_depth': 5, 'max_bin': 205, 'num_leaves': 134}. Best is trial 3 with value: 0.6586694733670738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:33:11,523]\u001b[0m Trial 6 finished with value: 0.6538349063483173 and parameters: {'n_estimators': 806, 'learning_rate': 0.11215644782013841, 'max_depth': 6, 'max_bin': 293, 'num_leaves': 552}. Best is trial 3 with value: 0.6586694733670738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:33:31,042]\u001b[0m Trial 7 finished with value: 0.636694650267742 and parameters: {'n_estimators': 216, 'learning_rate': 0.012777175693055, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 717}. Best is trial 3 with value: 0.6586694733670738.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:33:40,624]\u001b[0m Trial 8 finished with value: 0.6672365375142049 and parameters: {'n_estimators': 858, 'learning_rate': 0.0818668909764244, 'max_depth': 10, 'max_bin': 182, 'num_leaves': 137}. Best is trial 8 with value: 0.6672365375142049.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:33:45,878]\u001b[0m Trial 9 finished with value: 0.6597970499541411 and parameters: {'n_estimators': 425, 'learning_rate': 0.174426069086747, 'max_depth': 8, 'max_bin': 258, 'num_leaves': 195}. Best is trial 8 with value: 0.6672365375142049.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:33:54,497]\u001b[0m Trial 10 finished with value: 0.6407390382046481 and parameters: {'n_estimators': 604, 'learning_rate': 0.0438408033292696, 'max_depth': 3, 'max_bin': 166, 'num_leaves': 45}. Best is trial 8 with value: 0.6672365375142049.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:34:04,157]\u001b[0m Trial 11 finished with value: 0.6593721240924879 and parameters: {'n_estimators': 454, 'learning_rate': 0.13928894204873876, 'max_depth': 11, 'max_bin': 190, 'num_leaves': 202}. Best is trial 8 with value: 0.6672365375142049.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:34:10,294]\u001b[0m Trial 12 finished with value: 0.6588149483506311 and parameters: {'n_estimators': 603, 'learning_rate': 0.06503106526278, 'max_depth': 8, 'max_bin': 260, 'num_leaves': 224}. Best is trial 8 with value: 0.6672365375142049.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:34:14,843]\u001b[0m Trial 13 finished with value: 0.6428823849125169 and parameters: {'n_estimators': 54, 'learning_rate': 0.15520824672631764, 'max_depth': 10, 'max_bin': 188, 'num_leaves': 66}. Best is trial 8 with value: 0.6672365375142049.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:34:24,335]\u001b[0m Trial 14 finished with value: 0.6657159676049278 and parameters: {'n_estimators': 338, 'learning_rate': 0.08201338405372958, 'max_depth': 9, 'max_bin': 251, 'num_leaves': 234}. Best is trial 8 with value: 0.6672365375142049.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:34:34,647]\u001b[0m Trial 15 finished with value: 0.666281552473436 and parameters: {'n_estimators': 326, 'learning_rate': 0.08182732343251345, 'max_depth': 12, 'max_bin': 179, 'num_leaves': 414}. Best is trial 8 with value: 0.6672365375142049.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:35:04,734]\u001b[0m Trial 16 finished with value: 0.6700734864265175 and parameters: {'n_estimators': 585, 'learning_rate': 0.011710914270944422, 'max_depth': 12, 'max_bin': 179, 'num_leaves': 438}. Best is trial 16 with value: 0.6700734864265175.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:35:47,104]\u001b[0m Trial 17 finished with value: 0.08781961060550461 and parameters: {'n_estimators': 722, 'learning_rate': 0.00012915118796028024, 'max_depth': 11, 'max_bin': 151, 'num_leaves': 437}. Best is trial 16 with value: 0.6700734864265175.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:36:04,937]\u001b[0m Trial 18 finished with value: 0.6705452964035333 and parameters: {'n_estimators': 895, 'learning_rate': 0.02738541016797763, 'max_depth': 11, 'max_bin': 205, 'num_leaves': 322}. Best is trial 18 with value: 0.6705452964035333.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:36:22,207]\u001b[0m Trial 19 finished with value: 0.672453021935852 and parameters: {'n_estimators': 570, 'learning_rate': 0.031755090816953616, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 318}. Best is trial 19 with value: 0.672453021935852.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:36:37,155]\u001b[0m Trial 20 finished with value: 0.6753749598595974 and parameters: {'n_estimators': 531, 'learning_rate': 0.03639254208354144, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 343}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:36:58,844]\u001b[0m Trial 21 finished with value: 0.6735402187162369 and parameters: {'n_estimators': 526, 'learning_rate': 0.03393866178975444, 'max_depth': 11, 'max_bin': 208, 'num_leaves': 313}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:37:10,927]\u001b[0m Trial 22 finished with value: 0.6752737141362303 and parameters: {'n_estimators': 531, 'learning_rate': 0.04043132493473144, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 276}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:37:21,683]\u001b[0m Trial 23 finished with value: 0.6642673505620793 and parameters: {'n_estimators': 518, 'learning_rate': 0.05499208690684873, 'max_depth': 9, 'max_bin': 224, 'num_leaves': 277}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:37:42,397]\u001b[0m Trial 24 finished with value: 0.672042286984697 and parameters: {'n_estimators': 683, 'learning_rate': 0.028155849614168994, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 385}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:37:54,066]\u001b[0m Trial 25 finished with value: 0.666540534048043 and parameters: {'n_estimators': 491, 'learning_rate': 0.06696210507623837, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 369}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:38:05,791]\u001b[0m Trial 26 finished with value: 0.6707608205598052 and parameters: {'n_estimators': 534, 'learning_rate': 0.04195205971575772, 'max_depth': 11, 'max_bin': 238, 'num_leaves': 493}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:38:25,649]\u001b[0m Trial 27 finished with value: 0.6489273932535173 and parameters: {'n_estimators': 393, 'learning_rate': 0.014439144491185756, 'max_depth': 7, 'max_bin': 215, 'num_leaves': 278}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:38:36,066]\u001b[0m Trial 28 finished with value: 0.6690373136644894 and parameters: {'n_estimators': 737, 'learning_rate': 0.06105775860806255, 'max_depth': 9, 'max_bin': 231, 'num_leaves': 489}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:38:55,398]\u001b[0m Trial 29 finished with value: 0.6694878977360144 and parameters: {'n_estimators': 645, 'learning_rate': 0.03614645849780604, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 140}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:39:10,712]\u001b[0m Trial 30 finished with value: 0.3925129182040242 and parameters: {'n_estimators': 232, 'learning_rate': 0.002699064523411017, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 266}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 21:39:34,610]\u001b[0m Trial 31 finished with value: 0.673905636070432 and parameters: {'n_estimators': 549, 'learning_rate': 0.0247734809129036, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 353}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:39:50,523]\u001b[0m Trial 32 finished with value: 0.669054436560865 and parameters: {'n_estimators': 537, 'learning_rate': 0.02227420850987863, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 369}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:40:05,290]\u001b[0m Trial 33 finished with value: 0.6711180028071427 and parameters: {'n_estimators': 471, 'learning_rate': 0.04783120599460376, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 451}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:40:16,593]\u001b[0m Trial 34 finished with value: 0.6677440271000874 and parameters: {'n_estimators': 375, 'learning_rate': 0.07289809987918949, 'max_depth': 10, 'max_bin': 213, 'num_leaves': 578}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:40:36,892]\u001b[0m Trial 35 finished with value: 0.6708634536070861 and parameters: {'n_estimators': 656, 'learning_rate': 0.020456916673224454, 'max_depth': 11, 'max_bin': 200, 'num_leaves': 348}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:40:51,151]\u001b[0m Trial 36 finished with value: 0.6715814122169229 and parameters: {'n_estimators': 442, 'learning_rate': 0.03501826775995603, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 272}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:41:01,453]\u001b[0m Trial 37 finished with value: 0.6571500482306097 and parameters: {'n_estimators': 509, 'learning_rate': 0.11653332224253762, 'max_depth': 10, 'max_bin': 245, 'num_leaves': 180}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:41:05,031]\u001b[0m Trial 38 finished with value: 0.6438538746338557 and parameters: {'n_estimators': 571, 'learning_rate': 0.05489567310230997, 'max_depth': 3, 'max_bin': 218, 'num_leaves': 308}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:41:13,527]\u001b[0m Trial 39 finished with value: 0.6568233829512297 and parameters: {'n_estimators': 727, 'learning_rate': 0.09913092419810304, 'max_depth': 7, 'max_bin': 270, 'num_leaves': 553}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:41:46,370]\u001b[0m Trial 40 finished with value: 0.6566978113942497 and parameters: {'n_estimators': 628, 'learning_rate': 0.009278503241103086, 'max_depth': 9, 'max_bin': 173, 'num_leaves': 402}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:41:59,729]\u001b[0m Trial 41 finished with value: 0.6719079695921228 and parameters: {'n_estimators': 545, 'learning_rate': 0.03192220648834949, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 310}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:42:18,782]\u001b[0m Trial 42 finished with value: 0.6740683243109031 and parameters: {'n_estimators': 563, 'learning_rate': 0.04137970974281006, 'max_depth': 11, 'max_bin': 206, 'num_leaves': 352}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:42:34,603]\u001b[0m Trial 43 finished with value: 0.6739187758533924 and parameters: {'n_estimators': 479, 'learning_rate': 0.04256092765951904, 'max_depth': 11, 'max_bin': 191, 'num_leaves': 360}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:42:46,500]\u001b[0m Trial 44 finished with value: 0.6683931695367255 and parameters: {'n_estimators': 395, 'learning_rate': 0.04671169503554172, 'max_depth': 11, 'max_bin': 190, 'num_leaves': 353}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:42:56,903]\u001b[0m Trial 45 finished with value: 0.652466260757596 and parameters: {'n_estimators': 478, 'learning_rate': 0.07639433389169764, 'max_depth': 4, 'max_bin': 198, 'num_leaves': 740}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:43:08,778]\u001b[0m Trial 46 finished with value: 0.666437490491196 and parameters: {'n_estimators': 433, 'learning_rate': 0.056417776188627436, 'max_depth': 10, 'max_bin': 185, 'num_leaves': 239}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:43:27,358]\u001b[0m Trial 47 finished with value: 0.6691473993749407 and parameters: {'n_estimators': 683, 'learning_rate': 0.020276600220273343, 'max_depth': 9, 'max_bin': 167, 'num_leaves': 413}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:43:35,868]\u001b[0m Trial 48 finished with value: 0.6598587134226664 and parameters: {'n_estimators': 770, 'learning_rate': 0.19822866025843622, 'max_depth': 12, 'max_bin': 299, 'num_leaves': 474}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:43:47,486]\u001b[0m Trial 49 finished with value: 0.6687378976685807 and parameters: {'n_estimators': 294, 'learning_rate': 0.041100695228842334, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 169}. Best is trial 20 with value: 0.6753749598595974.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6754\n",
      "\tBest params:\n",
      "\t\tn_estimators: 531\n",
      "\t\tlearning_rate: 0.03639254208354144\n",
      "\t\tmax_depth: 11\n",
      "\t\tmax_bin: 208\n",
      "\t\tnum_leaves: 343\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.686631\n",
      "1                    TP   42.000000\n",
      "2                    TN  199.000000\n",
      "3                    FP    2.000000\n",
      "4                    FN   25.000000\n",
      "5              Accuracy    0.899254\n",
      "6             Precision    0.954545\n",
      "7           Sensitivity    0.626866\n",
      "8           Specificity    0.990000\n",
      "9              F1 score    0.756757\n",
      "10  F1 score (weighted)    0.891542\n",
      "11     F1 score (macro)    0.846614\n",
      "12    Balanced Accuracy    0.808458\n",
      "13                  MCC    0.721125\n",
      "14                  NPV    0.888400\n",
      "15              ROC_AUC    0.808458\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_lgbm_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_0_cat = np.where(((y_pred_lgbm_0 >= 2) | (y_pred_lgbm_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 21:44:04,854]\u001b[0m Trial 50 finished with value: 0.6820300961495451 and parameters: {'n_estimators': 604, 'learning_rate': 0.09406271344650544, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 653}. Best is trial 50 with value: 0.6820300961495451.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:44:15,699]\u001b[0m Trial 51 finished with value: 0.666215768220333 and parameters: {'n_estimators': 615, 'learning_rate': 0.1324088634236345, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 694}. Best is trial 50 with value: 0.6820300961495451.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:44:26,913]\u001b[0m Trial 52 finished with value: 0.6811506256716553 and parameters: {'n_estimators': 568, 'learning_rate': 0.0910735018276129, 'max_depth': 11, 'max_bin': 233, 'num_leaves': 657}. Best is trial 50 with value: 0.6820300961495451.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:44:37,623]\u001b[0m Trial 53 finished with value: 0.6693742260774407 and parameters: {'n_estimators': 586, 'learning_rate': 0.0944741178939212, 'max_depth': 10, 'max_bin': 236, 'num_leaves': 648}. Best is trial 50 with value: 0.6820300961495451.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:44:49,605]\u001b[0m Trial 54 finished with value: 0.6861088705303288 and parameters: {'n_estimators': 494, 'learning_rate': 0.08777190073649087, 'max_depth': 11, 'max_bin': 227, 'num_leaves': 611}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:45:00,839]\u001b[0m Trial 55 finished with value: 0.673216821025554 and parameters: {'n_estimators': 611, 'learning_rate': 0.11450378435133617, 'max_depth': 11, 'max_bin': 229, 'num_leaves': 649}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:45:10,512]\u001b[0m Trial 56 finished with value: 0.6673809563799914 and parameters: {'n_estimators': 502, 'learning_rate': 0.09097952629861512, 'max_depth': 8, 'max_bin': 244, 'num_leaves': 599}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:45:20,924]\u001b[0m Trial 57 finished with value: 0.6599218121926836 and parameters: {'n_estimators': 570, 'learning_rate': 0.1304365728452502, 'max_depth': 6, 'max_bin': 240, 'num_leaves': 687}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:45:29,168]\u001b[0m Trial 58 finished with value: 0.656034037752258 and parameters: {'n_estimators': 78, 'learning_rate': 0.10539301969729888, 'max_depth': 11, 'max_bin': 229, 'num_leaves': 527}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:45:37,723]\u001b[0m Trial 59 finished with value: 0.6754469039868174 and parameters: {'n_estimators': 656, 'learning_rate': 0.08747599213215657, 'max_depth': 10, 'max_bin': 256, 'num_leaves': 627}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:45:51,848]\u001b[0m Trial 60 finished with value: 0.6738486531307808 and parameters: {'n_estimators': 771, 'learning_rate': 0.08776029637725556, 'max_depth': 9, 'max_bin': 257, 'num_leaves': 618}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:46:04,741]\u001b[0m Trial 61 finished with value: 0.67958476092518 and parameters: {'n_estimators': 662, 'learning_rate': 0.10453372499627976, 'max_depth': 11, 'max_bin': 252, 'num_leaves': 679}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:46:16,275]\u001b[0m Trial 62 finished with value: 0.6771586681893818 and parameters: {'n_estimators': 654, 'learning_rate': 0.12278745600039313, 'max_depth': 10, 'max_bin': 252, 'num_leaves': 703}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:46:27,315]\u001b[0m Trial 63 finished with value: 0.6711222479170196 and parameters: {'n_estimators': 661, 'learning_rate': 0.1229968039206548, 'max_depth': 10, 'max_bin': 266, 'num_leaves': 692}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:46:38,292]\u001b[0m Trial 64 finished with value: 0.6664243436888864 and parameters: {'n_estimators': 700, 'learning_rate': 0.1066510094485131, 'max_depth': 10, 'max_bin': 253, 'num_leaves': 665}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:46:49,156]\u001b[0m Trial 65 finished with value: 0.674791433562823 and parameters: {'n_estimators': 630, 'learning_rate': 0.12172823643828243, 'max_depth': 10, 'max_bin': 283, 'num_leaves': 735}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:47:00,684]\u001b[0m Trial 66 finished with value: 0.670668554252047 and parameters: {'n_estimators': 655, 'learning_rate': 0.14154203115630465, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 613}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:47:11,907]\u001b[0m Trial 67 finished with value: 0.6731244793636344 and parameters: {'n_estimators': 587, 'learning_rate': 0.15373317509139495, 'max_depth': 11, 'max_bin': 281, 'num_leaves': 711}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:47:23,177]\u001b[0m Trial 68 finished with value: 0.6639959917464149 and parameters: {'n_estimators': 752, 'learning_rate': 0.10784453683226243, 'max_depth': 10, 'max_bin': 251, 'num_leaves': 670}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:47:33,074]\u001b[0m Trial 69 finished with value: 0.6738136450399126 and parameters: {'n_estimators': 693, 'learning_rate': 0.08590533328029445, 'max_depth': 9, 'max_bin': 277, 'num_leaves': 586}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:47:45,338]\u001b[0m Trial 70 finished with value: 0.6780334947326783 and parameters: {'n_estimators': 806, 'learning_rate': 0.07462994336204294, 'max_depth': 11, 'max_bin': 241, 'num_leaves': 720}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:47:56,858]\u001b[0m Trial 71 finished with value: 0.6740229957262762 and parameters: {'n_estimators': 878, 'learning_rate': 0.09727435743073937, 'max_depth': 11, 'max_bin': 242, 'num_leaves': 713}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:48:12,516]\u001b[0m Trial 72 finished with value: 0.676729070201716 and parameters: {'n_estimators': 823, 'learning_rate': 0.07650053083380284, 'max_depth': 11, 'max_bin': 235, 'num_leaves': 631}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:48:26,363]\u001b[0m Trial 73 finished with value: 0.676497821702819 and parameters: {'n_estimators': 855, 'learning_rate': 0.07380467436787187, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 634}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:48:39,856]\u001b[0m Trial 74 finished with value: 0.6790974766077009 and parameters: {'n_estimators': 839, 'learning_rate': 0.07153149950043179, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 670}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:48:52,624]\u001b[0m Trial 75 finished with value: 0.6836040725249652 and parameters: {'n_estimators': 872, 'learning_rate': 0.07936371986821461, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 669}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:49:04,046]\u001b[0m Trial 76 finished with value: 0.6798952716332016 and parameters: {'n_estimators': 804, 'learning_rate': 0.080690148515835, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 676}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:49:25,267]\u001b[0m Trial 77 finished with value: 0.6778765347997178 and parameters: {'n_estimators': 807, 'learning_rate': 0.06587468455371857, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 748}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:49:37,315]\u001b[0m Trial 78 finished with value: 0.674524806813803 and parameters: {'n_estimators': 842, 'learning_rate': 0.07927626943055437, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 668}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:49:59,018]\u001b[0m Trial 79 finished with value: 0.6762894771051691 and parameters: {'n_estimators': 794, 'learning_rate': 0.0685124639234661, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 565}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:50:10,765]\u001b[0m Trial 80 finished with value: 0.6828274387983141 and parameters: {'n_estimators': 899, 'learning_rate': 0.08226066832375079, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 677}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 21:50:24,182]\u001b[0m Trial 81 finished with value: 0.6807801221402856 and parameters: {'n_estimators': 884, 'learning_rate': 0.08133789312589723, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 729}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:50:38,533]\u001b[0m Trial 82 finished with value: 0.6814743061184945 and parameters: {'n_estimators': 873, 'learning_rate': 0.08288261862046414, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 676}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:50:52,210]\u001b[0m Trial 83 finished with value: 0.6764276313945213 and parameters: {'n_estimators': 891, 'learning_rate': 0.08195425612705806, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 651}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:51:06,466]\u001b[0m Trial 84 finished with value: 0.6756220948699165 and parameters: {'n_estimators': 870, 'learning_rate': 0.1020871084279781, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 681}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:51:15,678]\u001b[0m Trial 85 finished with value: 0.6754136293256069 and parameters: {'n_estimators': 862, 'learning_rate': 0.09293926886864921, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 726}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:51:31,926]\u001b[0m Trial 86 finished with value: 0.6784742980125801 and parameters: {'n_estimators': 898, 'learning_rate': 0.08434154152211615, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 591}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:51:46,955]\u001b[0m Trial 87 finished with value: 0.6814985643265213 and parameters: {'n_estimators': 788, 'learning_rate': 0.062324194200919954, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 652}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:52:03,789]\u001b[0m Trial 88 finished with value: 0.6776736331252923 and parameters: {'n_estimators': 772, 'learning_rate': 0.061119079429694766, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 522}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:52:17,928]\u001b[0m Trial 89 finished with value: 0.676079748386528 and parameters: {'n_estimators': 821, 'learning_rate': 0.061079715698039425, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 652}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:52:35,491]\u001b[0m Trial 90 finished with value: 0.6843040315458595 and parameters: {'n_estimators': 845, 'learning_rate': 0.09103874578842103, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 599}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:52:52,149]\u001b[0m Trial 91 finished with value: 0.683163629848085 and parameters: {'n_estimators': 848, 'learning_rate': 0.09227013869921977, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 610}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:53:05,065]\u001b[0m Trial 92 finished with value: 0.6790274962944854 and parameters: {'n_estimators': 878, 'learning_rate': 0.0897456074457226, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 606}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:53:15,709]\u001b[0m Trial 93 finished with value: 0.672031371969217 and parameters: {'n_estimators': 849, 'learning_rate': 0.09718922878523834, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 573}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:53:26,825]\u001b[0m Trial 94 finished with value: 0.6807048648135994 and parameters: {'n_estimators': 900, 'learning_rate': 0.0929116567420738, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 638}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:53:36,909]\u001b[0m Trial 95 finished with value: 0.6655068887682842 and parameters: {'n_estimators': 788, 'learning_rate': 0.10058556447591419, 'max_depth': 11, 'max_bin': 228, 'num_leaves': 700}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:53:55,850]\u001b[0m Trial 96 finished with value: 0.6787532303385819 and parameters: {'n_estimators': 830, 'learning_rate': 0.06957229548013925, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 617}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:54:04,937]\u001b[0m Trial 97 finished with value: 0.6744513635230704 and parameters: {'n_estimators': 866, 'learning_rate': 0.11140637210221573, 'max_depth': 11, 'max_bin': 244, 'num_leaves': 544}. Best is trial 54 with value: 0.6861088705303288.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:54:23,286]\u001b[0m Trial 98 finished with value: 0.6871978169312094 and parameters: {'n_estimators': 876, 'learning_rate': 0.07786588374393016, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 657}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:54:36,583]\u001b[0m Trial 99 finished with value: 0.6725936553215448 and parameters: {'n_estimators': 751, 'learning_rate': 0.07826882974096713, 'max_depth': 11, 'max_bin': 210, 'num_leaves': 654}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6872\n",
      "\tBest params:\n",
      "\t\tn_estimators: 876\n",
      "\t\tlearning_rate: 0.07786588374393016\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 222\n",
      "\t\tnum_leaves: 657\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.686631    0.637557\n",
      "1                    TP   42.000000   37.000000\n",
      "2                    TN  199.000000  198.000000\n",
      "3                    FP    2.000000    3.000000\n",
      "4                    FN   25.000000   30.000000\n",
      "5              Accuracy    0.899254    0.876866\n",
      "6             Precision    0.954545    0.925000\n",
      "7           Sensitivity    0.626866    0.552239\n",
      "8           Specificity    0.990000    0.985100\n",
      "9              F1 score    0.756757    0.691589\n",
      "10  F1 score (weighted)    0.891542    0.865205\n",
      "11     F1 score (macro)    0.846614    0.807333\n",
      "12    Balanced Accuracy    0.808458    0.768657\n",
      "13                  MCC    0.721125    0.652929\n",
      "14                  NPV    0.888400    0.868400\n",
      "15              ROC_AUC    0.808458    0.768657\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_lgbm_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_1_cat = np.where(((y_pred_lgbm_1 >= 2) | (y_pred_lgbm_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 21:54:44,417]\u001b[0m Trial 100 finished with value: 0.6321194031368945 and parameters: {'n_estimators': 854, 'learning_rate': 0.08640935851884363, 'max_depth': 5, 'max_bin': 215, 'num_leaves': 600}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:54:56,233]\u001b[0m Trial 101 finished with value: 0.6529095057221193 and parameters: {'n_estimators': 884, 'learning_rate': 0.09579353812296142, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 659}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:55:10,530]\u001b[0m Trial 102 finished with value: 0.6517552241038189 and parameters: {'n_estimators': 828, 'learning_rate': 0.08329795126649224, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 729}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:55:20,187]\u001b[0m Trial 103 finished with value: 0.6542721680287045 and parameters: {'n_estimators': 459, 'learning_rate': 0.06361708290358221, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 632}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:55:38,866]\u001b[0m Trial 104 finished with value: 0.6547394407702752 and parameters: {'n_estimators': 874, 'learning_rate': 0.05349038195665705, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 692}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:55:48,541]\u001b[0m Trial 105 finished with value: 0.6471003008622942 and parameters: {'n_estimators': 840, 'learning_rate': 0.09277112007932953, 'max_depth': 11, 'max_bin': 232, 'num_leaves': 709}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:55:59,950]\u001b[0m Trial 106 finished with value: 0.6523717694725519 and parameters: {'n_estimators': 859, 'learning_rate': 0.08878873134120684, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 621}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:56:13,188]\u001b[0m Trial 107 finished with value: 0.6551741079252033 and parameters: {'n_estimators': 815, 'learning_rate': 0.07932201172270678, 'max_depth': 11, 'max_bin': 242, 'num_leaves': 83}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:56:23,036]\u001b[0m Trial 108 finished with value: 0.6592940240447583 and parameters: {'n_estimators': 789, 'learning_rate': 0.07082548544029227, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 750}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:56:35,601]\u001b[0m Trial 109 finished with value: 0.6519611333342259 and parameters: {'n_estimators': 887, 'learning_rate': 0.10162152496217867, 'max_depth': 11, 'max_bin': 204, 'num_leaves': 683}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:56:48,359]\u001b[0m Trial 110 finished with value: 0.6511745523833827 and parameters: {'n_estimators': 715, 'learning_rate': 0.08398572446831155, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 641}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:56:59,044]\u001b[0m Trial 111 finished with value: 0.6565808077211625 and parameters: {'n_estimators': 895, 'learning_rate': 0.09143473589345286, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 640}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:57:09,419]\u001b[0m Trial 112 finished with value: 0.6561446400864395 and parameters: {'n_estimators': 847, 'learning_rate': 0.09517640412066238, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 588}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:57:23,259]\u001b[0m Trial 113 finished with value: 0.6539710918925141 and parameters: {'n_estimators': 874, 'learning_rate': 0.07725004827306876, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 664}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:57:35,836]\u001b[0m Trial 114 finished with value: 0.658811871311537 and parameters: {'n_estimators': 900, 'learning_rate': 0.11016844634746738, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 617}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:57:46,167]\u001b[0m Trial 115 finished with value: 0.6504018644708099 and parameters: {'n_estimators': 833, 'learning_rate': 0.09026513642408335, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 604}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:57:57,188]\u001b[0m Trial 116 finished with value: 0.657834385800865 and parameters: {'n_estimators': 863, 'learning_rate': 0.07348595334504544, 'max_depth': 11, 'max_bin': 237, 'num_leaves': 697}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:58:05,112]\u001b[0m Trial 117 finished with value: 0.6541457332007788 and parameters: {'n_estimators': 399, 'learning_rate': 0.09880702660136495, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 637}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:58:20,375]\u001b[0m Trial 118 finished with value: 0.6551745902752026 and parameters: {'n_estimators': 516, 'learning_rate': 0.08583243211173136, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 555}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:58:36,004]\u001b[0m Trial 119 finished with value: 0.6571403400468109 and parameters: {'n_estimators': 899, 'learning_rate': 0.08034797935196246, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 719}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:58:49,082]\u001b[0m Trial 120 finished with value: 0.654953087289489 and parameters: {'n_estimators': 817, 'learning_rate': 0.06604338728011909, 'max_depth': 11, 'max_bin': 228, 'num_leaves': 663}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:59:00,663]\u001b[0m Trial 121 finished with value: 0.6553081032826057 and parameters: {'n_estimators': 797, 'learning_rate': 0.08226673674967935, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 684}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:59:14,477]\u001b[0m Trial 122 finished with value: 0.6537646319698378 and parameters: {'n_estimators': 777, 'learning_rate': 0.0932256952161481, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 673}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:59:27,325]\u001b[0m Trial 123 finished with value: 0.653398150425729 and parameters: {'n_estimators': 753, 'learning_rate': 0.07574384956381117, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 646}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:59:39,275]\u001b[0m Trial 124 finished with value: 0.6560015816547189 and parameters: {'n_estimators': 849, 'learning_rate': 0.10281292101909785, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 626}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 21:59:51,148]\u001b[0m Trial 125 finished with value: 0.6513843202087556 and parameters: {'n_estimators': 873, 'learning_rate': 0.08640985940503688, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 706}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:00:02,321]\u001b[0m Trial 126 finished with value: 0.6566895830040963 and parameters: {'n_estimators': 802, 'learning_rate': 0.08022659637700486, 'max_depth': 11, 'max_bin': 234, 'num_leaves': 685}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:00:11,601]\u001b[0m Trial 127 finished with value: 0.6539551312020275 and parameters: {'n_estimators': 835, 'learning_rate': 0.08998984390792833, 'max_depth': 8, 'max_bin': 213, 'num_leaves': 658}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:00:23,341]\u001b[0m Trial 128 finished with value: 0.6487147810551392 and parameters: {'n_estimators': 884, 'learning_rate': 0.06947164430912117, 'max_depth': 7, 'max_bin': 218, 'num_leaves': 578}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:00:37,160]\u001b[0m Trial 129 finished with value: 0.6611503654139098 and parameters: {'n_estimators': 342, 'learning_rate': 0.05794029139362329, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 675}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:00:49,026]\u001b[0m Trial 130 finished with value: 0.6522037779953684 and parameters: {'n_estimators': 862, 'learning_rate': 0.08186146391289995, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 610}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 22:00:57,935]\u001b[0m Trial 131 finished with value: 0.6463407640291637 and parameters: {'n_estimators': 556, 'learning_rate': 0.10393256601543126, 'max_depth': 11, 'max_bin': 255, 'num_leaves': 735}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:01:07,926]\u001b[0m Trial 132 finished with value: 0.6485215914391257 and parameters: {'n_estimators': 638, 'learning_rate': 0.09707465357233055, 'max_depth': 11, 'max_bin': 247, 'num_leaves': 643}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:01:18,800]\u001b[0m Trial 133 finished with value: 0.6544335314168924 and parameters: {'n_estimators': 609, 'learning_rate': 0.11539290806078889, 'max_depth': 11, 'max_bin': 239, 'num_leaves': 676}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:01:26,867]\u001b[0m Trial 134 finished with value: 0.6508786073228962 and parameters: {'n_estimators': 816, 'learning_rate': 0.10727767531163797, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 706}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:01:35,678]\u001b[0m Trial 135 finished with value: 0.6585007792254863 and parameters: {'n_estimators': 580, 'learning_rate': 0.09499579361246324, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 656}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:01:45,096]\u001b[0m Trial 136 finished with value: 0.650347323780452 and parameters: {'n_estimators': 595, 'learning_rate': 0.0877613636447454, 'max_depth': 11, 'max_bin': 224, 'num_leaves': 628}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:02:01,875]\u001b[0m Trial 137 finished with value: 0.6601963921153198 and parameters: {'n_estimators': 494, 'learning_rate': 0.07405684032886939, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 695}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:02:11,475]\u001b[0m Trial 138 finished with value: 0.649613960876552 and parameters: {'n_estimators': 883, 'learning_rate': 0.09963581859558336, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 593}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:02:23,512]\u001b[0m Trial 139 finished with value: 0.6516150901701833 and parameters: {'n_estimators': 670, 'learning_rate': 0.05067252817747581, 'max_depth': 11, 'max_bin': 264, 'num_leaves': 675}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:02:31,206]\u001b[0m Trial 140 finished with value: 0.6330303053141828 and parameters: {'n_estimators': 852, 'learning_rate': 0.08392797976367064, 'max_depth': 6, 'max_bin': 240, 'num_leaves': 726}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:02:41,704]\u001b[0m Trial 141 finished with value: 0.6530269895073793 and parameters: {'n_estimators': 836, 'learning_rate': 0.07221323886406954, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 664}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:02:57,337]\u001b[0m Trial 142 finished with value: 0.6599318776344438 and parameters: {'n_estimators': 840, 'learning_rate': 0.07767817803300346, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 638}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:03:09,305]\u001b[0m Trial 143 finished with value: 0.6631785418494052 and parameters: {'n_estimators': 900, 'learning_rate': 0.06495066345084852, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 687}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:03:18,881]\u001b[0m Trial 144 finished with value: 0.6565731203799179 and parameters: {'n_estimators': 872, 'learning_rate': 0.09169109306421179, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 650}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:03:33,848]\u001b[0m Trial 145 finished with value: 0.6578271184413875 and parameters: {'n_estimators': 817, 'learning_rate': 0.06908145464367443, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 620}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:03:45,409]\u001b[0m Trial 146 finished with value: 0.6486838891314978 and parameters: {'n_estimators': 863, 'learning_rate': 0.08658084098727885, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 673}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:03:52,123]\u001b[0m Trial 147 finished with value: 0.627822988173898 and parameters: {'n_estimators': 739, 'learning_rate': 0.07900647978790754, 'max_depth': 4, 'max_bin': 222, 'num_leaves': 716}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:04:04,905]\u001b[0m Trial 148 finished with value: 0.6560059908955207 and parameters: {'n_estimators': 848, 'learning_rate': 0.0906981398289187, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 607}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:04:19,228]\u001b[0m Trial 149 finished with value: 0.6568935379491538 and parameters: {'n_estimators': 828, 'learning_rate': 0.07249158558494864, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 698}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6872\n",
      "\tBest params:\n",
      "\t\tn_estimators: 876\n",
      "\t\tlearning_rate: 0.07786588374393016\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 222\n",
      "\t\tnum_leaves: 657\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.686631    0.637557    0.707517\n",
      "1                    TP   42.000000   37.000000   39.000000\n",
      "2                    TN  199.000000  198.000000  198.000000\n",
      "3                    FP    2.000000    3.000000    3.000000\n",
      "4                    FN   25.000000   30.000000   28.000000\n",
      "5              Accuracy    0.899254    0.876866    0.884328\n",
      "6             Precision    0.954545    0.925000    0.928571\n",
      "7           Sensitivity    0.626866    0.552239    0.582090\n",
      "8           Specificity    0.990000    0.985100    0.985100\n",
      "9              F1 score    0.756757    0.691589    0.715596\n",
      "10  F1 score (weighted)    0.891542    0.865205    0.874449\n",
      "11     F1 score (macro)    0.846614    0.807333    0.821498\n",
      "12    Balanced Accuracy    0.808458    0.768657    0.783582\n",
      "13                  MCC    0.721125    0.652929    0.675562\n",
      "14                  NPV    0.888400    0.868400    0.876100\n",
      "15              ROC_AUC    0.808458    0.768657    0.783582\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_lgbm_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_2_cat = np.where(((y_pred_lgbm_2 >= 2) | (y_pred_lgbm_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 22:04:35,506]\u001b[0m Trial 150 finished with value: 0.6806522748093136 and parameters: {'n_estimators': 791, 'learning_rate': 0.08319829681911237, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 660}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:04:48,341]\u001b[0m Trial 151 finished with value: 0.6846815283693377 and parameters: {'n_estimators': 790, 'learning_rate': 0.08397143771540523, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 655}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:05:01,317]\u001b[0m Trial 152 finished with value: 0.6801693182381141 and parameters: {'n_estimators': 780, 'learning_rate': 0.08347130600257759, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 653}. Best is trial 98 with value: 0.6871978169312094.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:05:16,248]\u001b[0m Trial 153 finished with value: 0.6872552462516891 and parameters: {'n_estimators': 780, 'learning_rate': 0.08276532508888751, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 632}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:05:26,695]\u001b[0m Trial 154 finished with value: 0.6820529385333576 and parameters: {'n_estimators': 764, 'learning_rate': 0.08391390107657873, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 633}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:05:42,382]\u001b[0m Trial 155 finished with value: 0.6837887825798694 and parameters: {'n_estimators': 758, 'learning_rate': 0.08838935908443166, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 630}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:05:52,138]\u001b[0m Trial 156 finished with value: 0.6805968887167287 and parameters: {'n_estimators': 755, 'learning_rate': 0.09472018985101842, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 631}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:06:03,930]\u001b[0m Trial 157 finished with value: 0.6799108413345942 and parameters: {'n_estimators': 722, 'learning_rate': 0.089690392913215, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 596}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:06:16,916]\u001b[0m Trial 158 finished with value: 0.6845933327980993 and parameters: {'n_estimators': 755, 'learning_rate': 0.08763139266335235, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 614}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:06:28,081]\u001b[0m Trial 159 finished with value: 0.6809841399367601 and parameters: {'n_estimators': 730, 'learning_rate': 0.07671637262707502, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 570}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:06:42,184]\u001b[0m Trial 160 finished with value: 0.6796342943906282 and parameters: {'n_estimators': 703, 'learning_rate': 0.07536105337946396, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 565}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:06:53,094]\u001b[0m Trial 161 finished with value: 0.6861810491141126 and parameters: {'n_estimators': 757, 'learning_rate': 0.08692709704865687, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 618}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:07:06,279]\u001b[0m Trial 162 finished with value: 0.6816118902419622 and parameters: {'n_estimators': 760, 'learning_rate': 0.08698991378692217, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 583}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:07:18,110]\u001b[0m Trial 163 finished with value: 0.6828687268945809 and parameters: {'n_estimators': 762, 'learning_rate': 0.08730804049332255, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 583}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:07:33,695]\u001b[0m Trial 164 finished with value: 0.6838439760429147 and parameters: {'n_estimators': 747, 'learning_rate': 0.08630009500952746, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 610}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:07:43,621]\u001b[0m Trial 165 finished with value: 0.681397682012755 and parameters: {'n_estimators': 744, 'learning_rate': 0.08754254064209575, 'max_depth': 12, 'max_bin': 249, 'num_leaves': 540}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:07:57,316]\u001b[0m Trial 166 finished with value: 0.6821306929960678 and parameters: {'n_estimators': 760, 'learning_rate': 0.09814801915013145, 'max_depth': 12, 'max_bin': 249, 'num_leaves': 583}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:08:08,062]\u001b[0m Trial 167 finished with value: 0.6847665615488024 and parameters: {'n_estimators': 761, 'learning_rate': 0.09817451595089874, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 592}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:08:19,816]\u001b[0m Trial 168 finished with value: 0.681571216988179 and parameters: {'n_estimators': 707, 'learning_rate': 0.09995039036513312, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 604}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:08:31,083]\u001b[0m Trial 169 finished with value: 0.6772653584874733 and parameters: {'n_estimators': 767, 'learning_rate': 0.09658988766118781, 'max_depth': 12, 'max_bin': 249, 'num_leaves': 615}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:08:42,755]\u001b[0m Trial 170 finished with value: 0.6784061101027822 and parameters: {'n_estimators': 737, 'learning_rate': 0.09428865240560698, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 524}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:08:58,832]\u001b[0m Trial 171 finished with value: 0.6822782363735808 and parameters: {'n_estimators': 762, 'learning_rate': 0.08797514154228288, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 583}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:09:09,161]\u001b[0m Trial 172 finished with value: 0.6832375891979681 and parameters: {'n_estimators': 768, 'learning_rate': 0.08848682521560476, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 585}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:09:23,027]\u001b[0m Trial 173 finished with value: 0.6840247211284761 and parameters: {'n_estimators': 765, 'learning_rate': 0.08887130677773158, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 580}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:09:34,135]\u001b[0m Trial 174 finished with value: 0.684486347455934 and parameters: {'n_estimators': 778, 'learning_rate': 0.09044118105120774, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 584}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:09:45,500]\u001b[0m Trial 175 finished with value: 0.6759021669632651 and parameters: {'n_estimators': 196, 'learning_rate': 0.08927292914518216, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 544}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:09:57,241]\u001b[0m Trial 176 finished with value: 0.6843402525172945 and parameters: {'n_estimators': 722, 'learning_rate': 0.08668818093453294, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 554}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:10:12,213]\u001b[0m Trial 177 finished with value: 0.6840594470149657 and parameters: {'n_estimators': 778, 'learning_rate': 0.07935282183581972, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 507}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:10:27,141]\u001b[0m Trial 178 finished with value: 0.6831922846847176 and parameters: {'n_estimators': 727, 'learning_rate': 0.07952182123894308, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 563}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:10:41,102]\u001b[0m Trial 179 finished with value: 0.6805232510822794 and parameters: {'n_estimators': 720, 'learning_rate': 0.07947055646856563, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 504}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:10:52,874]\u001b[0m Trial 180 finished with value: 0.6811042281469717 and parameters: {'n_estimators': 779, 'learning_rate': 0.09136461310801211, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 596}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 22:11:04,735]\u001b[0m Trial 181 finished with value: 0.6853093183572452 and parameters: {'n_estimators': 775, 'learning_rate': 0.08526262299916779, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 558}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:11:20,373]\u001b[0m Trial 182 finished with value: 0.6798496329778626 and parameters: {'n_estimators': 729, 'learning_rate': 0.07887149876908589, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 554}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:11:34,620]\u001b[0m Trial 183 finished with value: 0.6823030129058265 and parameters: {'n_estimators': 686, 'learning_rate': 0.08457432578223757, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 563}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:11:46,705]\u001b[0m Trial 184 finished with value: 0.6815048615547483 and parameters: {'n_estimators': 779, 'learning_rate': 0.09297354663749144, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 535}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:12:00,470]\u001b[0m Trial 185 finished with value: 0.6858694761464897 and parameters: {'n_estimators': 742, 'learning_rate': 0.08582784150825151, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 558}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:12:08,591]\u001b[0m Trial 186 finished with value: 0.6795473346634365 and parameters: {'n_estimators': 743, 'learning_rate': 0.18493806314998532, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 515}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:12:19,764]\u001b[0m Trial 187 finished with value: 0.6771230828863206 and parameters: {'n_estimators': 712, 'learning_rate': 0.0816459768289806, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 482}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:12:33,595]\u001b[0m Trial 188 finished with value: 0.6831416028059079 and parameters: {'n_estimators': 799, 'learning_rate': 0.07550253533111381, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 560}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:12:47,571]\u001b[0m Trial 189 finished with value: 0.6815515133808083 and parameters: {'n_estimators': 746, 'learning_rate': 0.08577721326522907, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 455}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:13:03,412]\u001b[0m Trial 190 finished with value: 0.6833985722818887 and parameters: {'n_estimators': 787, 'learning_rate': 0.07861496946534811, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 571}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:13:15,087]\u001b[0m Trial 191 finished with value: 0.6837599373103698 and parameters: {'n_estimators': 785, 'learning_rate': 0.07901525138841618, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 577}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:13:26,568]\u001b[0m Trial 192 finished with value: 0.6818463628317166 and parameters: {'n_estimators': 785, 'learning_rate': 0.08578901619046285, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 574}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:13:40,396]\u001b[0m Trial 193 finished with value: 0.680628456821611 and parameters: {'n_estimators': 772, 'learning_rate': 0.07659233903022564, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 597}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:13:57,391]\u001b[0m Trial 194 finished with value: 0.6781838619917882 and parameters: {'n_estimators': 809, 'learning_rate': 0.0819316209011178, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 548}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:14:09,606]\u001b[0m Trial 195 finished with value: 0.685183953454867 and parameters: {'n_estimators': 802, 'learning_rate': 0.09085215977252287, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 574}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:14:20,033]\u001b[0m Trial 196 finished with value: 0.6813639315249931 and parameters: {'n_estimators': 794, 'learning_rate': 0.09577226933525147, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 613}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:14:32,562]\u001b[0m Trial 197 finished with value: 0.6862057495679076 and parameters: {'n_estimators': 747, 'learning_rate': 0.09079694968327252, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 568}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:14:44,174]\u001b[0m Trial 198 finished with value: 0.6812474664373936 and parameters: {'n_estimators': 748, 'learning_rate': 0.09129803255882421, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 556}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:14:55,938]\u001b[0m Trial 199 finished with value: 0.6805783966638635 and parameters: {'n_estimators': 805, 'learning_rate': 0.08479753845082423, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 597}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6873\n",
      "\tBest params:\n",
      "\t\tn_estimators: 780\n",
      "\t\tlearning_rate: 0.08276532508888751\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 232\n",
      "\t\tnum_leaves: 632\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.686631    0.637557    0.707517    0.672644\n",
      "1                    TP   42.000000   37.000000   39.000000   34.000000\n",
      "2                    TN  199.000000  198.000000  198.000000  195.000000\n",
      "3                    FP    2.000000    3.000000    3.000000    5.000000\n",
      "4                    FN   25.000000   30.000000   28.000000   34.000000\n",
      "5              Accuracy    0.899254    0.876866    0.884328    0.854478\n",
      "6             Precision    0.954545    0.925000    0.928571    0.871795\n",
      "7           Sensitivity    0.626866    0.552239    0.582090    0.500000\n",
      "8           Specificity    0.990000    0.985100    0.985100    0.975000\n",
      "9              F1 score    0.756757    0.691589    0.715596    0.635514\n",
      "10  F1 score (weighted)    0.891542    0.865205    0.874449    0.839676\n",
      "11     F1 score (macro)    0.846614    0.807333    0.821498    0.772302\n",
      "12    Balanced Accuracy    0.808458    0.768657    0.783582    0.737500\n",
      "13                  MCC    0.721125    0.652929    0.675562    0.586156\n",
      "14                  NPV    0.888400    0.868400    0.876100    0.851500\n",
      "15              ROC_AUC    0.808458    0.768657    0.783582    0.737500\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_lgbm_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_3_cat = np.where(((y_pred_lgbm_3 >= 2) | (y_pred_lgbm_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 22:15:13,130]\u001b[0m Trial 200 finished with value: 0.6736237174199573 and parameters: {'n_estimators': 749, 'learning_rate': 0.10089471295193461, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 532}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:15:27,271]\u001b[0m Trial 201 finished with value: 0.6715554661815374 and parameters: {'n_estimators': 797, 'learning_rate': 0.08984582163668478, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 571}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:15:40,217]\u001b[0m Trial 202 finished with value: 0.667341050622851 and parameters: {'n_estimators': 778, 'learning_rate': 0.08137875151071908, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 572}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:15:51,374]\u001b[0m Trial 203 finished with value: 0.6705201857731982 and parameters: {'n_estimators': 736, 'learning_rate': 0.08553079386305011, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 622}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:16:03,043]\u001b[0m Trial 204 finished with value: 0.6726344313303823 and parameters: {'n_estimators': 780, 'learning_rate': 0.07371600669418471, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 590}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:16:16,214]\u001b[0m Trial 205 finished with value: 0.6720089793439006 and parameters: {'n_estimators': 761, 'learning_rate': 0.07834181272636342, 'max_depth': 12, 'max_bin': 249, 'num_leaves': 510}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:16:34,602]\u001b[0m Trial 206 finished with value: 0.6723534178971112 and parameters: {'n_estimators': 784, 'learning_rate': 0.09419321625924607, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 548}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:16:45,966]\u001b[0m Trial 207 finished with value: 0.671358106869359 and parameters: {'n_estimators': 815, 'learning_rate': 0.08986524551761296, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 603}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:16:57,015]\u001b[0m Trial 208 finished with value: 0.6685103306052529 and parameters: {'n_estimators': 734, 'learning_rate': 0.08335980540387006, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 577}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:17:10,413]\u001b[0m Trial 209 finished with value: 0.6722375489644736 and parameters: {'n_estimators': 751, 'learning_rate': 0.09619679571767535, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 613}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:17:27,691]\u001b[0m Trial 210 finished with value: 0.6730923251627803 and parameters: {'n_estimators': 766, 'learning_rate': 0.08705311920259623, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 569}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:17:39,280]\u001b[0m Trial 211 finished with value: 0.6730547079972007 and parameters: {'n_estimators': 771, 'learning_rate': 0.08938388249649715, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 589}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:17:50,093]\u001b[0m Trial 212 finished with value: 0.6729828326402251 and parameters: {'n_estimators': 796, 'learning_rate': 0.08043228039056327, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 585}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:18:02,502]\u001b[0m Trial 213 finished with value: 0.6758168840993881 and parameters: {'n_estimators': 766, 'learning_rate': 0.08907583476361124, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 604}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:18:19,304]\u001b[0m Trial 214 finished with value: 0.6796309807813039 and parameters: {'n_estimators': 784, 'learning_rate': 0.09257755575661472, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 623}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:18:27,600]\u001b[0m Trial 215 finished with value: 0.6659110373790664 and parameters: {'n_estimators': 745, 'learning_rate': 0.08504870350836574, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 555}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:18:43,532]\u001b[0m Trial 216 finished with value: 0.6674732195288658 and parameters: {'n_estimators': 716, 'learning_rate': 0.07684743606500763, 'max_depth': 12, 'max_bin': 263, 'num_leaves': 536}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:18:57,310]\u001b[0m Trial 217 finished with value: 0.6675790031199722 and parameters: {'n_estimators': 804, 'learning_rate': 0.09746312771389698, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 587}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:19:07,598]\u001b[0m Trial 218 finished with value: 0.6659587909324757 and parameters: {'n_estimators': 766, 'learning_rate': 0.08187745348376828, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 572}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:19:19,341]\u001b[0m Trial 219 finished with value: 0.670590923121862 and parameters: {'n_estimators': 729, 'learning_rate': 0.08701097796616512, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 604}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:19:30,875]\u001b[0m Trial 220 finished with value: 0.6632168865435496 and parameters: {'n_estimators': 240, 'learning_rate': 0.07116137593108518, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 619}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:19:44,516]\u001b[0m Trial 221 finished with value: 0.6725834729913303 and parameters: {'n_estimators': 745, 'learning_rate': 0.08169132643142983, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 559}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:19:54,184]\u001b[0m Trial 222 finished with value: 0.6673443720355114 and parameters: {'n_estimators': 691, 'learning_rate': 0.07729068821240324, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 564}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:20:10,924]\u001b[0m Trial 223 finished with value: 0.6708790377024606 and parameters: {'n_estimators': 728, 'learning_rate': 0.07919328189452174, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 581}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:20:21,061]\u001b[0m Trial 224 finished with value: 0.6727290349780961 and parameters: {'n_estimators': 781, 'learning_rate': 0.09063435620812718, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 594}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:20:32,365]\u001b[0m Trial 225 finished with value: 0.6701000990005899 and parameters: {'n_estimators': 709, 'learning_rate': 0.08516770502275396, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 548}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:20:46,225]\u001b[0m Trial 226 finished with value: 0.6743495407167674 and parameters: {'n_estimators': 756, 'learning_rate': 0.09323461119626018, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 569}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:20:58,102]\u001b[0m Trial 227 finished with value: 0.6696974708141981 and parameters: {'n_estimators': 797, 'learning_rate': 0.0740763852293716, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 623}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:21:09,012]\u001b[0m Trial 228 finished with value: 0.6692495414672417 and parameters: {'n_estimators': 819, 'learning_rate': 0.08835123184537899, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 634}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:21:22,681]\u001b[0m Trial 229 finished with value: 0.6800529900227422 and parameters: {'n_estimators': 772, 'learning_rate': 0.08028527837920521, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 607}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:21:37,466]\u001b[0m Trial 230 finished with value: 0.6714706968687455 and parameters: {'n_estimators': 733, 'learning_rate': 0.08342788899013304, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 526}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 22:21:53,180]\u001b[0m Trial 231 finished with value: 0.6770576182236511 and parameters: {'n_estimators': 751, 'learning_rate': 0.09273288556985611, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 594}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:22:02,545]\u001b[0m Trial 232 finished with value: 0.6717583403510112 and parameters: {'n_estimators': 777, 'learning_rate': 0.08699328106686258, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 579}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:22:10,831]\u001b[0m Trial 233 finished with value: 0.6734800028506355 and parameters: {'n_estimators': 792, 'learning_rate': 0.09143851252501521, 'max_depth': 12, 'max_bin': 256, 'num_leaves': 610}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:22:24,671]\u001b[0m Trial 234 finished with value: 0.6717584049154421 and parameters: {'n_estimators': 760, 'learning_rate': 0.09788818352467558, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 554}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:22:36,923]\u001b[0m Trial 235 finished with value: 0.6739421460419001 and parameters: {'n_estimators': 830, 'learning_rate': 0.08468388681740271, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 642}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:22:42,415]\u001b[0m Trial 236 finished with value: 0.6497799424914696 and parameters: {'n_estimators': 92, 'learning_rate': 0.07827561277072508, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 591}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:22:54,297]\u001b[0m Trial 237 finished with value: 0.6729508230350507 and parameters: {'n_estimators': 811, 'learning_rate': 0.08997101501425518, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 571}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:23:09,544]\u001b[0m Trial 238 finished with value: 0.6711983491354655 and parameters: {'n_estimators': 723, 'learning_rate': 0.10320515352528604, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 612}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:23:25,454]\u001b[0m Trial 239 finished with value: 0.6734080834297667 and parameters: {'n_estimators': 749, 'learning_rate': 0.094493491642753, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 626}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:23:34,610]\u001b[0m Trial 240 finished with value: 0.6669400439932927 and parameters: {'n_estimators': 783, 'learning_rate': 0.082537014847721, 'max_depth': 12, 'max_bin': 243, 'num_leaves': 542}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:23:49,142]\u001b[0m Trial 241 finished with value: 0.6732441987557325 and parameters: {'n_estimators': 802, 'learning_rate': 0.07657528150398449, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 562}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:24:05,583]\u001b[0m Trial 242 finished with value: 0.6744669923870035 and parameters: {'n_estimators': 793, 'learning_rate': 0.07347594274612841, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 565}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:24:16,475]\u001b[0m Trial 243 finished with value: 0.6712280549983592 and parameters: {'n_estimators': 770, 'learning_rate': 0.08720937319674138, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 581}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:24:30,895]\u001b[0m Trial 244 finished with value: 0.6691826314150958 and parameters: {'n_estimators': 820, 'learning_rate': 0.07947553590256622, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 553}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:24:45,553]\u001b[0m Trial 245 finished with value: 0.6690828733287597 and parameters: {'n_estimators': 766, 'learning_rate': 0.08373019286660813, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 604}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:24:56,280]\u001b[0m Trial 246 finished with value: 0.6626238061143399 and parameters: {'n_estimators': 790, 'learning_rate': 0.06821185638480748, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 586}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:25:06,799]\u001b[0m Trial 247 finished with value: 0.6731391595641535 and parameters: {'n_estimators': 743, 'learning_rate': 0.07554819250287789, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 538}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:25:23,754]\u001b[0m Trial 248 finished with value: 0.6728687992472336 and parameters: {'n_estimators': 802, 'learning_rate': 0.08879211388955212, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 599}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:25:34,678]\u001b[0m Trial 249 finished with value: 0.675721795300238 and parameters: {'n_estimators': 759, 'learning_rate': 0.09494425762890915, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 637}. Best is trial 153 with value: 0.6872552462516891.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6873\n",
      "\tBest params:\n",
      "\t\tn_estimators: 780\n",
      "\t\tlearning_rate: 0.08276532508888751\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 232\n",
      "\t\tnum_leaves: 632\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686631    0.637557    0.707517    0.672644   \n",
      "1                    TP   42.000000   37.000000   39.000000   34.000000   \n",
      "2                    TN  199.000000  198.000000  198.000000  195.000000   \n",
      "3                    FP    2.000000    3.000000    3.000000    5.000000   \n",
      "4                    FN   25.000000   30.000000   28.000000   34.000000   \n",
      "5              Accuracy    0.899254    0.876866    0.884328    0.854478   \n",
      "6             Precision    0.954545    0.925000    0.928571    0.871795   \n",
      "7           Sensitivity    0.626866    0.552239    0.582090    0.500000   \n",
      "8           Specificity    0.990000    0.985100    0.985100    0.975000   \n",
      "9              F1 score    0.756757    0.691589    0.715596    0.635514   \n",
      "10  F1 score (weighted)    0.891542    0.865205    0.874449    0.839676   \n",
      "11     F1 score (macro)    0.846614    0.807333    0.821498    0.772302   \n",
      "12    Balanced Accuracy    0.808458    0.768657    0.783582    0.737500   \n",
      "13                  MCC    0.721125    0.652929    0.675562    0.586156   \n",
      "14                  NPV    0.888400    0.868400    0.876100    0.851500   \n",
      "15              ROC_AUC    0.808458    0.768657    0.783582    0.737500   \n",
      "\n",
      "          Set4  \n",
      "0     0.713204  \n",
      "1    37.000000  \n",
      "2   199.000000  \n",
      "3     4.000000  \n",
      "4    28.000000  \n",
      "5     0.880597  \n",
      "6     0.902439  \n",
      "7     0.569231  \n",
      "8     0.980300  \n",
      "9     0.698113  \n",
      "10    0.870412  \n",
      "11    0.811847  \n",
      "12    0.774763  \n",
      "13    0.654317  \n",
      "14    0.876700  \n",
      "15    0.774763  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_lgbm_4)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_4_cat = np.where(((y_pred_lgbm_4 >= 2) | (y_pred_lgbm_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 22:25:53,524]\u001b[0m Trial 250 finished with value: 0.7027502729362436 and parameters: {'n_estimators': 733, 'learning_rate': 0.08119676559665331, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 560}. Best is trial 250 with value: 0.7027502729362436.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:26:03,629]\u001b[0m Trial 251 finished with value: 0.7014544415246926 and parameters: {'n_estimators': 732, 'learning_rate': 0.08601511900708012, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 576}. Best is trial 250 with value: 0.7027502729362436.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:26:14,501]\u001b[0m Trial 252 finished with value: 0.6992079361836623 and parameters: {'n_estimators': 710, 'learning_rate': 0.08118937140442721, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 576}. Best is trial 250 with value: 0.7027502729362436.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:26:28,416]\u001b[0m Trial 253 finished with value: 0.7043361904301031 and parameters: {'n_estimators': 723, 'learning_rate': 0.08632504158407418, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 579}. Best is trial 253 with value: 0.7043361904301031.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:26:38,105]\u001b[0m Trial 254 finished with value: 0.7012950450414742 and parameters: {'n_estimators': 695, 'learning_rate': 0.08402938586554712, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 577}. Best is trial 253 with value: 0.7043361904301031.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:26:51,093]\u001b[0m Trial 255 finished with value: 0.7062988243573008 and parameters: {'n_estimators': 695, 'learning_rate': 0.08418518386220832, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 578}. Best is trial 255 with value: 0.7062988243573008.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:27:00,951]\u001b[0m Trial 256 finished with value: 0.7068424294960505 and parameters: {'n_estimators': 701, 'learning_rate': 0.08489109033868533, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 575}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:27:07,844]\u001b[0m Trial 257 finished with value: 0.6861995569501249 and parameters: {'n_estimators': 680, 'learning_rate': 0.08543817879735821, 'max_depth': 5, 'max_bin': 246, 'num_leaves': 593}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:27:21,648]\u001b[0m Trial 258 finished with value: 0.7012097399091719 and parameters: {'n_estimators': 687, 'learning_rate': 0.08346066045420522, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 579}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:27:29,697]\u001b[0m Trial 259 finished with value: 0.6952015251354255 and parameters: {'n_estimators': 683, 'learning_rate': 0.08381549793478649, 'max_depth': 7, 'max_bin': 245, 'num_leaves': 576}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:27:42,465]\u001b[0m Trial 260 finished with value: 0.6999747368572582 and parameters: {'n_estimators': 676, 'learning_rate': 0.08457937120110047, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 547}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:27:49,256]\u001b[0m Trial 261 finished with value: 0.6868709989372845 and parameters: {'n_estimators': 689, 'learning_rate': 0.08566108091346165, 'max_depth': 5, 'max_bin': 247, 'num_leaves': 555}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:27:58,611]\u001b[0m Trial 262 finished with value: 0.6928244690021106 and parameters: {'n_estimators': 682, 'learning_rate': 0.08421123803480327, 'max_depth': 7, 'max_bin': 246, 'num_leaves': 549}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:28:07,888]\u001b[0m Trial 263 finished with value: 0.6860306301396724 and parameters: {'n_estimators': 682, 'learning_rate': 0.08378853601333068, 'max_depth': 5, 'max_bin': 246, 'num_leaves': 530}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:28:15,996]\u001b[0m Trial 264 finished with value: 0.695077549203774 and parameters: {'n_estimators': 673, 'learning_rate': 0.08344768621032483, 'max_depth': 7, 'max_bin': 246, 'num_leaves': 526}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:28:22,982]\u001b[0m Trial 265 finished with value: 0.6849066897709766 and parameters: {'n_estimators': 677, 'learning_rate': 0.08293139454393243, 'max_depth': 5, 'max_bin': 246, 'num_leaves': 533}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:28:34,485]\u001b[0m Trial 266 finished with value: 0.6885916776726553 and parameters: {'n_estimators': 679, 'learning_rate': 0.08312773476606418, 'max_depth': 5, 'max_bin': 244, 'num_leaves': 525}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:28:45,209]\u001b[0m Trial 267 finished with value: 0.6904517359014176 and parameters: {'n_estimators': 675, 'learning_rate': 0.08228890246762295, 'max_depth': 5, 'max_bin': 245, 'num_leaves': 529}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:28:53,468]\u001b[0m Trial 268 finished with value: 0.687487767558909 and parameters: {'n_estimators': 663, 'learning_rate': 0.0827713467651363, 'max_depth': 5, 'max_bin': 243, 'num_leaves': 527}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:28:59,804]\u001b[0m Trial 269 finished with value: 0.6865343528680846 and parameters: {'n_estimators': 668, 'learning_rate': 0.08231763062965063, 'max_depth': 5, 'max_bin': 243, 'num_leaves': 515}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:29:08,435]\u001b[0m Trial 270 finished with value: 0.6862928666591188 and parameters: {'n_estimators': 659, 'learning_rate': 0.08207390751881938, 'max_depth': 5, 'max_bin': 243, 'num_leaves': 492}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:29:18,400]\u001b[0m Trial 271 finished with value: 0.68557089472765 and parameters: {'n_estimators': 662, 'learning_rate': 0.08149392878329387, 'max_depth': 5, 'max_bin': 243, 'num_leaves': 504}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:29:26,255]\u001b[0m Trial 272 finished with value: 0.6898722907280018 and parameters: {'n_estimators': 688, 'learning_rate': 0.08277791586471679, 'max_depth': 5, 'max_bin': 246, 'num_leaves': 523}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:29:33,535]\u001b[0m Trial 273 finished with value: 0.6894552829485332 and parameters: {'n_estimators': 640, 'learning_rate': 0.07363050752414688, 'max_depth': 5, 'max_bin': 243, 'num_leaves': 526}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:29:42,735]\u001b[0m Trial 274 finished with value: 0.686446374412318 and parameters: {'n_estimators': 634, 'learning_rate': 0.0756755259596417, 'max_depth': 5, 'max_bin': 244, 'num_leaves': 492}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:29:53,702]\u001b[0m Trial 275 finished with value: 0.6914423362918086 and parameters: {'n_estimators': 649, 'learning_rate': 0.07294492680968123, 'max_depth': 5, 'max_bin': 244, 'num_leaves': 496}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:30:01,545]\u001b[0m Trial 276 finished with value: 0.6906018157700216 and parameters: {'n_estimators': 628, 'learning_rate': 0.07142978380509632, 'max_depth': 5, 'max_bin': 243, 'num_leaves': 470}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:30:10,132]\u001b[0m Trial 277 finished with value: 0.6871771399945256 and parameters: {'n_estimators': 641, 'learning_rate': 0.06732596797936669, 'max_depth': 5, 'max_bin': 243, 'num_leaves': 487}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:30:20,784]\u001b[0m Trial 278 finished with value: 0.6936682921113786 and parameters: {'n_estimators': 636, 'learning_rate': 0.06832328693497008, 'max_depth': 7, 'max_bin': 243, 'num_leaves': 488}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:30:34,644]\u001b[0m Trial 279 finished with value: 0.6997479970716266 and parameters: {'n_estimators': 646, 'learning_rate': 0.06647274627176865, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 473}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:30:44,239]\u001b[0m Trial 280 finished with value: 0.6966013231568946 and parameters: {'n_estimators': 641, 'learning_rate': 0.06661852096598093, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 463}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 22:30:53,888]\u001b[0m Trial 281 finished with value: 0.7000206471738574 and parameters: {'n_estimators': 619, 'learning_rate': 0.06447356360909914, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 468}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:31:04,333]\u001b[0m Trial 282 finished with value: 0.6971005397651833 and parameters: {'n_estimators': 622, 'learning_rate': 0.057787711127494396, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 470}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:31:16,596]\u001b[0m Trial 283 finished with value: 0.700366852219874 and parameters: {'n_estimators': 619, 'learning_rate': 0.06150122704127495, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 456}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:31:29,244]\u001b[0m Trial 284 finished with value: 0.6959255738651501 and parameters: {'n_estimators': 624, 'learning_rate': 0.05864445902729336, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 468}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:31:39,840]\u001b[0m Trial 285 finished with value: 0.696074367553926 and parameters: {'n_estimators': 613, 'learning_rate': 0.05860563437944406, 'max_depth': 7, 'max_bin': 246, 'num_leaves': 466}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:31:50,201]\u001b[0m Trial 286 finished with value: 0.6936804280024177 and parameters: {'n_estimators': 626, 'learning_rate': 0.057858810769439085, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 449}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:32:04,134]\u001b[0m Trial 287 finished with value: 0.695082265368044 and parameters: {'n_estimators': 615, 'learning_rate': 0.057295402419450435, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 454}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:32:13,037]\u001b[0m Trial 288 finished with value: 0.6964415135295478 and parameters: {'n_estimators': 613, 'learning_rate': 0.05832790174925253, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 439}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:32:26,556]\u001b[0m Trial 289 finished with value: 0.6984581838606777 and parameters: {'n_estimators': 619, 'learning_rate': 0.057314722868218294, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 453}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:32:36,478]\u001b[0m Trial 290 finished with value: 0.6987362461452115 and parameters: {'n_estimators': 614, 'learning_rate': 0.056471627250760735, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 428}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:32:50,492]\u001b[0m Trial 291 finished with value: 0.6967907530842637 and parameters: {'n_estimators': 618, 'learning_rate': 0.05729835011366862, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 433}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:32:58,528]\u001b[0m Trial 292 finished with value: 0.6993349299493057 and parameters: {'n_estimators': 617, 'learning_rate': 0.05887179967433067, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 450}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:33:13,213]\u001b[0m Trial 293 finished with value: 0.6967311349099707 and parameters: {'n_estimators': 611, 'learning_rate': 0.056321222901714374, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 427}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:33:25,728]\u001b[0m Trial 294 finished with value: 0.6990914154900387 and parameters: {'n_estimators': 601, 'learning_rate': 0.05160924074874825, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 432}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:33:40,615]\u001b[0m Trial 295 finished with value: 0.6917426293177769 and parameters: {'n_estimators': 604, 'learning_rate': 0.050691601720564736, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 421}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:33:49,509]\u001b[0m Trial 296 finished with value: 0.6921328871919405 and parameters: {'n_estimators': 595, 'learning_rate': 0.05392666721194367, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 430}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:34:02,576]\u001b[0m Trial 297 finished with value: 0.7006209269032722 and parameters: {'n_estimators': 616, 'learning_rate': 0.05970753416685606, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 404}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:34:15,194]\u001b[0m Trial 298 finished with value: 0.6972700939349312 and parameters: {'n_estimators': 609, 'learning_rate': 0.04619725077657657, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 396}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:34:31,940]\u001b[0m Trial 299 finished with value: 0.6988507269721048 and parameters: {'n_estimators': 614, 'learning_rate': 0.04553962669000312, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 440}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7068\n",
      "\tBest params:\n",
      "\t\tn_estimators: 701\n",
      "\t\tlearning_rate: 0.08489109033868533\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 246\n",
      "\t\tnum_leaves: 575\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686631    0.637557    0.707517    0.672644   \n",
      "1                    TP   42.000000   37.000000   39.000000   34.000000   \n",
      "2                    TN  199.000000  198.000000  198.000000  195.000000   \n",
      "3                    FP    2.000000    3.000000    3.000000    5.000000   \n",
      "4                    FN   25.000000   30.000000   28.000000   34.000000   \n",
      "5              Accuracy    0.899254    0.876866    0.884328    0.854478   \n",
      "6             Precision    0.954545    0.925000    0.928571    0.871795   \n",
      "7           Sensitivity    0.626866    0.552239    0.582090    0.500000   \n",
      "8           Specificity    0.990000    0.985100    0.985100    0.975000   \n",
      "9              F1 score    0.756757    0.691589    0.715596    0.635514   \n",
      "10  F1 score (weighted)    0.891542    0.865205    0.874449    0.839676   \n",
      "11     F1 score (macro)    0.846614    0.807333    0.821498    0.772302   \n",
      "12    Balanced Accuracy    0.808458    0.768657    0.783582    0.737500   \n",
      "13                  MCC    0.721125    0.652929    0.675562    0.586156   \n",
      "14                  NPV    0.888400    0.868400    0.876100    0.851500   \n",
      "15              ROC_AUC    0.808458    0.768657    0.783582    0.737500   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.713204    0.640578  \n",
      "1    37.000000   30.000000  \n",
      "2   199.000000  200.000000  \n",
      "3     4.000000    1.000000  \n",
      "4    28.000000   37.000000  \n",
      "5     0.880597    0.858209  \n",
      "6     0.902439    0.967742  \n",
      "7     0.569231    0.447761  \n",
      "8     0.980300    0.995000  \n",
      "9     0.698113    0.612245  \n",
      "10    0.870412    0.837993  \n",
      "11    0.811847    0.762743  \n",
      "12    0.774763    0.721393  \n",
      "13    0.654317    0.599480  \n",
      "14    0.876700    0.843900  \n",
      "15    0.774763    0.721393  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_lgbm_5)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_5_cat = np.where(((y_pred_lgbm_5 >= 2) | (y_pred_lgbm_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 22:34:43,894]\u001b[0m Trial 300 finished with value: 0.6532547113681697 and parameters: {'n_estimators': 578, 'learning_rate': 0.045403953254718535, 'max_depth': 8, 'max_bin': 250, 'num_leaves': 394}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:34:50,928]\u001b[0m Trial 301 finished with value: 0.6461960028635751 and parameters: {'n_estimators': 608, 'learning_rate': 0.04894661521683459, 'max_depth': 6, 'max_bin': 248, 'num_leaves': 408}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:35:04,003]\u001b[0m Trial 302 finished with value: 0.6551955972524688 and parameters: {'n_estimators': 588, 'learning_rate': 0.06247274291162796, 'max_depth': 7, 'max_bin': 251, 'num_leaves': 438}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:35:16,330]\u001b[0m Trial 303 finished with value: 0.6573713957820153 and parameters: {'n_estimators': 614, 'learning_rate': 0.05199268780565596, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 442}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:35:26,468]\u001b[0m Trial 304 finished with value: 0.6476415398352499 and parameters: {'n_estimators': 598, 'learning_rate': 0.03815327609997676, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 426}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:35:35,223]\u001b[0m Trial 305 finished with value: 0.6547166617609331 and parameters: {'n_estimators': 622, 'learning_rate': 0.06119530662097745, 'max_depth': 7, 'max_bin': 252, 'num_leaves': 380}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:35:43,599]\u001b[0m Trial 306 finished with value: 0.6538445316925289 and parameters: {'n_estimators': 562, 'learning_rate': 0.054862166873859745, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 463}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:35:53,894]\u001b[0m Trial 307 finished with value: 0.6523613202400711 and parameters: {'n_estimators': 615, 'learning_rate': 0.046473972004105274, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 412}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:36:09,408]\u001b[0m Trial 308 finished with value: 0.657539418628079 and parameters: {'n_estimators': 649, 'learning_rate': 0.06063272816156727, 'max_depth': 7, 'max_bin': 240, 'num_leaves': 431}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:36:17,973]\u001b[0m Trial 309 finished with value: 0.6551711343707989 and parameters: {'n_estimators': 592, 'learning_rate': 0.05592604142638025, 'max_depth': 7, 'max_bin': 251, 'num_leaves': 455}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:36:27,514]\u001b[0m Trial 310 finished with value: 0.6550103793039678 and parameters: {'n_estimators': 609, 'learning_rate': 0.0644944347888389, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 397}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:36:42,276]\u001b[0m Trial 311 finished with value: 0.6530049892522275 and parameters: {'n_estimators': 642, 'learning_rate': 0.04223257417143108, 'max_depth': 8, 'max_bin': 241, 'num_leaves': 475}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:36:51,093]\u001b[0m Trial 312 finished with value: 0.6475380003539144 and parameters: {'n_estimators': 623, 'learning_rate': 0.050801058638386014, 'max_depth': 6, 'max_bin': 252, 'num_leaves': 446}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:37:04,215]\u001b[0m Trial 313 finished with value: 0.6587888878266894 and parameters: {'n_estimators': 589, 'learning_rate': 0.054566943046986, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 417}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:37:11,764]\u001b[0m Trial 314 finished with value: 0.6550479993051221 and parameters: {'n_estimators': 700, 'learning_rate': 0.059843332966495404, 'max_depth': 7, 'max_bin': 246, 'num_leaves': 463}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:37:23,435]\u001b[0m Trial 315 finished with value: 0.6562806081265296 and parameters: {'n_estimators': 644, 'learning_rate': 0.06246114835365041, 'max_depth': 7, 'max_bin': 251, 'num_leaves': 381}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:37:31,590]\u001b[0m Trial 316 finished with value: 0.6561623553607966 and parameters: {'n_estimators': 578, 'learning_rate': 0.056963847955271533, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 438}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:37:44,386]\u001b[0m Trial 317 finished with value: 0.6575754640666098 and parameters: {'n_estimators': 601, 'learning_rate': 0.048674859008227, 'max_depth': 8, 'max_bin': 239, 'num_leaves': 423}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:37:54,534]\u001b[0m Trial 318 finished with value: 0.6571848966143113 and parameters: {'n_estimators': 629, 'learning_rate': 0.06456779311492108, 'max_depth': 7, 'max_bin': 252, 'num_leaves': 443}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:38:02,376]\u001b[0m Trial 319 finished with value: 0.6506168210767085 and parameters: {'n_estimators': 653, 'learning_rate': 0.059635520466234236, 'max_depth': 6, 'max_bin': 241, 'num_leaves': 407}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:38:16,648]\u001b[0m Trial 320 finished with value: 0.6573551649390493 and parameters: {'n_estimators': 611, 'learning_rate': 0.054422937719380005, 'max_depth': 7, 'max_bin': 246, 'num_leaves': 459}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:38:26,227]\u001b[0m Trial 321 finished with value: 0.6511122019605892 and parameters: {'n_estimators': 703, 'learning_rate': 0.045351870382820285, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 430}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:38:39,730]\u001b[0m Trial 322 finished with value: 0.6580514027806652 and parameters: {'n_estimators': 625, 'learning_rate': 0.06520019199074595, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 472}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:38:48,990]\u001b[0m Trial 323 finished with value: 0.6542116382348784 and parameters: {'n_estimators': 545, 'learning_rate': 0.04946386573639057, 'max_depth': 7, 'max_bin': 245, 'num_leaves': 446}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:38:59,906]\u001b[0m Trial 324 finished with value: 0.6462709455321821 and parameters: {'n_estimators': 571, 'learning_rate': 0.05337728999350734, 'max_depth': 6, 'max_bin': 253, 'num_leaves': 474}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:39:10,313]\u001b[0m Trial 325 finished with value: 0.6569286487407568 and parameters: {'n_estimators': 601, 'learning_rate': 0.05826297746297583, 'max_depth': 7, 'max_bin': 242, 'num_leaves': 362}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:39:17,914]\u001b[0m Trial 326 finished with value: 0.6556135066279019 and parameters: {'n_estimators': 651, 'learning_rate': 0.06194847814756025, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 398}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:39:32,029]\u001b[0m Trial 327 finished with value: 0.660213314538393 and parameters: {'n_estimators': 638, 'learning_rate': 0.05238290552567629, 'max_depth': 8, 'max_bin': 245, 'num_leaves': 415}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:39:39,747]\u001b[0m Trial 328 finished with value: 0.6570349759437187 and parameters: {'n_estimators': 699, 'learning_rate': 0.043182962557788945, 'max_depth': 7, 'max_bin': 253, 'num_leaves': 435}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:39:49,959]\u001b[0m Trial 329 finished with value: 0.6541286168946565 and parameters: {'n_estimators': 619, 'learning_rate': 0.05788859141757702, 'max_depth': 7, 'max_bin': 241, 'num_leaves': 458}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:39:54,033]\u001b[0m Trial 330 finished with value: 0.6544480664700025 and parameters: {'n_estimators': 584, 'learning_rate': 0.06602521612038219, 'max_depth': 7, 'max_bin': 250, 'num_leaves': 448}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 22:40:04,347]\u001b[0m Trial 331 finished with value: 0.6561816779617251 and parameters: {'n_estimators': 602, 'learning_rate': 0.06207384347651529, 'max_depth': 7, 'max_bin': 246, 'num_leaves': 423}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:40:11,061]\u001b[0m Trial 332 finished with value: 0.6532214204671243 and parameters: {'n_estimators': 662, 'learning_rate': 0.051332430450651424, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 478}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:40:16,752]\u001b[0m Trial 333 finished with value: 0.6566646866270982 and parameters: {'n_estimators': 633, 'learning_rate': 0.05603915857613402, 'max_depth': 7, 'max_bin': 157, 'num_leaves': 334}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:40:25,158]\u001b[0m Trial 334 finished with value: 0.6527310067305778 and parameters: {'n_estimators': 619, 'learning_rate': 0.04692140482122665, 'max_depth': 7, 'max_bin': 245, 'num_leaves': 386}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:40:30,626]\u001b[0m Trial 335 finished with value: 0.6509815665028739 and parameters: {'n_estimators': 707, 'learning_rate': 0.06104955623056292, 'max_depth': 6, 'max_bin': 252, 'num_leaves': 466}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:40:43,184]\u001b[0m Trial 336 finished with value: 0.6453858778446822 and parameters: {'n_estimators': 657, 'learning_rate': 0.03183945520786781, 'max_depth': 7, 'max_bin': 241, 'num_leaves': 440}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:40:56,628]\u001b[0m Trial 337 finished with value: 0.6531499629808397 and parameters: {'n_estimators': 604, 'learning_rate': 0.04038435894965454, 'max_depth': 8, 'max_bin': 248, 'num_leaves': 405}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:41:02,754]\u001b[0m Trial 338 finished with value: 0.6545211269774862 and parameters: {'n_estimators': 640, 'learning_rate': 0.0667737205726035, 'max_depth': 7, 'max_bin': 244, 'num_leaves': 453}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:41:08,197]\u001b[0m Trial 339 finished with value: 0.6549787428838006 and parameters: {'n_estimators': 562, 'learning_rate': 0.05592777194403065, 'max_depth': 7, 'max_bin': 251, 'num_leaves': 426}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:41:15,239]\u001b[0m Trial 340 finished with value: 0.6534914089988387 and parameters: {'n_estimators': 619, 'learning_rate': 0.0633595263456866, 'max_depth': 7, 'max_bin': 246, 'num_leaves': 461}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:41:21,822]\u001b[0m Trial 341 finished with value: 0.6606504715685907 and parameters: {'n_estimators': 587, 'learning_rate': 0.05892360836187116, 'max_depth': 7, 'max_bin': 239, 'num_leaves': 480}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:41:28,453]\u001b[0m Trial 342 finished with value: 0.6566008773242203 and parameters: {'n_estimators': 701, 'learning_rate': 0.050539653807838, 'max_depth': 7, 'max_bin': 250, 'num_leaves': 428}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:41:36,224]\u001b[0m Trial 343 finished with value: 0.6522600365518132 and parameters: {'n_estimators': 633, 'learning_rate': 0.054155081539818586, 'max_depth': 7, 'max_bin': 243, 'num_leaves': 407}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:41:42,524]\u001b[0m Trial 344 finished with value: 0.6536901327274988 and parameters: {'n_estimators': 660, 'learning_rate': 0.06553076483185595, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 441}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:41:51,763]\u001b[0m Trial 345 finished with value: 0.6544271995494824 and parameters: {'n_estimators': 608, 'learning_rate': 0.04723188120140058, 'max_depth': 8, 'max_bin': 252, 'num_leaves': 450}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:41:59,486]\u001b[0m Trial 346 finished with value: 0.6506683195242067 and parameters: {'n_estimators': 648, 'learning_rate': 0.05959136584083269, 'max_depth': 6, 'max_bin': 245, 'num_leaves': 467}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:42:03,756]\u001b[0m Trial 347 finished with value: 0.6498578856982469 and parameters: {'n_estimators': 714, 'learning_rate': 0.15194147031628485, 'max_depth': 7, 'max_bin': 253, 'num_leaves': 419}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:42:12,263]\u001b[0m Trial 348 finished with value: 0.6537117433121787 and parameters: {'n_estimators': 590, 'learning_rate': 0.05465126510310223, 'max_depth': 7, 'max_bin': 241, 'num_leaves': 431}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:42:17,894]\u001b[0m Trial 349 finished with value: 0.657467017876905 and parameters: {'n_estimators': 627, 'learning_rate': 0.06801924687676537, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 376}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.706842\n",
      "\tBest params:\n",
      "\t\tn_estimators: 701\n",
      "\t\tlearning_rate: 0.08489109033868533\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 246\n",
      "\t\tnum_leaves: 575\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686631    0.637557    0.707517    0.672644   \n",
      "1                    TP   42.000000   37.000000   39.000000   34.000000   \n",
      "2                    TN  199.000000  198.000000  198.000000  195.000000   \n",
      "3                    FP    2.000000    3.000000    3.000000    5.000000   \n",
      "4                    FN   25.000000   30.000000   28.000000   34.000000   \n",
      "5              Accuracy    0.899254    0.876866    0.884328    0.854478   \n",
      "6             Precision    0.954545    0.925000    0.928571    0.871795   \n",
      "7           Sensitivity    0.626866    0.552239    0.582090    0.500000   \n",
      "8           Specificity    0.990000    0.985100    0.985100    0.975000   \n",
      "9              F1 score    0.756757    0.691589    0.715596    0.635514   \n",
      "10  F1 score (weighted)    0.891542    0.865205    0.874449    0.839676   \n",
      "11     F1 score (macro)    0.846614    0.807333    0.821498    0.772302   \n",
      "12    Balanced Accuracy    0.808458    0.768657    0.783582    0.737500   \n",
      "13                  MCC    0.721125    0.652929    0.675562    0.586156   \n",
      "14                  NPV    0.888400    0.868400    0.876100    0.851500   \n",
      "15              ROC_AUC    0.808458    0.768657    0.783582    0.737500   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.713204    0.640578    0.702467  \n",
      "1    37.000000   30.000000   35.000000  \n",
      "2   199.000000  200.000000  200.000000  \n",
      "3     4.000000    1.000000    2.000000  \n",
      "4    28.000000   37.000000   31.000000  \n",
      "5     0.880597    0.858209    0.876866  \n",
      "6     0.902439    0.967742    0.945946  \n",
      "7     0.569231    0.447761    0.530303  \n",
      "8     0.980300    0.995000    0.990100  \n",
      "9     0.698113    0.612245    0.679612  \n",
      "10    0.870412    0.837993    0.863655  \n",
      "11    0.811847    0.762743    0.801700  \n",
      "12    0.774763    0.721393    0.760201  \n",
      "13    0.654317    0.599480    0.649950  \n",
      "14    0.876700    0.843900    0.865800  \n",
      "15    0.774763    0.721393    0.760201  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_lgbm_6)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_6_cat = np.where(((y_pred_lgbm_6 >= 2) | (y_pred_lgbm_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 22:42:26,477]\u001b[0m Trial 350 finished with value: 0.6556291139010388 and parameters: {'n_estimators': 669, 'learning_rate': 0.03764507281013106, 'max_depth': 7, 'max_bin': 245, 'num_leaves': 393}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:42:34,152]\u001b[0m Trial 351 finished with value: 0.6591841800369367 and parameters: {'n_estimators': 693, 'learning_rate': 0.059117705114525326, 'max_depth': 7, 'max_bin': 250, 'num_leaves': 453}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:42:40,412]\u001b[0m Trial 352 finished with value: 0.6592236785608953 and parameters: {'n_estimators': 613, 'learning_rate': 0.063334813525141, 'max_depth': 7, 'max_bin': 242, 'num_leaves': 483}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:42:47,270]\u001b[0m Trial 353 finished with value: 0.6588777318079575 and parameters: {'n_estimators': 570, 'learning_rate': 0.05067879574007252, 'max_depth': 7, 'max_bin': 238, 'num_leaves': 410}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:42:53,713]\u001b[0m Trial 354 finished with value: 0.6592104146317047 and parameters: {'n_estimators': 640, 'learning_rate': 0.06970742093242628, 'max_depth': 8, 'max_bin': 247, 'num_leaves': 439}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:42:59,311]\u001b[0m Trial 355 finished with value: 0.6594148973986593 and parameters: {'n_estimators': 600, 'learning_rate': 0.05586579063968646, 'max_depth': 7, 'max_bin': 253, 'num_leaves': 463}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:43:06,312]\u001b[0m Trial 356 finished with value: 0.6556549506653182 and parameters: {'n_estimators': 627, 'learning_rate': 0.046409035648725123, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 439}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:43:12,352]\u001b[0m Trial 357 finished with value: 0.659472901482631 and parameters: {'n_estimators': 713, 'learning_rate': 0.06255385713344558, 'max_depth': 7, 'max_bin': 245, 'num_leaves': 41}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:43:17,663]\u001b[0m Trial 358 finished with value: 0.6529142423777986 and parameters: {'n_estimators': 653, 'learning_rate': 0.053110582388876976, 'max_depth': 6, 'max_bin': 243, 'num_leaves': 475}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:43:24,622]\u001b[0m Trial 359 finished with value: 0.661772483963556 and parameters: {'n_estimators': 669, 'learning_rate': 0.05919095142345316, 'max_depth': 7, 'max_bin': 250, 'num_leaves': 419}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:43:31,513]\u001b[0m Trial 360 finished with value: 0.6583544596560263 and parameters: {'n_estimators': 695, 'learning_rate': 0.04142554727557134, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 452}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:43:35,581]\u001b[0m Trial 361 finished with value: 0.6631320271354653 and parameters: {'n_estimators': 615, 'learning_rate': 0.06621711129984396, 'max_depth': 7, 'max_bin': 253, 'num_leaves': 107}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:43:42,372]\u001b[0m Trial 362 finished with value: 0.6586861559372664 and parameters: {'n_estimators': 584, 'learning_rate': 0.05652636023018554, 'max_depth': 7, 'max_bin': 245, 'num_leaves': 498}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:43:50,999]\u001b[0m Trial 363 finished with value: 0.6578905158431064 and parameters: {'n_estimators': 597, 'learning_rate': 0.048903532016270665, 'max_depth': 7, 'max_bin': 240, 'num_leaves': 430}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:43:59,362]\u001b[0m Trial 364 finished with value: 0.6633380755867033 and parameters: {'n_estimators': 633, 'learning_rate': 0.06030141710966201, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 465}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:44:04,600]\u001b[0m Trial 365 finished with value: 0.648927847974949 and parameters: {'n_estimators': 612, 'learning_rate': 0.052320100430852445, 'max_depth': 6, 'max_bin': 251, 'num_leaves': 398}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:44:09,628]\u001b[0m Trial 366 finished with value: 0.6621830628975773 and parameters: {'n_estimators': 650, 'learning_rate': 0.06857843468208398, 'max_depth': 7, 'max_bin': 243, 'num_leaves': 451}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:44:17,030]\u001b[0m Trial 367 finished with value: 0.6653495687452043 and parameters: {'n_estimators': 695, 'learning_rate': 0.0640149542546975, 'max_depth': 8, 'max_bin': 254, 'num_leaves': 419}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:44:20,508]\u001b[0m Trial 368 finished with value: 0.6546498957977844 and parameters: {'n_estimators': 531, 'learning_rate': 0.16999644263335195, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 438}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:44:30,077]\u001b[0m Trial 369 finished with value: 0.6559760907472609 and parameters: {'n_estimators': 675, 'learning_rate': 0.04398105706345589, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 473}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:44:35,661]\u001b[0m Trial 370 finished with value: 0.6566606344504355 and parameters: {'n_estimators': 626, 'learning_rate': 0.058265983162467935, 'max_depth': 7, 'max_bin': 184, 'num_leaves': 487}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:44:41,378]\u001b[0m Trial 371 finished with value: 0.6629507956216552 and parameters: {'n_estimators': 713, 'learning_rate': 0.054421501905851474, 'max_depth': 7, 'max_bin': 242, 'num_leaves': 407}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:44:47,468]\u001b[0m Trial 372 finished with value: 0.6611598034084197 and parameters: {'n_estimators': 574, 'learning_rate': 0.07000698074488881, 'max_depth': 7, 'max_bin': 245, 'num_leaves': 451}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:44:52,449]\u001b[0m Trial 373 finished with value: 0.6633734970030296 and parameters: {'n_estimators': 650, 'learning_rate': 0.06281799216206177, 'max_depth': 7, 'max_bin': 251, 'num_leaves': 431}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:45:01,371]\u001b[0m Trial 374 finished with value: 0.6566371331885522 and parameters: {'n_estimators': 589, 'learning_rate': 0.04773221431047273, 'max_depth': 7, 'max_bin': 238, 'num_leaves': 461}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:45:07,042]\u001b[0m Trial 375 finished with value: 0.6628610770944151 and parameters: {'n_estimators': 607, 'learning_rate': 0.05229362764609511, 'max_depth': 8, 'max_bin': 247, 'num_leaves': 390}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:45:12,689]\u001b[0m Trial 376 finished with value: 0.6543093280869468 and parameters: {'n_estimators': 673, 'learning_rate': 0.057380396818591105, 'max_depth': 6, 'max_bin': 254, 'num_leaves': 216}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:45:20,073]\u001b[0m Trial 377 finished with value: 0.663906347487505 and parameters: {'n_estimators': 640, 'learning_rate': 0.0650852111630084, 'max_depth': 7, 'max_bin': 244, 'num_leaves': 440}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:45:25,711]\u001b[0m Trial 378 finished with value: 0.6646898106329608 and parameters: {'n_estimators': 614, 'learning_rate': 0.06032072190814551, 'max_depth': 7, 'max_bin': 250, 'num_leaves': 481}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:45:33,248]\u001b[0m Trial 379 finished with value: 0.6564784010459804 and parameters: {'n_estimators': 691, 'learning_rate': 0.07008412929368543, 'max_depth': 7, 'max_bin': 241, 'num_leaves': 415}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:45:36,724]\u001b[0m Trial 380 finished with value: 0.6560794824990988 and parameters: {'n_estimators': 627, 'learning_rate': 0.056730426679234426, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 460}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 22:45:45,673]\u001b[0m Trial 381 finished with value: 0.6650855599440224 and parameters: {'n_estimators': 548, 'learning_rate': 0.035253477670534716, 'max_depth': 9, 'max_bin': 250, 'num_leaves': 498}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:45:52,376]\u001b[0m Trial 382 finished with value: 0.6299181977758006 and parameters: {'n_estimators': 724, 'learning_rate': 0.050467470102115566, 'max_depth': 3, 'max_bin': 169, 'num_leaves': 445}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:45:56,542]\u001b[0m Trial 383 finished with value: 0.6602787237894374 and parameters: {'n_estimators': 598, 'learning_rate': 0.06203491236201535, 'max_depth': 7, 'max_bin': 254, 'num_leaves': 369}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:46:05,114]\u001b[0m Trial 384 finished with value: 0.6553156653102847 and parameters: {'n_estimators': 657, 'learning_rate': 0.04552262535433253, 'max_depth': 7, 'max_bin': 244, 'num_leaves': 292}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:46:10,895]\u001b[0m Trial 385 finished with value: 0.6604614410688555 and parameters: {'n_estimators': 636, 'learning_rate': 0.05477382212901318, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 424}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:46:29,690]\u001b[0m Trial 386 finished with value: 0.6417737775778948 and parameters: {'n_estimators': 707, 'learning_rate': 0.006303666318056109, 'max_depth': 7, 'max_bin': 240, 'num_leaves': 474}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:46:39,555]\u001b[0m Trial 387 finished with value: 0.6627051361242386 and parameters: {'n_estimators': 668, 'learning_rate': 0.06722956367487769, 'max_depth': 7, 'max_bin': 251, 'num_leaves': 399}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:46:46,961]\u001b[0m Trial 388 finished with value: 0.6598258661834684 and parameters: {'n_estimators': 617, 'learning_rate': 0.05780740090139251, 'max_depth': 7, 'max_bin': 244, 'num_leaves': 433}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:46:51,686]\u001b[0m Trial 389 finished with value: 0.6638957262021117 and parameters: {'n_estimators': 580, 'learning_rate': 0.0628620235485775, 'max_depth': 8, 'max_bin': 248, 'num_leaves': 453}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:47:00,755]\u001b[0m Trial 390 finished with value: 0.6559988804965328 and parameters: {'n_estimators': 685, 'learning_rate': 0.04977425285473992, 'max_depth': 7, 'max_bin': 254, 'num_leaves': 419}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:47:03,779]\u001b[0m Trial 391 finished with value: 0.653196235485527 and parameters: {'n_estimators': 639, 'learning_rate': 0.05380362831761272, 'max_depth': 6, 'max_bin': 246, 'num_leaves': 463}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:47:11,791]\u001b[0m Trial 392 finished with value: 0.6616573286287412 and parameters: {'n_estimators': 600, 'learning_rate': 0.07188523021536537, 'max_depth': 7, 'max_bin': 237, 'num_leaves': 445}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:47:18,776]\u001b[0m Trial 393 finished with value: 0.6647665590153317 and parameters: {'n_estimators': 718, 'learning_rate': 0.061042770932342706, 'max_depth': 7, 'max_bin': 242, 'num_leaves': 486}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:47:25,779]\u001b[0m Trial 394 finished with value: 0.6630745094404247 and parameters: {'n_estimators': 617, 'learning_rate': 0.06595919943458202, 'max_depth': 7, 'max_bin': 251, 'num_leaves': 411}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:47:34,117]\u001b[0m Trial 395 finished with value: 0.6576437622042854 and parameters: {'n_estimators': 660, 'learning_rate': 0.04216512948100512, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 433}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:47:40,622]\u001b[0m Trial 396 finished with value: 0.6566840293399588 and parameters: {'n_estimators': 598, 'learning_rate': 0.05695435594267066, 'max_depth': 7, 'max_bin': 177, 'num_leaves': 467}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:47:47,197]\u001b[0m Trial 397 finished with value: 0.6598576622882393 and parameters: {'n_estimators': 624, 'learning_rate': 0.048732679773205334, 'max_depth': 8, 'max_bin': 246, 'num_leaves': 569}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:47:54,421]\u001b[0m Trial 398 finished with value: 0.6407246785916862 and parameters: {'n_estimators': 567, 'learning_rate': 0.053163250830216835, 'max_depth': 4, 'max_bin': 280, 'num_leaves': 451}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:48:04,232]\u001b[0m Trial 399 finished with value: 0.6534160489685117 and parameters: {'n_estimators': 697, 'learning_rate': 0.02818374763854088, 'max_depth': 7, 'max_bin': 243, 'num_leaves': 431}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7068424\n",
      "\tBest params:\n",
      "\t\tn_estimators: 701\n",
      "\t\tlearning_rate: 0.08489109033868533\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 246\n",
      "\t\tnum_leaves: 575\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686631    0.637557    0.707517    0.672644   \n",
      "1                    TP   42.000000   37.000000   39.000000   34.000000   \n",
      "2                    TN  199.000000  198.000000  198.000000  195.000000   \n",
      "3                    FP    2.000000    3.000000    3.000000    5.000000   \n",
      "4                    FN   25.000000   30.000000   28.000000   34.000000   \n",
      "5              Accuracy    0.899254    0.876866    0.884328    0.854478   \n",
      "6             Precision    0.954545    0.925000    0.928571    0.871795   \n",
      "7           Sensitivity    0.626866    0.552239    0.582090    0.500000   \n",
      "8           Specificity    0.990000    0.985100    0.985100    0.975000   \n",
      "9              F1 score    0.756757    0.691589    0.715596    0.635514   \n",
      "10  F1 score (weighted)    0.891542    0.865205    0.874449    0.839676   \n",
      "11     F1 score (macro)    0.846614    0.807333    0.821498    0.772302   \n",
      "12    Balanced Accuracy    0.808458    0.768657    0.783582    0.737500   \n",
      "13                  MCC    0.721125    0.652929    0.675562    0.586156   \n",
      "14                  NPV    0.888400    0.868400    0.876100    0.851500   \n",
      "15              ROC_AUC    0.808458    0.768657    0.783582    0.737500   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.713204    0.640578    0.702467    0.673087  \n",
      "1    37.000000   30.000000   35.000000   40.000000  \n",
      "2   199.000000  200.000000  200.000000  201.000000  \n",
      "3     4.000000    1.000000    2.000000    2.000000  \n",
      "4    28.000000   37.000000   31.000000   25.000000  \n",
      "5     0.880597    0.858209    0.876866    0.899254  \n",
      "6     0.902439    0.967742    0.945946    0.952381  \n",
      "7     0.569231    0.447761    0.530303    0.615385  \n",
      "8     0.980300    0.995000    0.990100    0.990100  \n",
      "9     0.698113    0.612245    0.679612    0.747664  \n",
      "10    0.870412    0.837993    0.863655    0.891127  \n",
      "11    0.811847    0.762743    0.801700    0.842363  \n",
      "12    0.774763    0.721393    0.760201    0.802766  \n",
      "13    0.654317    0.599480    0.649950    0.713942  \n",
      "14    0.876700    0.843900    0.865800    0.889400  \n",
      "15    0.774763    0.721393    0.760201    0.802766  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_lgbm_7)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_7_cat = np.where(((y_pred_lgbm_7 >= 2) | (y_pred_lgbm_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 22:48:09,916]\u001b[0m Trial 400 finished with value: 0.6803721717048464 and parameters: {'n_estimators': 645, 'learning_rate': 0.0598835820803506, 'max_depth': 6, 'max_bin': 251, 'num_leaves': 250}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:48:17,208]\u001b[0m Trial 401 finished with value: 0.68477192908236 and parameters: {'n_estimators': 628, 'learning_rate': 0.06904167617097677, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 478}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:48:25,532]\u001b[0m Trial 402 finished with value: 0.687943462926307 and parameters: {'n_estimators': 679, 'learning_rate': 0.06531834125240246, 'max_depth': 9, 'max_bin': 245, 'num_leaves': 381}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:48:30,668]\u001b[0m Trial 403 finished with value: 0.6850697699413898 and parameters: {'n_estimators': 607, 'learning_rate': 0.07359748569951645, 'max_depth': 7, 'max_bin': 240, 'num_leaves': 545}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:48:37,952]\u001b[0m Trial 404 finished with value: 0.6819805621816296 and parameters: {'n_estimators': 588, 'learning_rate': 0.05617182786663648, 'max_depth': 7, 'max_bin': 253, 'num_leaves': 407}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:48:43,489]\u001b[0m Trial 405 finished with value: 0.6782438933432762 and parameters: {'n_estimators': 719, 'learning_rate': 0.043982611162418264, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 500}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:48:50,309]\u001b[0m Trial 406 finished with value: 0.6834736789179323 and parameters: {'n_estimators': 654, 'learning_rate': 0.05997411247722065, 'max_depth': 7, 'max_bin': 245, 'num_leaves': 450}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:48:57,842]\u001b[0m Trial 407 finished with value: 0.6763148533890657 and parameters: {'n_estimators': 634, 'learning_rate': 0.052400843328685574, 'max_depth': 6, 'max_bin': 254, 'num_leaves': 564}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:49:03,240]\u001b[0m Trial 408 finished with value: 0.68915039414852 and parameters: {'n_estimators': 708, 'learning_rate': 0.06352409360907038, 'max_depth': 7, 'max_bin': 242, 'num_leaves': 420}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:49:11,044]\u001b[0m Trial 409 finished with value: 0.6818381455464154 and parameters: {'n_estimators': 666, 'learning_rate': 0.04845808466106766, 'max_depth': 8, 'max_bin': 247, 'num_leaves': 392}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:49:19,661]\u001b[0m Trial 410 finished with value: 0.6854289830986724 and parameters: {'n_estimators': 611, 'learning_rate': 0.07148682619287305, 'max_depth': 7, 'max_bin': 251, 'num_leaves': 348}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:49:27,758]\u001b[0m Trial 411 finished with value: 0.6798690959830391 and parameters: {'n_estimators': 686, 'learning_rate': 0.05642235154889889, 'max_depth': 7, 'max_bin': 244, 'num_leaves': 466}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:49:33,928]\u001b[0m Trial 412 finished with value: 0.6795976429554365 and parameters: {'n_estimators': 644, 'learning_rate': 0.060851114148902395, 'max_depth': 7, 'max_bin': 237, 'num_leaves': 439}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:49:39,566]\u001b[0m Trial 413 finished with value: 0.6866021753349653 and parameters: {'n_estimators': 591, 'learning_rate': 0.06572661497354426, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 580}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:49:48,668]\u001b[0m Trial 414 finished with value: 0.6922883835007074 and parameters: {'n_estimators': 558, 'learning_rate': 0.05210478868307512, 'max_depth': 11, 'max_bin': 240, 'num_leaves': 483}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:49:57,126]\u001b[0m Trial 415 finished with value: 0.6807332903046872 and parameters: {'n_estimators': 731, 'learning_rate': 0.03880014468124744, 'max_depth': 7, 'max_bin': 252, 'num_leaves': 452}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:50:04,667]\u001b[0m Trial 416 finished with value: 0.6788818705739978 and parameters: {'n_estimators': 622, 'learning_rate': 0.05702291993144755, 'max_depth': 7, 'max_bin': 245, 'num_leaves': 153}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:50:13,242]\u001b[0m Trial 417 finished with value: 0.6946505654858823 and parameters: {'n_estimators': 606, 'learning_rate': 0.06933317501361072, 'max_depth': 10, 'max_bin': 249, 'num_leaves': 429}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:50:19,647]\u001b[0m Trial 418 finished with value: 0.6822869796973782 and parameters: {'n_estimators': 663, 'learning_rate': 0.0757995828496977, 'max_depth': 7, 'max_bin': 255, 'num_leaves': 546}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:50:23,095]\u001b[0m Trial 419 finished with value: 0.6888734097490553 and parameters: {'n_estimators': 698, 'learning_rate': 0.19850804109374393, 'max_depth': 8, 'max_bin': 242, 'num_leaves': 585}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:50:30,532]\u001b[0m Trial 420 finished with value: 0.6805352842320472 and parameters: {'n_estimators': 635, 'learning_rate': 0.04614791514386915, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 510}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:50:33,615]\u001b[0m Trial 421 finished with value: 0.6871383349871965 and parameters: {'n_estimators': 574, 'learning_rate': 0.12894555627647444, 'max_depth': 6, 'max_bin': 250, 'num_leaves': 470}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:50:41,328]\u001b[0m Trial 422 finished with value: 0.6845538432906495 and parameters: {'n_estimators': 619, 'learning_rate': 0.0634497287344495, 'max_depth': 7, 'max_bin': 246, 'num_leaves': 406}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:50:51,413]\u001b[0m Trial 423 finished with value: 0.6829252526475686 and parameters: {'n_estimators': 451, 'learning_rate': 0.05111412844700849, 'max_depth': 7, 'max_bin': 252, 'num_leaves': 571}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:50:58,038]\u001b[0m Trial 424 finished with value: 0.6832365348355423 and parameters: {'n_estimators': 678, 'learning_rate': 0.05804013051993723, 'max_depth': 7, 'max_bin': 242, 'num_leaves': 441}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:51:04,352]\u001b[0m Trial 425 finished with value: 0.6848783732088946 and parameters: {'n_estimators': 359, 'learning_rate': 0.060685212079447645, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 458}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:51:12,106]\u001b[0m Trial 426 finished with value: 0.6810109408492554 and parameters: {'n_estimators': 650, 'learning_rate': 0.05347648462250633, 'max_depth': 7, 'max_bin': 245, 'num_leaves': 419}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:51:18,680]\u001b[0m Trial 427 finished with value: 0.6857707256551582 and parameters: {'n_estimators': 600, 'learning_rate': 0.07586679391762308, 'max_depth': 7, 'max_bin': 255, 'num_leaves': 556}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:51:23,914]\u001b[0m Trial 428 finished with value: 0.6827706891266372 and parameters: {'n_estimators': 720, 'learning_rate': 0.06730966695143141, 'max_depth': 7, 'max_bin': 238, 'num_leaves': 442}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:51:33,340]\u001b[0m Trial 429 finished with value: 0.6953589821420343 and parameters: {'n_estimators': 627, 'learning_rate': 0.05534556865813181, 'max_depth': 11, 'max_bin': 247, 'num_leaves': 493}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:51:39,438]\u001b[0m Trial 430 finished with value: 0.6804895212378432 and parameters: {'n_estimators': 588, 'learning_rate': 0.06500624776004837, 'max_depth': 6, 'max_bin': 243, 'num_leaves': 461}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 22:51:45,812]\u001b[0m Trial 431 finished with value: 0.681775332972039 and parameters: {'n_estimators': 701, 'learning_rate': 0.04795806824847052, 'max_depth': 7, 'max_bin': 300, 'num_leaves': 428}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:51:54,100]\u001b[0m Trial 432 finished with value: 0.6889097472966661 and parameters: {'n_estimators': 642, 'learning_rate': 0.07214211168259761, 'max_depth': 8, 'max_bin': 251, 'num_leaves': 370}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:51:57,647]\u001b[0m Trial 433 finished with value: 0.6797350484987521 and parameters: {'n_estimators': 671, 'learning_rate': 0.0585840429237748, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 584}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:52:05,133]\u001b[0m Trial 434 finished with value: 0.6850876612144181 and parameters: {'n_estimators': 623, 'learning_rate': 0.06304065453170965, 'max_depth': 7, 'max_bin': 245, 'num_leaves': 400}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:52:14,599]\u001b[0m Trial 435 finished with value: 0.6834499094036987 and parameters: {'n_estimators': 607, 'learning_rate': 0.07859457501022057, 'max_depth': 7, 'max_bin': 240, 'num_leaves': 567}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:52:22,510]\u001b[0m Trial 436 finished with value: 0.6843789928570561 and parameters: {'n_estimators': 581, 'learning_rate': 0.04477332426130007, 'max_depth': 7, 'max_bin': 194, 'num_leaves': 475}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:52:28,782]\u001b[0m Trial 437 finished with value: 0.6812527850330504 and parameters: {'n_estimators': 729, 'learning_rate': 0.05161888173495997, 'max_depth': 7, 'max_bin': 295, 'num_leaves': 440}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:52:36,217]\u001b[0m Trial 438 finished with value: 0.6794836162529476 and parameters: {'n_estimators': 689, 'learning_rate': 0.059833160027554164, 'max_depth': 7, 'max_bin': 252, 'num_leaves': 597}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:52:42,588]\u001b[0m Trial 439 finished with value: 0.6788872254579982 and parameters: {'n_estimators': 664, 'learning_rate': 0.05506951981376179, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 418}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:52:50,034]\u001b[0m Trial 440 finished with value: 0.6856662842170396 and parameters: {'n_estimators': 639, 'learning_rate': 0.07774679702541677, 'max_depth': 8, 'max_bin': 255, 'num_leaves': 455}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:52:57,556]\u001b[0m Trial 441 finished with value: 0.682968053757427 and parameters: {'n_estimators': 612, 'learning_rate': 0.06767603374060287, 'max_depth': 7, 'max_bin': 243, 'num_leaves': 546}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:53:03,099]\u001b[0m Trial 442 finished with value: 0.6751628277024573 and parameters: {'n_estimators': 712, 'learning_rate': 0.06197282625748513, 'max_depth': 6, 'max_bin': 247, 'num_leaves': 473}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:53:09,260]\u001b[0m Trial 443 finished with value: 0.6795986231511636 and parameters: {'n_estimators': 655, 'learning_rate': 0.04885449706502806, 'max_depth': 7, 'max_bin': 250, 'num_leaves': 573}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:53:17,354]\u001b[0m Trial 444 finished with value: 0.6828329613353871 and parameters: {'n_estimators': 599, 'learning_rate': 0.07102011671756689, 'max_depth': 7, 'max_bin': 244, 'num_leaves': 438}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:53:25,666]\u001b[0m Trial 445 finished with value: 0.683981950138844 and parameters: {'n_estimators': 627, 'learning_rate': 0.05652776121568092, 'max_depth': 7, 'max_bin': 235, 'num_leaves': 594}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:53:31,430]\u001b[0m Trial 446 finished with value: 0.6833481462640131 and parameters: {'n_estimators': 418, 'learning_rate': 0.04274706958543094, 'max_depth': 7, 'max_bin': 253, 'num_leaves': 395}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:53:40,842]\u001b[0m Trial 447 finished with value: 0.6934633347818098 and parameters: {'n_estimators': 684, 'learning_rate': 0.05992982197943993, 'max_depth': 11, 'max_bin': 240, 'num_leaves': 416}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:53:51,278]\u001b[0m Trial 448 finished with value: 0.6841826202776313 and parameters: {'n_estimators': 705, 'learning_rate': 0.05223321598664429, 'max_depth': 7, 'max_bin': 246, 'num_leaves': 558}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:53:57,894]\u001b[0m Trial 449 finished with value: 0.6917888587008116 and parameters: {'n_estimators': 646, 'learning_rate': 0.07527649886402901, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 457}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.70684243\n",
      "\tBest params:\n",
      "\t\tn_estimators: 701\n",
      "\t\tlearning_rate: 0.08489109033868533\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 246\n",
      "\t\tnum_leaves: 575\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686631    0.637557    0.707517    0.672644   \n",
      "1                    TP   42.000000   37.000000   39.000000   34.000000   \n",
      "2                    TN  199.000000  198.000000  198.000000  195.000000   \n",
      "3                    FP    2.000000    3.000000    3.000000    5.000000   \n",
      "4                    FN   25.000000   30.000000   28.000000   34.000000   \n",
      "5              Accuracy    0.899254    0.876866    0.884328    0.854478   \n",
      "6             Precision    0.954545    0.925000    0.928571    0.871795   \n",
      "7           Sensitivity    0.626866    0.552239    0.582090    0.500000   \n",
      "8           Specificity    0.990000    0.985100    0.985100    0.975000   \n",
      "9              F1 score    0.756757    0.691589    0.715596    0.635514   \n",
      "10  F1 score (weighted)    0.891542    0.865205    0.874449    0.839676   \n",
      "11     F1 score (macro)    0.846614    0.807333    0.821498    0.772302   \n",
      "12    Balanced Accuracy    0.808458    0.768657    0.783582    0.737500   \n",
      "13                  MCC    0.721125    0.652929    0.675562    0.586156   \n",
      "14                  NPV    0.888400    0.868400    0.876100    0.851500   \n",
      "15              ROC_AUC    0.808458    0.768657    0.783582    0.737500   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.713204    0.640578    0.702467    0.673087    0.554281  \n",
      "1    37.000000   30.000000   35.000000   40.000000   32.000000  \n",
      "2   199.000000  200.000000  200.000000  201.000000  197.000000  \n",
      "3     4.000000    1.000000    2.000000    2.000000    5.000000  \n",
      "4    28.000000   37.000000   31.000000   25.000000   34.000000  \n",
      "5     0.880597    0.858209    0.876866    0.899254    0.854478  \n",
      "6     0.902439    0.967742    0.945946    0.952381    0.864865  \n",
      "7     0.569231    0.447761    0.530303    0.615385    0.484848  \n",
      "8     0.980300    0.995000    0.990100    0.990100    0.975200  \n",
      "9     0.698113    0.612245    0.679612    0.747664    0.621359  \n",
      "10    0.870412    0.837993    0.863655    0.891127    0.838865  \n",
      "11    0.811847    0.762743    0.801700    0.842363    0.765645  \n",
      "12    0.774763    0.721393    0.760201    0.802766    0.730048  \n",
      "13    0.654317    0.599480    0.649950    0.713942    0.574631  \n",
      "14    0.876700    0.843900    0.865800    0.889400    0.852800  \n",
      "15    0.774763    0.721393    0.760201    0.802766    0.730048  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_lgbm_8)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_8_cat = np.where(((y_pred_lgbm_8 >= 2) | (y_pred_lgbm_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 22:54:04,855]\u001b[0m Trial 450 finished with value: 0.6549855483193224 and parameters: {'n_estimators': 612, 'learning_rate': 0.06536271348038034, 'max_depth': 7, 'max_bin': 243, 'num_leaves': 487}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:54:13,577]\u001b[0m Trial 451 finished with value: 0.6491409335238667 and parameters: {'n_estimators': 270, 'learning_rate': 0.05661670578809121, 'max_depth': 8, 'max_bin': 251, 'num_leaves': 425}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:54:22,613]\u001b[0m Trial 452 finished with value: 0.6439664395364594 and parameters: {'n_estimators': 560, 'learning_rate': 0.06189442883277645, 'max_depth': 7, 'max_bin': 246, 'num_leaves': 513}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:54:27,950]\u001b[0m Trial 453 finished with value: 0.6556489457767822 and parameters: {'n_estimators': 732, 'learning_rate': 0.07887348197736978, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 386}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:54:34,770]\u001b[0m Trial 454 finished with value: 0.6379431708147768 and parameters: {'n_estimators': 583, 'learning_rate': 0.05308052328987521, 'max_depth': 7, 'max_bin': 256, 'num_leaves': 445}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:54:44,167]\u001b[0m Trial 455 finished with value: 0.6308860203820517 and parameters: {'n_estimators': 631, 'learning_rate': 0.037468004659087055, 'max_depth': 6, 'max_bin': 241, 'num_leaves': 469}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:54:49,624]\u001b[0m Trial 456 finished with value: 0.6476468684320154 and parameters: {'n_estimators': 602, 'learning_rate': 0.06851366152935864, 'max_depth': 7, 'max_bin': 253, 'num_leaves': 578}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:54:56,668]\u001b[0m Trial 457 finished with value: 0.6396247367653369 and parameters: {'n_estimators': 670, 'learning_rate': 0.050148341364287745, 'max_depth': 7, 'max_bin': 245, 'num_leaves': 407}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:55:03,496]\u001b[0m Trial 458 finished with value: 0.6490714815736763 and parameters: {'n_estimators': 693, 'learning_rate': 0.0723457151837946, 'max_depth': 7, 'max_bin': 250, 'num_leaves': 436}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:55:08,168]\u001b[0m Trial 459 finished with value: 0.6437047838882108 and parameters: {'n_estimators': 620, 'learning_rate': 0.056794596511071646, 'max_depth': 7, 'max_bin': 238, 'num_leaves': 456}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:55:18,077]\u001b[0m Trial 460 finished with value: 0.6464602838006511 and parameters: {'n_estimators': 645, 'learning_rate': 0.0642039694721606, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 542}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:55:26,574]\u001b[0m Trial 461 finished with value: 0.6446386108931483 and parameters: {'n_estimators': 713, 'learning_rate': 0.04738982059777887, 'max_depth': 8, 'max_bin': 243, 'num_leaves': 497}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:55:28,699]\u001b[0m Trial 462 finished with value: 0.6278076226266724 and parameters: {'n_estimators': 121, 'learning_rate': 0.06071423853337979, 'max_depth': 7, 'max_bin': 252, 'num_leaves': 593}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:55:37,069]\u001b[0m Trial 463 finished with value: 0.6427574137404539 and parameters: {'n_estimators': 595, 'learning_rate': 0.05450646076408967, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 563}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:55:42,218]\u001b[0m Trial 464 finished with value: 0.6564050345785548 and parameters: {'n_estimators': 655, 'learning_rate': 0.08001523476786486, 'max_depth': 7, 'max_bin': 245, 'num_leaves': 429}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:55:54,099]\u001b[0m Trial 465 finished with value: 0.6594302058519566 and parameters: {'n_estimators': 629, 'learning_rate': 0.05982070416340001, 'max_depth': 11, 'max_bin': 241, 'num_leaves': 481}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:56:01,916]\u001b[0m Trial 466 finished with value: 0.6438542685255021 and parameters: {'n_estimators': 615, 'learning_rate': 0.07477468702221365, 'max_depth': 6, 'max_bin': 256, 'num_leaves': 446}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:56:11,053]\u001b[0m Trial 467 finished with value: 0.638198539664532 and parameters: {'n_estimators': 729, 'learning_rate': 0.040925943635782155, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 575}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:56:15,540]\u001b[0m Trial 468 finished with value: 0.6526417894648889 and parameters: {'n_estimators': 679, 'learning_rate': 0.06649303936329705, 'max_depth': 7, 'max_bin': 253, 'num_leaves': 420}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:56:22,264]\u001b[0m Trial 469 finished with value: 0.6441831655119783 and parameters: {'n_estimators': 588, 'learning_rate': 0.0453198636286935, 'max_depth': 7, 'max_bin': 244, 'num_leaves': 458}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:56:30,679]\u001b[0m Trial 470 finished with value: 0.6406605190020993 and parameters: {'n_estimators': 639, 'learning_rate': 0.05294965006867393, 'max_depth': 7, 'max_bin': 250, 'num_leaves': 466}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:56:33,934]\u001b[0m Trial 471 finished with value: 0.65343764275257 and parameters: {'n_estimators': 570, 'learning_rate': 0.08681866249055786, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 406}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:56:41,941]\u001b[0m Trial 472 finished with value: 0.6597903003878838 and parameters: {'n_estimators': 700, 'learning_rate': 0.19002875590485244, 'max_depth': 10, 'max_bin': 242, 'num_leaves': 433}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:56:47,092]\u001b[0m Trial 473 finished with value: 0.6486726680639158 and parameters: {'n_estimators': 657, 'learning_rate': 0.056980428351574075, 'max_depth': 8, 'max_bin': 199, 'num_leaves': 559}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:56:54,918]\u001b[0m Trial 474 finished with value: 0.6492589458204224 and parameters: {'n_estimators': 608, 'learning_rate': 0.06409473319897278, 'max_depth': 7, 'max_bin': 250, 'num_leaves': 588}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:56:59,601]\u001b[0m Trial 475 finished with value: 0.6450520199337523 and parameters: {'n_estimators': 628, 'learning_rate': 0.06914378111517144, 'max_depth': 6, 'max_bin': 239, 'num_leaves': 381}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:57:06,438]\u001b[0m Trial 476 finished with value: 0.6521516530635221 and parameters: {'n_estimators': 671, 'learning_rate': 0.09267013409283403, 'max_depth': 7, 'max_bin': 245, 'num_leaves': 481}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:57:13,454]\u001b[0m Trial 477 finished with value: 0.6624122712495615 and parameters: {'n_estimators': 597, 'learning_rate': 0.08025555726734371, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 454}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:57:17,861]\u001b[0m Trial 478 finished with value: 0.6407075035892106 and parameters: {'n_estimators': 479, 'learning_rate': 0.05128530053169668, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 443}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:57:27,879]\u001b[0m Trial 479 finished with value: 0.6473955061833669 and parameters: {'n_estimators': 724, 'learning_rate': 0.05902439321426329, 'max_depth': 7, 'max_bin': 244, 'num_leaves': 579}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:57:34,847]\u001b[0m Trial 480 finished with value: 0.6628386755903988 and parameters: {'n_estimators': 689, 'learning_rate': 0.08863483296672951, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 538}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 22:57:39,915]\u001b[0m Trial 481 finished with value: 0.6598918536525483 and parameters: {'n_estimators': 645, 'learning_rate': 0.06258415308229419, 'max_depth': 12, 'max_bin': 249, 'num_leaves': 601}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:57:50,594]\u001b[0m Trial 482 finished with value: 0.6352485328504426 and parameters: {'n_estimators': 621, 'learning_rate': 0.04927752590764787, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 416}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:58:01,379]\u001b[0m Trial 483 finished with value: 0.6631861790167379 and parameters: {'n_estimators': 581, 'learning_rate': 0.07193714300500009, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 469}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:58:07,755]\u001b[0m Trial 484 finished with value: 0.6453301467133687 and parameters: {'n_estimators': 608, 'learning_rate': 0.056579063295270966, 'max_depth': 7, 'max_bin': 251, 'num_leaves': 565}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:58:12,803]\u001b[0m Trial 485 finished with value: 0.6473374258888309 and parameters: {'n_estimators': 705, 'learning_rate': 0.07521843072341966, 'max_depth': 7, 'max_bin': 236, 'num_leaves': 497}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:58:22,126]\u001b[0m Trial 486 finished with value: 0.6580677884361492 and parameters: {'n_estimators': 658, 'learning_rate': 0.07884093827322054, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 361}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:58:30,109]\u001b[0m Trial 487 finished with value: 0.6479045834563245 and parameters: {'n_estimators': 632, 'learning_rate': 0.0609587364608325, 'max_depth': 7, 'max_bin': 254, 'num_leaves': 397}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:58:37,687]\u001b[0m Trial 488 finished with value: 0.6487484494202901 and parameters: {'n_estimators': 681, 'learning_rate': 0.05400596290332306, 'max_depth': 8, 'max_bin': 240, 'num_leaves': 430}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:58:51,671]\u001b[0m Trial 489 finished with value: 0.6346336101159034 and parameters: {'n_estimators': 614, 'learning_rate': 0.017957124668427887, 'max_depth': 7, 'max_bin': 258, 'num_leaves': 444}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:59:02,429]\u001b[0m Trial 490 finished with value: 0.667450098233939 and parameters: {'n_estimators': 739, 'learning_rate': 0.06740930232794527, 'max_depth': 12, 'max_bin': 250, 'num_leaves': 510}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:59:08,091]\u001b[0m Trial 491 finished with value: 0.6535480523605325 and parameters: {'n_estimators': 719, 'learning_rate': 0.08592911814630194, 'max_depth': 7, 'max_bin': 244, 'num_leaves': 581}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:59:10,969]\u001b[0m Trial 492 finished with value: 0.6441290926386142 and parameters: {'n_estimators': 545, 'learning_rate': 0.10810856386727552, 'max_depth': 6, 'max_bin': 248, 'num_leaves': 553}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:59:19,829]\u001b[0m Trial 493 finished with value: 0.6507180527878889 and parameters: {'n_estimators': 598, 'learning_rate': 0.04494742420938928, 'max_depth': 9, 'max_bin': 252, 'num_leaves': 464}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:59:32,625]\u001b[0m Trial 494 finished with value: 0.6585584113040426 and parameters: {'n_estimators': 637, 'learning_rate': 0.16704186980490687, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 427}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:59:40,549]\u001b[0m Trial 495 finished with value: 0.650215463360656 and parameters: {'n_estimators': 573, 'learning_rate': 0.058337577959612405, 'max_depth': 7, 'max_bin': 242, 'num_leaves': 482}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:59:46,864]\u001b[0m Trial 496 finished with value: 0.626809365976737 and parameters: {'n_estimators': 667, 'learning_rate': 0.06414543383305486, 'max_depth': 3, 'max_bin': 287, 'num_leaves': 454}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 22:59:52,162]\u001b[0m Trial 497 finished with value: 0.6384476111560035 and parameters: {'n_estimators': 522, 'learning_rate': 0.04915631352048239, 'max_depth': 7, 'max_bin': 248, 'num_leaves': 409}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:00:01,899]\u001b[0m Trial 498 finished with value: 0.6705433374497406 and parameters: {'n_estimators': 648, 'learning_rate': 0.08076125343058042, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 600}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:00:08,489]\u001b[0m Trial 499 finished with value: 0.6400986008226337 and parameters: {'n_estimators': 615, 'learning_rate': 0.055898461916042384, 'max_depth': 7, 'max_bin': 238, 'num_leaves': 442}. Best is trial 256 with value: 0.7068424294960505.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.706842429\n",
      "\tBest params:\n",
      "\t\tn_estimators: 701\n",
      "\t\tlearning_rate: 0.08489109033868533\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 246\n",
      "\t\tnum_leaves: 575\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.686631    0.637557    0.707517    0.672644   \n",
      "1                    TP   42.000000   37.000000   39.000000   34.000000   \n",
      "2                    TN  199.000000  198.000000  198.000000  195.000000   \n",
      "3                    FP    2.000000    3.000000    3.000000    5.000000   \n",
      "4                    FN   25.000000   30.000000   28.000000   34.000000   \n",
      "5              Accuracy    0.899254    0.876866    0.884328    0.854478   \n",
      "6             Precision    0.954545    0.925000    0.928571    0.871795   \n",
      "7           Sensitivity    0.626866    0.552239    0.582090    0.500000   \n",
      "8           Specificity    0.990000    0.985100    0.985100    0.975000   \n",
      "9              F1 score    0.756757    0.691589    0.715596    0.635514   \n",
      "10  F1 score (weighted)    0.891542    0.865205    0.874449    0.839676   \n",
      "11     F1 score (macro)    0.846614    0.807333    0.821498    0.772302   \n",
      "12    Balanced Accuracy    0.808458    0.768657    0.783582    0.737500   \n",
      "13                  MCC    0.721125    0.652929    0.675562    0.586156   \n",
      "14                  NPV    0.888400    0.868400    0.876100    0.851500   \n",
      "15              ROC_AUC    0.808458    0.768657    0.783582    0.737500   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.713204    0.640578    0.702467    0.673087    0.554281    0.751018  \n",
      "1    37.000000   30.000000   35.000000   40.000000   32.000000   41.000000  \n",
      "2   199.000000  200.000000  200.000000  201.000000  197.000000  201.000000  \n",
      "3     4.000000    1.000000    2.000000    2.000000    5.000000    1.000000  \n",
      "4    28.000000   37.000000   31.000000   25.000000   34.000000   25.000000  \n",
      "5     0.880597    0.858209    0.876866    0.899254    0.854478    0.902985  \n",
      "6     0.902439    0.967742    0.945946    0.952381    0.864865    0.976190  \n",
      "7     0.569231    0.447761    0.530303    0.615385    0.484848    0.621212  \n",
      "8     0.980300    0.995000    0.990100    0.990100    0.975200    0.995000  \n",
      "9     0.698113    0.612245    0.679612    0.747664    0.621359    0.759259  \n",
      "10    0.870412    0.837993    0.863655    0.891127    0.838865    0.894926  \n",
      "11    0.811847    0.762743    0.801700    0.842363    0.765645    0.849256  \n",
      "12    0.774763    0.721393    0.760201    0.802766    0.730048    0.808131  \n",
      "13    0.654317    0.599480    0.649950    0.713942    0.574631    0.730355  \n",
      "14    0.876700    0.843900    0.865800    0.889400    0.852800    0.889400  \n",
      "15    0.774763    0.721393    0.760201    0.802766    0.730048    0.808131  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_lgbm_9)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_9_cat = np.where(((y_pred_lgbm_9 >= 2) | (y_pred_lgbm_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABDmUlEQVR4nO3deVxU1f8/8NcswLDLDJs4mgiuuUG4hAsiiLmTmZbZN7XSyq1Ff4lblFlobmiuyUf9mPUxSzOz0ihDlEjUUMNSQHHBARxQQPbhnt8fODcGZmAYZoZl3s/Hw4fMXd9nlvu+95xzzxUwxhgIIYQQAMKmDoAQQkjzQUmBEEIIj5ICIYQQHiUFQgghPEoKhBBCeJQUCCGE8CgpEJMaNmwYXnnllWazneayn4bYs2cPxGJxU4dhdNOnT0doaGhTh0FqoKRgwbKzszFv3jx07NgR1tbWcHNzw6RJk5CcnNzgbX344Yfo2LFjremHDh3C+vXrGx2rsbajZup465ORkQGBQIDTp0/XmhcZGQlfX1/+9ZQpU5CZman3tkNDQzF9+nRjhGmw3377DQKBgP8nk8kQHByM+Pj4Rm3X19cXkZGRxgmSaEVJwULdvn0bAQEBSEhIwLZt25CWloZjx47BysoKAwcOxE8//WSU/UilUjg5OTWb7TSX/TSEra0tPDw8zL5fxhgqKioatY0LFy5AoVDgl19+ga2tLUaNGoWMjAzjBEhMgxGLNG7cOObh4cHy8/NrzRs1ahTz8PBgxcXFjDHG3nvvPebj48P279/PvL29mY2NDQsJCWHXr19njDG2e/duBkDj33vvvccYYywoKIi9/PLL/LaDgoLYzJkz2dKlS5mbmxtzdnZmS5YsYZWVlez9999n7u7uzNXVlS1ZskQjpurbOXnyZK39AWCPPfYYY4wxjuPYK6+8wjp16sQkEgnz9vZmERERrLS0tMHxlpeXs3fffZd5eXkxKysr1r17d7Z//36N2ACwLVu2sGnTpjEHBwcml8vZ6tWr63z/b9y4wQCw+Pj4WvPU77fa7t27mUgk4l/n5+ez6dOnMw8PD2Ztbc3kcjl76623GGOMvfTSS7XKdvLkScYYY//88w8bPXo0s7e3Z/b29mzs2LEsNTW11n5+/fVX1rdvX2ZlZcWio6OZQCBgZ86c0Yjxt99+YwKBgKWnp2stn/ozun37Nj/tzp07DADbvn07H2tISAg/n+M49sknnzBvb29mZWXFOnXqxDZs2MDPDwoKqlW2Gzdu1Pk+k4ajpGCB8vLymFAoZCtXrtQ6/9SpUwwAO3LkCGOs6iBlZ2fHBg0axM6ePcvOnj3L+vfvz3r37s04jmPFxcXs3XffZXK5nCkUCqZQKFhhYSFjTHtScHJyYv/v//0/dvXqVRYTE8MAsFGjRrFFixaxq1evsj179jAA7IcfftBYT72dsrIyfj8KhYKlpKQwLy8vNn36dMYYY5WVlWzp0qUsMTGR3bhxgx05coR5enqyFStWMMZYg+JduHAhk0ql7KuvvmJXr15lq1atYgKBgMXGxvLLAGDu7u5s586dLC0tjUVHRzMA7Ndff9X5GTQmKcybN4/17t2bJSYmsps3b7IzZ86wnTt3MsYYe/DgARsyZAibPHkyX7aysjJWXFzMOnTowIYPH87OnTvHzp07x4YNG8Z8fHxYWVkZvx+BQMACAgLYL7/8wtLT01lOTg4LCwvj31u1adOmsdDQUJ3l05YUcnNzGQC2efNmxljtpPDpp58yiUTCduzYwa5du8a2bdvGbGxs2K5du/j1O3bsyN555x2+bCqVSmcMxDCUFCzQH3/8wQCwQ4cOaZ2v/vGuWbOGMVZ1kAKgcVZ59epVBoD9/PPPjDHGVq5cyZ+pV6ctKfTp00djmR49erCePXtqTOvduzd75513dG5Hrby8nA0bNowNHjyYvxLQZv369czX15d/rU+8RUVFzNramm3ZskVjmfDwcBYcHMy/BsDmzZunsUzXrl3Z4sWLdcajTgq2trb8mbv6n5WVVZ1JYfz48eyll17Sue2QkJBa83ft2sVsbW3ZvXv3+GlZWVlMIpGwvXv38vsBwE6dOqWx7jfffMPs7OzYgwcPGGOM3b9/n9na2rKvvvpKZww1k0JBQQF75ZVXmFgsZpcvX2aM1U4KcrmcLVq0SGM7b775JvP29uZf+/j48Fd1xDSoTcECsXrGQBQIBLWmubm5aTR+dunSBa6urrhy5UqD99+nTx+N156enujdu3etaTk5OfVu6/XXX8ft27dx+PBh2NjY8NM/++wzDBgwAB4eHnBwcEBERARu3rzZoDjT0tJQXl6OoUOHakwPCgpCSkqKxrS+fftqvG7Xrh2ys7Pr3cfu3buRnJys8e+1116rc5033ngDX3/9NXr27IkFCxbgxx9/BMdxda6TkpKCHj16wNXVlZ/m4eGBrl271ipLv379NF6PHz8ezs7O+OKLLwAAn3/+ORwcHDBhwoR6y9e1a1c4ODjA2dkZx48fx3//+1/07Nmz1nIFBQW4c+eO1vc6IyMDxcXF9e6LGAclBQvUuXNnCIVC/PXXX1rnq6d37dq1zu3Ul1x0sbKy0ngtEAi0TqvvQLdmzRocOnQIx44d0zjYHTx4EHPmzMGUKVPwww8/4M8//8SKFSsMbjStmSQZY7WmWVtbNzh+oCp5+Pr6avyTSqV1rjNy5EjcunULS5cuRWlpKaZNm4bhw4ejsrKyQeXQVhaRSASJRKKxjFgsxssvv4zPPvsMALBr1y5Mnz69Vpm1OX78OC5evAilUolbt27h+eefb1CMhn7HiOEoKVggqVSKUaNGYcuWLSgoKKg1/6OPPoKHhwdGjBjBT7t37x7S09P519euXUNubi66d+8OoOqgWN9ByZi+/fZbrFixAocOHaqVvE6dOgU/Pz+8/fbbeOKJJ9C5c+daPV70idfX1xc2NjaIi4urtf3HH3/cKOUwlFQqxfPPP48dO3bg2LFjiIuL46/atJXt8ccfR0pKCpRKJT8tOzsb165d06ssr776Ki5evIjt27fj4sWLet/L0bFjR/j4+NSb6JycnCCXy7W+197e3rCzs9NZNmJclBQs1JYtWyASiTB8+HD89NNPuH37NpKSkjB16lScPHkSe/bsga2tLb+8nZ0dZsyYgfPnz+PcuXN46aWX0KtXL/7mI29vb2RlZeH333+HUqk06eV+SkoKpk2bhsjISHTr1g1ZWVnIysrCvXv3AFRd4Vy+fBlHjhxBeno6oqOjcejQIY1t6BOvnZ0d5s+fj+XLl+PgwYNITU3FRx99hCNHjmDJkiUmK199li5dikOHDuHq1atITU3F/v374eDggA4dOgCoKtv58+eRnp4OpVKJiooKTJ06FW5ubpgyZQouXLiA8+fP47nnnkO7du0wZcqUevfZoUMHPPXUU1iwYAGGDRuGLl26GL1cERER2Lx5Mz777DOkpqZix44d2LZtm8Z77e3tjTNnzuDWrVtQKpV6XY2RhqGkYKEee+wxnDt3DgMGDMDs2bPh4+ODUaNGoaysDL///jueeuopjeXbtm2LWbNm4ZlnnsGgQYNga2uLw4cP85f74eHhePbZZzFmzBi4ublhzZo1Jos9KSkJRUVFiIiIQNu2bfl/6rrw2bNn48UXX8SMGTPg5+eHP/74o9YNT/rGu2rVKrz66qt488038fjjj+Pzzz/H559/jpCQEJOVrz4SiQQrVqzAE088gYCAAFy6dAk//vgjnJ2dAQDvvPMOXF1d0adPH7i5ueHMmTOwtbXFiRMnYGNjg6FDhyIoKAj29vb46aef9KoGAoBZs2ahvLwcs2bNMkm5Xn/9dXzwwQf46KOP0KNHD6xevRpRUVF4+eWX+WXef/995Ofno2vXrnBzc8OtW7dMEoslEzCqtCP1iIyMxOeff460tLSmDoU0oa1bt2LFihXIzMzUaNQnrUvrG1CFEGJUDx8+RFpaGtauXYu5c+dSQmjlqPqIEFKnuXPnon///ujevTvefffdpg6HmBhVHxFCCOHRlQIhhBAeJQVCCCG8Ft/QfPfuXYPWc3V11biRxxJQmS0DldkyNKbMXl5eOufRlQIhhBAeJQVCCCE8SgqEEEJ4lBQIIYTwKCkQQgjhtfjeR4Q0Z4qL/+DGll3ocD0FEq4cQgACmPZsLN+E226uLK7MIhEKHRwg6NMHkhenQezjY7RNU1IgxETSd/wXNge/QDeuAtUfHSNA1VPnaz/yhhA9VVaCKywE/vgDJQ8ewHbeXKMlBrMlheTkZOzevRscxyEkJATh4eEa87/77jvEx8cDADiOw507dxATEwMHBwdzhUiIUSgu/oPsVavRQXmLP/BrSwD6JAZdY9DUXE/bcpR0WrlHIxRxmZlQxZ9uWUmB4zjExMRg2bJlkMlkiIiIQEBAAORyOb/M+PHjMX78eADAuXPncOzYMUoIpMVRXPwHGavWoovy33H+6zo415UY6hqUTJ8By+hqpJVTD1tXWgpOj+eB68ssDc1paWnw9PSEh4cHxGIxAgMDkZSUpHP5M2fOYNCgQeYIjRCjuvj1cbgW5kIA/Q/I2g7wxhqlkka7/BfT8k+fec2W+nnWEgmEHh5G26xZrhTy8vIgk8n41zKZDKmpqVqXLSsrQ3JyssbTlqqLjY1FbGwsACAqKkrjge0NIRaLDV63paIym57kfi4kXAV/YNG3+qjm3/XNr/m3rvmkSvXPQ9f705D3rWbSaJL3WyAAhALYdOwIt/HjYGOk77lZkoK20bnVj3Gs6fz58+jatavOqqPQ0FD+ucAADB77g8ZKMa+sH2KRv28/nO/dhRWnghCtsz90n0f/qw8+3KP/1WVVT6srGaCO+YYsaym9SSp1TK9+5l/z/dGWiNW09RRTf541rybM0atMg0gE4aPeR+IXp6FQKkVhA37bdY19ZJbvi0wmQ25uLv86NzcXLi4uWpc9c+YMBg8ebI6wmozi4j9I3fMV5KmX4FScDxE4o3yZ1F9Y9UGn+oEIAPJg5i9utbhsALhXm6Y+aOqrvrO4+rZnrjO5mr2MhKiKSwUhjngH4j99ws0USZWwri6IHNnRrPs0tz/vFOLt79JRpjKs0kckACp1rCoA8LinLVxsrVBUzuFuQTmyCsu1Lmvu99pUJ3lmSQo+Pj5QKBTIycmBVCpFQkIC5s+fX2u54uJiXLlyBfPmzTNHWGanuPgPbn8SDe+719Crxrz6Dnq6vu4111OfgVe/VBbWWNac1QvG2pehvXSMHUd9qicn9dlkqdAK6/2fQ4K8j+4VTcDD0QqzBrY16z7N7W5+Gd7+Ng1lnOHb0JUQgKrP76+sEgAlesXSGpglKYhEIsycOROrVq0Cx3EIDg5G+/btceLECQBAWFgYAODs2bPo06cPJBKJOcIyK3WvFF9lBkSPpunTrbA+Nc9Ma/6taz7RrxunIV091YmBQYBCazsUWUnMnhDc7MXYMrEzvJxb9/OUo0/daVRCMCZlkaqpQzAKs1U3+vv7w9/fX2OaOhmoDRs2DMOGDTNXSHrL+iEWBbv+gzYP7kEEjq+LbEg1jARAt0d/N7TqpKVqbmXUVSffmGW0zeP/FgohFAqhlDg1KE5tbERV/woe1VyIhUA3d1soCsqRW6xZm+7lLMGm8E6tPiHczS/DH7cKmzoMnsxOVP9CLYCltEEZLH3Hf2H31edwZVU/PG2NU9XVrD6o70zekhmjd01Dl625b13r17ePuhqL1dsoZ0KUQYDvvBvfRhbkq72++m5+GTaeuoOUrGIADD097RE5oRdsueJG77O525moQHlddT9m1q5N66jhoKSgg7r+3/fuNT4RNPQOVEOTQGMPig1dzxxq/nSr9+AwZ0+chi5r2HoCVIiscNvBDV91Ho4LHfsCBjaCAnW3DXg522DNOM07WV2ldlAqW39SUD6saOoQeO2crM3SfnM3vww7ExVQPqxAO5kCL/lJjX5FSEkB2nsDSQB0fjS/rr7mjVH9MKHuOWSug5cAgKkvdrlH/9TRVD9QmruOvSk5iYChPi5QFlXAzkqI0opKXLtXglIVg7UIsBIKUMExlFeCf81BAKEA6OlpjwVD5a2mKqj6Qc3VoSrZGVo2VwcrI0dnmID2Dlg8vIPRP6Oa79WEx2X4KPYWMh/VIV7IfIgLGXmIftrXqPu2+KSguPgPrq3dgnY5t+FUUaizEdhYqh+0KyFAQtte+F+3UGQ46+43bAr+cgd8OrFz/Qs2UPUvsp21EH9nF9Wq87Y0IqGo1XcLrYu6iuvS3Yd4WMahertwXOp9WIkFKK1gYAwQCgWwEQG21iK4O1jDxU4MAYC8ogrkllTCwVqAByWVqOAYGOMgEQtQ2oirMDUhAEPaq98f2QEjusrqX1AL9W8l834pX7aH5QwyOzFc7MRIVZYgu/Dfq6H49HyUqDSjzCwox85EhVG/XxafFC5+fRxtigphX1nKNxw35K5GQ6prSgVWKLK2xfZeE5rsjNnOSojI4xkNOmPTdpYHgJ8mEDCkZBVr/Ejd7cUIkNsjPbcMAEM7Jytk5pcjv5QDA2BvI4SflwOe83PXOAvSxkYISKxFYIxDmQooq1afLLMTwVsqQXpuGYrKK5tVXfPjnnZNHYLe6juTr2v+n3cK8d5PGcgrVoHDvzd01XVKUMYBZeX/flaVHEMFBzysUOGenr15BACcbcXwlVXFcUlRrPH5CwVVjfQcE0AkFMDOSoiOMgk4Brja/1sG/iD9oBS5xZVwtRfDRizAdWUJHjz6vqrZioVYO74T/OSODXo/Jzwuw5GUXGTeL8X1+2UoqaidinTdB1EzIagpi4xbjSZg2m43bkHu3r1r0HpFN+4iJWojOqYlQ8wqNRqQtalZ1aNtekPrmo2VEMRCoJIDxALASgwU1/MdkdmJIBYJNc5CgKrqJG1naurqjppneVKJEMUqVu+ZmkgAuDlYY/mIDnX+iPgfUFEFXO3//QGpX1c/ANVctua8F/f/o/NHZE4yOxF2PNvVrNU/JUI7rP7xCn+1JgBQVM7xfyvyy5CRV4ZKVH0/xQLA1loIjuNQ8/giAiC1F/PfhZpnr7ZiIRaHyPFdSi7O3ykyWxm1aedkjeinfQFA53fDUHV933T5804hFh69rnHgr+tGOUMZctNcXXc0W1xSUD/0pFPaRdg8OsTp03jMAFQKhNjdfRS+7RJsUKyN5SwR4aPR3joPlGrvHk1H/I0CrdsQ4d/2C3MTCYBNT/vWe3ZlDH/eKcTcQ2lN2i3W0UaI3c91M1tCUFfTnL1V2KyulMxpsLdTrYZ3U6lZ/SOzE6NdGxv+CtocJybqRNjQ71iTD3PRXKjbD3yy0mGtR0Ko3rW0WGSDL7qGak0I5rrvYMBjTvCTO9Z7UF0wVI7ruWkaVTHqGJuydr+SASt/voVDMx43+b785I74dKJvreEPhAJAIhbCSSLGlL4yRMcr6t2WAIC1SIDHXGzg7mjNn23fuF9W51nfkx2dzZoQFhxOq7P6zRKcvVWIu/llJn/ftb3fWYXlSMkuRoqiCJ1kEtMnhDYSRE8w/v0oFpUU1O0HVo+qi6rT1mWyQiDCfYkTLrXthvjuQ3De6t/Re4QCwNFahN5e9njOzx3LfryB+yXGOeTaigEIhBqXnQ3p8ublbIPop335y927+brHazG3h2Xmu+vTT+6IH+YGVlWl6Liy6uJmj6XHruOBlttia9YbazsQ2IqFkLexwp38CoM/L2PYmaiw+IQAAOWVzOgNr9rU9X5nFpSb5bNwdbA2SfKzqKRgnXsP1pwKHAS1GobVf5eIrHHL0QPnPbphf/eRAKrq7CIGtuUboa7nVTUQ5ZdVIv5GAa7nlqKnp73OKhtt3O3FKCir1FofP8THBbMe7c/QelEvZxv+hzH3m9RmkxQcbMz7lWsvtavzAOEnd8QPs6vaduqrN9Z2IChRcfCW2eHjMY37vBqrOfXZb2rGbnjVuo9m8H4rH5rmN21RSaFc5obyrDsoEVpBzFVABM3EoK4myrdxwBmvqiHr3O3F/A88cmRHRB7PQEq25uBYmQXl8JZJIJUIkVda/yWji60YWyd1QXZhea2GKPUZZvWDemM1l/7cIgGwfESHpg5Dp/rec10HAmVRhVE/L0M0l8+4OXC1N/170RzebzcHa5NstzUOaa9Tn0kjUWDjgHKxDUpFNqh4dMUAVNW159k4IdHrceztMYq/b6DmIV7XgaG4gsPjbfV7fGi/Do7wcraBn9wR+6Z2Q1hXF/jLHRDW1cXoN6IAwKyBbWHTxJ+0tUhgtkZmU9F1IDDHQag+swa2ha24Zf6cRQBsRMa5M8hcI8POGtgWwiYeq+aKohDvHk03+uisFnWl0LZPN+wf8SzuxZ9A73upsFeVocjaFhddffCDd6DWG8iURSqNOsq6Dgz6XFLWrGs2xxmml7MN1of7Yt7hNHBN1CllmG+bFp0QgKoDQYqiSKMKydxtB7p4Odugk0yClOymHd5CAOCJ9g6Y0c8TX/6Zw4/JJBYKtN534OlojU8nau9GGn3qjtYqWRdbEWzEIq1Vop1dbc1SbeflbANHaxHyy5qu60Z5JUP8jQJcU6YadURci0oKADD1mYFYIHTFlmo/7Pr6Dlevo6zrwLAzUXtPFk9Ha3g5WzdJXTNQVVd+JCUXXdzskXqvSKOsAgAONkKIhQL4yCSwtRKhqILTiFVbf+uGaC4Hzsaq2YDfVJ+nLu3a2OhMCmIBIBYJat1Y2MHFptbNXmphXV1QUl6p9cA82Ltq5NdLdx+iVMVgayVE77aaQ3JUPwnQ1khfsztlzZOjonLt3zdvmS3AtN/kVWzgd9QQvb0a1o5oKtmFFUZtXLe4pKD+Ye/9Mw+ZeQ/16plTvXqgrgODroRhiiohfenqMePjKuFjri82dTWXtj7wtmIhZj3pgZ2J2RpJQ333sVAAdJK1jtEjAfNc2RlK2/fPWiTAgA6OWDBUDkD7TV26DtjqRF6ze3M7J2u82cDxmBqSUNUN/jfySrVuq67qOnNW5S0YKkfizSswYx7SyZiN6xZ385qa+lF2c79JxYXMhzqX83C0atClmSF3PppS5PEMnLh6v9Z0Qx8dqK18OxMVWvdRXVMlR0t7Fvfd/DL+hKch37/67hA313e6vvstqt+1XN+VhznUdaNofWyEACcQoMIINxo29PdMN6/VQVcbgZVQgIGPOTZ4hMrmdiZZV48ZQ2grnz5tKaYYuIvU5uVsg3WTejU4Edb1vTXnd1pX/38XWxH6dXDSSEjVrzzaSR1MMox0fRYMleOPW3836A7ymldvcw+lNarLuLEb1y0+KTTHKh9jMkePGX2755mj/zhp2XSdYHjLbGslpurJqqmuCL2cbTCgg6NeVwsO1kIEejvXutL6dKJvrased3sxBEJBrfHJajLFY1ctPik098bDxjJHjxlt+9CmOXTdJM1bc+72q4u2YWW0CfR21nrFpesYBPzbBnQjt0TriAntXSRGP1ZZfJuCJVDXCeeXA87WMEnSq17vbGclrDWaJrUpmE9LLrM+vZS0aeoym/r7b+y2QRolVYum/hI1BXOWubk0uNPn3PIY8t1pbmU29vdfW7LsILXF+nHeBm2XkoIWze1LZA5UZstAZW6daiaad0f1gC1n2M2K1PuIEEJauJq9wFyldlAqjX8Hu9mSQnJyMnbv3g2O4xASEoLw8PBay6SkpGDPnj2orKyEo6Mj3n//fXOFRwghBGZKChzHISYmBsuWLYNMJkNERAQCAgIgl8v5ZYqKirBr1y4sXboUrq6uyM/PN0dohBBCqjHLsIppaWnw9PSEh4cHxGIxAgMDkZSUpLHM6dOnMWDAALi6ugIAnJ2dzREaIYSQasxypZCXlweZTMa/lslkSE1N1VhGoVBApVIhMjISJSUlGD16NIKCgmptKzY2FrGxsQCAqKgoPok0lFgsNnjdlorKbBmozJbBVGU2S1LQ1sFJINAcjLyyshI3btzA8uXLUV5ejmXLlqFz5861WslDQ0MRGhrKvza0x4El9FaoicpsGajMlqExZW7y3kcymQy5ubn869zcXLi4uNRaxtHRERKJBBKJBN27d8fNmzfrDJ4QQohxmaVNwcfHBwqFAjk5OVCpVEhISEBAQIDGMgEBAfjnn39QWVmJsrIypKWloV27duYIjxBCyCNmuVIQiUSYOXMmVq1aBY7jEBwcjPbt2+PEiRMAgLCwMMjlcvTt2xcLFy6EUCjE8OHD0aFD832eLyGEtEZ0R7MFoTJbBiqzZTBVm0LLfNI3IYQQk6CkQAghhEdJgRBCCI+SAiGEEB4lBUIIITxKCoQQQniUFAghhPAoKRBCCOFRUiCEEMKjpEAIIYRHSYEQQgiPkgIhhBAeJQVCCCE8SgqEEEJ4eicFlUqFv//+GwkJCQCA0tJSlJaWmiwwQggh5qfXQ3Zu3bqF1atXw8rKCrm5uQgMDMSVK1cQFxeHt956y9QxEkIIMRO9rhQ+++wzTJkyBRs3boRYXJVHevTogX/++cekwRFCCDEvvZLCnTt3MGTIEI1pEokE5eXlJgmKEEJI09ArKbi5ueH69esa09LS0uDp6WmSoAghhDQNvdoUpkyZgqioKIwYMQIqlQqHDx/Gzz//jNmzZ5s6PkIIIWak15XCE088gYiICBQUFKBHjx64d+8eFi5ciD59+pg6PkIIIWak15UCAHTq1AmdOnUyZSyEEEKamF5J4cCBAzrnTZkyxWjBEEIIaVp6JYXc3FyN1w8ePMCVK1fQv39/vXeUnJyM3bt3g+M4hISEIDw8XGN+SkoK1qxZA3d3dwDAgAEDMGnSJL23TwghpPH0SgpvvPFGrWnJyck4ffq0XjvhOA4xMTFYtmwZZDIZIiIiEBAQALlcrrFc9+7dsXjxYr22SQghxPgMHvuod+/eSEpK0mtZdfdVDw8PiMViBAYG6r0uIYQQ89HrSiE7O1vjdVlZGU6fPg1XV1e9dpKXlweZTMa/lslkSE1NrbXctWvXsGjRIri4uODFF19E+/btay0TGxuL2NhYAEBUVJTeMdQkFosNXrelojJbBiqzZTBVmfVKCvPnz9d4bW1tDW9vb8yZM0evnTDGak0TCAQar729vbF161ZIJBJcuHABn3zyCTZt2lRrvdDQUISGhvKvlUqlXjHU5OrqavC6LRWV2TJQmS1DY8rs5eWlc16jex/pQyaTaTRW5+bmwsXFRWMZOzs7/m9/f3/ExMSgoKAATk5Ojdo3IYQQ/ZnleQo+Pj5QKBTIycmBSqVCQkICAgICNJZ58OABf0WRlpYGjuPg6OhojvAIIYQ8ovNK4fXXX9drA9u2bat3GZFIhJkzZ2LVqlXgOA7BwcFo3749Tpw4AQAICwtDYmIiTpw4AZFIBGtra7z55pu1qpgIIYSYloBpq/AHcOXKFb020KNHD6MG1FB37941aD2qg7QMVGbLQGVuGIPaFJr6YE8IIcT89B77KCMjA3///TcKCws1ehPRMBeEENJ66JUUYmNjsXfvXvTu3RvJycno27cvLl26VKuxmBBCSMumV++jI0eOYMmSJVi0aBGsra2xaNEivP322xCJRKaOjxBCiBnplRQKCgrQvXt3AFU3nXEcBz8/P5w/f96kwRFCCDEvvaqPpFIpcnJy4O7ujrZt2+LcuXNwdHSEWKx3kwQhhJAWQK+j+oQJE5CZmQl3d3dMmjQJ69evh0qlwowZM0wdHyGEEDOqMymsX78ew4YNw9ChQyEUVtU0+fn5Yffu3VCpVJBIJGYJkhBCiHnUmRSkUim2b98OxhgGDx6MYcOG4bHHHoNYLKaqI0IIaYXqPLJPnz4d//d//4fk5GTEx8dj2bJl8PT0RFBQEAYPHow2bdqYKUxCCCHmUO/pvlAohL+/P/z9/VFcXIzExETEx8fjyy+/RK9evehJaYQQ0oo0qA7Izs4Ofn5+ePjwIbKzs/H333+bKi5CCCFNQK+kUF5ejrNnzyIuLg4pKSno3r07pkyZgoEDB5o6PkIIIWZUZ1JISUlBXFwc/vjjD7i4uGDo0KGYPXu2xT32jhBCLEWdSWHt2rUIDAzE0qVL0aVLF3PFRAghpInUmRR27twJKysrc8VCCCGkidU59hElBEIIsSxmeUYzIYSQloGSAiGEEF6DkoJSqcS1a9dMFQshhJAmptd9CkqlEtHR0cjIyAAA7Nu3D4mJiUhOTsZrr71myvgIIYSYkV5XCjt37oSfnx/27t3LD4TXu3dvXLp0yaTBEUIIMS+9kkJaWhrCw8P54bOBqiEviouLTRYYIYQQ89MrKTg7OyMrK0tj2p07dxp0Z3NycjIWLFiAefPm4dtvv9W5XFpaGqZMmYLExES9t00IIcQ49EoK48aNw+rVq3Hy5ElwHIfTp09jw4YNmDBhgl474TgOMTExWLJkCTZs2IAzZ87gzp07Wpfbv38/+vbt26BCEEIIMQ69GpqHDx8OBwcH/PLLL5DJZDh16hSmTJmC/v3767WTtLQ0eHp6wsPDAwAQGBiIpKQkyOVyjeV+/PFHDBgwAOnp6Q0sBiGEEGPQKylwHIf+/fvrnQRqysvLg0wm41/LZDKkpqbWWubs2bN47733sG3bNp3bio2NRWxsLAAgKirK4MH5xGJxowf2u51XjI2/piOnoAzuTjZ4c7gP2kvtGrVNUzJGmVsaKrNloDIbcbv6LPTqq6/iySefxODBg9GtW7cG74QxVmuaQCDQeL1nzx688MILGo3Z2oSGhiI0NJR/rVQqGxwPALi6uhq8LgDczS/DgsNpyCwo56ddyMhD9NO+8HK2MXi7ptTYMrdEVGbLQGVuGC8vL53z9EoKy5Ytw5kzZxAdHQ2hUIhBgwZh8ODB6NChg14ByGQy5Obm8q9zc3Ph4uKisUx6ejqio6MBAAUFBfjzzz8hFAoNvjoxtZ2JCo2EAACZBeXYmahA5MiOTRMUIYQ0kl5JwdvbG97e3pg2bRquXLmC06dP44MPPkCbNm2wdu3aetf38fGBQqFATk4OpFIpEhISMH/+fI1ltmzZovH3E0880WwTAgAoH1Zon16kfTohhLQEDXocJ1B12SGXy5Genl6rm6ouIpEIM2fOxKpVq8BxHIKDg9G+fXucOHECABAWFtbQMJqcq4P2EWRd7WlkWUJIy6VXUigqKsIff/yB06dPIzU1Fb1798aECRMQEBCg9478/f3h7++vMU1XMpgzZ47e220qswa2RYqiSKMKqZ2TNWYNbNuEURFCSOPolRRmz56Nrl27YvDgwVi4cCHs7JpvDxtz8XK2QfTTvtiZqICyqAKu9laYNbBts21kJoQQfeiVFDZv3lyrYZhUJQZqVCaEtCY6k8KVK1fQo0cPAEBmZiYyMzO1LtezZ0/TREYIIcTsdCaFmJgYrFu3DgB03kwmEAjw6aefmiYyQgghZqczKagTAqDZXZQQQkjrpdeAeGvWrNE6XZ97FAghhLQceiWFlJSUBk0nhBDSMtXZ++jAgQMAAJVKxf+tlp2dDTc3N9NFRgghxOzqTArq8Yo4jtMYuwioGoxp8uTJpouMEEKI2dWZFN544w0AQJcuXTRGJiWEENI66dWmYGVlhZs3b2pMy8jIwKlTp0wSFCGEkKahV1I4cOCAxkNygKrqo//9738mCYoQQkjT0CsplJSU1BrvyM7ODkVFRSYJihBCSNPQKynI5XIkJiZqTDt79mytZywTQghp2fQaEO+FF17Axx9/jISEBHh6eiIrKwuXL19GRESEqeMjhBBiRnolhW7dumHdunU4ffo0lEolfH19MX36dIt7UDYhhLR2ej95zdXVFePHj0d+fj4No00IIa2U3k9e27VrFxITEyEWi7Fv3z6cO3cOaWlpeO6550wdIyGEEDPRq6H5s88+g52dHbZu3QqxuCqPdOnSBQkJCSYNjhBCiHnpdaVw+fJl7Nixg08IAODk5IT8/HyTBUYIIcT89LpSsLOzQ2FhocY0pVJJbQuEENLK6JUUQkJCsG7dOvz1119gjOHatWvYsmULRowYYer4CCGEmJFe1UcTJkyAlZUVYmJiUFlZiW3btiE0NBSjR482dXyEEELMSK+kIBAIMGbMGIwZM8bgHSUnJ2P37t3gOA4hISEIDw/XmJ+UlIQDBw5AIBBAJBJh+vTp6Natm8H7I4QQ0nA6k8KVK1fQo0cPAMBff/2lewNiMdzc3GoNmFcdx3GIiYnBsmXLIJPJEBERgYCAAI1hMnr16oWAgAAIBALcvHkTGzZswMaNGw0oEiGEEEPpTAoxMTFYt24dAGDbtm06N8AYQ2FhIUaNGoWpU6dqXSYtLQ2enp7w8PAAAAQGBiIpKUkjKUgkEv7vsrIyCASChpWEEEJIo+lMCuqEAABbtmypcyMFBQVYsGCBzqSQl5encSUhk8mQmppaa7mzZ8/iiy++QH5+vs5xlWJjYxEbGwsAiIqKMnioDbFYbHHDdFCZLQOV2TKYqsx6D3PBcRyuXbuG+/fvQyqVonPnzhAKqzovOTk5YdmyZTrXZYzVmqbtSqB///7o378/rly5ggMHDmD58uW1lgkNDdV4CpxSqdS3CBpcXV0NXrelojJbBiqzZWhMmb28vHTO0ysp3Lx5E5988gkqKioglUqRl5cHKysrLFy4EB07dgQA+Pj46FxfJpNpPOM5Nze3znscevTogS1btqCgoABOTk76hEgIIcQI9EoK27Ztw8iRIzF27FgIBAIwxnDs2DFs27YNq1evrnd9Hx8fKBQK5OTkQCqVIiEhAfPnz9dYJisrCx4eHhAIBLh+/TpUKhUcHR0NKxUhhBCD6JUUFAoFxowZw1f5CAQCjB49GgcPHtRrJyKRCDNnzsSqVavAcRyCg4PRvn17nDhxAgAQFhaGxMREnDp1CiKRCNbW1njrrbeosZkQQsxMr6Tg5+eHc+fOoX///vy0c+fOwc/PT+8d+fv7w9/fX2NaWFgY/3d4eHitexcIIYSYl86ksHnzZv5MneM4bNy4EZ06deLbB65fv46AgACzBUoIIcT0dCYFT09Pjdft27fn/5bL5ejTp4/poiKEENIkdCaFZ5991pxxEEIIaQbqbVOorKxEfHw8Ll26hMLCQjg6OqJXr14YMmSIxvMVCCGEtHx1Dp1dXFyMZcuWYf/+/RCJRPD29oZIJMIXX3yB5cuXo7i42FxxEkIIMYM6T/W/+OILODk54b333tMYm6i0tBQbNmzAF198gVdeecXkQRJCCDGPOq8UkpKS8Oqrr2okBKBq8LqXX34ZZ8+eNWlwhBBCzKve6iOpVKp1nkwmQ0lJiUmCIoQQ0jTqTAoeHh46n6Vw+fJluLu7myQoQgghTaPOpDB27Fh8+umnSExMBMdxAKpuZEtMTMTWrVsxduxYswRJCCHEPOpsaB42bBgKCwuxdetWREdHw8nJCQUFBbCyssKkSZMQHBxsrjgJIYSYQb03GowbNw6hoaG4evUqf59Cly5dYGdnZ474CCGEmJFed5/Z2tqib9++Jg6FEEJIU6uzTYEQQohloaRACCGER0mBEEIIj5ICIYQQHiUFQgghPEoKhBBCeJQUCCGE8CgpEEII4VFSIIQQwqOkQAghhGe2hywnJydj9+7d4DgOISEhCA8P15gfHx+PI0eOAKh6iM8rr7yCjh07mis8QgghMNOVAsdxiImJwZIlS7BhwwacOXMGd+7c0VjG3d0dkZGRWLt2LZ555hns3LnTHKERQgipxixJIS0tDZ6envDw8IBYLEZgYCCSkpI0lunatSscHBwAAJ07d0Zubq45QiOEEFKNWaqP8vLyIJPJ+NcymQypqak6l//111/h5+endV5sbCxiY2MBAFFRUXB1dTUoJrFYbPC6LRWV2TJQmS2DqcpslqTAGKs1TSAQaF32r7/+wsmTJ/HBBx9onR8aGorQ0FD+tVKpNCgmV1dXg9dtqajMloHKbBkaU2YvLy+d88xSfSSTyTSqg3Jzc+Hi4lJruZs3b2LHjh1YtGgRHB0dzREaIYSQasySFHx8fKBQKJCTkwOVSoWEhAQEBARoLKNUKrF27VrMnTu3zixGCCHEdMxSfSQSiTBz5kysWrUKHMchODgY7du3x4kTJwAAYWFh+Prrr/Hw4UPs2rWLXycqKsoc4RFCCHlEwLRV+Lcgd+/eNWg9qoO0DFRmy0Blbpgmb1MghBDSMlBSIIQQwjPbMBeEkJaFMYbS0lJwHKezC3lzkZ2djbKysqYOw6zqKzNjDEKhEBKJpEGfHyUFQohWpaWlsLKygljc/A8TYrEYIpGoqcMwK33KrFKpUFpaCltbW723S9VHhBCtOI5rEQmB6CYWi8FxXIPWoaRACNGquVcZEf009HOkpEAIIYRHSYEQ0mzdvXsXM2bMwKBBgxAYGIgVK1agvLwcAHDgwAEsXbpU63rjx483aH8//fQTrl27xr/+5JNPcOrUKYO2pXbgwAG88cYbGtPy8vLQq1cvnQ3FdZXN1CgpEEKM4m5+GSKPZ2DuN6mIPJ6Bu/mN6w3EGMOrr76Kp556CmfOnEF8fDyKioqwevXqetf97rvvDNpnzaSwaNEiDB061KBtqY0ePRqnTp1CSUkJP+37779HWFgYbGxsGrVtU6CkQAhptLv5ZVhwOA0nrt7HhcyHOHH1PhYcTmtUYjh9+jRsbGwwZcoUAFVD30RGRuJ///sff4C9e/cuXnjhBQQGBmL9+vX8up07d+b/3rZtG0aPHo3Q0FCsXbuWn37w4EF+1OV58+YhKSkJP//8Mz788EOMGDECGRkZePPNN/H999/j119/xezZs/l1ExIS8NJLLwEA4uLiMG7cOIwcORKzZs1CUVGRRjkcHR0xcOBAflgfoCppTZgwASdOnMDYsWMRFhaGKVOm4N69e7XeB3UMDSlbY1BSIIQ02s5EBTILyjWmZRaUY2eiwuBtXrt2Db169dKY5ujoiHbt2uHGjRsAqh7zu3nzZvzyyy/4/vvvcfHiRY3l4+LicOPGDRw7dgwnTpzApUuXkJiYiKtXr2LTpk346quvEBsbiw8++AD9+vXDiBEjsGzZMvz8888ajwMeOnQoLly4gOLiYgBVB/Xx48cjLy8P0dHROHDgAI4fP44+ffpofWrkhAkT+KuXrKwsXL9+HYMGDUL//v1x9OhRnDhxAhMmTMDWrVv1fn9+++03rWVrLOpvRghpNOXDCu3Ti7RP1wdjTGvPmerThwwZAqlUCrFYjFGjRuHs2bPo06cPv2xcXBzi4uIQFhYGACguLsaNGzdw5coVjBkzBlKpFAC0DuVfnVgsRnBwMH7++WeMGTMGv/zyC5YtW4bff/8d165dw4QJEwAAFRUVeOKJJ2qtHxoaiiVLlqCwsBBHjx7FmDFjIBKJoFAo8PrrryMnJwfl5eXo0KGD3u/Pb7/9prVsAwcO1HsbWsvaqLUJIQSAq4OV9un22qfro0uXLvjhhx80phUWFuLu3bvo2LEjLl26VCtp1HzNGMPcuXPx4osvakyPiYlpcFfNcePGYe/evWjTpg369u0LBwcHMMYwdOjQes/wbW1tMWzYMPz44484cuQIIiMjAQDLly/HrFmzEBYWhoSEBI0qMLXq9xowxlBRUVFn2RqLqo8IIY02a2BbtHOy1pjWzskaswa2NXibQ4YMQUlJCQ4ePAgAqKysxAcffIDJkyfzd+jGx8fj/v37KCkpwfHjx9GvXz+NbQwbNgwHDhzg6/kVCgWUSiUGDx6Mo0ePIi8vDwBw//59AICDg0OtNgG1wMBAXL58Gfv378e4ceMAAE888QSSkpL46qySkhKkp6drXT88PBw7d+6EUqnkryYKCgrg6ekJAHw5a5LL5bh8+TIA4Pjx43xSCA4O1lq2xqKkQAhpNC9nG0Q/7Yuwri7wlzsgrKsLop/2hZez4b1rBAIBdu3ahe+//x6DBg3CkCFDYGNjg8WLF/PL9OvXD/Pnz0dISAhGjx7NVx2prwKCgoIQHh6O8ePHIyQkBLNmzcLDhw/RtWtXzJ8/H5MmTUJoaCjef/99AFV1/9u2bUNYWBgyMjI04hGJRAgNDcXJkycxYsQIAFVPldywYQPmzJmD0NBQjBs3TmdSCAoKQnZ2NsaPH8/H984772D27Nl4+umn+aqsml544QX8/vvvGDNmDP7880/Y2dkBqEp42srWWPQ8BQtCZbYMxipzcXExfwBq7sRiMVQqFYCqewCeeuopnD17tomjMq3qZa6Lts+RnqdACLEIWVlZGD9+PF577bWmDqXFooZmQkir4enpidOnTzd1GC0aXSkQQgjhUVIghBDCo6RACCGER0mBEEIIj5ICIcQoVOnpKN2zF8Wr16B0z16odPTXb4j27dtjxIgRCA0NxciRI5GUlGTQdj777DONUUrV1q1bh48//lhj2l9//YWgoCCd21q3bh22b99uUBwtgdmSQnJyMhYsWIB58+bh22+/rTU/MzMTS5cuxdSpUw0e9pYQ0jRU6eko/+ogWGEhBG5uYIWFKP/qYKMTg0Qiwc8//4zY2FhEREQgKirKoO3s2rVLa1KoPlCd2nfffYfw8HCD9tMamKVLKsdxiImJwbJlyyCTyRAREYGAgADI5XJ+GQcHB8yYMcPgMwFCiOlUnD0L9mhICK3zT58BKy2FoNoQEay0FGW794AbPEjrOgKpFFb9++sdQ2FhIZydnfnX27Ztw9GjR1FeXo7Ro0fj7bffRnFxMWbPng2FQgGO47BgwQIolUpkZ2fj2WefhYuLC77++mt+G76+vnBycsKFCxfg7+8PADh69Cj279/P/ysvL4e3tzc2bdrED6+hNmnSJCxfvhx9+vRBXl4eRo0ahT/++AOVlZX46KOP8Pvvv6O8vBwvvfSS0ccoMhWzJIW0tDR4enrCw8MDQNUYIklJSRpJwdnZGc7Ozrhw4YI5QiKEGBErKAAcHTUn2thUTW+E0tJSjBgxAmVlZcjJycFXX30FQHNIbMYYZsyYgcTEROTm5sLT0xP79u0DUDW2kJOTE3bu3ImDBw9qHUoiPDwcR44cgb+/P86fPw8XFxd06tQJbdq0wQsvvAAAWL16Nb788kvMnDlTr7i//PJLODo64ocffkBZWRnCw8MRFBTUoFFQm4pZkkJeXh5kMhn/WiaTITU11aBtxcbGIjY2FgAQFRUFV1dXg7YjFosNXrelojJbBmOVOTs7G2Jx1SFCHBhY98I598AVFkJYLTGoX9uNHWtwDBKJBCdPngQAJCUl4c0330RcXBzi4+Nx6tQpjBw5EgBQVFSEmzdvYuDAgVi5ciU+/vhjjBgxgh9GWiAQQCQS8eWpbuLEiRg7dixWrlyJo0ePYuLEiRCLxUhLS0NUVBTy8/NRVFSE4OBgiMViCIVCCIVCiMVije2KRCIIBAKIxWLEx8fjypUr/CivBQUFuHXrFjp16mTwe6GNtvLUZGNj06Dvg1mSgrbhlRo6bK2a+klJaoaO8aLP+DB388uwM1EB5cMKuDpYYdbAto0a4Kup0ThAlsFYZS4rK4NIJNJrWeGgQKi+OgjGcYC9PVBUBPbwIcRPjdRrfJ66qNf38/NDbm4usrOzUVlZiTlz5vBVMtXHAfrxxx/x66+/4sMPP0RQUBDeeustMMZQWVmpNRYPDw/I5XLEx8fj+++/x3fffQeVSoX58+cjJiYGjz/+OA4cOIDff/8dKpUKHMeB4zioVCoIhUJUVFRApVKhqKgIjDF+mZUrV2LYsGFay2IM+o59VFZWVuv70ORjH8lkMuTm5vKvc3Nz632oRVMzxeMFCWmtxD4+sJ78LASOjmD37kHg6Ajryc9C7ONjtH2kpaWhsrISLi4uOofEzsrKgq2tLZ555hm89tpr/JDTDg4OdY4gOmHCBERGRqJjx478AfPhw4fw8PBARUUFDh8+rHW99u3b49KlSwCAY8eO8dODgoLw3//+lx/mOj09nX9qW3NnlisFHx8fKBQK5OTkQCqVIiEhAfPnzzfHrg1W1+MFI0d2bJqgCGnGxD4+Rk0CwL9tCkBVjcPGjRshEokQFBSE1NRUjB8/HgBgb2+PTZs2ISMjAx9++CEEAgGsrKz47qYvvPACpk2bBnd3d42GZrVx48bhvffew8qVK/lpixYtwtixYyGXy9GtWzetSeW1117Da6+9hm+++QaDBv3boD516lTcvn0bTz31FBhjkEql+M9//mPU98ZUzDZ09oULF7B3715wHIfg4GBMnDiRf5B1WFgYHjx4gMWLF6OkpAQCgQASiQTr16+vd+heUw2dPfebVFzIrP0l8Jc74NOJnbWs0fxRVYplsPShsy2FqYbONtsoqf7+/nyXLzX1s0UBoE2bNs3qhhBTPF6QEEKaO7qjWQdTPF6QEEKaO3qegg7qxwvuTFRAWVQBV/uW3/uIkIZo4Q9lJI809HOkpFAHL2cbalQmFksoFEKlUunVF540T+pusw1BnzYhRCuJRILS0lKUlZUZfF+RudjY2KCszLK6i9dXZsYYhEIhJBJJg7ZLSYEQopVAIKg11k9zRb3MjIcamgkhhPAoKRBCCOFRUiCEEMIz2x3NhBBCmj+LvVJYvHhxU4dgdlRmy0BltgymKrPFJgVCCCG1UVIghBDCs9ikUP1BPZaCymwZqMyWwVRlpoZmQgghPIu9UiCEEFIbJQVCCCE8ixz7KDk5Gbt37wbHcQgJCUF4eHhTh2QUW7duxYULF+Ds7Ix169YBqHrO7IYNG3Dv3j24ubnhrbfegoODAwDg8OHD+PXXXyEUCjFjxgz07du3CaM3jFKpxJYtW/DgwQMIBAKEhoZi9OjRrbrc5eXleO+996BSqVBZWYmBAwdi8uTJrbrMAMBxHBYvXgypVIrFixe3+vICwJw5cyCRSCAUCiESiRAVFWX6cjMLU1lZyebOncuysrJYRUUFW7hwIbt9+3ZTh2UUKSkpLD09nb399tv8tH379rHDhw8zxhg7fPgw27dvH2OMsdu3b7OFCxey8vJylp2dzebOncsqKyubIuxGycvLY+np6YwxxoqLi9n8+fPZ7du3W3W5OY5jJSUljDHGKioqWEREBLt69WqrLjNjjB09epRt3LiRffzxx4yx1v/dZoyxN954g+Xn52tMM3W5La76KC0tDZ6envDw8IBYLEZgYCCSkpKaOiyj6NGjB3/GoJaUlISgoCAAQFBQEF/WpKQkBAYGwsrKCu7u7vD09ERaWprZY24sFxcXdOrUCQBga2uLdu3aIS8vr1WXW/0McwCorKxEZWUlBAJBqy5zbm4uLly4gJCQEH5aay5vXUxdbotLCnl5eZDJZPxrmUyGvLy8JozItPLz8+Hi4gKg6gBaUFAAoPb7IJVKW/z7kJOTgxs3bsDX17fVl5vjOCxatAivvPIKevXqhc6dO7fqMu/ZswfTpk3TeK5Day5vdatWrcK7776L2NhYAKYvt8W1KTAtPXCb+wNETEHb+9CSlZaWYt26dZg+fTrs7Ox0Ltdayi0UCvHJJ5+gqKgIa9euxa1bt3Qu29LLfP78eTg7O6NTp05ISUmpd/mWXt7qVq5cCalUivz8fHz44Yfw8vLSuayxym1xSUEmkyE3N5d/nZuby2fd1sjZ2Rn379+Hi4sL7t+/DycnJwC134e8vDxIpdKmCrNRVCoV1q1bhyFDhmDAgAEALKPcAGBvb48ePXogOTm51Zb56tWrOHfuHP7880+Ul5ejpKQEmzZtarXlrU4dt7OzM/r164e0tDSTl9viqo98fHygUCiQk5MDlUqFhIQEBAQENHVYJhMQEIC4uDgAQFxcHPr168dPT0hIQEVFBXJycqBQKODr69uUoRqEMYbt27ejXbt2GDt2LD+9NZe7oKAARUVFAKp6Il2+fBnt2rVrtWWeOnUqtm/fji1btuDNN99Ez549MX/+/FZbXrXS0lKUlJTwf1+6dAkdOnQwebkt8o7mCxcuYO/eveA4DsHBwZg4cWJTh2QUGzduxJUrV1BYWAhnZ2dMnjwZ/fr1w4YNG6BUKuHq6oq3336bb4w+dOgQTp48CaFQiOnTp8PPz6+JS9Bw//zzD1asWIEOHTrw1YDPP/88Onfu3GrLffPmTWzZsgUcx4ExhieffBKTJk1CYWFhqy2zWkpKCo4ePYrFixe3+vJmZ2dj7dq1AKo6FAwePBgTJ040ebktMikQQgjRzuKqjwghhOhGSYEQQgiPkgIhhBAeJQVCCCE8SgqEEEJ4lBQIMZG///4bCxYs0GvZ3377DcuXLzdxRITUz+LuaCZEXxEREZg/fz6EQiHWr1+P1atX48UXX+Tnl5eXQywWQyisOreaNWsWhgwZws/v3r07oqOjzR43IY1BSYEQLVQqFZRKJTw9PZGYmAhvb28AwL59+/hl5syZg9mzZ6N379611q+srIRIJDJbvIQYCyUFQrS4ffs25HI5BAIB0tPT+aSgS0pKCjZv3oynnnoKx44dQ+/evTF8+HBs3rwZ27dvBwB8++23+OWXX5Cfnw+ZTIbnn38e/fv3r7Utxhj27t2L06dPo6KiAm5ubpg/fz46dOhgkrISUh0lBUKqOXnyJPbu3QuVSgXGGKZPn47S0lJYW1vjyy+/xJo1a+Du7q513QcPHuDhw4fYunUrGGNITU3VmO/h4YH3338fbdq0QWJiIjZv3oxNmzbVGpDx4sWL+PvvvxEdHQ07OztkZmbC3t7eZGUmpDpqaCakmuDgYOzZswedOnXCqlWrsHbtWrRv3x579+7Fnj17dCYEoGoI9smTJ8PKygrW1ta15j/55JOQSqUQCoUIDAzU+RAUsViM0tJSZGZmgjEGuVzeqkfyJc0LXSkQ8sjDhw8xd+5cMMZQWlqKyMhIVFRUAABmzJiBZ599FmPGjNG5vpOTk9ZkoBYXF4fvv/8e9+7dA1A18mVhYWGt5Xr27ImRI0ciJiYGSqUS/fv3x4svvljncyIIMRZKCoQ84uDggD179uDMmTNISUnBrFmz8Mknn2DkyJFaG5NrquthTffu3cOOHTuwYsUKdOnSBUKhEIsWLdL5YJTRo0dj9OjRyM/Px4YNG/Ddd9/hueeeM7hshOiLkgIhNVy/fp1vWM7IyOCfAd0YZWVlEAgE/ANRTp48idu3b2tdNi0tDYwxeHt7w8bGBlZWVny3V0JMjZICITVcv34dTz75JAoLCyEUCvmx6htDLpdj7NixWLp0KYRCIYYOHYquXbtqXbakpAR79+5FdnY2rK2t0adPH4wfP77RMRCiD3qeAiGEEB5dkxJCCOFRUiCEEMKjpEAIIYRHSYEQQgiPkgIhhBAeJQVCCCE8SgqEEEJ4lBQIIYTw/j/hzbitZWugAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.706807     0.058992\n",
      "1                    TP        19.100000     4.532598\n",
      "2                    TN        98.800000     1.316561\n",
      "3                    FP         1.700000     1.159502\n",
      "4                    FN        14.300000     4.620005\n",
      "5              Accuracy         0.880524     0.038491\n",
      "6             Precision         0.912936     0.064803\n",
      "7           Sensitivity         0.572215     0.136154\n",
      "8           Specificity         0.983080     0.011553\n",
      "9              F1 score         0.697505     0.117449\n",
      "10  F1 score (weighted)         0.868529     0.046592\n",
      "11     F1 score (macro)         0.811474     0.070053\n",
      "12    Balanced Accuracy         0.777647     0.070419\n",
      "13                  MCC         0.659419     0.120487\n",
      "14                  NPV         0.874750     0.036277\n",
      "15              ROC_AUC         0.777647     0.070419\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.686631</td>\n",
       "      <td>0.637557</td>\n",
       "      <td>0.707517</td>\n",
       "      <td>0.672644</td>\n",
       "      <td>0.713204</td>\n",
       "      <td>0.640578</td>\n",
       "      <td>0.702467</td>\n",
       "      <td>0.673087</td>\n",
       "      <td>0.554281</td>\n",
       "      <td>0.751018</td>\n",
       "      <td>0.673898</td>\n",
       "      <td>0.054101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>36.700000</td>\n",
       "      <td>3.945462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>198.800000</td>\n",
       "      <td>1.873796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.475730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>29.700000</td>\n",
       "      <td>4.270051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.854478</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.854478</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.878731</td>\n",
       "      <td>0.018469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.928947</td>\n",
       "      <td>0.038473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.552993</td>\n",
       "      <td>0.061604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.980300</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.990100</td>\n",
       "      <td>0.990100</td>\n",
       "      <td>0.975200</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.986090</td>\n",
       "      <td>0.007328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.691589</td>\n",
       "      <td>0.715596</td>\n",
       "      <td>0.635514</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.679612</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.691771</td>\n",
       "      <td>0.054878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.891542</td>\n",
       "      <td>0.865205</td>\n",
       "      <td>0.874449</td>\n",
       "      <td>0.839676</td>\n",
       "      <td>0.870412</td>\n",
       "      <td>0.837993</td>\n",
       "      <td>0.863655</td>\n",
       "      <td>0.891127</td>\n",
       "      <td>0.838865</td>\n",
       "      <td>0.894926</td>\n",
       "      <td>0.866785</td>\n",
       "      <td>0.022173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.846614</td>\n",
       "      <td>0.807333</td>\n",
       "      <td>0.821498</td>\n",
       "      <td>0.772302</td>\n",
       "      <td>0.811847</td>\n",
       "      <td>0.762743</td>\n",
       "      <td>0.801700</td>\n",
       "      <td>0.842363</td>\n",
       "      <td>0.765645</td>\n",
       "      <td>0.849256</td>\n",
       "      <td>0.808130</td>\n",
       "      <td>0.032876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.808458</td>\n",
       "      <td>0.768657</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.774763</td>\n",
       "      <td>0.721393</td>\n",
       "      <td>0.760201</td>\n",
       "      <td>0.802766</td>\n",
       "      <td>0.730048</td>\n",
       "      <td>0.808131</td>\n",
       "      <td>0.769550</td>\n",
       "      <td>0.032188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.721125</td>\n",
       "      <td>0.652929</td>\n",
       "      <td>0.675562</td>\n",
       "      <td>0.586156</td>\n",
       "      <td>0.654317</td>\n",
       "      <td>0.599480</td>\n",
       "      <td>0.649950</td>\n",
       "      <td>0.713942</td>\n",
       "      <td>0.574631</td>\n",
       "      <td>0.730355</td>\n",
       "      <td>0.655845</td>\n",
       "      <td>0.056028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.888400</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>0.876100</td>\n",
       "      <td>0.851500</td>\n",
       "      <td>0.876700</td>\n",
       "      <td>0.843900</td>\n",
       "      <td>0.865800</td>\n",
       "      <td>0.889400</td>\n",
       "      <td>0.852800</td>\n",
       "      <td>0.889400</td>\n",
       "      <td>0.870240</td>\n",
       "      <td>0.016708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.808458</td>\n",
       "      <td>0.768657</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.774763</td>\n",
       "      <td>0.721393</td>\n",
       "      <td>0.760201</td>\n",
       "      <td>0.802766</td>\n",
       "      <td>0.730048</td>\n",
       "      <td>0.808131</td>\n",
       "      <td>0.769550</td>\n",
       "      <td>0.032188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.686631    0.637557    0.707517    0.672644   \n",
       "1                    TP   42.000000   37.000000   39.000000   34.000000   \n",
       "2                    TN  199.000000  198.000000  198.000000  195.000000   \n",
       "3                    FP    2.000000    3.000000    3.000000    5.000000   \n",
       "4                    FN   25.000000   30.000000   28.000000   34.000000   \n",
       "5              Accuracy    0.899254    0.876866    0.884328    0.854478   \n",
       "6             Precision    0.954545    0.925000    0.928571    0.871795   \n",
       "7           Sensitivity    0.626866    0.552239    0.582090    0.500000   \n",
       "8           Specificity    0.990000    0.985100    0.985100    0.975000   \n",
       "9              F1 score    0.756757    0.691589    0.715596    0.635514   \n",
       "10  F1 score (weighted)    0.891542    0.865205    0.874449    0.839676   \n",
       "11     F1 score (macro)    0.846614    0.807333    0.821498    0.772302   \n",
       "12    Balanced Accuracy    0.808458    0.768657    0.783582    0.737500   \n",
       "13                  MCC    0.721125    0.652929    0.675562    0.586156   \n",
       "14                  NPV    0.888400    0.868400    0.876100    0.851500   \n",
       "15              ROC_AUC    0.808458    0.768657    0.783582    0.737500   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.713204    0.640578    0.702467    0.673087    0.554281    0.751018   \n",
       "1    37.000000   30.000000   35.000000   40.000000   32.000000   41.000000   \n",
       "2   199.000000  200.000000  200.000000  201.000000  197.000000  201.000000   \n",
       "3     4.000000    1.000000    2.000000    2.000000    5.000000    1.000000   \n",
       "4    28.000000   37.000000   31.000000   25.000000   34.000000   25.000000   \n",
       "5     0.880597    0.858209    0.876866    0.899254    0.854478    0.902985   \n",
       "6     0.902439    0.967742    0.945946    0.952381    0.864865    0.976190   \n",
       "7     0.569231    0.447761    0.530303    0.615385    0.484848    0.621212   \n",
       "8     0.980300    0.995000    0.990100    0.990100    0.975200    0.995000   \n",
       "9     0.698113    0.612245    0.679612    0.747664    0.621359    0.759259   \n",
       "10    0.870412    0.837993    0.863655    0.891127    0.838865    0.894926   \n",
       "11    0.811847    0.762743    0.801700    0.842363    0.765645    0.849256   \n",
       "12    0.774763    0.721393    0.760201    0.802766    0.730048    0.808131   \n",
       "13    0.654317    0.599480    0.649950    0.713942    0.574631    0.730355   \n",
       "14    0.876700    0.843900    0.865800    0.889400    0.852800    0.889400   \n",
       "15    0.774763    0.721393    0.760201    0.802766    0.730048    0.808131   \n",
       "\n",
       "           ave       std  \n",
       "0     0.673898  0.054101  \n",
       "1    36.700000  3.945462  \n",
       "2   198.800000  1.873796  \n",
       "3     2.800000  1.475730  \n",
       "4    29.700000  4.270051  \n",
       "5     0.878731  0.018469  \n",
       "6     0.928947  0.038473  \n",
       "7     0.552993  0.061604  \n",
       "8     0.986090  0.007328  \n",
       "9     0.691771  0.054878  \n",
       "10    0.866785  0.022173  \n",
       "11    0.808130  0.032876  \n",
       "12    0.769550  0.032188  \n",
       "13    0.655845  0.056028  \n",
       "14    0.870240  0.016708  \n",
       "15    0.769550  0.032188  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test['std'] = mat_met_lgbm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_lgbm0</th>\n",
       "      <th>y_pred_lgbm1</th>\n",
       "      <th>y_pred_lgbm2</th>\n",
       "      <th>y_pred_lgbm3</th>\n",
       "      <th>y_pred_lgbm4</th>\n",
       "      <th>y_pred_lgbm_ave</th>\n",
       "      <th>y_pred_lgbm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4635479</td>\n",
       "      <td>0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.679267</td>\n",
       "      <td>0.656525</td>\n",
       "      <td>0.752455</td>\n",
       "      <td>0.713056</td>\n",
       "      <td>0.286577</td>\n",
       "      <td>0.584647</td>\n",
       "      <td>0.170647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4299417</td>\n",
       "      <td>1</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.610825</td>\n",
       "      <td>0.678077</td>\n",
       "      <td>0.743565</td>\n",
       "      <td>0.632929</td>\n",
       "      <td>0.623071</td>\n",
       "      <td>0.711411</td>\n",
       "      <td>0.128040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4225331</td>\n",
       "      <td>2</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.593105</td>\n",
       "      <td>1.262545</td>\n",
       "      <td>1.034978</td>\n",
       "      <td>1.691965</td>\n",
       "      <td>1.817377</td>\n",
       "      <td>1.373328</td>\n",
       "      <td>0.355482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL1094710</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.016776</td>\n",
       "      <td>0.197949</td>\n",
       "      <td>-0.223303</td>\n",
       "      <td>-0.285149</td>\n",
       "      <td>-0.270049</td>\n",
       "      <td>-0.024555</td>\n",
       "      <td>0.271554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3287256</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.068452</td>\n",
       "      <td>-0.254472</td>\n",
       "      <td>-0.226262</td>\n",
       "      <td>0.012476</td>\n",
       "      <td>-0.014633</td>\n",
       "      <td>-0.146891</td>\n",
       "      <td>0.129386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>CHEMBL3769491</td>\n",
       "      <td>1334</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.688502</td>\n",
       "      <td>-0.083767</td>\n",
       "      <td>-0.185900</td>\n",
       "      <td>-0.033903</td>\n",
       "      <td>-0.373522</td>\n",
       "      <td>-0.112599</td>\n",
       "      <td>0.419585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>CHEMBL482095</td>\n",
       "      <td>1335</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.634463</td>\n",
       "      <td>0.812666</td>\n",
       "      <td>0.540992</td>\n",
       "      <td>0.783357</td>\n",
       "      <td>0.586538</td>\n",
       "      <td>0.633003</td>\n",
       "      <td>0.130870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>CHEMBL4095596</td>\n",
       "      <td>1336</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.661237</td>\n",
       "      <td>1.130119</td>\n",
       "      <td>0.804389</td>\n",
       "      <td>0.982399</td>\n",
       "      <td>1.263233</td>\n",
       "      <td>0.936896</td>\n",
       "      <td>0.209787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>CHEMBL4072925</td>\n",
       "      <td>1337</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.707674</td>\n",
       "      <td>0.819515</td>\n",
       "      <td>0.933265</td>\n",
       "      <td>0.980230</td>\n",
       "      <td>0.756062</td>\n",
       "      <td>0.802791</td>\n",
       "      <td>0.124787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>CHEMBL3774993</td>\n",
       "      <td>1338</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.883388</td>\n",
       "      <td>1.022834</td>\n",
       "      <td>0.683974</td>\n",
       "      <td>0.917414</td>\n",
       "      <td>0.994151</td>\n",
       "      <td>0.911960</td>\n",
       "      <td>0.111983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1339 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_lgbm0  y_pred_lgbm1  \\\n",
       "0         CHEMBL4635479            0     0.42      0.679267      0.656525   \n",
       "1         CHEMBL4299417            1     0.98      0.610825      0.678077   \n",
       "2         CHEMBL4225331            2     0.84      1.593105      1.262545   \n",
       "3         CHEMBL1094710            3     0.45     -0.016776      0.197949   \n",
       "4         CHEMBL3287256            4    -0.33     -0.068452     -0.254472   \n",
       "...                 ...          ...      ...           ...           ...   \n",
       "1334      CHEMBL3769491         1334     0.69     -0.688502     -0.083767   \n",
       "1335       CHEMBL482095         1335     0.44      0.634463      0.812666   \n",
       "1336      CHEMBL4095596         1336     0.78      0.661237      1.130119   \n",
       "1337      CHEMBL4072925         1337     0.62      0.707674      0.819515   \n",
       "1338      CHEMBL3774993         1338     0.97      0.883388      1.022834   \n",
       "\n",
       "      y_pred_lgbm2  y_pred_lgbm3  y_pred_lgbm4  y_pred_lgbm_ave  \\\n",
       "0         0.752455      0.713056      0.286577         0.584647   \n",
       "1         0.743565      0.632929      0.623071         0.711411   \n",
       "2         1.034978      1.691965      1.817377         1.373328   \n",
       "3        -0.223303     -0.285149     -0.270049        -0.024555   \n",
       "4        -0.226262      0.012476     -0.014633        -0.146891   \n",
       "...            ...           ...           ...              ...   \n",
       "1334     -0.185900     -0.033903     -0.373522        -0.112599   \n",
       "1335      0.540992      0.783357      0.586538         0.633003   \n",
       "1336      0.804389      0.982399      1.263233         0.936896   \n",
       "1337      0.933265      0.980230      0.756062         0.802791   \n",
       "1338      0.683974      0.917414      0.994151         0.911960   \n",
       "\n",
       "      y_pred_lgbm_std  \n",
       "0            0.170647  \n",
       "1            0.128040  \n",
       "2            0.355482  \n",
       "3            0.271554  \n",
       "4            0.129386  \n",
       "...               ...  \n",
       "1334         0.419585  \n",
       "1335         0.130870  \n",
       "1336         0.209787  \n",
       "1337         0.124787  \n",
       "1338         0.111983  \n",
       "\n",
       "[1339 rows x 10 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_lgbm_cat = np.where(((y_pred_optimized_lgbm >= 2) | (y_pred_optimized_lgbm <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_lgbm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_lgbm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "lgbm_run0 = data_lgbm[['y_test_idx0', 'y_test0', 'y_pred_lgbm0']]\n",
    "lgbm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "lgbm_run0.reset_index(inplace=True, drop=True)\n",
    "lgbm_run1 = data_lgbm[['y_test_idx1', 'y_test1', 'y_pred_lgbm1']]\n",
    "lgbm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "lgbm_run1.reset_index(inplace=True, drop=True)\n",
    "lgbm_run2 = data_lgbm[['y_test_idx2', 'y_test2', 'y_pred_lgbm2']]\n",
    "lgbm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "lgbm_run2.reset_index(inplace=True, drop=True)\n",
    "lgbm_run3 = data_lgbm[['y_test_idx3', 'y_test3', 'y_pred_lgbm3']]\n",
    "lgbm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "lgbm_run3.reset_index(inplace=True, drop=True)\n",
    "lgbm_run4 = data_lgbm[['y_test_idx4', 'y_test4', 'y_pred_lgbm4']]\n",
    "lgbm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "lgbm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "lgbm_5preds = pd.concat([chembl_id, lgbm_run0, lgbm_run1, lgbm_run2, lgbm_run3, lgbm_run4], axis=1)\n",
    "lgbm_5preds = lgbm_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_lgbm0', 'y_pred_lgbm1', 'y_pred_lgbm2', 'y_pred_lgbm3', 'y_pred_lgbm4']]\n",
    "lgbm_5preds['y_pred_lgbm_ave'] = lgbm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "lgbm_5preds['y_pred_lgbm_std'] = lgbm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "lgbm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c16510fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_lgbm.to_csv(output/'mat_met_lgbm_opt.csv')\n",
    "lgbm_5preds.to_csv(output/'lgbm_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db4ac315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model r2_score 0.6822 with a standard deviation of 0.0715\n",
      "LightGBM optimized model r2_score 0.6772 with a standard deviation of 0.0632\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#r2_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"r2\")\n",
    "r2_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"r2\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)r2_score %0.4f with a standard deviation of %0.4f\" % (r2_cv_lgbm_opt_testSet.mean(), r2_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(r2_cv_lgbm_opt), np.std(r2_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_lgbm_noSemiSel.joblib']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm_reg, output/\"lgbm_reg.joblib\")\n",
    "#joblib.dump(optimized_lgbm, output/\"optimized_lgbm.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, output/\"optimizedCV_lgbm.joblib\") \n",
    "#loaded_rf = joblib.load(output/\"optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.709165     0.044926\n",
      "1                    TP        18.700000     3.233505\n",
      "2                    TN        98.600000     1.429841\n",
      "3                    FP         1.900000     1.370320\n",
      "4                    FN        14.700000     3.267687\n",
      "5              Accuracy         0.876035     0.030063\n",
      "6             Precision         0.905787     0.070225\n",
      "7           Sensitivity         0.560004     0.096416\n",
      "8           Specificity         0.981100     0.013582\n",
      "9              F1 score         0.689195     0.087868\n",
      "10  F1 score (weighted)         0.864317     0.035421\n",
      "11     F1 score (macro)         0.805856     0.052858\n",
      "12    Balanced Accuracy         0.770552     0.051561\n",
      "13                  MCC         0.647270     0.094934\n",
      "14                  NPV         0.870800     0.025752\n",
      "15              ROC_AUC         0.770552     0.051561\n",
      "CPU times: user 2h 21min 20s, sys: 5.22 s, total: 2h 21min 25s\n",
      "Wall time: 6min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_reg.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores),np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=24, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=24, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 23:08:57,747]\u001b[0m A new study created in memory with name: XGBRegressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:09:21,603]\u001b[0m Trial 0 finished with value: 0.6827092947827509 and parameters: {'n_estimators': 482, 'eta': 0.03402893476978763, 'max_depth': 5, 'alpha': 0.448, 'lambda': 7.958478585760083, 'max_bin': 468}. Best is trial 0 with value: 0.6827092947827509.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:09:38,695]\u001b[0m Trial 1 finished with value: 0.6890287916009374 and parameters: {'n_estimators': 262, 'eta': 0.04595099059360414, 'max_depth': 7, 'alpha': 0.5686, 'lambda': 2.9521031344408315, 'max_bin': 359}. Best is trial 1 with value: 0.6890287916009374.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:10:06,112]\u001b[0m Trial 2 finished with value: 0.6938284290986481 and parameters: {'n_estimators': 748, 'eta': 0.06621039753260158, 'max_depth': 9, 'alpha': 0.8646, 'lambda': 17.069465070938083, 'max_bin': 433}. Best is trial 2 with value: 0.6938284290986481.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:10:26,095]\u001b[0m Trial 3 finished with value: 0.6892085401433536 and parameters: {'n_estimators': 762, 'eta': 0.09211006635307434, 'max_depth': 12, 'alpha': 0.6567000000000001, 'lambda': 22.173292615596292, 'max_bin': 376}. Best is trial 2 with value: 0.6938284290986481.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:10:43,020]\u001b[0m Trial 4 finished with value: 0.6901936563871418 and parameters: {'n_estimators': 888, 'eta': 0.08584225632660569, 'max_depth': 6, 'alpha': 0.3244, 'lambda': 22.942559570595098, 'max_bin': 392}. Best is trial 2 with value: 0.6938284290986481.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:11:10,519]\u001b[0m Trial 5 finished with value: 0.6925025706251117 and parameters: {'n_estimators': 418, 'eta': 0.05074399971462485, 'max_depth': 10, 'alpha': 0.5206000000000001, 'lambda': 2.7348896479617335, 'max_bin': 452}. Best is trial 2 with value: 0.6938284290986481.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:11:32,602]\u001b[0m Trial 6 finished with value: 0.6840038187541688 and parameters: {'n_estimators': 816, 'eta': 0.04330462846975853, 'max_depth': 5, 'alpha': 0.3864, 'lambda': 7.471079728847228, 'max_bin': 271}. Best is trial 2 with value: 0.6938284290986481.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:11:52,921]\u001b[0m Trial 7 finished with value: 0.6835529477013218 and parameters: {'n_estimators': 337, 'eta': 0.047790058192688244, 'max_depth': 5, 'alpha': 0.6434000000000001, 'lambda': 20.343191252371128, 'max_bin': 429}. Best is trial 2 with value: 0.6938284290986481.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:12:43,367]\u001b[0m Trial 8 finished with value: 0.6781625076047987 and parameters: {'n_estimators': 617, 'eta': 0.015117320380055822, 'max_depth': 7, 'alpha': 0.1557, 'lambda': 32.62257344034907, 'max_bin': 450}. Best is trial 2 with value: 0.6938284290986481.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:12:52,760]\u001b[0m Trial 9 finished with value: 0.4435401173724921 and parameters: {'n_estimators': 86, 'eta': 0.012161511229609852, 'max_depth': 9, 'alpha': 0.35700000000000004, 'lambda': 20.19391453973463, 'max_bin': 253}. Best is trial 2 with value: 0.6938284290986481.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:13:26,724]\u001b[0m Trial 10 finished with value: 0.6925367467522257 and parameters: {'n_estimators': 667, 'eta': 0.07552287480079808, 'max_depth': 11, 'alpha': 0.9670000000000001, 'lambda': 38.731729765852364, 'max_bin': 309}. Best is trial 2 with value: 0.6938284290986481.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:13:57,899]\u001b[0m Trial 11 finished with value: 0.6933183611872801 and parameters: {'n_estimators': 639, 'eta': 0.07437726012116772, 'max_depth': 11, 'alpha': 0.9708, 'lambda': 37.59785994149565, 'max_bin': 315}. Best is trial 2 with value: 0.6938284290986481.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:14:24,118]\u001b[0m Trial 12 finished with value: 0.6904175127965055 and parameters: {'n_estimators': 590, 'eta': 0.07140250605433897, 'max_depth': 9, 'alpha': 0.9890000000000001, 'lambda': 28.640665848601394, 'max_bin': 321}. Best is trial 2 with value: 0.6938284290986481.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:14:49,199]\u001b[0m Trial 13 finished with value: 0.6948756854822173 and parameters: {'n_estimators': 686, 'eta': 0.06371086130457396, 'max_depth': 11, 'alpha': 0.8143, 'lambda': 12.972259278883232, 'max_bin': 403}. Best is trial 13 with value: 0.6948756854822173.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:15:12,979]\u001b[0m Trial 14 finished with value: 0.6974887659457893 and parameters: {'n_estimators': 736, 'eta': 0.06297753193509197, 'max_depth': 8, 'alpha': 0.8122, 'lambda': 12.657556226356792, 'max_bin': 414}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:15:36,134]\u001b[0m Trial 15 finished with value: 0.696265291648277 and parameters: {'n_estimators': 506, 'eta': 0.05934649952007727, 'max_depth': 8, 'alpha': 0.7782, 'lambda': 12.873587033014614, 'max_bin': 500}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:16:09,742]\u001b[0m Trial 16 finished with value: 0.6963854038080883 and parameters: {'n_estimators': 514, 'eta': 0.024993669831425046, 'max_depth': 8, 'alpha': 0.7614000000000001, 'lambda': 12.67759205581483, 'max_bin': 496}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:16:30,384]\u001b[0m Trial 17 finished with value: 0.681640674768133 and parameters: {'n_estimators': 282, 'eta': 0.02506731066626241, 'max_depth': 7, 'alpha': 0.7150000000000001, 'lambda': 12.819996560713403, 'max_bin': 500}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:17:20,570]\u001b[0m Trial 18 finished with value: 0.657134912064482 and parameters: {'n_estimators': 560, 'eta': 0.005656386374221185, 'max_depth': 8, 'alpha': 0.8373, 'lambda': 7.653228107412666, 'max_bin': 351}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:17:52,391]\u001b[0m Trial 19 finished with value: 0.6947579249488907 and parameters: {'n_estimators': 410, 'eta': 0.02917403107279689, 'max_depth': 8, 'alpha': 0.0315, 'lambda': 15.78634007984459, 'max_bin': 481}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:18:30,701]\u001b[0m Trial 20 finished with value: 0.6919186327046452 and parameters: {'n_estimators': 897, 'eta': 0.0367057394121975, 'max_depth': 6, 'alpha': 0.6153000000000001, 'lambda': 26.247452773604156, 'max_bin': 412}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:18:56,501]\u001b[0m Trial 21 finished with value: 0.6963370612583546 and parameters: {'n_estimators': 512, 'eta': 0.058843793612834205, 'max_depth': 8, 'alpha': 0.7486, 'lambda': 11.947232458682443, 'max_bin': 496}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:19:20,358]\u001b[0m Trial 22 finished with value: 0.6925414525400837 and parameters: {'n_estimators': 537, 'eta': 0.056436403872488564, 'max_depth': 10, 'alpha': 0.7259, 'lambda': 10.157063496586401, 'max_bin': 472}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:19:33,086]\u001b[0m Trial 23 finished with value: 0.6889092991180197 and parameters: {'n_estimators': 177, 'eta': 0.0843960459164371, 'max_depth': 8, 'alpha': 0.8636, 'lambda': 15.770033436634527, 'max_bin': 447}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:20:03,273]\u001b[0m Trial 24 finished with value: 0.6879826394338793 and parameters: {'n_estimators': 408, 'eta': 0.019379290219100873, 'max_depth': 7, 'alpha': 0.7308, 'lambda': 5.387860013011794, 'max_bin': 481}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:20:37,317]\u001b[0m Trial 25 finished with value: 0.6950838530555454 and parameters: {'n_estimators': 729, 'eta': 0.05515533295003647, 'max_depth': 10, 'alpha': 0.882, 'lambda': 10.57720698468829, 'max_bin': 423}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:20:56,783]\u001b[0m Trial 26 finished with value: 0.6908623183947327 and parameters: {'n_estimators': 814, 'eta': 0.09966429308757463, 'max_depth': 6, 'alpha': 0.9139, 'lambda': 17.384576998924704, 'max_bin': 500}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:21:29,751]\u001b[0m Trial 27 finished with value: 0.6955268493214308 and parameters: {'n_estimators': 454, 'eta': 0.038980891000771546, 'max_depth': 9, 'alpha': 0.7738, 'lambda': 10.165855879449719, 'max_bin': 458}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:22:06,040]\u001b[0m Trial 28 finished with value: 0.22068320445675366 and parameters: {'n_estimators': 334, 'eta': 0.0010258456395925292, 'max_depth': 8, 'alpha': 0.5587, 'lambda': 14.136012769821166, 'max_bin': 393}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 23:22:35,287]\u001b[0m Trial 29 finished with value: 0.6933253324123434 and parameters: {'n_estimators': 485, 'eta': 0.032394866264748995, 'max_depth': 7, 'alpha': 0.46580000000000005, 'lambda': 4.30346507940205, 'max_bin': 477}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:22:55,278]\u001b[0m Trial 30 finished with value: 0.6926848145997498 and parameters: {'n_estimators': 536, 'eta': 0.0659382647650673, 'max_depth': 9, 'alpha': 0.6542, 'lambda': 1.3070950703156434, 'max_bin': 463}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:23:24,347]\u001b[0m Trial 31 finished with value: 0.6956456059411404 and parameters: {'n_estimators': 484, 'eta': 0.05917800684840519, 'max_depth': 8, 'alpha': 0.7857000000000001, 'lambda': 10.133777872401094, 'max_bin': 491}. Best is trial 14 with value: 0.6974887659457893.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:23:48,381]\u001b[0m Trial 32 finished with value: 0.7001396862668093 and parameters: {'n_estimators': 500, 'eta': 0.05306786980781708, 'max_depth': 8, 'alpha': 0.7111000000000001, 'lambda': 12.58188570907714, 'max_bin': 491}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:24:14,403]\u001b[0m Trial 33 finished with value: 0.6893195806677908 and parameters: {'n_estimators': 347, 'eta': 0.05438666324623164, 'max_depth': 7, 'alpha': 0.7059000000000001, 'lambda': 18.567788171346887, 'max_bin': 439}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:24:44,173]\u001b[0m Trial 34 finished with value: 0.6962529921757095 and parameters: {'n_estimators': 702, 'eta': 0.045143809330050536, 'max_depth': 9, 'alpha': 0.5961000000000001, 'lambda': 6.409110211750514, 'max_bin': 467}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:25:12,979]\u001b[0m Trial 35 finished with value: 0.6954656094299025 and parameters: {'n_estimators': 588, 'eta': 0.06872793509197665, 'max_depth': 8, 'alpha': 0.907, 'lambda': 23.192940134990906, 'max_bin': 357}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:25:27,366]\u001b[0m Trial 36 finished with value: 0.6897743468562565 and parameters: {'n_estimators': 237, 'eta': 0.08092644322305109, 'max_depth': 6, 'alpha': 0.51, 'lambda': 15.101147386020468, 'max_bin': 485}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:26:01,793]\u001b[0m Trial 37 finished with value: 0.6977009541799144 and parameters: {'n_estimators': 807, 'eta': 0.04394267778657876, 'max_depth': 7, 'alpha': 0.6952, 'lambda': 11.646110800221894, 'max_bin': 368}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:26:50,480]\u001b[0m Trial 38 finished with value: 0.6966402207699821 and parameters: {'n_estimators': 800, 'eta': 0.024957627166317928, 'max_depth': 7, 'alpha': 0.6779000000000001, 'lambda': 18.34807679369529, 'max_bin': 377}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:27:28,048]\u001b[0m Trial 39 finished with value: 0.6951077654682452 and parameters: {'n_estimators': 798, 'eta': 0.04093343208141085, 'max_depth': 7, 'alpha': 0.6697000000000001, 'lambda': 17.97624216727542, 'max_bin': 379}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:28:01,727]\u001b[0m Trial 40 finished with value: 0.6907805407227168 and parameters: {'n_estimators': 848, 'eta': 0.050916341277834815, 'max_depth': 6, 'alpha': 0.5690000000000001, 'lambda': 24.949059504563657, 'max_bin': 338}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:28:45,931]\u001b[0m Trial 41 finished with value: 0.6972283123316783 and parameters: {'n_estimators': 757, 'eta': 0.022014919659932145, 'max_depth': 7, 'alpha': 0.6716000000000001, 'lambda': 8.531342067405898, 'max_bin': 371}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:29:33,453]\u001b[0m Trial 42 finished with value: 0.696918642260728 and parameters: {'n_estimators': 768, 'eta': 0.021157789004152272, 'max_depth': 7, 'alpha': 0.6846, 'lambda': 8.423670230089309, 'max_bin': 370}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:30:10,577]\u001b[0m Trial 43 finished with value: 0.682724146548283 and parameters: {'n_estimators': 760, 'eta': 0.016454147470257495, 'max_depth': 5, 'alpha': 0.4423, 'lambda': 4.404023782303277, 'max_bin': 366}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:30:51,566]\u001b[0m Trial 44 finished with value: 0.6930713479354544 and parameters: {'n_estimators': 860, 'eta': 0.03285280715091834, 'max_depth': 7, 'alpha': 0.6078, 'lambda': 8.543309883532798, 'max_bin': 342}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:31:19,625]\u001b[0m Trial 45 finished with value: 0.6948004770310893 and parameters: {'n_estimators': 740, 'eta': 0.04944171934277668, 'max_depth': 6, 'alpha': 0.8114, 'lambda': 9.264953071526303, 'max_bin': 388}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:31:57,760]\u001b[0m Trial 46 finished with value: 0.6946505002191785 and parameters: {'n_estimators': 629, 'eta': 0.021227916205977346, 'max_depth': 7, 'alpha': 0.6754, 'lambda': 7.170580558818377, 'max_bin': 298}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:32:48,228]\u001b[0m Trial 47 finished with value: 0.6718924203183878 and parameters: {'n_estimators': 656, 'eta': 0.008153637274544275, 'max_depth': 6, 'alpha': 0.5637, 'lambda': 2.8324435988814507, 'max_bin': 366}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:33:15,156]\u001b[0m Trial 48 finished with value: 0.6969470127394323 and parameters: {'n_estimators': 706, 'eta': 0.06378017101923092, 'max_depth': 7, 'alpha': 0.256, 'lambda': 11.382556181300439, 'max_bin': 333}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:33:54,865]\u001b[0m Trial 49 finished with value: 0.6944973844314075 and parameters: {'n_estimators': 709, 'eta': 0.06305227795013238, 'max_depth': 12, 'alpha': 0.24430000000000002, 'lambda': 11.687203729773177, 'max_bin': 333}. Best is trial 32 with value: 0.7001396862668093.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.7001\n",
      "\tBest params:\n",
      "\t\tn_estimators: 500\n",
      "\t\teta: 0.05306786980781708\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.7111000000000001\n",
      "\t\tlambda: 12.58188570907714\n",
      "\t\tmax_bin: 491\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBRegressor\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.706611\n",
      "1                    TP   42.000000\n",
      "2                    TN  197.000000\n",
      "3                    FP    4.000000\n",
      "4                    FN   25.000000\n",
      "5              Accuracy    0.891791\n",
      "6             Precision    0.913043\n",
      "7           Sensitivity    0.626866\n",
      "8           Specificity    0.980100\n",
      "9              F1 score    0.743363\n",
      "10  F1 score (weighted)    0.884422\n",
      "11     F1 score (macro)    0.837402\n",
      "12    Balanced Accuracy    0.803483\n",
      "13                  MCC    0.697018\n",
      "14                  NPV    0.887400\n",
      "15              ROC_AUC    0.803483\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=24,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_xgb_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_xgb_0_cat = np.where(((y_pred_xgb_0 >= 2) | (y_pred_xgb_0 <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 23:34:28,884]\u001b[0m Trial 50 finished with value: 0.7088453281643636 and parameters: {'n_estimators': 848, 'eta': 0.07572317879188396, 'max_depth': 9, 'alpha': 0.2033, 'lambda': 14.310195881509664, 'max_bin': 290}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:35:01,654]\u001b[0m Trial 51 finished with value: 0.7062613423958494 and parameters: {'n_estimators': 861, 'eta': 0.07620539043698157, 'max_depth': 9, 'alpha': 0.1976, 'lambda': 14.047509870913695, 'max_bin': 294}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:35:24,623]\u001b[0m Trial 52 finished with value: 0.7029013714322039 and parameters: {'n_estimators': 854, 'eta': 0.07994390575995526, 'max_depth': 10, 'alpha': 0.14700000000000002, 'lambda': 15.090829316516743, 'max_bin': 288}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:35:49,925]\u001b[0m Trial 53 finished with value: 0.704337631280443 and parameters: {'n_estimators': 867, 'eta': 0.07922376401738031, 'max_depth': 9, 'alpha': 0.10350000000000001, 'lambda': 14.637097272536586, 'max_bin': 286}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:36:18,889]\u001b[0m Trial 54 finished with value: 0.7034661503651508 and parameters: {'n_estimators': 855, 'eta': 0.07770826296337191, 'max_depth': 10, 'alpha': 0.0942, 'lambda': 19.875567344205223, 'max_bin': 282}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:36:53,031]\u001b[0m Trial 55 finished with value: 0.7033474900879175 and parameters: {'n_estimators': 863, 'eta': 0.07794585834840684, 'max_depth': 10, 'alpha': 0.08650000000000001, 'lambda': 21.12043420963653, 'max_bin': 284}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:37:29,347]\u001b[0m Trial 56 finished with value: 0.7050237233779075 and parameters: {'n_estimators': 858, 'eta': 0.0781711948542928, 'max_depth': 10, 'alpha': 0.0926, 'lambda': 20.72370853203891, 'max_bin': 275}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:38:07,842]\u001b[0m Trial 57 finished with value: 0.7033650831524024 and parameters: {'n_estimators': 883, 'eta': 0.08961979677678072, 'max_depth': 10, 'alpha': 0.0742, 'lambda': 21.32563000460036, 'max_bin': 263}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:38:41,922]\u001b[0m Trial 58 finished with value: 0.7058336858789749 and parameters: {'n_estimators': 837, 'eta': 0.08998840148767231, 'max_depth': 11, 'alpha': 0.0256, 'lambda': 19.698237220239395, 'max_bin': 252}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:39:04,930]\u001b[0m Trial 59 finished with value: 0.7046626917753684 and parameters: {'n_estimators': 895, 'eta': 0.0955152661835808, 'max_depth': 11, 'alpha': 0.0041, 'lambda': 19.11234780346667, 'max_bin': 270}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:39:41,991]\u001b[0m Trial 60 finished with value: 0.7018281564885347 and parameters: {'n_estimators': 898, 'eta': 0.09707003644129959, 'max_depth': 11, 'alpha': 0.0056, 'lambda': 23.835970969201227, 'max_bin': 261}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:40:17,825]\u001b[0m Trial 61 finished with value: 0.7046051559781663 and parameters: {'n_estimators': 836, 'eta': 0.08807587776526976, 'max_depth': 11, 'alpha': 0.1574, 'lambda': 19.170813989167137, 'max_bin': 277}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:40:49,833]\u001b[0m Trial 62 finished with value: 0.7018393158817796 and parameters: {'n_estimators': 835, 'eta': 0.0921353756527453, 'max_depth': 11, 'alpha': 0.17550000000000002, 'lambda': 19.486245129144553, 'max_bin': 251}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:41:17,710]\u001b[0m Trial 63 finished with value: 0.7013663791061702 and parameters: {'n_estimators': 825, 'eta': 0.08542527592691314, 'max_depth': 11, 'alpha': 0.20040000000000002, 'lambda': 17.03528690836292, 'max_bin': 275}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:42:03,289]\u001b[0m Trial 64 finished with value: 0.6986008468605475 and parameters: {'n_estimators': 780, 'eta': 0.07264948072323485, 'max_depth': 12, 'alpha': 0.1192, 'lambda': 29.052562593212365, 'max_bin': 296}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:42:28,561]\u001b[0m Trial 65 finished with value: 0.7061329032284713 and parameters: {'n_estimators': 877, 'eta': 0.08966581066959288, 'max_depth': 9, 'alpha': 0.0446, 'lambda': 16.700883719105093, 'max_bin': 269}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:43:01,422]\u001b[0m Trial 66 finished with value: 0.7063647101599683 and parameters: {'n_estimators': 897, 'eta': 0.09007547228150227, 'max_depth': 12, 'alpha': 0.0357, 'lambda': 16.942855973898475, 'max_bin': 268}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:43:30,210]\u001b[0m Trial 67 finished with value: 0.7043137728801214 and parameters: {'n_estimators': 898, 'eta': 0.09391107906788666, 'max_depth': 12, 'alpha': 0.0409, 'lambda': 16.811791209865245, 'max_bin': 259}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:43:59,069]\u001b[0m Trial 68 finished with value: 0.6998935442481904 and parameters: {'n_estimators': 789, 'eta': 0.09577132972284151, 'max_depth': 9, 'alpha': 0.0519, 'lambda': 21.697965869081983, 'max_bin': 269}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:44:25,297]\u001b[0m Trial 69 finished with value: 0.7052375599046721 and parameters: {'n_estimators': 882, 'eta': 0.08402914893131973, 'max_depth': 12, 'alpha': 0.011300000000000001, 'lambda': 13.744741631435314, 'max_bin': 308}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:44:56,717]\u001b[0m Trial 70 finished with value: 0.6994033610786201 and parameters: {'n_estimators': 821, 'eta': 0.08424099910257826, 'max_depth': 12, 'alpha': 0.31670000000000004, 'lambda': 13.63317667373282, 'max_bin': 311}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:45:24,303]\u001b[0m Trial 71 finished with value: 0.704060020810818 and parameters: {'n_estimators': 876, 'eta': 0.08797097531524763, 'max_depth': 11, 'alpha': 0.014400000000000001, 'lambda': 16.447263797583844, 'max_bin': 299}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:45:57,337]\u001b[0m Trial 72 finished with value: 0.7071077016012675 and parameters: {'n_estimators': 878, 'eta': 0.08194815911550765, 'max_depth': 12, 'alpha': 0.06280000000000001, 'lambda': 15.904022626552745, 'max_bin': 268}. Best is trial 50 with value: 0.7088453281643636.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:46:35,431]\u001b[0m Trial 73 finished with value: 0.7103595409809851 and parameters: {'n_estimators': 831, 'eta': 0.08246437897815631, 'max_depth': 12, 'alpha': 0.0635, 'lambda': 15.826964136949897, 'max_bin': 322}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:47:10,478]\u001b[0m Trial 74 finished with value: 0.7078928457495052 and parameters: {'n_estimators': 831, 'eta': 0.08225814423394458, 'max_depth': 12, 'alpha': 0.058, 'lambda': 16.090811894907564, 'max_bin': 323}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:47:55,546]\u001b[0m Trial 75 finished with value: 0.7082261088248315 and parameters: {'n_estimators': 818, 'eta': 0.07466244500125785, 'max_depth': 12, 'alpha': 0.057100000000000005, 'lambda': 16.02743469243185, 'max_bin': 323}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:48:23,296]\u001b[0m Trial 76 finished with value: 0.7035520436549814 and parameters: {'n_estimators': 790, 'eta': 0.0699526439660637, 'max_depth': 12, 'alpha': 0.054900000000000004, 'lambda': 16.071098545757117, 'max_bin': 294}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:48:32,431]\u001b[0m Trial 77 finished with value: 0.6823476787364982 and parameters: {'n_estimators': 67, 'eta': 0.07479692910599559, 'max_depth': 12, 'alpha': 0.1272, 'lambda': 15.542090379086579, 'max_bin': 323}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:49:05,809]\u001b[0m Trial 78 finished with value: 0.7046526377945468 and parameters: {'n_estimators': 822, 'eta': 0.08107081119794686, 'max_depth': 12, 'alpha': 0.1965, 'lambda': 17.91415310153737, 'max_bin': 324}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-21 23:49:34,215]\u001b[0m Trial 79 finished with value: 0.7056380503100386 and parameters: {'n_estimators': 775, 'eta': 0.07226653023271953, 'max_depth': 12, 'alpha': 0.0627, 'lambda': 13.726332959849595, 'max_bin': 314}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:49:58,017]\u001b[0m Trial 80 finished with value: 0.6961125439276176 and parameters: {'n_estimators': 798, 'eta': 0.09960646227483466, 'max_depth': 12, 'alpha': 0.23420000000000002, 'lambda': 17.526206881501302, 'max_bin': 305}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:50:29,822]\u001b[0m Trial 81 finished with value: 0.7052316055754969 and parameters: {'n_estimators': 832, 'eta': 0.09069549280011382, 'max_depth': 12, 'alpha': 0.029300000000000003, 'lambda': 16.25076513594569, 'max_bin': 255}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:50:55,840]\u001b[0m Trial 82 finished with value: 0.703197717594471 and parameters: {'n_estimators': 839, 'eta': 0.08199483546823835, 'max_depth': 12, 'alpha': 0.11520000000000001, 'lambda': 14.62061822468316, 'max_bin': 266}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:51:28,650]\u001b[0m Trial 83 finished with value: 0.7038735545071122 and parameters: {'n_estimators': 734, 'eta': 0.08702601878422739, 'max_depth': 9, 'alpha': 0.2812, 'lambda': 22.712522954967618, 'max_bin': 290}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:51:51,776]\u001b[0m Trial 84 finished with value: 0.7022366257954904 and parameters: {'n_estimators': 875, 'eta': 0.07547888581533764, 'max_depth': 12, 'alpha': 0.07400000000000001, 'lambda': 12.901498690655512, 'max_bin': 304}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:52:23,611]\u001b[0m Trial 85 finished with value: 0.7036976588978553 and parameters: {'n_estimators': 813, 'eta': 0.08397418034461324, 'max_depth': 11, 'alpha': 0.14250000000000002, 'lambda': 18.482226764898154, 'max_bin': 279}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:52:50,557]\u001b[0m Trial 86 finished with value: 0.7045883487681749 and parameters: {'n_estimators': 847, 'eta': 0.09226313570294263, 'max_depth': 11, 'alpha': 0.1728, 'lambda': 15.556472181482027, 'max_bin': 319}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:53:21,851]\u001b[0m Trial 87 finished with value: 0.7039186255454462 and parameters: {'n_estimators': 755, 'eta': 0.08280624061478813, 'max_depth': 12, 'alpha': 0.0334, 'lambda': 17.22354951629036, 'max_bin': 258}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:53:59,737]\u001b[0m Trial 88 finished with value: 0.7030969469616657 and parameters: {'n_estimators': 874, 'eta': 0.06770645423140215, 'max_depth': 9, 'alpha': 0.2031, 'lambda': 20.32899968729514, 'max_bin': 346}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:54:19,021]\u001b[0m Trial 89 finished with value: 0.6973423416422746 and parameters: {'n_estimators': 140, 'eta': 0.0895246980372919, 'max_depth': 12, 'alpha': 0.40040000000000003, 'lambda': 14.03170218052365, 'max_bin': 330}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:54:46,091]\u001b[0m Trial 90 finished with value: 0.7065845787207669 and parameters: {'n_estimators': 810, 'eta': 0.07709152462295878, 'max_depth': 11, 'alpha': 0.063, 'lambda': 14.876047948498991, 'max_bin': 250}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:55:15,005]\u001b[0m Trial 91 finished with value: 0.7024668322332537 and parameters: {'n_estimators': 807, 'eta': 0.0760634899426901, 'max_depth': 11, 'alpha': 0.0594, 'lambda': 14.82746843358077, 'max_bin': 253}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:55:42,850]\u001b[0m Trial 92 finished with value: 0.7073044272593283 and parameters: {'n_estimators': 900, 'eta': 0.08605147695935686, 'max_depth': 9, 'alpha': 0.12990000000000002, 'lambda': 16.469145901022248, 'max_bin': 264}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:56:18,560]\u001b[0m Trial 93 finished with value: 0.7042991409754249 and parameters: {'n_estimators': 899, 'eta': 0.07276168481252614, 'max_depth': 9, 'alpha': 0.1373, 'lambda': 16.622525563562196, 'max_bin': 268}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:56:49,037]\u001b[0m Trial 94 finished with value: 0.7046861785517775 and parameters: {'n_estimators': 866, 'eta': 0.08694305537437343, 'max_depth': 9, 'alpha': 0.1013, 'lambda': 11.847108241565973, 'max_bin': 273}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:57:14,354]\u001b[0m Trial 95 finished with value: 0.7055718756675132 and parameters: {'n_estimators': 774, 'eta': 0.08052693710528476, 'max_depth': 9, 'alpha': 0.16720000000000002, 'lambda': 10.954584015277545, 'max_bin': 291}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:57:58,158]\u001b[0m Trial 96 finished with value: 0.7034814049993359 and parameters: {'n_estimators': 846, 'eta': 0.07724325171358272, 'max_depth': 10, 'alpha': 0.0806, 'lambda': 36.612531603680594, 'max_bin': 282}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:58:32,149]\u001b[0m Trial 97 finished with value: 0.7034778021825876 and parameters: {'n_estimators': 677, 'eta': 0.07020370490588446, 'max_depth': 12, 'alpha': 0.0438, 'lambda': 13.186247161949558, 'max_bin': 358}. Best is trial 73 with value: 0.7103595409809851.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:59:14,397]\u001b[0m Trial 98 finished with value: 0.7119772408753071 and parameters: {'n_estimators': 880, 'eta': 0.06674460905913907, 'max_depth': 9, 'alpha': 0.11510000000000001, 'lambda': 15.478008077026015, 'max_bin': 264}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-21 23:59:48,068]\u001b[0m Trial 99 finished with value: 0.7057194270119067 and parameters: {'n_estimators': 806, 'eta': 0.07366276727010715, 'max_depth': 12, 'alpha': 0.1317, 'lambda': 14.578519294284444, 'max_bin': 260}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7120\n",
      "\tBest params:\n",
      "\t\tn_estimators: 880\n",
      "\t\teta: 0.06674460905913907\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.11510000000000001\n",
      "\t\tlambda: 15.478008077026015\n",
      "\t\tmax_bin: 264\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.706611    0.686641\n",
      "1                    TP   42.000000   44.000000\n",
      "2                    TN  197.000000  198.000000\n",
      "3                    FP    4.000000    3.000000\n",
      "4                    FN   25.000000   23.000000\n",
      "5              Accuracy    0.891791    0.902985\n",
      "6             Precision    0.913043    0.936170\n",
      "7           Sensitivity    0.626866    0.656716\n",
      "8           Specificity    0.980100    0.985100\n",
      "9              F1 score    0.743363    0.771930\n",
      "10  F1 score (weighted)    0.884422    0.896774\n",
      "11     F1 score (macro)    0.837402    0.855159\n",
      "12    Balanced Accuracy    0.803483    0.820896\n",
      "13                  MCC    0.697018    0.730776\n",
      "14                  NPV    0.887400    0.895900\n",
      "15              ROC_AUC    0.803483    0.820896\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=24,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_xgb_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_1_cat = np.where(((y_pred_xgb_1 >= 2) | (y_pred_xgb_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 00:00:19,010]\u001b[0m Trial 100 finished with value: 0.6670379679564606 and parameters: {'n_estimators': 856, 'eta': 0.06807601219797128, 'max_depth': 8, 'alpha': 0.2222, 'lambda': 15.556631036818114, 'max_bin': 250}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:00:45,032]\u001b[0m Trial 101 finished with value: 0.6671189603717347 and parameters: {'n_estimators': 882, 'eta': 0.08227288617286971, 'max_depth': 9, 'alpha': 0.09920000000000001, 'lambda': 17.845284480803297, 'max_bin': 264}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:01:12,026]\u001b[0m Trial 102 finished with value: 0.6705392823811585 and parameters: {'n_estimators': 900, 'eta': 0.08590619382298968, 'max_depth': 9, 'alpha': 0.11270000000000001, 'lambda': 16.456005730726787, 'max_bin': 279}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:01:37,030]\u001b[0m Trial 103 finished with value: 0.6660771516230403 and parameters: {'n_estimators': 826, 'eta': 0.07897683629574972, 'max_depth': 9, 'alpha': 0.0852, 'lambda': 18.426622715298073, 'max_bin': 302}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:02:04,327]\u001b[0m Trial 104 finished with value: 0.6710208993977582 and parameters: {'n_estimators': 872, 'eta': 0.06629323315239127, 'max_depth': 9, 'alpha': 0.0665, 'lambda': 12.496957563953034, 'max_bin': 273}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:02:32,505]\u001b[0m Trial 105 finished with value: 0.6712124855056063 and parameters: {'n_estimators': 847, 'eta': 0.07509681657694843, 'max_depth': 10, 'alpha': 0.1849, 'lambda': 14.428156690550827, 'max_bin': 264}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:02:56,649]\u001b[0m Trial 106 finished with value: 0.6674325137448808 and parameters: {'n_estimators': 881, 'eta': 0.0704238127123983, 'max_depth': 12, 'alpha': 0.1486, 'lambda': 15.591994612192325, 'max_bin': 257}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:03:23,663]\u001b[0m Trial 107 finished with value: 0.6694994711397291 and parameters: {'n_estimators': 786, 'eta': 0.07997196796688284, 'max_depth': 8, 'alpha': 0.031200000000000002, 'lambda': 17.41507495199389, 'max_bin': 284}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:03:44,794]\u001b[0m Trial 108 finished with value: 0.6667138977500202 and parameters: {'n_estimators': 817, 'eta': 0.07703574586333409, 'max_depth': 9, 'alpha': 0.0001, 'lambda': 12.280667788999995, 'max_bin': 317}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:04:11,709]\u001b[0m Trial 109 finished with value: 0.673162245128156 and parameters: {'n_estimators': 860, 'eta': 0.061435047587179756, 'max_depth': 10, 'alpha': 0.28140000000000004, 'lambda': 13.384728937237066, 'max_bin': 270}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:04:37,569]\u001b[0m Trial 110 finished with value: 0.6667652910041834 and parameters: {'n_estimators': 882, 'eta': 0.09288278455737316, 'max_depth': 12, 'alpha': 0.0524, 'lambda': 18.7769002073989, 'max_bin': 288}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:05:03,660]\u001b[0m Trial 111 finished with value: 0.6686249243183437 and parameters: {'n_estimators': 842, 'eta': 0.08848679656259896, 'max_depth': 11, 'alpha': 0.029400000000000003, 'lambda': 15.091922369755135, 'max_bin': 254}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:05:34,793]\u001b[0m Trial 112 finished with value: 0.6677971591661048 and parameters: {'n_estimators': 830, 'eta': 0.08228814254457291, 'max_depth': 12, 'alpha': 0.020200000000000003, 'lambda': 20.012670847105944, 'max_bin': 265}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:06:04,796]\u001b[0m Trial 113 finished with value: 0.6700846894905089 and parameters: {'n_estimators': 863, 'eta': 0.09103400811710931, 'max_depth': 10, 'alpha': 0.0742, 'lambda': 17.05686914651162, 'max_bin': 327}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:06:34,223]\u001b[0m Trial 114 finished with value: 0.6692681766965707 and parameters: {'n_estimators': 838, 'eta': 0.08627522438300214, 'max_depth': 11, 'alpha': 0.1116, 'lambda': 16.29169331353933, 'max_bin': 351}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:06:56,863]\u001b[0m Trial 115 finished with value: 0.6651674249072895 and parameters: {'n_estimators': 803, 'eta': 0.09394374214060007, 'max_depth': 12, 'alpha': 0.0407, 'lambda': 19.63495117282171, 'max_bin': 250}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:07:25,622]\u001b[0m Trial 116 finished with value: 0.6673851821174723 and parameters: {'n_estimators': 887, 'eta': 0.08482954332912103, 'max_depth': 9, 'alpha': 0.1258, 'lambda': 14.087730940638203, 'max_bin': 258}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:07:49,297]\u001b[0m Trial 117 finished with value: 0.6689963570873356 and parameters: {'n_estimators': 725, 'eta': 0.08936821110690124, 'max_depth': 11, 'alpha': 0.0857, 'lambda': 18.02489136312459, 'max_bin': 295}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:08:16,589]\u001b[0m Trial 118 finished with value: 0.6717654599611912 and parameters: {'n_estimators': 764, 'eta': 0.0836133147057125, 'max_depth': 12, 'alpha': 0.158, 'lambda': 15.934486340022058, 'max_bin': 338}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:08:36,201]\u001b[0m Trial 119 finished with value: 0.6647599886550133 and parameters: {'n_estimators': 434, 'eta': 0.09676902022071862, 'max_depth': 9, 'alpha': 0.0669, 'lambda': 15.184473568871194, 'max_bin': 279}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:09:08,166]\u001b[0m Trial 120 finished with value: 0.6692377127976569 and parameters: {'n_estimators': 900, 'eta': 0.07870150172899762, 'max_depth': 10, 'alpha': 0.0227, 'lambda': 18.93467084531933, 'max_bin': 270}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:09:34,037]\u001b[0m Trial 121 finished with value: 0.6728415948137978 and parameters: {'n_estimators': 810, 'eta': 0.07406433871354562, 'max_depth': 12, 'alpha': 0.12190000000000001, 'lambda': 14.25555717435165, 'max_bin': 259}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:10:03,651]\u001b[0m Trial 122 finished with value: 0.6714136750422298 and parameters: {'n_estimators': 789, 'eta': 0.073350562434623, 'max_depth': 12, 'alpha': 0.0534, 'lambda': 16.855978967602393, 'max_bin': 260}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:10:30,157]\u001b[0m Trial 123 finished with value: 0.6735376396545896 and parameters: {'n_estimators': 853, 'eta': 0.07647533146540876, 'max_depth': 12, 'alpha': 0.13620000000000002, 'lambda': 14.91478307611914, 'max_bin': 264}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:11:03,704]\u001b[0m Trial 124 finished with value: 0.6695509823979674 and parameters: {'n_estimators': 816, 'eta': 0.08021696313331263, 'max_depth': 12, 'alpha': 0.0969, 'lambda': 16.00415308584511, 'max_bin': 273}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:11:28,574]\u001b[0m Trial 125 finished with value: 0.6676257069967189 and parameters: {'n_estimators': 869, 'eta': 0.08574343682905168, 'max_depth': 12, 'alpha': 0.2152, 'lambda': 17.520966220777932, 'max_bin': 254}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:11:51,132]\u001b[0m Trial 126 finished with value: 0.6739627868370411 and parameters: {'n_estimators': 831, 'eta': 0.07203040263719776, 'max_depth': 11, 'alpha': 0.1809, 'lambda': 13.24268533480484, 'max_bin': 276}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:12:23,560]\u001b[0m Trial 127 finished with value: 0.6744959336947068 and parameters: {'n_estimators': 800, 'eta': 0.08144742785177336, 'max_depth': 12, 'alpha': 0.0051, 'lambda': 14.541841282126288, 'max_bin': 310}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:12:46,330]\u001b[0m Trial 128 finished with value: 0.6676424614057443 and parameters: {'n_estimators': 746, 'eta': 0.09064066015078118, 'max_depth': 9, 'alpha': 0.045000000000000005, 'lambda': 13.641088199490495, 'max_bin': 267}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 00:13:15,316]\u001b[0m Trial 129 finished with value: 0.6691245407836017 and parameters: {'n_estimators': 848, 'eta': 0.07562921687145228, 'max_depth': 12, 'alpha': 0.0782, 'lambda': 16.239388082386345, 'max_bin': 250}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:13:37,631]\u001b[0m Trial 130 finished with value: 0.6774100815279968 and parameters: {'n_estimators': 885, 'eta': 0.08777553608692838, 'max_depth': 11, 'alpha': 0.1044, 'lambda': 10.411946172257506, 'max_bin': 260}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:14:04,367]\u001b[0m Trial 131 finished with value: 0.6698570093185096 and parameters: {'n_estimators': 778, 'eta': 0.071288509940101, 'max_depth': 12, 'alpha': 0.063, 'lambda': 13.91666362840462, 'max_bin': 312}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:14:30,370]\u001b[0m Trial 132 finished with value: 0.6683498261003307 and parameters: {'n_estimators': 833, 'eta': 0.07839187991440744, 'max_depth': 12, 'alpha': 0.13290000000000002, 'lambda': 12.340754143087175, 'max_bin': 322}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:14:57,300]\u001b[0m Trial 133 finished with value: 0.6712246350919139 and parameters: {'n_estimators': 367, 'eta': 0.07334782080214015, 'max_depth': 12, 'alpha': 0.058600000000000006, 'lambda': 15.474618685393033, 'max_bin': 336}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:15:31,435]\u001b[0m Trial 134 finished with value: 0.6732485986147154 and parameters: {'n_estimators': 865, 'eta': 0.0683235083368215, 'max_depth': 12, 'alpha': 0.022600000000000002, 'lambda': 16.785707978522275, 'max_bin': 329}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:16:00,006]\u001b[0m Trial 135 finished with value: 0.6706858745771005 and parameters: {'n_estimators': 775, 'eta': 0.08361325361197258, 'max_depth': 9, 'alpha': 0.0964, 'lambda': 11.139116078400463, 'max_bin': 314}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:16:30,750]\u001b[0m Trial 136 finished with value: 0.6707139801528229 and parameters: {'n_estimators': 821, 'eta': 0.0666861230832456, 'max_depth': 12, 'alpha': 0.1534, 'lambda': 9.48615122382286, 'max_bin': 300}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:16:56,539]\u001b[0m Trial 137 finished with value: 0.6685105169669423 and parameters: {'n_estimators': 847, 'eta': 0.07691370217446758, 'max_depth': 12, 'alpha': 0.0746, 'lambda': 14.661244467103817, 'max_bin': 269}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:17:22,756]\u001b[0m Trial 138 finished with value: 0.6688074751327384 and parameters: {'n_estimators': 600, 'eta': 0.07028646030766247, 'max_depth': 12, 'alpha': 0.033600000000000005, 'lambda': 12.90113531432508, 'max_bin': 307}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:17:52,259]\u001b[0m Trial 139 finished with value: 0.668033094307874 and parameters: {'n_estimators': 797, 'eta': 0.0652645409479595, 'max_depth': 8, 'alpha': 0.046200000000000005, 'lambda': 17.903271446085736, 'max_bin': 283}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:18:23,159]\u001b[0m Trial 140 finished with value: 0.6708368963212661 and parameters: {'n_estimators': 873, 'eta': 0.07442650236213907, 'max_depth': 9, 'alpha': 0.1231, 'lambda': 15.372892574717056, 'max_bin': 263}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:18:45,557]\u001b[0m Trial 141 finished with value: 0.6719948746680162 and parameters: {'n_estimators': 769, 'eta': 0.08116065503803543, 'max_depth': 9, 'alpha': 0.164, 'lambda': 13.801819974274501, 'max_bin': 293}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:19:10,073]\u001b[0m Trial 142 finished with value: 0.6672794449374202 and parameters: {'n_estimators': 818, 'eta': 0.07965895106659886, 'max_depth': 9, 'alpha': 0.18580000000000002, 'lambda': 17.142781399978663, 'max_bin': 291}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:19:37,323]\u001b[0m Trial 143 finished with value: 0.6653906643029367 and parameters: {'n_estimators': 900, 'eta': 0.08293283310313025, 'max_depth': 9, 'alpha': 0.2534, 'lambda': 16.147374102332563, 'max_bin': 316}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:19:54,880]\u001b[0m Trial 144 finished with value: 0.6662318380255294 and parameters: {'n_estimators': 269, 'eta': 0.08613587722866485, 'max_depth': 9, 'alpha': 0.08170000000000001, 'lambda': 11.023392362041445, 'max_bin': 287}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:20:22,529]\u001b[0m Trial 145 finished with value: 0.6734472058904782 and parameters: {'n_estimators': 721, 'eta': 0.07223138838649705, 'max_depth': 9, 'alpha': 0.1472, 'lambda': 14.774948080085741, 'max_bin': 278}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:20:46,074]\u001b[0m Trial 146 finished with value: 0.672251419943897 and parameters: {'n_estimators': 752, 'eta': 0.09448454629216327, 'max_depth': 12, 'alpha': 0.0032, 'lambda': 11.588507657544794, 'max_bin': 255}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:21:10,845]\u001b[0m Trial 147 finished with value: 0.6652831808032424 and parameters: {'n_estimators': 857, 'eta': 0.08094348060409547, 'max_depth': 8, 'alpha': 0.11, 'lambda': 13.181020910207618, 'max_bin': 271}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:21:26,589]\u001b[0m Trial 148 finished with value: 0.6517118567954763 and parameters: {'n_estimators': 786, 'eta': 0.0760048414234466, 'max_depth': 5, 'alpha': 0.1683, 'lambda': 9.620850585406698, 'max_bin': 345}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:21:53,546]\u001b[0m Trial 149 finished with value: 0.6727834584962693 and parameters: {'n_estimators': 836, 'eta': 0.07851297530947919, 'max_depth': 10, 'alpha': 0.0625, 'lambda': 18.78733934213294, 'max_bin': 303}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7120\n",
      "\tBest params:\n",
      "\t\tn_estimators: 880\n",
      "\t\teta: 0.06674460905913907\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.11510000000000001\n",
      "\t\tlambda: 15.478008077026015\n",
      "\t\tmax_bin: 264\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.706611    0.686641    0.737877\n",
      "1                    TP   42.000000   44.000000   43.000000\n",
      "2                    TN  197.000000  198.000000  197.000000\n",
      "3                    FP    4.000000    3.000000    4.000000\n",
      "4                    FN   25.000000   23.000000   24.000000\n",
      "5              Accuracy    0.891791    0.902985    0.895522\n",
      "6             Precision    0.913043    0.936170    0.914894\n",
      "7           Sensitivity    0.626866    0.656716    0.641791\n",
      "8           Specificity    0.980100    0.985100    0.980100\n",
      "9              F1 score    0.743363    0.771930    0.754386\n",
      "10  F1 score (weighted)    0.884422    0.896774    0.888833\n",
      "11     F1 score (macro)    0.837402    0.855159    0.844018\n",
      "12    Balanced Accuracy    0.803483    0.820896    0.810945\n",
      "13                  MCC    0.697018    0.730776    0.708116\n",
      "14                  NPV    0.887400    0.895900    0.891400\n",
      "15              ROC_AUC    0.803483    0.820896    0.810945\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=24,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_xgb_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_2_cat = np.where(((y_pred_xgb_2 >= 2) | (y_pred_xgb_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 00:22:31,209]\u001b[0m Trial 150 finished with value: 0.692148017321211 and parameters: {'n_estimators': 882, 'eta': 0.09063956730443534, 'max_depth': 12, 'alpha': 0.22540000000000002, 'lambda': 15.699038486121939, 'max_bin': 320}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:23:08,571]\u001b[0m Trial 151 finished with value: 0.694907781967949 and parameters: {'n_estimators': 879, 'eta': 0.0845642057958712, 'max_depth': 12, 'alpha': 0.024200000000000003, 'lambda': 13.992370737601064, 'max_bin': 300}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:23:42,354]\u001b[0m Trial 152 finished with value: 0.6919386588080096 and parameters: {'n_estimators': 863, 'eta': 0.08870427702731823, 'max_depth': 12, 'alpha': 0.0164, 'lambda': 14.89525336874396, 'max_bin': 308}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:24:22,933]\u001b[0m Trial 153 finished with value: 0.6987608889809506 and parameters: {'n_estimators': 811, 'eta': 0.08258057899168042, 'max_depth': 12, 'alpha': 0.0454, 'lambda': 13.506131557305778, 'max_bin': 295}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:24:48,526]\u001b[0m Trial 154 finished with value: 0.6981624153029937 and parameters: {'n_estimators': 834, 'eta': 0.08708734900622185, 'max_depth': 11, 'alpha': 0.09670000000000001, 'lambda': 12.294019564053611, 'max_bin': 262}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:25:28,755]\u001b[0m Trial 155 finished with value: 0.6946760492338123 and parameters: {'n_estimators': 885, 'eta': 0.07954373726386404, 'max_depth': 12, 'alpha': 0.069, 'lambda': 16.670391268510308, 'max_bin': 314}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:25:55,530]\u001b[0m Trial 156 finished with value: 0.6963361011106171 and parameters: {'n_estimators': 851, 'eta': 0.08510001094512712, 'max_depth': 9, 'alpha': 0.5325, 'lambda': 18.03412064968449, 'max_bin': 255}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:26:38,087]\u001b[0m Trial 157 finished with value: 0.7010723099534544 and parameters: {'n_estimators': 805, 'eta': 0.07424781743603354, 'max_depth': 12, 'alpha': 0.038, 'lambda': 14.289060077241468, 'max_bin': 281}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:27:07,901]\u001b[0m Trial 158 finished with value: 0.6978099466978931 and parameters: {'n_estimators': 875, 'eta': 0.09244215046549133, 'max_depth': 11, 'alpha': 0.0873, 'lambda': 15.826584054818394, 'max_bin': 267}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:27:43,290]\u001b[0m Trial 159 finished with value: 0.6965900720812155 and parameters: {'n_estimators': 846, 'eta': 0.078039290811884, 'max_depth': 12, 'alpha': 0.1267, 'lambda': 16.92787340983526, 'max_bin': 326}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:28:14,407]\u001b[0m Trial 160 finished with value: 0.6959321151059449 and parameters: {'n_estimators': 897, 'eta': 0.08353271445428133, 'max_depth': 9, 'alpha': 0.0161, 'lambda': 15.26869775526619, 'max_bin': 306}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:28:44,587]\u001b[0m Trial 161 finished with value: 0.6971771172301738 and parameters: {'n_estimators': 825, 'eta': 0.09890563315991124, 'max_depth': 12, 'alpha': 0.054, 'lambda': 16.274385559451872, 'max_bin': 255}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:29:20,815]\u001b[0m Trial 162 finished with value: 0.6874507493837319 and parameters: {'n_estimators': 833, 'eta': 0.09102831103710747, 'max_depth': 12, 'alpha': 0.0008, 'lambda': 17.44348474572105, 'max_bin': 275}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:29:51,243]\u001b[0m Trial 163 finished with value: 0.6992588838035473 and parameters: {'n_estimators': 856, 'eta': 0.08840101650378865, 'max_depth': 12, 'alpha': 0.0291, 'lambda': 12.84442561026199, 'max_bin': 259}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:30:28,124]\u001b[0m Trial 164 finished with value: 0.6930689522841321 and parameters: {'n_estimators': 869, 'eta': 0.08075554215588304, 'max_depth': 12, 'alpha': 0.065, 'lambda': 14.357427720374254, 'max_bin': 250}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:31:02,083]\u001b[0m Trial 165 finished with value: 0.697075239625762 and parameters: {'n_estimators': 781, 'eta': 0.07646936347160521, 'max_depth': 12, 'alpha': 0.0388, 'lambda': 16.225854724788952, 'max_bin': 262}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:31:30,912]\u001b[0m Trial 166 finished with value: 0.6997508524222662 and parameters: {'n_estimators': 800, 'eta': 0.08971784292832863, 'max_depth': 9, 'alpha': 0.10830000000000001, 'lambda': 15.255360462528062, 'max_bin': 289}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:32:04,962]\u001b[0m Trial 167 finished with value: 0.6899326968038436 and parameters: {'n_estimators': 821, 'eta': 0.08631729864713468, 'max_depth': 12, 'alpha': 0.08320000000000001, 'lambda': 22.131421397590742, 'max_bin': 319}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:32:45,959]\u001b[0m Trial 168 finished with value: 0.691780590807947 and parameters: {'n_estimators': 886, 'eta': 0.09335654501994434, 'max_depth': 12, 'alpha': 0.1952, 'lambda': 20.624092636682008, 'max_bin': 267}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:33:24,398]\u001b[0m Trial 169 finished with value: 0.6992232928697762 and parameters: {'n_estimators': 843, 'eta': 0.07196027306917689, 'max_depth': 11, 'alpha': 0.3917, 'lambda': 13.679731776013227, 'max_bin': 333}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:33:49,066]\u001b[0m Trial 170 finished with value: 0.6970146355128561 and parameters: {'n_estimators': 218, 'eta': 0.06960144580420008, 'max_depth': 9, 'alpha': 0.3376, 'lambda': 18.29580233667492, 'max_bin': 257}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:34:23,697]\u001b[0m Trial 171 finished with value: 0.6991806089107906 and parameters: {'n_estimators': 861, 'eta': 0.07752430738283461, 'max_depth': 10, 'alpha': 0.0976, 'lambda': 19.839636600543916, 'max_bin': 273}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:35:01,157]\u001b[0m Trial 172 finished with value: 0.6957154973834604 and parameters: {'n_estimators': 857, 'eta': 0.08184584729434446, 'max_depth': 10, 'alpha': 0.0511, 'lambda': 20.96585230925099, 'max_bin': 285}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:35:39,480]\u001b[0m Trial 173 finished with value: 0.6981713714966247 and parameters: {'n_estimators': 899, 'eta': 0.0743714266751746, 'max_depth': 9, 'alpha': 0.1403, 'lambda': 19.14060442551005, 'max_bin': 276}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:36:10,183]\u001b[0m Trial 174 finished with value: 0.6966426262319734 and parameters: {'n_estimators': 834, 'eta': 0.07916810935464677, 'max_depth': 10, 'alpha': 0.0235, 'lambda': 16.767053245878596, 'max_bin': 408}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:36:59,779]\u001b[0m Trial 175 finished with value: 0.6942324489972652 and parameters: {'n_estimators': 867, 'eta': 0.0828003744277905, 'max_depth': 12, 'alpha': 0.077, 'lambda': 25.221422812238664, 'max_bin': 265}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:37:30,055]\u001b[0m Trial 176 finished with value: 0.6971193840971607 and parameters: {'n_estimators': 812, 'eta': 0.07716134765858357, 'max_depth': 9, 'alpha': 0.1144, 'lambda': 14.894042255010607, 'max_bin': 255}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:38:05,321]\u001b[0m Trial 177 finished with value: 0.697386833868377 and parameters: {'n_estimators': 555, 'eta': 0.0857231315920313, 'max_depth': 11, 'alpha': 0.056900000000000006, 'lambda': 15.78324392100291, 'max_bin': 270}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:38:50,177]\u001b[0m Trial 178 finished with value: 0.6935446713793911 and parameters: {'n_estimators': 876, 'eta': 0.080198122928602, 'max_depth': 12, 'alpha': 0.0857, 'lambda': 22.96699748961226, 'max_bin': 261}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 00:39:19,041]\u001b[0m Trial 179 finished with value: 0.6991326360523763 and parameters: {'n_estimators': 771, 'eta': 0.07528702062754353, 'max_depth': 11, 'alpha': 0.0026000000000000003, 'lambda': 17.5328028959665, 'max_bin': 250}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:39:54,552]\u001b[0m Trial 180 finished with value: 0.694229439370279 and parameters: {'n_estimators': 837, 'eta': 0.08419236821104786, 'max_depth': 9, 'alpha': 0.032600000000000004, 'lambda': 32.377415464930834, 'max_bin': 424}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:40:19,947]\u001b[0m Trial 181 finished with value: 0.6960770130536607 and parameters: {'n_estimators': 866, 'eta': 0.08779019002679339, 'max_depth': 9, 'alpha': 0.10880000000000001, 'lambda': 11.810556969888747, 'max_bin': 273}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:40:45,974]\u001b[0m Trial 182 finished with value: 0.7021160802585807 and parameters: {'n_estimators': 885, 'eta': 0.09148241923596034, 'max_depth': 9, 'alpha': 0.166, 'lambda': 10.665707628822108, 'max_bin': 280}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:41:19,081]\u001b[0m Trial 183 finished with value: 0.6960221104957997 and parameters: {'n_estimators': 851, 'eta': 0.08763132838408588, 'max_depth': 9, 'alpha': 0.133, 'lambda': 12.035463097745401, 'max_bin': 297}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:41:45,901]\u001b[0m Trial 184 finished with value: 0.6983980730499666 and parameters: {'n_estimators': 823, 'eta': 0.08922980742191236, 'max_depth': 10, 'alpha': 0.0721, 'lambda': 13.687993536842008, 'max_bin': 267}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:42:20,333]\u001b[0m Trial 185 finished with value: 0.69811518980359 and parameters: {'n_estimators': 797, 'eta': 0.07253949730665721, 'max_depth': 9, 'alpha': 0.0473, 'lambda': 12.592559887836826, 'max_bin': 276}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:43:02,189]\u001b[0m Trial 186 finished with value: 0.698130371829467 and parameters: {'n_estimators': 868, 'eta': 0.08499658516808359, 'max_depth': 12, 'alpha': 0.4692, 'lambda': 14.260215911566096, 'max_bin': 291}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:43:36,960]\u001b[0m Trial 187 finished with value: 0.7028247996443842 and parameters: {'n_estimators': 884, 'eta': 0.08196133929236137, 'max_depth': 12, 'alpha': 0.09720000000000001, 'lambda': 15.542820746306612, 'max_bin': 261}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:44:06,526]\u001b[0m Trial 188 finished with value: 0.6932774535484645 and parameters: {'n_estimators': 853, 'eta': 0.09625106225241535, 'max_depth': 8, 'alpha': 0.063, 'lambda': 13.132385618346932, 'max_bin': 310}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:44:37,293]\u001b[0m Trial 189 finished with value: 0.702831788718523 and parameters: {'n_estimators': 841, 'eta': 0.07855758195733777, 'max_depth': 9, 'alpha': 0.14650000000000002, 'lambda': 14.839170090218008, 'max_bin': 256}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:45:21,648]\u001b[0m Trial 190 finished with value: 0.7011823646695062 and parameters: {'n_estimators': 899, 'eta': 0.04680332025743025, 'max_depth': 12, 'alpha': 0.0227, 'lambda': 11.183223916258108, 'max_bin': 273}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:45:54,232]\u001b[0m Trial 191 finished with value: 0.6887363829426116 and parameters: {'n_estimators': 900, 'eta': 0.0927949684464191, 'max_depth': 11, 'alpha': 0.006900000000000001, 'lambda': 16.270635406508852, 'max_bin': 270}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:46:22,977]\u001b[0m Trial 192 finished with value: 0.6933874810110765 and parameters: {'n_estimators': 876, 'eta': 0.09510870863618465, 'max_depth': 11, 'alpha': 0.025400000000000002, 'lambda': 19.053106096252368, 'max_bin': 284}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:46:55,624]\u001b[0m Trial 193 finished with value: 0.686167718626857 and parameters: {'n_estimators': 862, 'eta': 0.09094414058672172, 'max_depth': 12, 'alpha': 0.0405, 'lambda': 20.160783432639686, 'max_bin': 265}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:47:33,718]\u001b[0m Trial 194 finished with value: 0.6927193842119139 and parameters: {'n_estimators': 818, 'eta': 0.08681963936912013, 'max_depth': 11, 'alpha': 0.0007, 'lambda': 21.424432060454926, 'max_bin': 261}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:48:11,360]\u001b[0m Trial 195 finished with value: 0.7012175193433536 and parameters: {'n_estimators': 887, 'eta': 0.09783650889746932, 'max_depth': 9, 'alpha': 0.0892, 'lambda': 17.20923746075566, 'max_bin': 325}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:48:49,669]\u001b[0m Trial 196 finished with value: 0.6982866997376209 and parameters: {'n_estimators': 846, 'eta': 0.07584511542645099, 'max_depth': 12, 'alpha': 0.0555, 'lambda': 18.30552279530104, 'max_bin': 279}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:49:21,668]\u001b[0m Trial 197 finished with value: 0.7009456897121255 and parameters: {'n_estimators': 875, 'eta': 0.08386496027578046, 'max_depth': 11, 'alpha': 0.1165, 'lambda': 15.987279588727013, 'max_bin': 269}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:49:58,865]\u001b[0m Trial 198 finished with value: 0.7010816667726388 and parameters: {'n_estimators': 833, 'eta': 0.09523642524689749, 'max_depth': 9, 'alpha': 0.2132, 'lambda': 14.484412280642642, 'max_bin': 302}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:50:36,901]\u001b[0m Trial 199 finished with value: 0.6944461087916545 and parameters: {'n_estimators': 792, 'eta': 0.08053570615159858, 'max_depth': 12, 'alpha': 0.0717, 'lambda': 16.788455234220056, 'max_bin': 254}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7120\n",
      "\tBest params:\n",
      "\t\tn_estimators: 880\n",
      "\t\teta: 0.06674460905913907\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.11510000000000001\n",
      "\t\tlambda: 15.478008077026015\n",
      "\t\tmax_bin: 264\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.706611    0.686641    0.737877    0.695883\n",
      "1                    TP   42.000000   44.000000   43.000000   36.000000\n",
      "2                    TN  197.000000  198.000000  197.000000  194.000000\n",
      "3                    FP    4.000000    3.000000    4.000000    6.000000\n",
      "4                    FN   25.000000   23.000000   24.000000   32.000000\n",
      "5              Accuracy    0.891791    0.902985    0.895522    0.858209\n",
      "6             Precision    0.913043    0.936170    0.914894    0.857143\n",
      "7           Sensitivity    0.626866    0.656716    0.641791    0.529412\n",
      "8           Specificity    0.980100    0.985100    0.980100    0.970000\n",
      "9              F1 score    0.743363    0.771930    0.754386    0.654545\n",
      "10  F1 score (weighted)    0.884422    0.896774    0.888833    0.845779\n",
      "11     F1 score (macro)    0.837402    0.855159    0.844018    0.782672\n",
      "12    Balanced Accuracy    0.803483    0.820896    0.810945    0.749706\n",
      "13                  MCC    0.697018    0.730776    0.708116    0.597791\n",
      "14                  NPV    0.887400    0.895900    0.891400    0.858400\n",
      "15              ROC_AUC    0.803483    0.820896    0.810945    0.749706\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=24,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_xgb_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_3_cat = np.where(((y_pred_xgb_3 >= 2) | (y_pred_xgb_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 00:51:11,192]\u001b[0m Trial 200 finished with value: 0.6869390997424303 and parameters: {'n_estimators': 812, 'eta': 0.08883271753150236, 'max_depth': 12, 'alpha': 0.037200000000000004, 'lambda': 15.163053843341363, 'max_bin': 265}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:51:42,774]\u001b[0m Trial 201 finished with value: 0.681493711087788 and parameters: {'n_estimators': 825, 'eta': 0.08214307519110377, 'max_depth': 12, 'alpha': 0.1751, 'lambda': 17.865853736492603, 'max_bin': 330}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:52:16,173]\u001b[0m Trial 202 finished with value: 0.6877981405234104 and parameters: {'n_estimators': 859, 'eta': 0.0783170732909094, 'max_depth': 12, 'alpha': 0.2051, 'lambda': 17.299822613787608, 'max_bin': 318}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:52:46,258]\u001b[0m Trial 203 finished with value: 0.6874539635159103 and parameters: {'n_estimators': 885, 'eta': 0.08057381097041862, 'max_depth': 12, 'alpha': 0.2716, 'lambda': 18.427534745476105, 'max_bin': 321}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:53:17,945]\u001b[0m Trial 204 finished with value: 0.6893864653671927 and parameters: {'n_estimators': 850, 'eta': 0.07360642074699841, 'max_depth': 12, 'alpha': 0.1976, 'lambda': 16.404099126747294, 'max_bin': 312}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:53:47,076]\u001b[0m Trial 205 finished with value: 0.6824559285708194 and parameters: {'n_estimators': 803, 'eta': 0.08537976192082594, 'max_depth': 9, 'alpha': 0.1563, 'lambda': 20.40669010470247, 'max_bin': 259}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:54:19,053]\u001b[0m Trial 206 finished with value: 0.6820038658736508 and parameters: {'n_estimators': 827, 'eta': 0.07681764139499626, 'max_depth': 12, 'alpha': 0.23970000000000002, 'lambda': 19.107561502577894, 'max_bin': 275}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:54:46,407]\u001b[0m Trial 207 finished with value: 0.6838505158446163 and parameters: {'n_estimators': 900, 'eta': 0.08352137707389494, 'max_depth': 12, 'alpha': 0.09380000000000001, 'lambda': 15.820889240982297, 'max_bin': 323}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:55:10,505]\u001b[0m Trial 208 finished with value: 0.6892095760702702 and parameters: {'n_estimators': 867, 'eta': 0.09021213417933528, 'max_depth': 9, 'alpha': 0.020200000000000003, 'lambda': 13.946243377180217, 'max_bin': 315}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:55:36,293]\u001b[0m Trial 209 finished with value: 0.6876225338326613 and parameters: {'n_estimators': 785, 'eta': 0.08719564157649345, 'max_depth': 10, 'alpha': 0.058600000000000006, 'lambda': 17.717702127064978, 'max_bin': 328}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:56:15,719]\u001b[0m Trial 210 finished with value: 0.6907483517373757 and parameters: {'n_estimators': 842, 'eta': 0.0577344052686414, 'max_depth': 12, 'alpha': 0.043300000000000005, 'lambda': 14.99919721842067, 'max_bin': 290}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:56:52,660]\u001b[0m Trial 211 finished with value: 0.6869472349844518 and parameters: {'n_estimators': 811, 'eta': 0.09226573846360843, 'max_depth': 11, 'alpha': 0.18150000000000002, 'lambda': 19.95096570620503, 'max_bin': 272}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:57:20,996]\u001b[0m Trial 212 finished with value: 0.683022057750475 and parameters: {'n_estimators': 839, 'eta': 0.08696503281476523, 'max_depth': 11, 'alpha': 0.1296, 'lambda': 19.631669533984198, 'max_bin': 280}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:57:47,031]\u001b[0m Trial 213 finished with value: 0.685000778577859 and parameters: {'n_estimators': 871, 'eta': 0.08020531822623346, 'max_depth': 11, 'alpha': 0.151, 'lambda': 16.80436188256224, 'max_bin': 283}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:58:24,034]\u001b[0m Trial 214 finished with value: 0.6865690708146761 and parameters: {'n_estimators': 828, 'eta': 0.08982332073362295, 'max_depth': 11, 'alpha': 0.07740000000000001, 'lambda': 19.162494356934193, 'max_bin': 266}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:58:56,169]\u001b[0m Trial 215 finished with value: 0.6867163341306595 and parameters: {'n_estimators': 763, 'eta': 0.07479328667525331, 'max_depth': 11, 'alpha': 0.16490000000000002, 'lambda': 18.2799516109192, 'max_bin': 250}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:59:28,059]\u001b[0m Trial 216 finished with value: 0.6878608106181386 and parameters: {'n_estimators': 855, 'eta': 0.08474807420075121, 'max_depth': 12, 'alpha': 0.1202, 'lambda': 21.943741116971164, 'max_bin': 275}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 00:59:53,289]\u001b[0m Trial 217 finished with value: 0.6860736769719171 and parameters: {'n_estimators': 885, 'eta': 0.0709605320021732, 'max_depth': 9, 'alpha': 0.1061, 'lambda': 15.617001036891036, 'max_bin': 286}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:00:34,666]\u001b[0m Trial 218 finished with value: 0.6862425891755617 and parameters: {'n_estimators': 867, 'eta': 0.08231903035193598, 'max_depth': 12, 'alpha': 0.020300000000000002, 'lambda': 20.993475038428173, 'max_bin': 257}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:01:06,643]\u001b[0m Trial 219 finished with value: 0.6896499732904694 and parameters: {'n_estimators': 813, 'eta': 0.07878940993695241, 'max_depth': 9, 'alpha': 0.0717, 'lambda': 13.475743684187343, 'max_bin': 384}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:01:30,893]\u001b[0m Trial 220 finished with value: 0.6868134813819877 and parameters: {'n_estimators': 843, 'eta': 0.09378802035292423, 'max_depth': 11, 'alpha': 0.1923, 'lambda': 17.31639923174515, 'max_bin': 269}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:01:57,457]\u001b[0m Trial 221 finished with value: 0.6904769347735064 and parameters: {'n_estimators': 853, 'eta': 0.09218461020554795, 'max_depth': 11, 'alpha': 0.17950000000000002, 'lambda': 16.399826716541977, 'max_bin': 317}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:02:21,623]\u001b[0m Trial 222 finished with value: 0.6858419235673442 and parameters: {'n_estimators': 827, 'eta': 0.08866622037559259, 'max_depth': 11, 'alpha': 0.1394, 'lambda': 15.356531597250216, 'max_bin': 322}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:02:46,804]\u001b[0m Trial 223 finished with value: 0.6902118786624026 and parameters: {'n_estimators': 883, 'eta': 0.09088541546412177, 'max_depth': 11, 'alpha': 0.21030000000000001, 'lambda': 14.490479339529893, 'max_bin': 336}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:03:17,492]\u001b[0m Trial 224 finished with value: 0.6856245883904466 and parameters: {'n_estimators': 796, 'eta': 0.09400176004502261, 'max_depth': 12, 'alpha': 0.16740000000000002, 'lambda': 15.605908037411066, 'max_bin': 307}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:03:41,019]\u001b[0m Trial 225 finished with value: 0.6877208767409012 and parameters: {'n_estimators': 844, 'eta': 0.09664119149498426, 'max_depth': 11, 'alpha': 0.0419, 'lambda': 16.45641437070851, 'max_bin': 263}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:04:07,299]\u001b[0m Trial 226 finished with value: 0.6863202869702676 and parameters: {'n_estimators': 869, 'eta': 0.0864897023875961, 'max_depth': 9, 'alpha': 0.22940000000000002, 'lambda': 14.470446554067786, 'max_bin': 324}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:04:37,424]\u001b[0m Trial 227 finished with value: 0.6879253046832876 and parameters: {'n_estimators': 824, 'eta': 0.07693480735524572, 'max_depth': 12, 'alpha': 0.0925, 'lambda': 12.859824327693241, 'max_bin': 278}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 01:05:11,410]\u001b[0m Trial 228 finished with value: 0.6874905352188676 and parameters: {'n_estimators': 854, 'eta': 0.0810529867718171, 'max_depth': 12, 'alpha': 0.0625, 'lambda': 17.667176603529725, 'max_bin': 296}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:05:43,601]\u001b[0m Trial 229 finished with value: 0.6942699464155997 and parameters: {'n_estimators': 886, 'eta': 0.08862259058364776, 'max_depth': 10, 'alpha': 0.12510000000000002, 'lambda': 15.935379486161525, 'max_bin': 259}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:06:27,017]\u001b[0m Trial 230 finished with value: 0.6814024959232443 and parameters: {'n_estimators': 869, 'eta': 0.07315986992187927, 'max_depth': 11, 'alpha': 0.153, 'lambda': 39.58847028545583, 'max_bin': 312}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:06:52,007]\u001b[0m Trial 231 finished with value: 0.6865989475995014 and parameters: {'n_estimators': 643, 'eta': 0.07841185524558263, 'max_depth': 9, 'alpha': 0.109, 'lambda': 15.034815086745015, 'max_bin': 288}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:07:22,715]\u001b[0m Trial 232 finished with value: 0.689505696975284 and parameters: {'n_estimators': 888, 'eta': 0.07592302749563987, 'max_depth': 9, 'alpha': 0.0368, 'lambda': 14.274865679870658, 'max_bin': 273}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:07:48,636]\u001b[0m Trial 233 finished with value: 0.6864730360934947 and parameters: {'n_estimators': 842, 'eta': 0.08339119989483391, 'max_depth': 9, 'alpha': 0.0753, 'lambda': 13.82291297752812, 'max_bin': 280}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:08:12,913]\u001b[0m Trial 234 finished with value: 0.6919147535092673 and parameters: {'n_estimators': 861, 'eta': 0.08049206462710913, 'max_depth': 9, 'alpha': 0.0902, 'lambda': 15.179323528315457, 'max_bin': 294}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:08:51,145]\u001b[0m Trial 235 finished with value: 0.6894014690693554 and parameters: {'n_estimators': 808, 'eta': 0.05292988415524333, 'max_depth': 9, 'alpha': 0.053500000000000006, 'lambda': 11.938225213366163, 'max_bin': 285}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:09:20,579]\u001b[0m Trial 236 finished with value: 0.690177766483054 and parameters: {'n_estimators': 899, 'eta': 0.07881742895196522, 'max_depth': 12, 'alpha': 0.014400000000000001, 'lambda': 17.06328352072797, 'max_bin': 269}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:09:43,038]\u001b[0m Trial 237 finished with value: 0.6918933504206748 and parameters: {'n_estimators': 874, 'eta': 0.08594169063843428, 'max_depth': 9, 'alpha': 0.4184, 'lambda': 9.884084628460652, 'max_bin': 304}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:10:17,533]\u001b[0m Trial 238 finished with value: 0.6914759463428434 and parameters: {'n_estimators': 833, 'eta': 0.07547997397554912, 'max_depth': 12, 'alpha': 0.0, 'lambda': 16.112434829939307, 'max_bin': 264}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:10:56,146]\u001b[0m Trial 239 finished with value: 0.69125069844655 and parameters: {'n_estimators': 857, 'eta': 0.06093477363117051, 'max_depth': 12, 'alpha': 0.1448, 'lambda': 13.484469122883267, 'max_bin': 397}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:11:24,201]\u001b[0m Trial 240 finished with value: 0.6891765123581999 and parameters: {'n_estimators': 819, 'eta': 0.09163506958688185, 'max_depth': 10, 'alpha': 0.1907, 'lambda': 18.832444983282393, 'max_bin': 330}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:11:50,510]\u001b[0m Trial 241 finished with value: 0.6878536630208693 and parameters: {'n_estimators': 899, 'eta': 0.09360240126843301, 'max_depth': 12, 'alpha': 0.038400000000000004, 'lambda': 16.659960608428154, 'max_bin': 254}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:12:20,238]\u001b[0m Trial 242 finished with value: 0.6870864552501419 and parameters: {'n_estimators': 879, 'eta': 0.09625437671140856, 'max_depth': 12, 'alpha': 0.0644, 'lambda': 15.559230069130397, 'max_bin': 259}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:12:52,238]\u001b[0m Trial 243 finished with value: 0.6866623472440226 and parameters: {'n_estimators': 462, 'eta': 0.0899089198135889, 'max_depth': 12, 'alpha': 0.0239, 'lambda': 14.816155186189249, 'max_bin': 265}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:13:27,120]\u001b[0m Trial 244 finished with value: 0.6876259342022357 and parameters: {'n_estimators': 878, 'eta': 0.08195842013218273, 'max_depth': 12, 'alpha': 0.054700000000000006, 'lambda': 17.867273717438874, 'max_bin': 255}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:13:40,201]\u001b[0m Trial 245 finished with value: 0.6826506781527232 and parameters: {'n_estimators': 119, 'eta': 0.09226334462151134, 'max_depth': 9, 'alpha': 0.1056, 'lambda': 16.709917994432445, 'max_bin': 320}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:14:37,881]\u001b[0m Trial 246 finished with value: 0.6895742579951449 and parameters: {'n_estimators': 857, 'eta': 0.041974524596435664, 'max_depth': 12, 'alpha': 0.0356, 'lambda': 15.562313980116926, 'max_bin': 276}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:15:33,631]\u001b[0m Trial 247 finished with value: 0.6871008701394087 and parameters: {'n_estimators': 891, 'eta': 0.03513479749928518, 'max_depth': 11, 'alpha': 0.085, 'lambda': 19.50800568655782, 'max_bin': 270}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:15:58,648]\u001b[0m Trial 248 finished with value: 0.6820682718951083 and parameters: {'n_estimators': 840, 'eta': 0.09468807782128152, 'max_depth': 12, 'alpha': 0.1356, 'lambda': 14.376547552541519, 'max_bin': 250}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:16:23,895]\u001b[0m Trial 249 finished with value: 0.6862133771404662 and parameters: {'n_estimators': 870, 'eta': 0.08440604300288038, 'max_depth': 9, 'alpha': 0.1718, 'lambda': 17.311833038169915, 'max_bin': 261}. Best is trial 98 with value: 0.7119772408753071.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7120\n",
      "\tBest params:\n",
      "\t\tn_estimators: 880\n",
      "\t\teta: 0.06674460905913907\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.11510000000000001\n",
      "\t\tlambda: 15.478008077026015\n",
      "\t\tmax_bin: 264\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.706611    0.686641    0.737877    0.695883   \n",
      "1                    TP   42.000000   44.000000   43.000000   36.000000   \n",
      "2                    TN  197.000000  198.000000  197.000000  194.000000   \n",
      "3                    FP    4.000000    3.000000    4.000000    6.000000   \n",
      "4                    FN   25.000000   23.000000   24.000000   32.000000   \n",
      "5              Accuracy    0.891791    0.902985    0.895522    0.858209   \n",
      "6             Precision    0.913043    0.936170    0.914894    0.857143   \n",
      "7           Sensitivity    0.626866    0.656716    0.641791    0.529412   \n",
      "8           Specificity    0.980100    0.985100    0.980100    0.970000   \n",
      "9              F1 score    0.743363    0.771930    0.754386    0.654545   \n",
      "10  F1 score (weighted)    0.884422    0.896774    0.888833    0.845779   \n",
      "11     F1 score (macro)    0.837402    0.855159    0.844018    0.782672   \n",
      "12    Balanced Accuracy    0.803483    0.820896    0.810945    0.749706   \n",
      "13                  MCC    0.697018    0.730776    0.708116    0.597791   \n",
      "14                  NPV    0.887400    0.895900    0.891400    0.858400   \n",
      "15              ROC_AUC    0.803483    0.820896    0.810945    0.749706   \n",
      "\n",
      "          Set4  \n",
      "0     0.730519  \n",
      "1    42.000000  \n",
      "2   198.000000  \n",
      "3     5.000000  \n",
      "4    23.000000  \n",
      "5     0.895522  \n",
      "6     0.893617  \n",
      "7     0.646154  \n",
      "8     0.975400  \n",
      "9     0.750000  \n",
      "10    0.889345  \n",
      "11    0.841981  \n",
      "12    0.810762  \n",
      "13    0.700514  \n",
      "14    0.895900  \n",
      "15    0.810762  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=24,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_xgb_4)\n",
    "# now convert the resuls to binary with cutoff 6.4\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_4_cat = np.where(((y_pred_xgb_4 >= 2) | (y_pred_xgb_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 01:17:09,711]\u001b[0m Trial 250 finished with value: 0.7193447157888191 and parameters: {'n_estimators': 781, 'eta': 0.08794052503838098, 'max_depth': 12, 'alpha': 0.050300000000000004, 'lambda': 16.111663744985297, 'max_bin': 290}. Best is trial 250 with value: 0.7193447157888191.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:17:42,338]\u001b[0m Trial 251 finished with value: 0.7187538882291743 and parameters: {'n_estimators': 787, 'eta': 0.0871104019347991, 'max_depth': 12, 'alpha': 0.054900000000000004, 'lambda': 16.098264384746948, 'max_bin': 292}. Best is trial 250 with value: 0.7193447157888191.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:18:21,359]\u001b[0m Trial 252 finished with value: 0.7135173232632894 and parameters: {'n_estimators': 747, 'eta': 0.08763874442038816, 'max_depth': 12, 'alpha': 0.0562, 'lambda': 16.018189105615217, 'max_bin': 300}. Best is trial 250 with value: 0.7193447157888191.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:19:02,195]\u001b[0m Trial 253 finished with value: 0.7170599526070658 and parameters: {'n_estimators': 784, 'eta': 0.08853792975589707, 'max_depth': 12, 'alpha': 0.055900000000000005, 'lambda': 16.269744845129015, 'max_bin': 305}. Best is trial 250 with value: 0.7193447157888191.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:19:36,931]\u001b[0m Trial 254 finished with value: 0.7196585078286221 and parameters: {'n_estimators': 773, 'eta': 0.086966054469444, 'max_depth': 12, 'alpha': 0.0516, 'lambda': 16.311475590735377, 'max_bin': 300}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:20:16,458]\u001b[0m Trial 255 finished with value: 0.7163797232941549 and parameters: {'n_estimators': 741, 'eta': 0.08711626634080713, 'max_depth': 12, 'alpha': 0.049800000000000004, 'lambda': 16.434209352617575, 'max_bin': 299}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:20:56,440]\u001b[0m Trial 256 finished with value: 0.7155476293874722 and parameters: {'n_estimators': 739, 'eta': 0.08785496313717123, 'max_depth': 12, 'alpha': 0.0529, 'lambda': 16.067428161566458, 'max_bin': 293}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:21:37,257]\u001b[0m Trial 257 finished with value: 0.7169630697734626 and parameters: {'n_estimators': 748, 'eta': 0.08616689096290171, 'max_depth': 12, 'alpha': 0.0538, 'lambda': 16.241168549844932, 'max_bin': 299}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:22:12,902]\u001b[0m Trial 258 finished with value: 0.7123472449876489 and parameters: {'n_estimators': 723, 'eta': 0.08805039604618135, 'max_depth': 12, 'alpha': 0.0538, 'lambda': 16.224398524871937, 'max_bin': 299}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:22:54,306]\u001b[0m Trial 259 finished with value: 0.7185929593615132 and parameters: {'n_estimators': 742, 'eta': 0.08745350890701385, 'max_depth': 12, 'alpha': 0.056, 'lambda': 16.27991922939068, 'max_bin': 295}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:23:31,878]\u001b[0m Trial 260 finished with value: 0.7160133350084834 and parameters: {'n_estimators': 710, 'eta': 0.08685008215149997, 'max_depth': 12, 'alpha': 0.054400000000000004, 'lambda': 16.137560970198876, 'max_bin': 298}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:24:11,648]\u001b[0m Trial 261 finished with value: 0.7136571911936251 and parameters: {'n_estimators': 732, 'eta': 0.08865926153548699, 'max_depth': 12, 'alpha': 0.0485, 'lambda': 16.266246487412282, 'max_bin': 297}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:24:43,201]\u001b[0m Trial 262 finished with value: 0.7160347285534516 and parameters: {'n_estimators': 713, 'eta': 0.0875627753947512, 'max_depth': 12, 'alpha': 0.0534, 'lambda': 16.599732912456318, 'max_bin': 298}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:25:20,230]\u001b[0m Trial 263 finished with value: 0.7170471238051092 and parameters: {'n_estimators': 686, 'eta': 0.08792047571032943, 'max_depth': 12, 'alpha': 0.0507, 'lambda': 16.301923027528083, 'max_bin': 299}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:25:56,230]\u001b[0m Trial 264 finished with value: 0.7186773521575028 and parameters: {'n_estimators': 696, 'eta': 0.08784128565139365, 'max_depth': 12, 'alpha': 0.054, 'lambda': 16.355303582700607, 'max_bin': 298}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:26:35,218]\u001b[0m Trial 265 finished with value: 0.7166475724974245 and parameters: {'n_estimators': 704, 'eta': 0.0873058506580504, 'max_depth': 12, 'alpha': 0.0575, 'lambda': 16.057728413834628, 'max_bin': 299}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:27:06,282]\u001b[0m Trial 266 finished with value: 0.7179672869992472 and parameters: {'n_estimators': 709, 'eta': 0.08733910757469966, 'max_depth': 12, 'alpha': 0.0568, 'lambda': 16.402692038400396, 'max_bin': 300}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:27:40,797]\u001b[0m Trial 267 finished with value: 0.7154431019385586 and parameters: {'n_estimators': 689, 'eta': 0.08756602166009796, 'max_depth': 12, 'alpha': 0.055, 'lambda': 16.228748316686673, 'max_bin': 300}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:28:19,759]\u001b[0m Trial 268 finished with value: 0.7191698137313494 and parameters: {'n_estimators': 702, 'eta': 0.08713925817318555, 'max_depth': 12, 'alpha': 0.0567, 'lambda': 16.218305176806595, 'max_bin': 296}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:28:58,298]\u001b[0m Trial 269 finished with value: 0.7182686300061126 and parameters: {'n_estimators': 699, 'eta': 0.08790731176433378, 'max_depth': 12, 'alpha': 0.053700000000000005, 'lambda': 16.296178280335972, 'max_bin': 298}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:29:36,545]\u001b[0m Trial 270 finished with value: 0.7153130808651773 and parameters: {'n_estimators': 695, 'eta': 0.08779343469190168, 'max_depth': 12, 'alpha': 0.052000000000000005, 'lambda': 16.248284615381404, 'max_bin': 300}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:30:19,177]\u001b[0m Trial 271 finished with value: 0.7193509665880227 and parameters: {'n_estimators': 693, 'eta': 0.08786029075776478, 'max_depth': 12, 'alpha': 0.051800000000000006, 'lambda': 16.26614075554476, 'max_bin': 299}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:30:53,638]\u001b[0m Trial 272 finished with value: 0.7140002012506228 and parameters: {'n_estimators': 693, 'eta': 0.08784077331602617, 'max_depth': 12, 'alpha': 0.0509, 'lambda': 16.58111711275139, 'max_bin': 297}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:31:27,777]\u001b[0m Trial 273 finished with value: 0.7132458221390252 and parameters: {'n_estimators': 687, 'eta': 0.08799468284058454, 'max_depth': 12, 'alpha': 0.0512, 'lambda': 16.869461024416157, 'max_bin': 299}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:32:03,685]\u001b[0m Trial 274 finished with value: 0.7164073910096086 and parameters: {'n_estimators': 698, 'eta': 0.08770412882948489, 'max_depth': 12, 'alpha': 0.0468, 'lambda': 16.740402861506936, 'max_bin': 300}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:32:38,779]\u001b[0m Trial 275 finished with value: 0.7161978781692193 and parameters: {'n_estimators': 687, 'eta': 0.08854414788830074, 'max_depth': 12, 'alpha': 0.049800000000000004, 'lambda': 16.85569562148397, 'max_bin': 299}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:33:21,055]\u001b[0m Trial 276 finished with value: 0.7156217544418846 and parameters: {'n_estimators': 693, 'eta': 0.08817661644777336, 'max_depth': 12, 'alpha': 0.045700000000000005, 'lambda': 17.203339394762327, 'max_bin': 299}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:33:59,029]\u001b[0m Trial 277 finished with value: 0.7166028015554721 and parameters: {'n_estimators': 700, 'eta': 0.08758408827373583, 'max_depth': 12, 'alpha': 0.0721, 'lambda': 17.209385302780582, 'max_bin': 301}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:34:38,841]\u001b[0m Trial 278 finished with value: 0.7126324616447705 and parameters: {'n_estimators': 697, 'eta': 0.08676503034723337, 'max_depth': 12, 'alpha': 0.0367, 'lambda': 17.104059528921425, 'max_bin': 295}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 01:35:14,279]\u001b[0m Trial 279 finished with value: 0.7127454199370387 and parameters: {'n_estimators': 672, 'eta': 0.08892932358956239, 'max_depth': 12, 'alpha': 0.07400000000000001, 'lambda': 17.52474389060688, 'max_bin': 303}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:35:47,696]\u001b[0m Trial 280 finished with value: 0.7163516014081558 and parameters: {'n_estimators': 707, 'eta': 0.0862584483743298, 'max_depth': 12, 'alpha': 0.0419, 'lambda': 16.82518498635766, 'max_bin': 298}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:36:27,213]\u001b[0m Trial 281 finished with value: 0.7114166253176413 and parameters: {'n_estimators': 703, 'eta': 0.08570582532439791, 'max_depth': 12, 'alpha': 0.6365000000000001, 'lambda': 17.255368230438055, 'max_bin': 304}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:37:02,054]\u001b[0m Trial 282 finished with value: 0.7142486308350013 and parameters: {'n_estimators': 710, 'eta': 0.08676053036722672, 'max_depth': 12, 'alpha': 0.029900000000000003, 'lambda': 16.832627675893523, 'max_bin': 293}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:37:46,679]\u001b[0m Trial 283 finished with value: 0.7154539111347669 and parameters: {'n_estimators': 658, 'eta': 0.08645116775534832, 'max_depth': 12, 'alpha': 0.0292, 'lambda': 18.076207461656544, 'max_bin': 292}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:38:24,939]\u001b[0m Trial 284 finished with value: 0.7174033440785952 and parameters: {'n_estimators': 659, 'eta': 0.09001229001757513, 'max_depth': 12, 'alpha': 0.076, 'lambda': 18.040899101454563, 'max_bin': 301}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:39:07,161]\u001b[0m Trial 285 finished with value: 0.7149746188967651 and parameters: {'n_estimators': 653, 'eta': 0.09027385222010488, 'max_depth': 12, 'alpha': 0.07400000000000001, 'lambda': 18.192767626682876, 'max_bin': 306}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:39:46,555]\u001b[0m Trial 286 finished with value: 0.7186882010281294 and parameters: {'n_estimators': 676, 'eta': 0.08538272538489872, 'max_depth': 12, 'alpha': 0.0201, 'lambda': 17.84168025330631, 'max_bin': 293}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:40:26,276]\u001b[0m Trial 287 finished with value: 0.7153774208691449 and parameters: {'n_estimators': 673, 'eta': 0.08554203426057633, 'max_depth': 12, 'alpha': 0.0206, 'lambda': 18.000044800611942, 'max_bin': 296}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:41:01,405]\u001b[0m Trial 288 finished with value: 0.7194719858405015 and parameters: {'n_estimators': 714, 'eta': 0.08979566929715571, 'max_depth': 12, 'alpha': 0.026000000000000002, 'lambda': 17.61323860687579, 'max_bin': 291}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:41:31,427]\u001b[0m Trial 289 finished with value: 0.7107769698131424 and parameters: {'n_estimators': 713, 'eta': 0.09085555343031047, 'max_depth': 12, 'alpha': 0.0752, 'lambda': 17.451218035160487, 'max_bin': 291}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:42:12,971]\u001b[0m Trial 290 finished with value: 0.7163697671731442 and parameters: {'n_estimators': 735, 'eta': 0.08970264971271047, 'max_depth': 12, 'alpha': 0.0198, 'lambda': 17.31439853329315, 'max_bin': 304}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:42:51,594]\u001b[0m Trial 291 finished with value: 0.7157155125116292 and parameters: {'n_estimators': 725, 'eta': 0.09022233038214891, 'max_depth': 12, 'alpha': 0.0269, 'lambda': 17.62421435141544, 'max_bin': 304}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:43:30,048]\u001b[0m Trial 292 finished with value: 0.7157815868935571 and parameters: {'n_estimators': 722, 'eta': 0.08978402203849514, 'max_depth': 12, 'alpha': 0.009000000000000001, 'lambda': 18.438296193245094, 'max_bin': 306}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:44:06,443]\u001b[0m Trial 293 finished with value: 0.7132350001480147 and parameters: {'n_estimators': 708, 'eta': 0.09008039512116255, 'max_depth': 12, 'alpha': 0.013300000000000001, 'lambda': 18.512386949096992, 'max_bin': 305}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:44:52,316]\u001b[0m Trial 294 finished with value: 0.7184759209245776 and parameters: {'n_estimators': 672, 'eta': 0.08498752042967729, 'max_depth': 12, 'alpha': 0.0102, 'lambda': 18.34874097365676, 'max_bin': 308}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:45:28,286]\u001b[0m Trial 295 finished with value: 0.7179942377166155 and parameters: {'n_estimators': 677, 'eta': 0.08560871483906424, 'max_depth': 12, 'alpha': 0.032100000000000004, 'lambda': 17.03715486295085, 'max_bin': 309}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:46:07,796]\u001b[0m Trial 296 finished with value: 0.71626906959659 and parameters: {'n_estimators': 679, 'eta': 0.08508272606479239, 'max_depth': 12, 'alpha': 0.0008, 'lambda': 17.157823556313915, 'max_bin': 310}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:46:48,667]\u001b[0m Trial 297 finished with value: 0.7168677550675431 and parameters: {'n_estimators': 676, 'eta': 0.08493354728533604, 'max_depth': 12, 'alpha': 0.0011, 'lambda': 17.749748931721488, 'max_bin': 309}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:47:26,281]\u001b[0m Trial 298 finished with value: 0.7148997363647408 and parameters: {'n_estimators': 672, 'eta': 0.0846723957129671, 'max_depth': 12, 'alpha': 0.0033, 'lambda': 17.81397082012535, 'max_bin': 309}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:48:09,861]\u001b[0m Trial 299 finished with value: 0.719129194003727 and parameters: {'n_estimators': 623, 'eta': 0.08501097798049397, 'max_depth': 12, 'alpha': 0.016900000000000002, 'lambda': 18.796196570805677, 'max_bin': 309}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7197\n",
      "\tBest params:\n",
      "\t\tn_estimators: 773\n",
      "\t\teta: 0.086966054469444\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.0516\n",
      "\t\tlambda: 16.311475590735377\n",
      "\t\tmax_bin: 300\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.706611    0.686641    0.737877    0.695883   \n",
      "1                    TP   42.000000   44.000000   43.000000   36.000000   \n",
      "2                    TN  197.000000  198.000000  197.000000  194.000000   \n",
      "3                    FP    4.000000    3.000000    4.000000    6.000000   \n",
      "4                    FN   25.000000   23.000000   24.000000   32.000000   \n",
      "5              Accuracy    0.891791    0.902985    0.895522    0.858209   \n",
      "6             Precision    0.913043    0.936170    0.914894    0.857143   \n",
      "7           Sensitivity    0.626866    0.656716    0.641791    0.529412   \n",
      "8           Specificity    0.980100    0.985100    0.980100    0.970000   \n",
      "9              F1 score    0.743363    0.771930    0.754386    0.654545   \n",
      "10  F1 score (weighted)    0.884422    0.896774    0.888833    0.845779   \n",
      "11     F1 score (macro)    0.837402    0.855159    0.844018    0.782672   \n",
      "12    Balanced Accuracy    0.803483    0.820896    0.810945    0.749706   \n",
      "13                  MCC    0.697018    0.730776    0.708116    0.597791   \n",
      "14                  NPV    0.887400    0.895900    0.891400    0.858400   \n",
      "15              ROC_AUC    0.803483    0.820896    0.810945    0.749706   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.730519    0.701294  \n",
      "1    42.000000   39.000000  \n",
      "2   198.000000  199.000000  \n",
      "3     5.000000    2.000000  \n",
      "4    23.000000   28.000000  \n",
      "5     0.895522    0.888060  \n",
      "6     0.893617    0.951220  \n",
      "7     0.646154    0.582090  \n",
      "8     0.975400    0.990000  \n",
      "9     0.750000    0.722222  \n",
      "10    0.889345    0.877985  \n",
      "11    0.841981    0.826064  \n",
      "12    0.810762    0.786070  \n",
      "13    0.700514    0.688228  \n",
      "14    0.895900    0.876700  \n",
      "15    0.810762    0.786070  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=24,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_xgb_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_5_cat = np.where(((y_pred_xgb_5 >= 2) | (y_pred_xgb_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 01:48:43,288]\u001b[0m Trial 300 finished with value: 0.6833643235769459 and parameters: {'n_estimators': 656, 'eta': 0.08508162793015696, 'max_depth': 12, 'alpha': 0.0218, 'lambda': 18.836525806948597, 'max_bin': 303}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:49:23,247]\u001b[0m Trial 301 finished with value: 0.6824298617611485 and parameters: {'n_estimators': 627, 'eta': 0.08512575806490182, 'max_depth': 12, 'alpha': 0.016300000000000002, 'lambda': 18.415651157928547, 'max_bin': 308}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:50:35,319]\u001b[0m Trial 302 finished with value: 0.6889574469287887 and parameters: {'n_estimators': 747, 'eta': 0.027771859169054135, 'max_depth': 12, 'alpha': 0.029, 'lambda': 18.07535671344687, 'max_bin': 289}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:51:13,306]\u001b[0m Trial 303 finished with value: 0.6865046902029367 and parameters: {'n_estimators': 665, 'eta': 0.08451964398733121, 'max_depth': 12, 'alpha': 0.029900000000000003, 'lambda': 17.457207605745186, 'max_bin': 293}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:51:42,053]\u001b[0m Trial 304 finished with value: 0.6853901309110064 and parameters: {'n_estimators': 644, 'eta': 0.09188853064661397, 'max_depth': 12, 'alpha': 0.0005, 'lambda': 17.567317324716225, 'max_bin': 302}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:52:14,780]\u001b[0m Trial 305 finished with value: 0.6806327260737561 and parameters: {'n_estimators': 741, 'eta': 0.08922709598848318, 'max_depth': 12, 'alpha': 0.9375, 'lambda': 19.08933674144126, 'max_bin': 308}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:52:52,389]\u001b[0m Trial 306 finished with value: 0.6872847759619359 and parameters: {'n_estimators': 677, 'eta': 0.08617179301972536, 'max_depth': 12, 'alpha': 0.0354, 'lambda': 16.912113488394716, 'max_bin': 293}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:53:41,989]\u001b[0m Trial 307 finished with value: 0.6881156228154126 and parameters: {'n_estimators': 726, 'eta': 0.08331593494394739, 'max_depth': 12, 'alpha': 0.0756, 'lambda': 18.470751921513212, 'max_bin': 302}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:54:41,825]\u001b[0m Trial 308 finished with value: 0.6805087037995159 and parameters: {'n_estimators': 753, 'eta': 0.09010887306383467, 'max_depth': 12, 'alpha': 0.0247, 'lambda': 17.74340246868563, 'max_bin': 288}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:55:40,878]\u001b[0m Trial 309 finished with value: 0.6797644922017799 and parameters: {'n_estimators': 698, 'eta': 0.0855195617974824, 'max_depth': 12, 'alpha': 0.0335, 'lambda': 15.590944293414799, 'max_bin': 312}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:56:31,678]\u001b[0m Trial 310 finished with value: 0.6795613152907216 and parameters: {'n_estimators': 625, 'eta': 0.09171556150001761, 'max_depth': 12, 'alpha': 0.07690000000000001, 'lambda': 16.820611364315802, 'max_bin': 304}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:58:53,020]\u001b[0m Trial 311 finished with value: 0.6840909490594196 and parameters: {'n_estimators': 734, 'eta': 0.01265397869582565, 'max_depth': 12, 'alpha': 0.020300000000000002, 'lambda': 17.67523556336256, 'max_bin': 295}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 01:59:37,205]\u001b[0m Trial 312 finished with value: 0.6830887362598055 and parameters: {'n_estimators': 666, 'eta': 0.0867902358732773, 'max_depth': 12, 'alpha': 0.07, 'lambda': 16.737361874565604, 'max_bin': 308}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:00:32,454]\u001b[0m Trial 313 finished with value: 0.684990790072999 and parameters: {'n_estimators': 705, 'eta': 0.08396160565028253, 'max_depth': 12, 'alpha': 0.038, 'lambda': 18.70192669806389, 'max_bin': 296}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:01:22,081]\u001b[0m Trial 314 finished with value: 0.6826242992303775 and parameters: {'n_estimators': 679, 'eta': 0.08948522121600512, 'max_depth': 12, 'alpha': 0.0031000000000000003, 'lambda': 15.487998004799639, 'max_bin': 302}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:02:07,717]\u001b[0m Trial 315 finished with value: 0.6853319809980424 and parameters: {'n_estimators': 758, 'eta': 0.086408786253495, 'max_depth': 12, 'alpha': 0.0373, 'lambda': 17.036780451966973, 'max_bin': 289}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:03:02,373]\u001b[0m Trial 316 finished with value: 0.6768322327697787 and parameters: {'n_estimators': 718, 'eta': 0.08874111740574976, 'max_depth': 12, 'alpha': 0.0666, 'lambda': 15.853408817901077, 'max_bin': 299}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:04:01,037]\u001b[0m Trial 317 finished with value: 0.6816898592779068 and parameters: {'n_estimators': 641, 'eta': 0.09165398225931966, 'max_depth': 12, 'alpha': 0.018500000000000003, 'lambda': 17.640420621970193, 'max_bin': 312}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:05:20,750]\u001b[0m Trial 318 finished with value: 0.6846604902519253 and parameters: {'n_estimators': 689, 'eta': 0.0841952300509024, 'max_depth': 12, 'alpha': 0.0859, 'lambda': 19.37124695371079, 'max_bin': 292}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:06:15,129]\u001b[0m Trial 319 finished with value: 0.6799533520063944 and parameters: {'n_estimators': 736, 'eta': 0.08648521359117745, 'max_depth': 12, 'alpha': 0.0446, 'lambda': 16.2901144949935, 'max_bin': 305}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:07:05,506]\u001b[0m Trial 320 finished with value: 0.6839779054635835 and parameters: {'n_estimators': 706, 'eta': 0.08900812484900136, 'max_depth': 12, 'alpha': 0.0678, 'lambda': 18.18899521610535, 'max_bin': 296}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:07:50,836]\u001b[0m Trial 321 finished with value: 0.6823041821744805 and parameters: {'n_estimators': 667, 'eta': 0.08394988492234118, 'max_depth': 12, 'alpha': 0.040400000000000005, 'lambda': 15.343885960890823, 'max_bin': 301}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:08:39,525]\u001b[0m Trial 322 finished with value: 0.6787960711874079 and parameters: {'n_estimators': 765, 'eta': 0.08708905009170297, 'max_depth': 12, 'alpha': 0.0154, 'lambda': 16.678780964822252, 'max_bin': 287}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:09:43,662]\u001b[0m Trial 323 finished with value: 0.6821031081994902 and parameters: {'n_estimators': 611, 'eta': 0.09074514569883377, 'max_depth': 12, 'alpha': 0.06430000000000001, 'lambda': 17.40955027137131, 'max_bin': 309}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:10:14,514]\u001b[0m Trial 324 finished with value: 0.6838764609792223 and parameters: {'n_estimators': 692, 'eta': 0.08593640364805175, 'max_depth': 12, 'alpha': 0.09000000000000001, 'lambda': 1.5160760245148808, 'max_bin': 295}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:12:58,673]\u001b[0m Trial 325 finished with value: 0.6034399120194587 and parameters: {'n_estimators': 707, 'eta': 0.003291517500463234, 'max_depth': 12, 'alpha': 0.0405, 'lambda': 16.024813749864947, 'max_bin': 302}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:13:57,900]\u001b[0m Trial 326 finished with value: 0.6800143312616848 and parameters: {'n_estimators': 725, 'eta': 0.08916964006035166, 'max_depth': 12, 'alpha': 0.0007, 'lambda': 18.624941403402367, 'max_bin': 291}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:14:43,429]\u001b[0m Trial 327 finished with value: 0.6801558077506934 and parameters: {'n_estimators': 654, 'eta': 0.09251695165677652, 'max_depth': 12, 'alpha': 0.0244, 'lambda': 16.746380474776444, 'max_bin': 314}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:15:36,449]\u001b[0m Trial 328 finished with value: 0.6824764099372449 and parameters: {'n_estimators': 750, 'eta': 0.08333286181536276, 'max_depth': 12, 'alpha': 0.0669, 'lambda': 15.331247771018885, 'max_bin': 306}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 02:16:28,180]\u001b[0m Trial 329 finished with value: 0.6838618958799785 and parameters: {'n_estimators': 733, 'eta': 0.08797776697778703, 'max_depth': 12, 'alpha': 0.041100000000000005, 'lambda': 17.938020792416875, 'max_bin': 297}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:17:17,086]\u001b[0m Trial 330 finished with value: 0.6828501147745342 and parameters: {'n_estimators': 683, 'eta': 0.08548281680133508, 'max_depth': 12, 'alpha': 0.0881, 'lambda': 17.173817760799512, 'max_bin': 300}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:18:06,544]\u001b[0m Trial 331 finished with value: 0.679932851467766 and parameters: {'n_estimators': 712, 'eta': 0.09010611916321756, 'max_depth': 12, 'alpha': 0.0193, 'lambda': 16.217397872321154, 'max_bin': 285}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:18:54,414]\u001b[0m Trial 332 finished with value: 0.6783290258050266 and parameters: {'n_estimators': 688, 'eta': 0.08768583300669548, 'max_depth': 12, 'alpha': 0.060700000000000004, 'lambda': 15.661803966056041, 'max_bin': 291}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:19:36,143]\u001b[0m Trial 333 finished with value: 0.6843918741458663 and parameters: {'n_estimators': 660, 'eta': 0.08558425406327394, 'max_depth': 12, 'alpha': 0.0375, 'lambda': 19.50514619331378, 'max_bin': 305}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:19:43,961]\u001b[0m Trial 334 finished with value: 0.6859179826888584 and parameters: {'n_estimators': 772, 'eta': 0.08344484953301057, 'max_depth': 12, 'alpha': 0.018600000000000002, 'lambda': 16.990204562618175, 'max_bin': 297}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:19:52,575]\u001b[0m Trial 335 finished with value: 0.6754157239636571 and parameters: {'n_estimators': 678, 'eta': 0.09223519562242853, 'max_depth': 12, 'alpha': 0.3627, 'lambda': 36.04161677824495, 'max_bin': 308}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:19:59,115]\u001b[0m Trial 336 finished with value: 0.682800597086676 and parameters: {'n_estimators': 699, 'eta': 0.0885659527563811, 'max_depth': 12, 'alpha': 0.8293, 'lambda': 18.114856797491864, 'max_bin': 300}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:21:11,249]\u001b[0m Trial 337 finished with value: 0.6788360371111741 and parameters: {'n_estimators': 721, 'eta': 0.08681177291674742, 'max_depth': 12, 'alpha': 0.0821, 'lambda': 28.979403399237675, 'max_bin': 312}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:22:20,965]\u001b[0m Trial 338 finished with value: 0.6819306676422369 and parameters: {'n_estimators': 747, 'eta': 0.09068199708865644, 'max_depth': 12, 'alpha': 0.048, 'lambda': 16.28411131897811, 'max_bin': 293}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:23:43,131]\u001b[0m Trial 339 finished with value: 0.6813904826828678 and parameters: {'n_estimators': 702, 'eta': 0.08481867139546699, 'max_depth': 12, 'alpha': 0.0018000000000000002, 'lambda': 15.205168797006326, 'max_bin': 454}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:25:09,727]\u001b[0m Trial 340 finished with value: 0.6785503102509698 and parameters: {'n_estimators': 640, 'eta': 0.08882909925258171, 'max_depth': 12, 'alpha': 0.0666, 'lambda': 17.584894657586474, 'max_bin': 287}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:26:39,384]\u001b[0m Trial 341 finished with value: 0.6870920808637968 and parameters: {'n_estimators': 763, 'eta': 0.08685183846879492, 'max_depth': 12, 'alpha': 0.0363, 'lambda': 18.847485126557537, 'max_bin': 441}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:28:06,643]\u001b[0m Trial 342 finished with value: 0.6824689564818445 and parameters: {'n_estimators': 731, 'eta': 0.09371244694593259, 'max_depth': 12, 'alpha': 0.0575, 'lambda': 16.821590833077963, 'max_bin': 302}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:29:31,053]\u001b[0m Trial 343 finished with value: 0.6784810927925102 and parameters: {'n_estimators': 670, 'eta': 0.09050835006152876, 'max_depth': 12, 'alpha': 0.014400000000000001, 'lambda': 15.919170451444256, 'max_bin': 295}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:31:03,322]\u001b[0m Trial 344 finished with value: 0.6876193681751538 and parameters: {'n_estimators': 682, 'eta': 0.08262729044954463, 'max_depth': 12, 'alpha': 0.0816, 'lambda': 17.837035797709298, 'max_bin': 306}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:32:38,481]\u001b[0m Trial 345 finished with value: 0.684359471134204 and parameters: {'n_estimators': 715, 'eta': 0.08458315041298238, 'max_depth': 12, 'alpha': 0.040400000000000005, 'lambda': 16.61487028946581, 'max_bin': 314}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:33:58,767]\u001b[0m Trial 346 finished with value: 0.6769999669422477 and parameters: {'n_estimators': 746, 'eta': 0.08790268315415896, 'max_depth': 12, 'alpha': 0.021500000000000002, 'lambda': 15.281348119334714, 'max_bin': 299}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:35:11,043]\u001b[0m Trial 347 finished with value: 0.6818443875011668 and parameters: {'n_estimators': 698, 'eta': 0.08946630689515617, 'max_depth': 12, 'alpha': 0.059000000000000004, 'lambda': 17.29641282266123, 'max_bin': 289}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:36:35,040]\u001b[0m Trial 348 finished with value: 0.6825635742684689 and parameters: {'n_estimators': 663, 'eta': 0.08581599893055707, 'max_depth': 12, 'alpha': 0.09340000000000001, 'lambda': 18.68858646928215, 'max_bin': 303}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:37:45,341]\u001b[0m Trial 349 finished with value: 0.6806531269883408 and parameters: {'n_estimators': 724, 'eta': 0.09247670198926633, 'max_depth': 12, 'alpha': 0.0443, 'lambda': 16.30284656944079, 'max_bin': 294}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7197\n",
      "\tBest params:\n",
      "\t\tn_estimators: 773\n",
      "\t\teta: 0.086966054469444\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.0516\n",
      "\t\tlambda: 16.311475590735377\n",
      "\t\tmax_bin: 300\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.706611    0.686641    0.737877    0.695883   \n",
      "1                    TP   42.000000   44.000000   43.000000   36.000000   \n",
      "2                    TN  197.000000  198.000000  197.000000  194.000000   \n",
      "3                    FP    4.000000    3.000000    4.000000    6.000000   \n",
      "4                    FN   25.000000   23.000000   24.000000   32.000000   \n",
      "5              Accuracy    0.891791    0.902985    0.895522    0.858209   \n",
      "6             Precision    0.913043    0.936170    0.914894    0.857143   \n",
      "7           Sensitivity    0.626866    0.656716    0.641791    0.529412   \n",
      "8           Specificity    0.980100    0.985100    0.980100    0.970000   \n",
      "9              F1 score    0.743363    0.771930    0.754386    0.654545   \n",
      "10  F1 score (weighted)    0.884422    0.896774    0.888833    0.845779   \n",
      "11     F1 score (macro)    0.837402    0.855159    0.844018    0.782672   \n",
      "12    Balanced Accuracy    0.803483    0.820896    0.810945    0.749706   \n",
      "13                  MCC    0.697018    0.730776    0.708116    0.597791   \n",
      "14                  NPV    0.887400    0.895900    0.891400    0.858400   \n",
      "15              ROC_AUC    0.803483    0.820896    0.810945    0.749706   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.730519    0.701294    0.749091  \n",
      "1    42.000000   39.000000   40.000000  \n",
      "2   198.000000  199.000000  198.000000  \n",
      "3     5.000000    2.000000    4.000000  \n",
      "4    23.000000   28.000000   26.000000  \n",
      "5     0.895522    0.888060    0.888060  \n",
      "6     0.893617    0.951220    0.909091  \n",
      "7     0.646154    0.582090    0.606061  \n",
      "8     0.975400    0.990000    0.980200  \n",
      "9     0.750000    0.722222    0.727273  \n",
      "10    0.889345    0.877985    0.879756  \n",
      "11    0.841981    0.826064    0.828425  \n",
      "12    0.810762    0.786070    0.793129  \n",
      "13    0.700514    0.688228    0.681846  \n",
      "14    0.895900    0.876700    0.883900  \n",
      "15    0.810762    0.786070    0.793129  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=24,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_xgb_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_6_cat = np.where(((y_pred_xgb_6 >= 2) | (y_pred_xgb_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 02:40:21,935]\u001b[0m Trial 350 finished with value: 0.6973689700203398 and parameters: {'n_estimators': 770, 'eta': 0.039227117831662876, 'max_depth': 12, 'alpha': 0.0015, 'lambda': 17.232735731951863, 'max_bin': 309}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:41:48,064]\u001b[0m Trial 351 finished with value: 0.6915246194199314 and parameters: {'n_estimators': 685, 'eta': 0.08683961600524699, 'max_depth': 12, 'alpha': 0.0702, 'lambda': 15.65691531627272, 'max_bin': 299}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:43:17,436]\u001b[0m Trial 352 finished with value: 0.6960589919148927 and parameters: {'n_estimators': 581, 'eta': 0.08887840593376505, 'max_depth': 12, 'alpha': 0.027800000000000002, 'lambda': 19.87998727157258, 'max_bin': 290}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:44:32,072]\u001b[0m Trial 353 finished with value: 0.6927199616014333 and parameters: {'n_estimators': 737, 'eta': 0.08339694235934676, 'max_depth': 12, 'alpha': 0.0512, 'lambda': 17.91949968177714, 'max_bin': 303}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:45:37,159]\u001b[0m Trial 354 finished with value: 0.6897988445423594 and parameters: {'n_estimators': 709, 'eta': 0.09151748605525148, 'max_depth': 12, 'alpha': 0.5807, 'lambda': 15.01941649744132, 'max_bin': 296}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:47:00,324]\u001b[0m Trial 355 finished with value: 0.6962518813964429 and parameters: {'n_estimators': 650, 'eta': 0.08609535755834131, 'max_depth': 12, 'alpha': 0.09580000000000001, 'lambda': 16.51289340046204, 'max_bin': 285}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:48:27,350]\u001b[0m Trial 356 finished with value: 0.6898632180846652 and parameters: {'n_estimators': 759, 'eta': 0.08781847732800206, 'max_depth': 12, 'alpha': 0.0247, 'lambda': 18.58173651475522, 'max_bin': 308}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:49:28,384]\u001b[0m Trial 357 finished with value: 0.6906971308259829 and parameters: {'n_estimators': 696, 'eta': 0.0843462416331935, 'max_depth': 6, 'alpha': 0.0758, 'lambda': 17.2701351265621, 'max_bin': 293}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:50:36,652]\u001b[0m Trial 358 finished with value: 0.6903602830159291 and parameters: {'n_estimators': 783, 'eta': 0.08986788642002813, 'max_depth': 12, 'alpha': 0.044700000000000004, 'lambda': 15.919900775053888, 'max_bin': 315}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:52:06,902]\u001b[0m Trial 359 finished with value: 0.6975152396083647 and parameters: {'n_estimators': 718, 'eta': 0.08250093357967528, 'max_depth': 12, 'alpha': 0.0632, 'lambda': 16.646154504211125, 'max_bin': 302}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:53:45,354]\u001b[0m Trial 360 finished with value: 0.6925869806843603 and parameters: {'n_estimators': 671, 'eta': 0.08723490984191935, 'max_depth': 12, 'alpha': 0.0194, 'lambda': 18.083579660593326, 'max_bin': 297}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:55:09,371]\u001b[0m Trial 361 finished with value: 0.6927432284055889 and parameters: {'n_estimators': 737, 'eta': 0.08520621295128729, 'max_depth': 12, 'alpha': 0.0378, 'lambda': 15.051761670815129, 'max_bin': 306}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:56:39,618]\u001b[0m Trial 362 finished with value: 0.6914209855035357 and parameters: {'n_estimators': 700, 'eta': 0.09053824099017224, 'max_depth': 12, 'alpha': 0.0005, 'lambda': 19.360459216466893, 'max_bin': 291}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:57:37,817]\u001b[0m Trial 363 finished with value: 0.6895804287173776 and parameters: {'n_estimators': 317, 'eta': 0.08856460948734612, 'max_depth': 12, 'alpha': 0.48800000000000004, 'lambda': 17.124781996692267, 'max_bin': 310}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:57:53,438]\u001b[0m Trial 364 finished with value: 0.6950430333722661 and parameters: {'n_estimators': 520, 'eta': 0.0862202578326016, 'max_depth': 12, 'alpha': 0.060500000000000005, 'lambda': 15.94531640845628, 'max_bin': 300}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:57:59,333]\u001b[0m Trial 365 finished with value: 0.6863915874916808 and parameters: {'n_estimators': 627, 'eta': 0.09324932402052723, 'max_depth': 12, 'alpha': 0.7614000000000001, 'lambda': 18.126937801526196, 'max_bin': 284}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:58:06,395]\u001b[0m Trial 366 finished with value: 0.6908893123962321 and parameters: {'n_estimators': 679, 'eta': 0.08318809932519004, 'max_depth': 12, 'alpha': 0.0902, 'lambda': 16.683240129310626, 'max_bin': 304}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:58:14,699]\u001b[0m Trial 367 finished with value: 0.6910826350979293 and parameters: {'n_estimators': 657, 'eta': 0.0886331272024561, 'max_depth': 12, 'alpha': 0.0395, 'lambda': 17.499946368145054, 'max_bin': 293}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:58:52,261]\u001b[0m Trial 368 finished with value: 0.6919847451048693 and parameters: {'n_estimators': 716, 'eta': 0.08505731611800565, 'max_depth': 12, 'alpha': 0.0223, 'lambda': 15.710890134519126, 'max_bin': 297}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:58:58,865]\u001b[0m Trial 369 finished with value: 0.6899308072385629 and parameters: {'n_estimators': 685, 'eta': 0.09103090201996285, 'max_depth': 12, 'alpha': 0.0727, 'lambda': 15.07013914103198, 'max_bin': 315}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:59:07,467]\u001b[0m Trial 370 finished with value: 0.6910100507765193 and parameters: {'n_estimators': 739, 'eta': 0.08695058202553015, 'max_depth': 12, 'alpha': 0.0545, 'lambda': 18.94690440068139, 'max_bin': 288}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:59:17,577]\u001b[0m Trial 371 finished with value: 0.6880361670747337 and parameters: {'n_estimators': 763, 'eta': 0.08961317457309705, 'max_depth': 12, 'alpha': 0.0286, 'lambda': 30.29750811594907, 'max_bin': 310}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:59:25,664]\u001b[0m Trial 372 finished with value: 0.6955290401214503 and parameters: {'n_estimators': 706, 'eta': 0.08215977241830609, 'max_depth': 12, 'alpha': 0.1017, 'lambda': 16.587289227566746, 'max_bin': 302}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:59:35,377]\u001b[0m Trial 373 finished with value: 0.6973599921106189 and parameters: {'n_estimators': 642, 'eta': 0.08463607389243007, 'max_depth': 12, 'alpha': 0.0002, 'lambda': 17.588399030130997, 'max_bin': 296}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:59:46,822]\u001b[0m Trial 374 finished with value: 0.6934569575458039 and parameters: {'n_estimators': 725, 'eta': 0.04902038046420518, 'max_depth': 12, 'alpha': 0.078, 'lambda': 16.079924749654584, 'max_bin': 305}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 02:59:55,442]\u001b[0m Trial 375 finished with value: 0.6916231716999691 and parameters: {'n_estimators': 671, 'eta': 0.0879606477188989, 'max_depth': 12, 'alpha': 0.046, 'lambda': 14.602220906695965, 'max_bin': 299}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:00:06,086]\u001b[0m Trial 376 finished with value: 0.6857801771438639 and parameters: {'n_estimators': 780, 'eta': 0.09207272833098087, 'max_depth': 12, 'alpha': 0.020200000000000003, 'lambda': 18.317529556632444, 'max_bin': 289}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:00:14,092]\u001b[0m Trial 377 finished with value: 0.6932462955011591 and parameters: {'n_estimators': 751, 'eta': 0.08643117727016611, 'max_depth': 12, 'alpha': 0.0599, 'lambda': 17.089057957199937, 'max_bin': 294}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:00:18,965]\u001b[0m Trial 378 finished with value: 0.6812975146089345 and parameters: {'n_estimators': 693, 'eta': 0.09480311966023647, 'max_depth': 5, 'alpha': 0.0362, 'lambda': 15.761422702757582, 'max_bin': 310}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:00:27,517]\u001b[0m Trial 379 finished with value: 0.6930033919783698 and parameters: {'n_estimators': 717, 'eta': 0.08961857561754899, 'max_depth': 12, 'alpha': 0.079, 'lambda': 20.204500237960243, 'max_bin': 301}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:00:34,186]\u001b[0m Trial 380 finished with value: 0.6924778301687006 and parameters: {'n_estimators': 686, 'eta': 0.08432567572638996, 'max_depth': 12, 'alpha': 0.8844000000000001, 'lambda': 16.60956281951974, 'max_bin': 285}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:00:44,240]\u001b[0m Trial 381 finished with value: 0.6964284539748974 and parameters: {'n_estimators': 704, 'eta': 0.08805649696528314, 'max_depth': 12, 'alpha': 0.0494, 'lambda': 17.699309538661826, 'max_bin': 306}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:00:52,313]\u001b[0m Trial 382 finished with value: 0.6949540605528282 and parameters: {'n_estimators': 611, 'eta': 0.08628027436603153, 'max_depth': 12, 'alpha': 0.02, 'lambda': 14.862043895868432, 'max_bin': 297}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:00:59,833]\u001b[0m Trial 383 finished with value: 0.6913951404166354 and parameters: {'n_estimators': 659, 'eta': 0.0910827036330967, 'max_depth': 12, 'alpha': 0.10110000000000001, 'lambda': 18.872184549032475, 'max_bin': 317}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:01:04,804]\u001b[0m Trial 384 finished with value: 0.6855002020786569 and parameters: {'n_estimators': 733, 'eta': 0.08849486098630989, 'max_depth': 12, 'alpha': 0.0651, 'lambda': 4.625423094699421, 'max_bin': 291}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:01:14,824]\u001b[0m Trial 385 finished with value: 0.6936756020629496 and parameters: {'n_estimators': 751, 'eta': 0.08212908496361748, 'max_depth': 12, 'alpha': 0.0325, 'lambda': 16.231744317737608, 'max_bin': 301}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:01:35,057]\u001b[0m Trial 386 finished with value: 0.6907536349524863 and parameters: {'n_estimators': 695, 'eta': 0.017264387785433224, 'max_depth': 12, 'alpha': 0.0162, 'lambda': 17.085057436691102, 'max_bin': 306}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:01:45,305]\u001b[0m Trial 387 finished with value: 0.7012560476717618 and parameters: {'n_estimators': 673, 'eta': 0.085263915159005, 'max_depth': 12, 'alpha': 0.0489, 'lambda': 18.090637065333677, 'max_bin': 295}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:01:51,512]\u001b[0m Trial 388 finished with value: 0.695142743731603 and parameters: {'n_estimators': 775, 'eta': 0.09359055120297216, 'max_depth': 12, 'alpha': 0.5424, 'lambda': 15.481870695449462, 'max_bin': 312}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:02:01,051]\u001b[0m Trial 389 finished with value: 0.6923520276034126 and parameters: {'n_estimators': 719, 'eta': 0.09054117725701477, 'max_depth': 12, 'alpha': 0.0004, 'lambda': 17.198051459277377, 'max_bin': 288}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:02:08,265]\u001b[0m Trial 390 finished with value: 0.693312909788281 and parameters: {'n_estimators': 682, 'eta': 0.08736211880342278, 'max_depth': 12, 'alpha': 0.0835, 'lambda': 16.373935096679933, 'max_bin': 299}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:02:17,581]\u001b[0m Trial 391 finished with value: 0.6908138517319362 and parameters: {'n_estimators': 704, 'eta': 0.0838337471500685, 'max_depth': 12, 'alpha': 0.06470000000000001, 'lambda': 19.503735217695706, 'max_bin': 292}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:02:26,388]\u001b[0m Trial 392 finished with value: 0.6921900533877007 and parameters: {'n_estimators': 640, 'eta': 0.0892572270765272, 'max_depth': 12, 'alpha': 0.0332, 'lambda': 15.56178678876864, 'max_bin': 304}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:02:35,069]\u001b[0m Trial 393 finished with value: 0.6994857376944121 and parameters: {'n_estimators': 740, 'eta': 0.08631774684037719, 'max_depth': 12, 'alpha': 0.0472, 'lambda': 18.235907031546887, 'max_bin': 284}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:02:43,196]\u001b[0m Trial 394 finished with value: 0.6965956471686271 and parameters: {'n_estimators': 660, 'eta': 0.08338037291463603, 'max_depth': 12, 'alpha': 0.018000000000000002, 'lambda': 16.750423413193236, 'max_bin': 297}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:02:50,929]\u001b[0m Trial 395 finished with value: 0.6949505388294117 and parameters: {'n_estimators': 723, 'eta': 0.09248488242597055, 'max_depth': 12, 'alpha': 0.08310000000000001, 'lambda': 17.346793401115455, 'max_bin': 308}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:02:57,244]\u001b[0m Trial 396 finished with value: 0.6899049374588423 and parameters: {'n_estimators': 757, 'eta': 0.08715156288568669, 'max_depth': 12, 'alpha': 0.06330000000000001, 'lambda': 16.00375224156705, 'max_bin': 301}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:03:05,119]\u001b[0m Trial 397 finished with value: 0.691282550693894 and parameters: {'n_estimators': 693, 'eta': 0.08942346124727267, 'max_depth': 12, 'alpha': 0.039400000000000004, 'lambda': 14.808397268915124, 'max_bin': 293}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:03:14,673]\u001b[0m Trial 398 finished with value: 0.6955509737262755 and parameters: {'n_estimators': 781, 'eta': 0.08517093339012852, 'max_depth': 12, 'alpha': 0.0195, 'lambda': 17.839223745129317, 'max_bin': 311}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:03:20,194]\u001b[0m Trial 399 finished with value: 0.6875678202175004 and parameters: {'n_estimators': 708, 'eta': 0.09132322585771945, 'max_depth': 12, 'alpha': 0.309, 'lambda': 16.508016426737207, 'max_bin': 303}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7197\n",
      "\tBest params:\n",
      "\t\tn_estimators: 773\n",
      "\t\teta: 0.086966054469444\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.0516\n",
      "\t\tlambda: 16.311475590735377\n",
      "\t\tmax_bin: 300\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.706611    0.686641    0.737877    0.695883   \n",
      "1                    TP   42.000000   44.000000   43.000000   36.000000   \n",
      "2                    TN  197.000000  198.000000  197.000000  194.000000   \n",
      "3                    FP    4.000000    3.000000    4.000000    6.000000   \n",
      "4                    FN   25.000000   23.000000   24.000000   32.000000   \n",
      "5              Accuracy    0.891791    0.902985    0.895522    0.858209   \n",
      "6             Precision    0.913043    0.936170    0.914894    0.857143   \n",
      "7           Sensitivity    0.626866    0.656716    0.641791    0.529412   \n",
      "8           Specificity    0.980100    0.985100    0.980100    0.970000   \n",
      "9              F1 score    0.743363    0.771930    0.754386    0.654545   \n",
      "10  F1 score (weighted)    0.884422    0.896774    0.888833    0.845779   \n",
      "11     F1 score (macro)    0.837402    0.855159    0.844018    0.782672   \n",
      "12    Balanced Accuracy    0.803483    0.820896    0.810945    0.749706   \n",
      "13                  MCC    0.697018    0.730776    0.708116    0.597791   \n",
      "14                  NPV    0.887400    0.895900    0.891400    0.858400   \n",
      "15              ROC_AUC    0.803483    0.820896    0.810945    0.749706   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.730519    0.701294    0.749091    0.714606  \n",
      "1    42.000000   39.000000   40.000000   38.000000  \n",
      "2   198.000000  199.000000  198.000000  197.000000  \n",
      "3     5.000000    2.000000    4.000000    6.000000  \n",
      "4    23.000000   28.000000   26.000000   27.000000  \n",
      "5     0.895522    0.888060    0.888060    0.876866  \n",
      "6     0.893617    0.951220    0.909091    0.863636  \n",
      "7     0.646154    0.582090    0.606061    0.584615  \n",
      "8     0.975400    0.990000    0.980200    0.970400  \n",
      "9     0.750000    0.722222    0.727273    0.697248  \n",
      "10    0.889345    0.877985    0.879756    0.868032  \n",
      "11    0.841981    0.826064    0.828425    0.809982  \n",
      "12    0.810762    0.786070    0.793129    0.777529  \n",
      "13    0.700514    0.688228    0.681846    0.642234  \n",
      "14    0.895900    0.876700    0.883900    0.879500  \n",
      "15    0.810762    0.786070    0.793129    0.777529  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=24,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_xgb_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_7_cat = np.where(((y_pred_xgb_7 >= 2) | (y_pred_xgb_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:03:29,495]\u001b[0m Trial 400 finished with value: 0.7191801837329177 and parameters: {'n_estimators': 675, 'eta': 0.08149112645604287, 'max_depth': 12, 'alpha': 0.7274, 'lambda': 18.959818421003185, 'max_bin': 297}. Best is trial 254 with value: 0.7196585078286221.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:03:37,879]\u001b[0m Trial 401 finished with value: 0.7238977530764508 and parameters: {'n_estimators': 669, 'eta': 0.08269674088356668, 'max_depth': 12, 'alpha': 0.1081, 'lambda': 19.13708068570362, 'max_bin': 290}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:03:46,314]\u001b[0m Trial 402 finished with value: 0.7227145362137045 and parameters: {'n_estimators': 646, 'eta': 0.08139165308152164, 'max_depth': 12, 'alpha': 0.8573000000000001, 'lambda': 20.71069974836426, 'max_bin': 284}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:03:53,289]\u001b[0m Trial 403 finished with value: 0.7190199541828078 and parameters: {'n_estimators': 394, 'eta': 0.08193264185775442, 'max_depth': 12, 'alpha': 0.9650000000000001, 'lambda': 20.546523950857317, 'max_bin': 286}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:04:01,577]\u001b[0m Trial 404 finished with value: 0.7218267225763969 and parameters: {'n_estimators': 647, 'eta': 0.08238725119259088, 'max_depth': 12, 'alpha': 0.9326000000000001, 'lambda': 20.930447413650715, 'max_bin': 286}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:04:08,876]\u001b[0m Trial 405 finished with value: 0.7177910726241657 and parameters: {'n_estimators': 394, 'eta': 0.08012832811955355, 'max_depth': 12, 'alpha': 0.9764, 'lambda': 22.47883166803758, 'max_bin': 281}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:04:16,808]\u001b[0m Trial 406 finished with value: 0.7163162174250703 and parameters: {'n_estimators': 646, 'eta': 0.07972860204151219, 'max_depth': 12, 'alpha': 0.9842000000000001, 'lambda': 24.046538087158606, 'max_bin': 283}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:04:23,597]\u001b[0m Trial 407 finished with value: 0.7215001591184393 and parameters: {'n_estimators': 403, 'eta': 0.08077828931418693, 'max_depth': 12, 'alpha': 0.9522, 'lambda': 22.161227549134058, 'max_bin': 283}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:04:30,169]\u001b[0m Trial 408 finished with value: 0.7179009757459076 and parameters: {'n_estimators': 398, 'eta': 0.08083114660175891, 'max_depth': 12, 'alpha': 0.9475, 'lambda': 22.566255098711824, 'max_bin': 278}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:04:36,834]\u001b[0m Trial 409 finished with value: 0.7172163059942533 and parameters: {'n_estimators': 379, 'eta': 0.08102157608007167, 'max_depth': 12, 'alpha': 0.9726, 'lambda': 22.835462757720425, 'max_bin': 282}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:04:43,574]\u001b[0m Trial 410 finished with value: 0.71904040526408 and parameters: {'n_estimators': 391, 'eta': 0.07934331315908338, 'max_depth': 12, 'alpha': 0.9574, 'lambda': 22.39201822537343, 'max_bin': 279}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:04:50,829]\u001b[0m Trial 411 finished with value: 0.7147394290064211 and parameters: {'n_estimators': 396, 'eta': 0.079236876071203, 'max_depth': 12, 'alpha': 0.9638000000000001, 'lambda': 23.647702323972652, 'max_bin': 280}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:04:56,868]\u001b[0m Trial 412 finished with value: 0.7169307192465393 and parameters: {'n_estimators': 397, 'eta': 0.08023461169901579, 'max_depth': 12, 'alpha': 0.9505, 'lambda': 21.850907859057777, 'max_bin': 280}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:05:03,560]\u001b[0m Trial 413 finished with value: 0.7180743850999501 and parameters: {'n_estimators': 378, 'eta': 0.08064138424609495, 'max_depth': 12, 'alpha': 0.9745, 'lambda': 21.461076886701267, 'max_bin': 280}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:05:10,664]\u001b[0m Trial 414 finished with value: 0.7228855312665997 and parameters: {'n_estimators': 426, 'eta': 0.08020002749439256, 'max_depth': 12, 'alpha': 0.9253, 'lambda': 20.9817477628125, 'max_bin': 284}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:05:18,083]\u001b[0m Trial 415 finished with value: 0.7196480759270469 and parameters: {'n_estimators': 357, 'eta': 0.08153804382532902, 'max_depth': 12, 'alpha': 0.9005000000000001, 'lambda': 21.29132023943564, 'max_bin': 279}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:05:26,093]\u001b[0m Trial 416 finished with value: 0.7164559308736027 and parameters: {'n_estimators': 430, 'eta': 0.08118586126816882, 'max_depth': 12, 'alpha': 0.9255, 'lambda': 22.05404246902853, 'max_bin': 279}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:05:32,335]\u001b[0m Trial 417 finished with value: 0.7185887422249699 and parameters: {'n_estimators': 357, 'eta': 0.08168289126857509, 'max_depth': 12, 'alpha': 0.9033, 'lambda': 21.08412833790998, 'max_bin': 277}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:05:39,268]\u001b[0m Trial 418 finished with value: 0.7201657385642001 and parameters: {'n_estimators': 354, 'eta': 0.08126576573531676, 'max_depth': 12, 'alpha': 0.9025000000000001, 'lambda': 21.09822598027794, 'max_bin': 285}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:05:46,780]\u001b[0m Trial 419 finished with value: 0.7177565610962032 and parameters: {'n_estimators': 377, 'eta': 0.08123338264247404, 'max_depth': 12, 'alpha': 0.9002, 'lambda': 21.056277897227694, 'max_bin': 283}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:05:53,378]\u001b[0m Trial 420 finished with value: 0.7212953325511864 and parameters: {'n_estimators': 324, 'eta': 0.07920758810424919, 'max_depth': 12, 'alpha': 0.8643000000000001, 'lambda': 21.130099426006023, 'max_bin': 277}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:06:00,359]\u001b[0m Trial 421 finished with value: 0.7199453632720105 and parameters: {'n_estimators': 343, 'eta': 0.07816541900743564, 'max_depth': 12, 'alpha': 0.8636, 'lambda': 21.419855103452424, 'max_bin': 284}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:06:07,281]\u001b[0m Trial 422 finished with value: 0.718452027675187 and parameters: {'n_estimators': 354, 'eta': 0.07717041679524063, 'max_depth': 12, 'alpha': 0.8638, 'lambda': 20.808982829219868, 'max_bin': 277}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:06:14,384]\u001b[0m Trial 423 finished with value: 0.7207192123644071 and parameters: {'n_estimators': 352, 'eta': 0.07807680917472817, 'max_depth': 12, 'alpha': 0.866, 'lambda': 20.71974527756412, 'max_bin': 276}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:06:21,223]\u001b[0m Trial 424 finished with value: 0.7217253174670735 and parameters: {'n_estimators': 318, 'eta': 0.07802028387332342, 'max_depth': 12, 'alpha': 0.8414, 'lambda': 21.367295815171012, 'max_bin': 275}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:06:27,353]\u001b[0m Trial 425 finished with value: 0.7205030312710363 and parameters: {'n_estimators': 319, 'eta': 0.07860681523886111, 'max_depth': 12, 'alpha': 0.8478, 'lambda': 21.357219023621145, 'max_bin': 275}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:06:34,057]\u001b[0m Trial 426 finished with value: 0.7185597481162244 and parameters: {'n_estimators': 339, 'eta': 0.07905266855615992, 'max_depth': 12, 'alpha': 0.8407, 'lambda': 21.547986693789042, 'max_bin': 284}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:06:40,736]\u001b[0m Trial 427 finished with value: 0.7202871575450618 and parameters: {'n_estimators': 315, 'eta': 0.0777306579368682, 'max_depth': 12, 'alpha': 0.8434, 'lambda': 20.623701984267562, 'max_bin': 276}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:06:47,760]\u001b[0m Trial 428 finished with value: 0.7185404842308822 and parameters: {'n_estimators': 307, 'eta': 0.0774952597084836, 'max_depth': 12, 'alpha': 0.8034, 'lambda': 20.443642211539863, 'max_bin': 275}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:06:54,648]\u001b[0m Trial 429 finished with value: 0.723531729965012 and parameters: {'n_estimators': 311, 'eta': 0.07777154760124211, 'max_depth': 12, 'alpha': 0.8804000000000001, 'lambda': 21.708823296851932, 'max_bin': 280}. Best is trial 401 with value: 0.7238977530764508.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:07:00,963]\u001b[0m Trial 430 finished with value: 0.724395502726619 and parameters: {'n_estimators': 321, 'eta': 0.07704702607196212, 'max_depth': 12, 'alpha': 0.8491000000000001, 'lambda': 21.620750417035236, 'max_bin': 274}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:07:08,470]\u001b[0m Trial 431 finished with value: 0.7200382707176753 and parameters: {'n_estimators': 312, 'eta': 0.07823728139162286, 'max_depth': 12, 'alpha': 0.8273, 'lambda': 21.867202148156085, 'max_bin': 274}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:07:15,081]\u001b[0m Trial 432 finished with value: 0.7205548086809735 and parameters: {'n_estimators': 315, 'eta': 0.07740329534202676, 'max_depth': 12, 'alpha': 0.8542000000000001, 'lambda': 21.896303834649178, 'max_bin': 273}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:07:21,530]\u001b[0m Trial 433 finished with value: 0.7150364294346583 and parameters: {'n_estimators': 299, 'eta': 0.07796144710762931, 'max_depth': 12, 'alpha': 0.8442000000000001, 'lambda': 23.200880329337842, 'max_bin': 273}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:07:27,886]\u001b[0m Trial 434 finished with value: 0.7213784980913335 and parameters: {'n_estimators': 323, 'eta': 0.07754562023343523, 'max_depth': 12, 'alpha': 0.8784000000000001, 'lambda': 21.972027067203445, 'max_bin': 273}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:07:34,245]\u001b[0m Trial 435 finished with value: 0.7178623807190195 and parameters: {'n_estimators': 330, 'eta': 0.07630028115145535, 'max_depth': 12, 'alpha': 0.8691000000000001, 'lambda': 21.738015321575, 'max_bin': 273}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:07:39,307]\u001b[0m Trial 436 finished with value: 0.7133840581601938 and parameters: {'n_estimators': 288, 'eta': 0.07492430947733941, 'max_depth': 7, 'alpha': 0.8884000000000001, 'lambda': 24.513762073213307, 'max_bin': 273}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:07:46,609]\u001b[0m Trial 437 finished with value: 0.7232691640926452 and parameters: {'n_estimators': 326, 'eta': 0.07759981480136481, 'max_depth': 12, 'alpha': 0.8540000000000001, 'lambda': 21.13548619845468, 'max_bin': 275}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:07:53,798]\u001b[0m Trial 438 finished with value: 0.7216395182475693 and parameters: {'n_estimators': 326, 'eta': 0.07823191174428977, 'max_depth': 12, 'alpha': 0.8534, 'lambda': 21.44737223435141, 'max_bin': 275}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:08:00,974]\u001b[0m Trial 439 finished with value: 0.7238286181429845 and parameters: {'n_estimators': 324, 'eta': 0.07745907831128346, 'max_depth': 12, 'alpha': 0.8533000000000001, 'lambda': 21.21842377565474, 'max_bin': 275}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:08:05,361]\u001b[0m Trial 440 finished with value: 0.7102437675942673 and parameters: {'n_estimators': 331, 'eta': 0.07746588613031519, 'max_depth': 6, 'alpha': 0.8549, 'lambda': 21.333841554019074, 'max_bin': 273}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:08:12,953]\u001b[0m Trial 441 finished with value: 0.7164594705985605 and parameters: {'n_estimators': 320, 'eta': 0.07528008501663312, 'max_depth': 12, 'alpha': 0.8734000000000001, 'lambda': 23.070707193221576, 'max_bin': 277}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:08:19,340]\u001b[0m Trial 442 finished with value: 0.7195237438626347 and parameters: {'n_estimators': 283, 'eta': 0.07715457294016946, 'max_depth': 12, 'alpha': 0.8156, 'lambda': 22.07141409457639, 'max_bin': 270}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:08:25,524]\u001b[0m Trial 443 finished with value: 0.7216370749897631 and parameters: {'n_estimators': 260, 'eta': 0.07759506260853101, 'max_depth': 12, 'alpha': 0.8191, 'lambda': 21.774522829556503, 'max_bin': 270}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:08:31,654]\u001b[0m Trial 444 finished with value: 0.7175450544643764 and parameters: {'n_estimators': 248, 'eta': 0.07727340391136299, 'max_depth': 12, 'alpha': 0.8169000000000001, 'lambda': 22.141588773765307, 'max_bin': 269}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:08:37,712]\u001b[0m Trial 445 finished with value: 0.7175649698049245 and parameters: {'n_estimators': 274, 'eta': 0.07333831077608749, 'max_depth': 12, 'alpha': 0.8027000000000001, 'lambda': 20.862130572188935, 'max_bin': 271}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:08:44,249]\u001b[0m Trial 446 finished with value: 0.7175288987158284 and parameters: {'n_estimators': 292, 'eta': 0.07626828056224111, 'max_depth': 12, 'alpha': 0.9188000000000001, 'lambda': 22.35290329765077, 'max_bin': 275}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:08:50,409]\u001b[0m Trial 447 finished with value: 0.721629174477823 and parameters: {'n_estimators': 310, 'eta': 0.07846034808983786, 'max_depth': 12, 'alpha': 0.8266, 'lambda': 21.660861316105237, 'max_bin': 269}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:08:57,236]\u001b[0m Trial 448 finished with value: 0.7207405843933514 and parameters: {'n_estimators': 311, 'eta': 0.07864496827185326, 'max_depth': 12, 'alpha': 0.8230000000000001, 'lambda': 21.703529782159887, 'max_bin': 269}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:09:04,150]\u001b[0m Trial 449 finished with value: 0.7210829488520202 and parameters: {'n_estimators': 313, 'eta': 0.07837334874061876, 'max_depth': 12, 'alpha': 0.8502000000000001, 'lambda': 21.309867301209568, 'max_bin': 276}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7244\n",
      "\tBest params:\n",
      "\t\tn_estimators: 321\n",
      "\t\teta: 0.07704702607196212\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.8491000000000001\n",
      "\t\tlambda: 21.620750417035236\n",
      "\t\tmax_bin: 274\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.706611    0.686641    0.737877    0.695883   \n",
      "1                    TP   42.000000   44.000000   43.000000   36.000000   \n",
      "2                    TN  197.000000  198.000000  197.000000  194.000000   \n",
      "3                    FP    4.000000    3.000000    4.000000    6.000000   \n",
      "4                    FN   25.000000   23.000000   24.000000   32.000000   \n",
      "5              Accuracy    0.891791    0.902985    0.895522    0.858209   \n",
      "6             Precision    0.913043    0.936170    0.914894    0.857143   \n",
      "7           Sensitivity    0.626866    0.656716    0.641791    0.529412   \n",
      "8           Specificity    0.980100    0.985100    0.980100    0.970000   \n",
      "9              F1 score    0.743363    0.771930    0.754386    0.654545   \n",
      "10  F1 score (weighted)    0.884422    0.896774    0.888833    0.845779   \n",
      "11     F1 score (macro)    0.837402    0.855159    0.844018    0.782672   \n",
      "12    Balanced Accuracy    0.803483    0.820896    0.810945    0.749706   \n",
      "13                  MCC    0.697018    0.730776    0.708116    0.597791   \n",
      "14                  NPV    0.887400    0.895900    0.891400    0.858400   \n",
      "15              ROC_AUC    0.803483    0.820896    0.810945    0.749706   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.730519    0.701294    0.749091    0.714606    0.592601  \n",
      "1    42.000000   39.000000   40.000000   38.000000   29.000000  \n",
      "2   198.000000  199.000000  198.000000  197.000000  198.000000  \n",
      "3     5.000000    2.000000    4.000000    6.000000    4.000000  \n",
      "4    23.000000   28.000000   26.000000   27.000000   37.000000  \n",
      "5     0.895522    0.888060    0.888060    0.876866    0.847015  \n",
      "6     0.893617    0.951220    0.909091    0.863636    0.878788  \n",
      "7     0.646154    0.582090    0.606061    0.584615    0.439394  \n",
      "8     0.975400    0.990000    0.980200    0.970400    0.980200  \n",
      "9     0.750000    0.722222    0.727273    0.697248    0.585859  \n",
      "10    0.889345    0.877985    0.879756    0.868032    0.827294  \n",
      "11    0.841981    0.826064    0.828425    0.809982    0.746019  \n",
      "12    0.810762    0.786070    0.793129    0.777529    0.709796  \n",
      "13    0.700514    0.688228    0.681846    0.642234    0.550154  \n",
      "14    0.895900    0.876700    0.883900    0.879500    0.842600  \n",
      "15    0.810762    0.786070    0.793129    0.777529    0.709796  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=24,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_xgb_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_8_cat = np.where(((y_pred_xgb_8 >= 2) | (y_pred_xgb_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:09:13,174]\u001b[0m Trial 450 finished with value: 0.6961748543447625 and parameters: {'n_estimators': 316, 'eta': 0.0785181897443499, 'max_depth': 12, 'alpha': 0.8494, 'lambda': 23.747361298202247, 'max_bin': 266}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:09:19,622]\u001b[0m Trial 451 finished with value: 0.6960194875850952 and parameters: {'n_estimators': 341, 'eta': 0.07532680496908319, 'max_depth': 12, 'alpha': 0.8280000000000001, 'lambda': 20.5425935499964, 'max_bin': 275}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:09:26,204]\u001b[0m Trial 452 finished with value: 0.6977964475625327 and parameters: {'n_estimators': 301, 'eta': 0.07863478808711077, 'max_depth': 12, 'alpha': 0.7863, 'lambda': 21.33789902307932, 'max_bin': 268}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:09:33,078]\u001b[0m Trial 453 finished with value: 0.6938165620175724 and parameters: {'n_estimators': 326, 'eta': 0.07367435620521298, 'max_depth': 12, 'alpha': 0.8583000000000001, 'lambda': 23.05836032406683, 'max_bin': 273}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:09:39,664]\u001b[0m Trial 454 finished with value: 0.6929393606417673 and parameters: {'n_estimators': 309, 'eta': 0.0785037417447112, 'max_depth': 12, 'alpha': 0.8793000000000001, 'lambda': 21.796659529048114, 'max_bin': 277}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:09:45,758]\u001b[0m Trial 455 finished with value: 0.6946634985142048 and parameters: {'n_estimators': 254, 'eta': 0.07547630121746686, 'max_depth': 12, 'alpha': 0.8362, 'lambda': 20.43463015392787, 'max_bin': 269}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:09:51,518]\u001b[0m Trial 456 finished with value: 0.6963879017689197 and parameters: {'n_estimators': 223, 'eta': 0.07749769842565338, 'max_depth': 12, 'alpha': 0.8677, 'lambda': 21.47843882845494, 'max_bin': 280}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:09:58,509]\u001b[0m Trial 457 finished with value: 0.6948511548534159 and parameters: {'n_estimators': 345, 'eta': 0.07873179862915741, 'max_depth': 12, 'alpha': 0.8332, 'lambda': 22.539480423562857, 'max_bin': 273}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:10:05,479]\u001b[0m Trial 458 finished with value: 0.6970822944859194 and parameters: {'n_estimators': 325, 'eta': 0.07610892744304158, 'max_depth': 12, 'alpha': 0.7895000000000001, 'lambda': 20.70929930800041, 'max_bin': 281}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:10:12,360]\u001b[0m Trial 459 finished with value: 0.69295029432811 and parameters: {'n_estimators': 305, 'eta': 0.07389855485989212, 'max_depth': 12, 'alpha': 0.889, 'lambda': 23.281842134054557, 'max_bin': 267}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:10:19,620]\u001b[0m Trial 460 finished with value: 0.6958705433770408 and parameters: {'n_estimators': 333, 'eta': 0.07861536431997776, 'max_depth': 12, 'alpha': 0.8532000000000001, 'lambda': 21.895313872138065, 'max_bin': 276}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:10:26,251]\u001b[0m Trial 461 finished with value: 0.6964749422459191 and parameters: {'n_estimators': 279, 'eta': 0.07128152166347697, 'max_depth': 12, 'alpha': 0.9182, 'lambda': 20.256794758831646, 'max_bin': 272}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:10:33,572]\u001b[0m Trial 462 finished with value: 0.6988595716852071 and parameters: {'n_estimators': 349, 'eta': 0.07934800859110788, 'max_depth': 12, 'alpha': 0.8208000000000001, 'lambda': 21.177733941439786, 'max_bin': 281}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:10:40,314]\u001b[0m Trial 463 finished with value: 0.6948098702709989 and parameters: {'n_estimators': 313, 'eta': 0.07680966982821069, 'max_depth': 12, 'alpha': 0.8493, 'lambda': 22.660589997065408, 'max_bin': 277}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:10:46,313]\u001b[0m Trial 464 finished with value: 0.6951412159446215 and parameters: {'n_estimators': 265, 'eta': 0.07951188975615747, 'max_depth': 12, 'alpha': 0.8767, 'lambda': 20.16720241459383, 'max_bin': 267}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:10:53,429]\u001b[0m Trial 465 finished with value: 0.69017801010362 and parameters: {'n_estimators': 488, 'eta': 0.07500864060752066, 'max_depth': 12, 'alpha': 0.8955000000000001, 'lambda': 25.826872657388606, 'max_bin': 283}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:11:00,368]\u001b[0m Trial 466 finished with value: 0.6945839667094831 and parameters: {'n_estimators': 290, 'eta': 0.07690712990611008, 'max_depth': 12, 'alpha': 0.9293, 'lambda': 21.775351210202693, 'max_bin': 271}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:11:07,446]\u001b[0m Trial 467 finished with value: 0.6991438807794218 and parameters: {'n_estimators': 365, 'eta': 0.07959552572207362, 'max_depth': 12, 'alpha': 0.8619, 'lambda': 20.887915333322304, 'max_bin': 275}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:11:14,028]\u001b[0m Trial 468 finished with value: 0.695687215747834 and parameters: {'n_estimators': 340, 'eta': 0.07305852674206288, 'max_depth': 12, 'alpha': 0.8405, 'lambda': 22.30857852728655, 'max_bin': 264}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:11:21,151]\u001b[0m Trial 469 finished with value: 0.6975288990522233 and parameters: {'n_estimators': 451, 'eta': 0.07822821226584126, 'max_depth': 12, 'alpha': 0.8159000000000001, 'lambda': 21.248655622668362, 'max_bin': 280}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:11:27,548]\u001b[0m Trial 470 finished with value: 0.6944056638080965 and parameters: {'n_estimators': 319, 'eta': 0.07539953136286902, 'max_depth': 12, 'alpha': 0.874, 'lambda': 23.4277293225279, 'max_bin': 270}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:11:34,839]\u001b[0m Trial 471 finished with value: 0.6938470181442147 and parameters: {'n_estimators': 419, 'eta': 0.07984790066159005, 'max_depth': 12, 'alpha': 0.9078, 'lambda': 20.126300861024244, 'max_bin': 276}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:11:40,940]\u001b[0m Trial 472 finished with value: 0.6953690347217345 and parameters: {'n_estimators': 300, 'eta': 0.07767672082622118, 'max_depth': 12, 'alpha': 0.7956000000000001, 'lambda': 21.87929526507501, 'max_bin': 284}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:11:47,495]\u001b[0m Trial 473 finished with value: 0.6930915400294693 and parameters: {'n_estimators': 330, 'eta': 0.07675415925572829, 'max_depth': 12, 'alpha': 0.7674000000000001, 'lambda': 24.095050482574855, 'max_bin': 277}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:11:54,145]\u001b[0m Trial 474 finished with value: 0.6957651857382712 and parameters: {'n_estimators': 361, 'eta': 0.07971340440443006, 'max_depth': 12, 'alpha': 0.8360000000000001, 'lambda': 20.853423860351977, 'max_bin': 271}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:12:00,775]\u001b[0m Trial 475 finished with value: 0.6986490403871034 and parameters: {'n_estimators': 312, 'eta': 0.07430826927968547, 'max_depth': 12, 'alpha': 0.8675, 'lambda': 22.696229040605964, 'max_bin': 283}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:12:08,094]\u001b[0m Trial 476 finished with value: 0.6957924725901383 and parameters: {'n_estimators': 344, 'eta': 0.07976380193066916, 'max_depth': 12, 'alpha': 0.8886000000000001, 'lambda': 21.541678685479535, 'max_bin': 267}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:12:14,796]\u001b[0m Trial 477 finished with value: 0.6934979752517051 and parameters: {'n_estimators': 297, 'eta': 0.07210960989779469, 'max_depth': 12, 'alpha': 0.8549, 'lambda': 20.060494719717617, 'max_bin': 280}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:12:21,363]\u001b[0m Trial 478 finished with value: 0.6968371831929648 and parameters: {'n_estimators': 328, 'eta': 0.07716963529343585, 'max_depth': 12, 'alpha': 0.8297, 'lambda': 20.90710857952067, 'max_bin': 274}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:12:28,165]\u001b[0m Trial 479 finished with value: 0.6992684411794405 and parameters: {'n_estimators': 274, 'eta': 0.07554152288813487, 'max_depth': 12, 'alpha': 0.8084, 'lambda': 22.22543316126508, 'max_bin': 264}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:12:35,130]\u001b[0m Trial 480 finished with value: 0.6980347285138364 and parameters: {'n_estimators': 312, 'eta': 0.08000566574736685, 'max_depth': 12, 'alpha': 0.9102, 'lambda': 22.995675981432424, 'max_bin': 284}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:12:42,007]\u001b[0m Trial 481 finished with value: 0.6952955272332151 and parameters: {'n_estimators': 369, 'eta': 0.07869803391121252, 'max_depth': 12, 'alpha': 0.8532000000000001, 'lambda': 21.443611624606778, 'max_bin': 276}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:12:48,761]\u001b[0m Trial 482 finished with value: 0.6936296773843459 and parameters: {'n_estimators': 342, 'eta': 0.08089452881608777, 'max_depth': 12, 'alpha': 0.8819, 'lambda': 20.201928903995125, 'max_bin': 271}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:12:55,025]\u001b[0m Trial 483 finished with value: 0.6929211924363645 and parameters: {'n_estimators': 287, 'eta': 0.07736925396950504, 'max_depth': 12, 'alpha': 0.8305, 'lambda': 22.01267340858684, 'max_bin': 280}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:13:01,524]\u001b[0m Trial 484 finished with value: 0.6938637373330633 and parameters: {'n_estimators': 319, 'eta': 0.07495702199718293, 'max_depth': 12, 'alpha': 0.8494, 'lambda': 20.890273074154052, 'max_bin': 263}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:13:07,489]\u001b[0m Trial 485 finished with value: 0.6946697021229504 and parameters: {'n_estimators': 353, 'eta': 0.07818013570225689, 'max_depth': 12, 'alpha': 0.8701000000000001, 'lambda': 22.416816979608967, 'max_bin': 354}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:13:14,368]\u001b[0m Trial 486 finished with value: 0.6956097374417743 and parameters: {'n_estimators': 304, 'eta': 0.08113022013227904, 'max_depth': 12, 'alpha': 0.8956000000000001, 'lambda': 21.537672106827536, 'max_bin': 283}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:13:20,694]\u001b[0m Trial 487 finished with value: 0.6930946632368695 and parameters: {'n_estimators': 332, 'eta': 0.07657507553470169, 'max_depth': 12, 'alpha': 0.9337000000000001, 'lambda': 24.861508678103878, 'max_bin': 271}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:13:26,860]\u001b[0m Trial 488 finished with value: 0.6923057353013827 and parameters: {'n_estimators': 317, 'eta': 0.07396355957468459, 'max_depth': 12, 'alpha': 0.8214, 'lambda': 19.961385747292113, 'max_bin': 277}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:13:32,892]\u001b[0m Trial 489 finished with value: 0.69434493313052 and parameters: {'n_estimators': 296, 'eta': 0.08031006370256552, 'max_depth': 12, 'alpha': 0.8478, 'lambda': 20.73061070915995, 'max_bin': 285}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:13:39,154]\u001b[0m Trial 490 finished with value: 0.695262587930419 and parameters: {'n_estimators': 340, 'eta': 0.07812216509125255, 'max_depth': 12, 'alpha': 0.916, 'lambda': 22.94931529161527, 'max_bin': 267}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:13:45,043]\u001b[0m Trial 491 finished with value: 0.6953443754321733 and parameters: {'n_estimators': 238, 'eta': 0.07000025879308654, 'max_depth': 12, 'alpha': 0.8761, 'lambda': 21.602818532388543, 'max_bin': 275}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:13:51,249]\u001b[0m Trial 492 finished with value: 0.6981415466514788 and parameters: {'n_estimators': 261, 'eta': 0.0761460102180293, 'max_depth': 12, 'alpha': 0.7966000000000001, 'lambda': 20.680155537055025, 'max_bin': 280}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:13:58,129]\u001b[0m Trial 493 finished with value: 0.6954482250469464 and parameters: {'n_estimators': 350, 'eta': 0.0814649668167706, 'max_depth': 12, 'alpha': 0.8371000000000001, 'lambda': 23.535274563262725, 'max_bin': 272}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:14:05,555]\u001b[0m Trial 494 finished with value: 0.6970900690778823 and parameters: {'n_estimators': 373, 'eta': 0.07910382647316977, 'max_depth': 12, 'alpha': 0.8584, 'lambda': 22.173333679415734, 'max_bin': 284}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:14:11,833]\u001b[0m Trial 495 finished with value: 0.6918099886415787 and parameters: {'n_estimators': 285, 'eta': 0.07377086369186431, 'max_depth': 12, 'alpha': 0.937, 'lambda': 19.84628662939229, 'max_bin': 269}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:14:18,285]\u001b[0m Trial 496 finished with value: 0.6976236085706822 and parameters: {'n_estimators': 324, 'eta': 0.0778370174487661, 'max_depth': 11, 'alpha': 0.8143, 'lambda': 21.256774034132164, 'max_bin': 277}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:14:26,333]\u001b[0m Trial 497 finished with value: 0.693857158772755 and parameters: {'n_estimators': 453, 'eta': 0.08035214285806162, 'max_depth': 12, 'alpha': 0.8863000000000001, 'lambda': 22.4924730168583, 'max_bin': 264}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:14:33,006]\u001b[0m Trial 498 finished with value: 0.6976938788514595 and parameters: {'n_estimators': 305, 'eta': 0.07595318131345588, 'max_depth': 12, 'alpha': 0.7483000000000001, 'lambda': 20.902559966718997, 'max_bin': 285}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:14:38,891]\u001b[0m Trial 499 finished with value: 0.6936184484072607 and parameters: {'n_estimators': 327, 'eta': 0.07212343478802936, 'max_depth': 12, 'alpha': 0.9066000000000001, 'lambda': 20.01129511720961, 'max_bin': 274}. Best is trial 430 with value: 0.724395502726619.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7244\n",
      "\tBest params:\n",
      "\t\tn_estimators: 321\n",
      "\t\teta: 0.07704702607196212\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.8491000000000001\n",
      "\t\tlambda: 21.620750417035236\n",
      "\t\tmax_bin: 274\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.706611    0.686641    0.737877    0.695883   \n",
      "1                    TP   42.000000   44.000000   43.000000   36.000000   \n",
      "2                    TN  197.000000  198.000000  197.000000  194.000000   \n",
      "3                    FP    4.000000    3.000000    4.000000    6.000000   \n",
      "4                    FN   25.000000   23.000000   24.000000   32.000000   \n",
      "5              Accuracy    0.891791    0.902985    0.895522    0.858209   \n",
      "6             Precision    0.913043    0.936170    0.914894    0.857143   \n",
      "7           Sensitivity    0.626866    0.656716    0.641791    0.529412   \n",
      "8           Specificity    0.980100    0.985100    0.980100    0.970000   \n",
      "9              F1 score    0.743363    0.771930    0.754386    0.654545   \n",
      "10  F1 score (weighted)    0.884422    0.896774    0.888833    0.845779   \n",
      "11     F1 score (macro)    0.837402    0.855159    0.844018    0.782672   \n",
      "12    Balanced Accuracy    0.803483    0.820896    0.810945    0.749706   \n",
      "13                  MCC    0.697018    0.730776    0.708116    0.597791   \n",
      "14                  NPV    0.887400    0.895900    0.891400    0.858400   \n",
      "15              ROC_AUC    0.803483    0.820896    0.810945    0.749706   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.730519    0.701294    0.749091    0.714606    0.592601    0.765842  \n",
      "1    42.000000   39.000000   40.000000   38.000000   29.000000   37.000000  \n",
      "2   198.000000  199.000000  198.000000  197.000000  198.000000  200.000000  \n",
      "3     5.000000    2.000000    4.000000    6.000000    4.000000    2.000000  \n",
      "4    23.000000   28.000000   26.000000   27.000000   37.000000   29.000000  \n",
      "5     0.895522    0.888060    0.888060    0.876866    0.847015    0.884328  \n",
      "6     0.893617    0.951220    0.909091    0.863636    0.878788    0.948718  \n",
      "7     0.646154    0.582090    0.606061    0.584615    0.439394    0.560606  \n",
      "8     0.975400    0.990000    0.980200    0.970400    0.980200    0.990100  \n",
      "9     0.750000    0.722222    0.727273    0.697248    0.585859    0.704762  \n",
      "10    0.889345    0.877985    0.879756    0.868032    0.827294    0.873079  \n",
      "11    0.841981    0.826064    0.828425    0.809982    0.746019    0.816418  \n",
      "12    0.810762    0.786070    0.793129    0.777529    0.709796    0.775353  \n",
      "13    0.700514    0.688228    0.681846    0.642234    0.550154    0.672848  \n",
      "14    0.895900    0.876700    0.883900    0.879500    0.842600    0.873400  \n",
      "15    0.810762    0.786070    0.793129    0.777529    0.709796    0.775353  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=24,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_xgb_9)\n",
    "# now convert the resuls to binary with cutoff 6.9\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_9_cat = np.where(((y_pred_xgb_9 >= 2) | (y_pred_xgb_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.726160</td>\n",
       "      <td>0.060424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>3.431877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>98.600000</td>\n",
       "      <td>1.074968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.994429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>13.400000</td>\n",
       "      <td>3.502380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.885748</td>\n",
       "      <td>0.031036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.909763</td>\n",
       "      <td>0.050908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.599047</td>\n",
       "      <td>0.103768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.981100</td>\n",
       "      <td>0.009911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.719440</td>\n",
       "      <td>0.089156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.876117</td>\n",
       "      <td>0.036256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.823822</td>\n",
       "      <td>0.053831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.790073</td>\n",
       "      <td>0.054987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.676354</td>\n",
       "      <td>0.094685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.881000</td>\n",
       "      <td>0.028199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.790073</td>\n",
       "      <td>0.054987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.726160     0.060424\n",
       "1                    TP        20.000000     3.431877\n",
       "2                    TN        98.600000     1.074968\n",
       "3                    FP         1.900000     0.994429\n",
       "4                    FN        13.400000     3.502380\n",
       "5              Accuracy         0.885748     0.031036\n",
       "6             Precision         0.909763     0.050908\n",
       "7           Sensitivity         0.599047     0.103768\n",
       "8           Specificity         0.981100     0.009911\n",
       "9              F1 score         0.719440     0.089156\n",
       "10  F1 score (weighted)         0.876117     0.036256\n",
       "11     F1 score (macro)         0.823822     0.053831\n",
       "12    Balanced Accuracy         0.790073     0.054987\n",
       "13                  MCC         0.676354     0.094685\n",
       "14                  NPV         0.881000     0.028199\n",
       "15              ROC_AUC         0.790073     0.054987"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.706611</td>\n",
       "      <td>0.686641</td>\n",
       "      <td>0.737877</td>\n",
       "      <td>0.695883</td>\n",
       "      <td>0.730519</td>\n",
       "      <td>0.701294</td>\n",
       "      <td>0.749091</td>\n",
       "      <td>0.714606</td>\n",
       "      <td>0.592601</td>\n",
       "      <td>0.765842</td>\n",
       "      <td>0.708097</td>\n",
       "      <td>0.047661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>4.396969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>197.600000</td>\n",
       "      <td>1.577621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>4.402020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.847015</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.882836</td>\n",
       "      <td>0.017607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.906632</td>\n",
       "      <td>0.033316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>0.587370</td>\n",
       "      <td>0.065949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.980100</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.980100</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.975400</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>0.970400</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>0.990100</td>\n",
       "      <td>0.980160</td>\n",
       "      <td>0.007001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.743363</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.697248</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.711159</td>\n",
       "      <td>0.055407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.884422</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.888833</td>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.889345</td>\n",
       "      <td>0.877985</td>\n",
       "      <td>0.879756</td>\n",
       "      <td>0.868032</td>\n",
       "      <td>0.827294</td>\n",
       "      <td>0.873079</td>\n",
       "      <td>0.873130</td>\n",
       "      <td>0.021444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.837402</td>\n",
       "      <td>0.855159</td>\n",
       "      <td>0.844018</td>\n",
       "      <td>0.782672</td>\n",
       "      <td>0.841981</td>\n",
       "      <td>0.826064</td>\n",
       "      <td>0.828425</td>\n",
       "      <td>0.809982</td>\n",
       "      <td>0.746019</td>\n",
       "      <td>0.816418</td>\n",
       "      <td>0.818814</td>\n",
       "      <td>0.032783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.803483</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.810945</td>\n",
       "      <td>0.749706</td>\n",
       "      <td>0.810762</td>\n",
       "      <td>0.786070</td>\n",
       "      <td>0.793129</td>\n",
       "      <td>0.777529</td>\n",
       "      <td>0.709796</td>\n",
       "      <td>0.775353</td>\n",
       "      <td>0.783767</td>\n",
       "      <td>0.033448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.697018</td>\n",
       "      <td>0.730776</td>\n",
       "      <td>0.708116</td>\n",
       "      <td>0.597791</td>\n",
       "      <td>0.700514</td>\n",
       "      <td>0.688228</td>\n",
       "      <td>0.681846</td>\n",
       "      <td>0.642234</td>\n",
       "      <td>0.550154</td>\n",
       "      <td>0.672848</td>\n",
       "      <td>0.666952</td>\n",
       "      <td>0.055321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.887400</td>\n",
       "      <td>0.895900</td>\n",
       "      <td>0.891400</td>\n",
       "      <td>0.858400</td>\n",
       "      <td>0.895900</td>\n",
       "      <td>0.876700</td>\n",
       "      <td>0.883900</td>\n",
       "      <td>0.879500</td>\n",
       "      <td>0.842600</td>\n",
       "      <td>0.873400</td>\n",
       "      <td>0.878510</td>\n",
       "      <td>0.017008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.803483</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.810945</td>\n",
       "      <td>0.749706</td>\n",
       "      <td>0.810762</td>\n",
       "      <td>0.786070</td>\n",
       "      <td>0.793129</td>\n",
       "      <td>0.777529</td>\n",
       "      <td>0.709796</td>\n",
       "      <td>0.775353</td>\n",
       "      <td>0.783767</td>\n",
       "      <td>0.033448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.706611    0.686641    0.737877    0.695883   \n",
       "1                    TP   42.000000   44.000000   43.000000   36.000000   \n",
       "2                    TN  197.000000  198.000000  197.000000  194.000000   \n",
       "3                    FP    4.000000    3.000000    4.000000    6.000000   \n",
       "4                    FN   25.000000   23.000000   24.000000   32.000000   \n",
       "5              Accuracy    0.891791    0.902985    0.895522    0.858209   \n",
       "6             Precision    0.913043    0.936170    0.914894    0.857143   \n",
       "7           Sensitivity    0.626866    0.656716    0.641791    0.529412   \n",
       "8           Specificity    0.980100    0.985100    0.980100    0.970000   \n",
       "9              F1 score    0.743363    0.771930    0.754386    0.654545   \n",
       "10  F1 score (weighted)    0.884422    0.896774    0.888833    0.845779   \n",
       "11     F1 score (macro)    0.837402    0.855159    0.844018    0.782672   \n",
       "12    Balanced Accuracy    0.803483    0.820896    0.810945    0.749706   \n",
       "13                  MCC    0.697018    0.730776    0.708116    0.597791   \n",
       "14                  NPV    0.887400    0.895900    0.891400    0.858400   \n",
       "15              ROC_AUC    0.803483    0.820896    0.810945    0.749706   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.730519    0.701294    0.749091    0.714606    0.592601    0.765842   \n",
       "1    42.000000   39.000000   40.000000   38.000000   29.000000   37.000000   \n",
       "2   198.000000  199.000000  198.000000  197.000000  198.000000  200.000000   \n",
       "3     5.000000    2.000000    4.000000    6.000000    4.000000    2.000000   \n",
       "4    23.000000   28.000000   26.000000   27.000000   37.000000   29.000000   \n",
       "5     0.895522    0.888060    0.888060    0.876866    0.847015    0.884328   \n",
       "6     0.893617    0.951220    0.909091    0.863636    0.878788    0.948718   \n",
       "7     0.646154    0.582090    0.606061    0.584615    0.439394    0.560606   \n",
       "8     0.975400    0.990000    0.980200    0.970400    0.980200    0.990100   \n",
       "9     0.750000    0.722222    0.727273    0.697248    0.585859    0.704762   \n",
       "10    0.889345    0.877985    0.879756    0.868032    0.827294    0.873079   \n",
       "11    0.841981    0.826064    0.828425    0.809982    0.746019    0.816418   \n",
       "12    0.810762    0.786070    0.793129    0.777529    0.709796    0.775353   \n",
       "13    0.700514    0.688228    0.681846    0.642234    0.550154    0.672848   \n",
       "14    0.895900    0.876700    0.883900    0.879500    0.842600    0.873400   \n",
       "15    0.810762    0.786070    0.793129    0.777529    0.709796    0.775353   \n",
       "\n",
       "           ave       std  \n",
       "0     0.708097  0.047661  \n",
       "1    39.000000  4.396969  \n",
       "2   197.600000  1.577621  \n",
       "3     4.000000  1.414214  \n",
       "4    27.400000  4.402020  \n",
       "5     0.882836  0.017607  \n",
       "6     0.906632  0.033316  \n",
       "7     0.587370  0.065949  \n",
       "8     0.980160  0.007001  \n",
       "9     0.711159  0.055407  \n",
       "10    0.873130  0.021444  \n",
       "11    0.818814  0.032783  \n",
       "12    0.783767  0.033448  \n",
       "13    0.666952  0.055321  \n",
       "14    0.878510  0.017008  \n",
       "15    0.783767  0.033448  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test['std'] = mat_met_xgb_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_xgb0</th>\n",
       "      <th>y_pred_xgb1</th>\n",
       "      <th>y_pred_xgb2</th>\n",
       "      <th>y_pred_xgb3</th>\n",
       "      <th>y_pred_xgb4</th>\n",
       "      <th>y_pred_xgb_ave</th>\n",
       "      <th>y_pred_xgb_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4635479</td>\n",
       "      <td>0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.864195</td>\n",
       "      <td>0.759444</td>\n",
       "      <td>0.731960</td>\n",
       "      <td>0.782119</td>\n",
       "      <td>0.168248</td>\n",
       "      <td>0.620994</td>\n",
       "      <td>0.245617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4299417</td>\n",
       "      <td>1</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.532398</td>\n",
       "      <td>0.604213</td>\n",
       "      <td>0.741320</td>\n",
       "      <td>0.672058</td>\n",
       "      <td>0.713656</td>\n",
       "      <td>0.707274</td>\n",
       "      <td>0.140251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4225331</td>\n",
       "      <td>2</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.634318</td>\n",
       "      <td>0.885098</td>\n",
       "      <td>0.848559</td>\n",
       "      <td>1.051753</td>\n",
       "      <td>1.086280</td>\n",
       "      <td>0.891001</td>\n",
       "      <td>0.149610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL1094710</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.194353</td>\n",
       "      <td>0.080873</td>\n",
       "      <td>0.254829</td>\n",
       "      <td>-0.013314</td>\n",
       "      <td>-0.064724</td>\n",
       "      <td>0.150336</td>\n",
       "      <td>0.173451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3287256</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.133026</td>\n",
       "      <td>-0.133698</td>\n",
       "      <td>-0.055169</td>\n",
       "      <td>-0.098350</td>\n",
       "      <td>-0.086947</td>\n",
       "      <td>-0.139532</td>\n",
       "      <td>0.089380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>CHEMBL3769491</td>\n",
       "      <td>1334</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.448810</td>\n",
       "      <td>-0.019706</td>\n",
       "      <td>-0.428303</td>\n",
       "      <td>-0.220375</td>\n",
       "      <td>-0.302046</td>\n",
       "      <td>-0.121540</td>\n",
       "      <td>0.389984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>CHEMBL482095</td>\n",
       "      <td>1335</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.126903</td>\n",
       "      <td>1.043857</td>\n",
       "      <td>1.231881</td>\n",
       "      <td>0.937111</td>\n",
       "      <td>0.929925</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.251732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>CHEMBL4095596</td>\n",
       "      <td>1336</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.007316</td>\n",
       "      <td>1.086926</td>\n",
       "      <td>0.893390</td>\n",
       "      <td>1.282278</td>\n",
       "      <td>1.100579</td>\n",
       "      <td>1.025082</td>\n",
       "      <td>0.159895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>CHEMBL4072925</td>\n",
       "      <td>1337</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.512044</td>\n",
       "      <td>0.592815</td>\n",
       "      <td>0.723611</td>\n",
       "      <td>0.433262</td>\n",
       "      <td>0.415881</td>\n",
       "      <td>0.549602</td>\n",
       "      <td>0.107958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>CHEMBL3774993</td>\n",
       "      <td>1338</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.714461</td>\n",
       "      <td>0.613323</td>\n",
       "      <td>0.713033</td>\n",
       "      <td>0.704026</td>\n",
       "      <td>0.630381</td>\n",
       "      <td>0.724204</td>\n",
       "      <td>0.117004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1339 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_xgb0  y_pred_xgb1  \\\n",
       "0         CHEMBL4635479            0     0.42     0.864195     0.759444   \n",
       "1         CHEMBL4299417            1     0.98     0.532398     0.604213   \n",
       "2         CHEMBL4225331            2     0.84     0.634318     0.885098   \n",
       "3         CHEMBL1094710            3     0.45     0.194353     0.080873   \n",
       "4         CHEMBL3287256            4    -0.33    -0.133026    -0.133698   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "1334      CHEMBL3769491         1334     0.69    -0.448810    -0.019706   \n",
       "1335       CHEMBL482095         1335     0.44     1.126903     1.043857   \n",
       "1336      CHEMBL4095596         1336     0.78     1.007316     1.086926   \n",
       "1337      CHEMBL4072925         1337     0.62     0.512044     0.592815   \n",
       "1338      CHEMBL3774993         1338     0.97     0.714461     0.613323   \n",
       "\n",
       "      y_pred_xgb2  y_pred_xgb3  y_pred_xgb4  y_pred_xgb_ave  y_pred_xgb_std  \n",
       "0        0.731960     0.782119     0.168248        0.620994        0.245617  \n",
       "1        0.741320     0.672058     0.713656        0.707274        0.140251  \n",
       "2        0.848559     1.051753     1.086280        0.891001        0.149610  \n",
       "3        0.254829    -0.013314    -0.064724        0.150336        0.173451  \n",
       "4       -0.055169    -0.098350    -0.086947       -0.139532        0.089380  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "1334    -0.428303    -0.220375    -0.302046       -0.121540        0.389984  \n",
       "1335     1.231881     0.937111     0.929925        0.951613        0.251732  \n",
       "1336     0.893390     1.282278     1.100579        1.025082        0.159895  \n",
       "1337     0.723611     0.433262     0.415881        0.549602        0.107958  \n",
       "1338     0.713033     0.704026     0.630381        0.724204        0.117004  \n",
       "\n",
       "[1339 rows x 10 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=24,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"rmse\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_xgb_cat = np.where(((y_pred_optimized_xgb >= 2) | (y_pred_optimized_xgb <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_xgb_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_xgb))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "xgb_run0 = data_xgb[['y_test_idx0', 'y_test0', 'y_pred_xgb0']]\n",
    "xgb_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "xgb_run0.reset_index(inplace=True, drop=True)\n",
    "xgb_run1 = data_xgb[['y_test_idx1', 'y_test1', 'y_pred_xgb1']]\n",
    "xgb_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "xgb_run1.reset_index(inplace=True, drop=True)\n",
    "xgb_run2 = data_xgb[['y_test_idx2', 'y_test2', 'y_pred_xgb2']]\n",
    "xgb_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "xgb_run2.reset_index(inplace=True, drop=True)\n",
    "xgb_run3 = data_xgb[['y_test_idx3', 'y_test3', 'y_pred_xgb3']]\n",
    "xgb_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "xgb_run3.reset_index(inplace=True, drop=True)\n",
    "xgb_run4 = data_xgb[['y_test_idx4', 'y_test4', 'y_pred_xgb4']]\n",
    "xgb_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "xgb_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "xgb_5preds = pd.concat([chembl_id, xgb_run0, xgb_run1, xgb_run2, xgb_run3, xgb_run4], axis=1)\n",
    "xgb_5preds = xgb_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_xgb0', 'y_pred_xgb1', 'y_pred_xgb2', 'y_pred_xgb3', 'y_pred_xgb4']]\n",
    "xgb_5preds['y_pred_xgb_ave'] = xgb_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "xgb_5preds['y_pred_xgb_std'] = xgb_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "xgb_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c2883ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_xgb.to_csv(output/'mat_met_xgb_opt.csv')\n",
    "xgb_5preds.to_csv(output/'xgb_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "02aaad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b19aca7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt  \n",
    "# retrieve performance metrics\n",
    "results = optimized_xgb_0.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "    \n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "pyplot.show()\n",
    "\n",
    " # plot classification error\n",
    "#fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "#ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "#ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "#ax.legend()\n",
    "    \n",
    "#pyplot.ylabel('Classification Error')\n",
    "#pyplot.title('XGBoost Classification Error')\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost baseline model r2_score 0.6657 with a standard deviation of 0.0603\n",
      "XGBoost optimized model r2_score 0.7079 with a standard deviation of 0.0457\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_reg, X, Y, cv=10, scoring=\"r2\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_xgb_noSemiSel.joblib']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_reg, output/\"xgb_reg.joblib\")\n",
    "#joblib.dump(optimized_xgb, output/\"optimized_xgb.joblib\")\n",
    "joblib.dump(optimizedCV_xgb, output/\"optimizedCV_xgb.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.684852     0.058986\n",
      "1                    TP        21.500000     2.953341\n",
      "2                    TN        98.300000     1.702939\n",
      "3                    FP         2.200000     1.549193\n",
      "4                    FN        11.900000     3.247221\n",
      "5              Accuracy         0.894720     0.030496\n",
      "6             Precision         0.907275     0.067761\n",
      "7           Sensitivity         0.644508     0.093455\n",
      "8           Specificity         0.978100     0.015435\n",
      "9              F1 score         0.751137     0.078640\n",
      "10  F1 score (weighted)         0.887661     0.034191\n",
      "11     F1 score (macro)         0.842165     0.048677\n",
      "12    Balanced Accuracy         0.811303     0.050116\n",
      "13                  MCC         0.704934     0.090514\n",
      "14                  NPV         0.892520     0.027160\n",
      "15              ROC_AUC         0.811303     0.050116\n",
      "CPU times: user 2.1 s, sys: 0 ns, total: 2.1 s\n",
      "Wall time: 2.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    \n",
    "    knn_reg.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=24)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=24)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:21:52,855]\u001b[0m A new study created in memory with name: KNNregressor\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:21:53,281]\u001b[0m Trial 0 finished with value: 0.6235008340843227 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 81}. Best is trial 0 with value: 0.6235008340843227.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:21:53,794]\u001b[0m Trial 1 finished with value: 0.5383815255328213 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 0 with value: 0.6235008340843227.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:21:54,187]\u001b[0m Trial 2 finished with value: 0.6481400708932503 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 76}. Best is trial 2 with value: 0.6481400708932503.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:21:54,702]\u001b[0m Trial 3 finished with value: 0.5659416708561007 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 2 with value: 0.6481400708932503.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:21:55,105]\u001b[0m Trial 4 finished with value: 0.6481400708932503 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 73}. Best is trial 2 with value: 0.6481400708932503.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:21:55,522]\u001b[0m Trial 5 finished with value: 0.6481400708932503 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 64}. Best is trial 2 with value: 0.6481400708932503.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:21:56,023]\u001b[0m Trial 6 finished with value: 0.5659416708561007 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 2 with value: 0.6481400708932503.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:21:56,438]\u001b[0m Trial 7 finished with value: 0.5398835256107873 and parameters: {'n_neighbors': 18, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 2 with value: 0.6481400708932503.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:21:56,853]\u001b[0m Trial 8 finished with value: 0.5938784683865613 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 97}. Best is trial 2 with value: 0.6481400708932503.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:21:57,337]\u001b[0m Trial 9 finished with value: 0.604507987116049 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 75}. Best is trial 2 with value: 0.6481400708932503.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:21:57,933]\u001b[0m Trial 10 finished with value: 0.5163242724587979 and parameters: {'n_neighbors': 21, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 2 with value: 0.6481400708932503.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:21:58,339]\u001b[0m Trial 11 finished with value: 0.6596807924044807 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 78}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:21:58,754]\u001b[0m Trial 12 finished with value: 0.604507987116049 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 85}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:21:59,238]\u001b[0m Trial 13 finished with value: 0.6481400708932503 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 61}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:21:59,720]\u001b[0m Trial 14 finished with value: 0.5945075839627186 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 88}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:00,161]\u001b[0m Trial 15 finished with value: 0.6596807924044807 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 70}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:00,693]\u001b[0m Trial 16 finished with value: 0.5680659760986219 and parameters: {'n_neighbors': 14, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 52}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:01,142]\u001b[0m Trial 17 finished with value: 0.6156109622436506 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 67}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:01,665]\u001b[0m Trial 18 finished with value: 0.5022626131111286 and parameters: {'n_neighbors': 23, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 53}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:02,120]\u001b[0m Trial 19 finished with value: 0.5569392705340193 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 91}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:02,622]\u001b[0m Trial 20 finished with value: 0.6280813816511238 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:03,080]\u001b[0m Trial 21 finished with value: 0.6596807924044807 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 75}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:03,543]\u001b[0m Trial 22 finished with value: 0.6596807924044807 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 69}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:04,003]\u001b[0m Trial 23 finished with value: 0.6290165323381072 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:04,554]\u001b[0m Trial 24 finished with value: 0.587515689735199 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 60}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:05,041]\u001b[0m Trial 25 finished with value: 0.6290165323381072 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:05,527]\u001b[0m Trial 26 finished with value: 0.6596807924044807 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 70}. Best is trial 11 with value: 0.6596807924044807.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:06,066]\u001b[0m Trial 27 finished with value: 0.6606079054164563 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 56}. Best is trial 27 with value: 0.6606079054164563.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:06,499]\u001b[0m Trial 28 finished with value: 0.6613870839614401 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 62}. Best is trial 28 with value: 0.6613870839614401.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:07,045]\u001b[0m Trial 29 finished with value: 0.6212762463991963 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 58}. Best is trial 28 with value: 0.6613870839614401.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:07,578]\u001b[0m Trial 30 finished with value: 0.6006564119698345 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 45}. Best is trial 28 with value: 0.6613870839614401.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:08,110]\u001b[0m Trial 31 finished with value: 0.6606079054164563 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 56}. Best is trial 28 with value: 0.6613870839614401.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:08,620]\u001b[0m Trial 32 finished with value: 0.6606079054164563 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 53}. Best is trial 28 with value: 0.6613870839614401.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:09,127]\u001b[0m Trial 33 finished with value: 0.6305937824582205 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 38}. Best is trial 28 with value: 0.6613870839614401.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:09,660]\u001b[0m Trial 34 finished with value: 0.6606079054164563 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 53}. Best is trial 28 with value: 0.6613870839614401.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:22:10,183]\u001b[0m Trial 35 finished with value: 0.6434191795337003 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 47}. Best is trial 28 with value: 0.6613870839614401.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:10,728]\u001b[0m Trial 36 finished with value: 0.6717365451476869 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 55}. Best is trial 36 with value: 0.6717365451476869.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:11,239]\u001b[0m Trial 37 finished with value: 0.6833432757890224 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 37 with value: 0.6833432757890224.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:11,740]\u001b[0m Trial 38 finished with value: 0.6194723524355592 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 37 with value: 0.6833432757890224.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:12,290]\u001b[0m Trial 39 finished with value: 0.5786553749731767 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 37 with value: 0.6833432757890224.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:12,728]\u001b[0m Trial 40 finished with value: 0.6833735206068852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 40 with value: 0.6833735206068852.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:13,135]\u001b[0m Trial 41 finished with value: 0.6833735206068852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 40 with value: 0.6833735206068852.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:13,558]\u001b[0m Trial 42 finished with value: 0.6833735206068852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 40 with value: 0.6833735206068852.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:14,009]\u001b[0m Trial 43 finished with value: 0.6833735206068852 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 40 with value: 0.6833735206068852.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:14,420]\u001b[0m Trial 44 finished with value: 0.6651169403942092 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 40 with value: 0.6833735206068852.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:14,865]\u001b[0m Trial 45 finished with value: 0.6823474015855272 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 40 with value: 0.6833735206068852.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:15,499]\u001b[0m Trial 46 finished with value: 0.658866969897334 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 40 with value: 0.6833735206068852.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:16,017]\u001b[0m Trial 47 finished with value: 0.6833432757890224 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 40 with value: 0.6833735206068852.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:16,625]\u001b[0m Trial 48 finished with value: 0.6837123740829366 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 48 with value: 0.6837123740829366.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:17,083]\u001b[0m Trial 49 finished with value: 0.6702528315058208 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 48 with value: 0.6837123740829366.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6837\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 48\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNregressor\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.658472\n",
      "1                    TP   47.000000\n",
      "2                    TN  199.000000\n",
      "3                    FP    2.000000\n",
      "4                    FN   20.000000\n",
      "5              Accuracy    0.917910\n",
      "6             Precision    0.959184\n",
      "7           Sensitivity    0.701493\n",
      "8           Specificity    0.990000\n",
      "9              F1 score    0.810345\n",
      "10  F1 score (weighted)    0.913300\n",
      "11     F1 score (macro)    0.878982\n",
      "12    Balanced Accuracy    0.845771\n",
      "13                  MCC    0.774701\n",
      "14                  NPV    0.908700\n",
      "15              ROC_AUC    0.845771\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=24,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_knn_0)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_knn_0_cat = np.where(((y_pred_knn_0 >= 2) | (y_pred_knn_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_knn_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:22:17,650]\u001b[0m Trial 50 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:18,099]\u001b[0m Trial 51 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:18,548]\u001b[0m Trial 52 finished with value: 0.7038607365373963 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:19,040]\u001b[0m Trial 53 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:19,485]\u001b[0m Trial 54 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:19,950]\u001b[0m Trial 55 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:20,442]\u001b[0m Trial 56 finished with value: 0.7038607365373963 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:20,849]\u001b[0m Trial 57 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:21,251]\u001b[0m Trial 58 finished with value: 0.6829675080943731 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:21,654]\u001b[0m Trial 59 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:22,077]\u001b[0m Trial 60 finished with value: 0.6003046242294114 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:22,495]\u001b[0m Trial 61 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:22,960]\u001b[0m Trial 62 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:23,361]\u001b[0m Trial 63 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:23,793]\u001b[0m Trial 64 finished with value: 0.6829675080943731 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:24,313]\u001b[0m Trial 65 finished with value: 0.7038607365373963 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:24,751]\u001b[0m Trial 66 finished with value: 0.6766321814034408 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:25,212]\u001b[0m Trial 67 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:25,684]\u001b[0m Trial 68 finished with value: 0.7038607365373963 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:26,132]\u001b[0m Trial 69 finished with value: 0.6829675080943731 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:26,558]\u001b[0m Trial 70 finished with value: 0.6702220113449201 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:27,022]\u001b[0m Trial 71 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:27,444]\u001b[0m Trial 72 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:27,870]\u001b[0m Trial 73 finished with value: 0.7038607365373963 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:28,292]\u001b[0m Trial 74 finished with value: 0.7012056438401573 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:28,713]\u001b[0m Trial 75 finished with value: 0.5715925528197433 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:29,147]\u001b[0m Trial 76 finished with value: 0.6959971756716732 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 92}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:29,550]\u001b[0m Trial 77 finished with value: 0.6936740481452774 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:29,951]\u001b[0m Trial 78 finished with value: 0.6227674384968038 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:30,353]\u001b[0m Trial 79 finished with value: 0.7012056438401573 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:30,776]\u001b[0m Trial 80 finished with value: 0.7038607365373963 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:31,180]\u001b[0m Trial 81 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:31,658]\u001b[0m Trial 82 finished with value: 0.6936740481452774 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:32,185]\u001b[0m Trial 83 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:32,626]\u001b[0m Trial 84 finished with value: 0.7012056438401573 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:22:33,123]\u001b[0m Trial 85 finished with value: 0.6936740481452774 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:33,557]\u001b[0m Trial 86 finished with value: 0.6980040535733897 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 87}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:34,003]\u001b[0m Trial 87 finished with value: 0.6829675080943731 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:34,461]\u001b[0m Trial 88 finished with value: 0.7012056438401573 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:34,884]\u001b[0m Trial 89 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:35,293]\u001b[0m Trial 90 finished with value: 0.7012056438401573 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:35,738]\u001b[0m Trial 91 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:36,144]\u001b[0m Trial 92 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:36,547]\u001b[0m Trial 93 finished with value: 0.7038607365373963 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:36,958]\u001b[0m Trial 94 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:37,361]\u001b[0m Trial 95 finished with value: 0.6936740481452774 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:37,806]\u001b[0m Trial 96 finished with value: 0.7038607365373963 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:38,239]\u001b[0m Trial 97 finished with value: 0.7040062642130659 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:38,710]\u001b[0m Trial 98 finished with value: 0.5189181158772846 and parameters: {'n_neighbors': 22, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:39,127]\u001b[0m Trial 99 finished with value: 0.6252996718765805 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 86}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7040\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 83\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.658472    0.663120\n",
      "1                    TP   47.000000   44.000000\n",
      "2                    TN  199.000000  200.000000\n",
      "3                    FP    2.000000    1.000000\n",
      "4                    FN   20.000000   23.000000\n",
      "5              Accuracy    0.917910    0.910448\n",
      "6             Precision    0.959184    0.977778\n",
      "7           Sensitivity    0.701493    0.656716\n",
      "8           Specificity    0.990000    0.995000\n",
      "9              F1 score    0.810345    0.785714\n",
      "10  F1 score (weighted)    0.913300    0.903976\n",
      "11     F1 score (macro)    0.878982    0.864555\n",
      "12    Balanced Accuracy    0.845771    0.825871\n",
      "13                  MCC    0.774701    0.755009\n",
      "14                  NPV    0.908700    0.896900\n",
      "15              ROC_AUC    0.845771    0.825871\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=24,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_knn_1)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet1_cat = np.where(((Y_testSet1>=2) | (Y_testSet1<=-2)), 1, 0) \n",
    "y_pred_knn_1_cat = np.where(((y_pred_knn_1 >= 2) | (y_pred_knn_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_knn_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:22:39,652]\u001b[0m Trial 100 finished with value: 0.6538253612592986 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:40,135]\u001b[0m Trial 101 finished with value: 0.6570739541587143 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:40,576]\u001b[0m Trial 102 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:41,049]\u001b[0m Trial 103 finished with value: 0.6538253612592986 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:41,536]\u001b[0m Trial 104 finished with value: 0.5685886970937601 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:42,011]\u001b[0m Trial 105 finished with value: 0.6460539053649066 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:42,504]\u001b[0m Trial 106 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:42,962]\u001b[0m Trial 107 finished with value: 0.649924897857331 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:43,389]\u001b[0m Trial 108 finished with value: 0.6570739541587143 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:43,882]\u001b[0m Trial 109 finished with value: 0.6538253612592986 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:44,373]\u001b[0m Trial 110 finished with value: 0.5432427126142223 and parameters: {'n_neighbors': 17, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:44,805]\u001b[0m Trial 111 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:45,249]\u001b[0m Trial 112 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:45,681]\u001b[0m Trial 113 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:46,148]\u001b[0m Trial 114 finished with value: 0.6570739541587143 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:46,587]\u001b[0m Trial 115 finished with value: 0.6538253612592986 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:47,062]\u001b[0m Trial 116 finished with value: 0.649924897857331 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:47,571]\u001b[0m Trial 117 finished with value: 0.6570739541587143 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:47,986]\u001b[0m Trial 118 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:48,403]\u001b[0m Trial 119 finished with value: 0.6444586003490331 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:48,868]\u001b[0m Trial 120 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:49,366]\u001b[0m Trial 121 finished with value: 0.6570739541587143 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:49,851]\u001b[0m Trial 122 finished with value: 0.6538253612592986 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:50,301]\u001b[0m Trial 123 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:50,788]\u001b[0m Trial 124 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:51,257]\u001b[0m Trial 125 finished with value: 0.649924897857331 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:51,769]\u001b[0m Trial 126 finished with value: 0.6538253612592986 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:52,283]\u001b[0m Trial 127 finished with value: 0.6570739541587143 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:52,802]\u001b[0m Trial 128 finished with value: 0.6570739541587143 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:53,258]\u001b[0m Trial 129 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:53,706]\u001b[0m Trial 130 finished with value: 0.624184787593577 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:54,185]\u001b[0m Trial 131 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:54,650]\u001b[0m Trial 132 finished with value: 0.6570739541587143 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:55,089]\u001b[0m Trial 133 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:55,621]\u001b[0m Trial 134 finished with value: 0.649924897857331 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:22:56,081]\u001b[0m Trial 135 finished with value: 0.6538253612592986 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:56,539]\u001b[0m Trial 136 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:56,980]\u001b[0m Trial 137 finished with value: 0.6570739541587143 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:57,460]\u001b[0m Trial 138 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:57,910]\u001b[0m Trial 139 finished with value: 0.6444586003490331 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 91}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:58,374]\u001b[0m Trial 140 finished with value: 0.649924897857331 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:58,848]\u001b[0m Trial 141 finished with value: 0.6570739541587143 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:59,315]\u001b[0m Trial 142 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:22:59,780]\u001b[0m Trial 143 finished with value: 0.6538253612592986 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:00,230]\u001b[0m Trial 144 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:00,695]\u001b[0m Trial 145 finished with value: 0.6570739541587143 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:01,140]\u001b[0m Trial 146 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:01,590]\u001b[0m Trial 147 finished with value: 0.6444586003490331 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:02,040]\u001b[0m Trial 148 finished with value: 0.6570739541587143 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:02,535]\u001b[0m Trial 149 finished with value: 0.6554136220283987 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7040\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 83\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.658472    0.663120    0.695179\n",
      "1                    TP   47.000000   44.000000   46.000000\n",
      "2                    TN  199.000000  200.000000  195.000000\n",
      "3                    FP    2.000000    1.000000    6.000000\n",
      "4                    FN   20.000000   23.000000   21.000000\n",
      "5              Accuracy    0.917910    0.910448    0.899254\n",
      "6             Precision    0.959184    0.977778    0.884615\n",
      "7           Sensitivity    0.701493    0.656716    0.686567\n",
      "8           Specificity    0.990000    0.995000    0.970100\n",
      "9              F1 score    0.810345    0.785714    0.773109\n",
      "10  F1 score (weighted)    0.913300    0.903976    0.894716\n",
      "11     F1 score (macro)    0.878982    0.864555    0.854181\n",
      "12    Balanced Accuracy    0.845771    0.825871    0.828358\n",
      "13                  MCC    0.774701    0.755009    0.719092\n",
      "14                  NPV    0.908700    0.896900    0.902800\n",
      "15              ROC_AUC    0.845771    0.825871    0.828358\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=24,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_knn_2)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet2_cat = np.where(((Y_testSet2>=2) | (Y_testSet2<=-2)), 1, 0) \n",
    "y_pred_knn_2_cat = np.where(((y_pred_knn_2 >= 2) | (y_pred_knn_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_knn_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:23:03,118]\u001b[0m Trial 150 finished with value: 0.6631608571187319 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:03,619]\u001b[0m Trial 151 finished with value: 0.6736512672604336 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:04,093]\u001b[0m Trial 152 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:04,586]\u001b[0m Trial 153 finished with value: 0.6699003682771585 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:05,070]\u001b[0m Trial 154 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:05,582]\u001b[0m Trial 155 finished with value: 0.6736512672604336 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:06,008]\u001b[0m Trial 156 finished with value: 0.5062671510841613 and parameters: {'n_neighbors': 20, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:06,423]\u001b[0m Trial 157 finished with value: 0.6702939163737209 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:06,841]\u001b[0m Trial 158 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:07,263]\u001b[0m Trial 159 finished with value: 0.6699003682771585 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:07,694]\u001b[0m Trial 160 finished with value: 0.6736512672604336 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:08,132]\u001b[0m Trial 161 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:08,582]\u001b[0m Trial 162 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:09,051]\u001b[0m Trial 163 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:09,570]\u001b[0m Trial 164 finished with value: 0.6699003682771585 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:10,064]\u001b[0m Trial 165 finished with value: 0.6314131040518136 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:10,539]\u001b[0m Trial 166 finished with value: 0.6736512672604336 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:11,019]\u001b[0m Trial 167 finished with value: 0.6550024032266027 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 98}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:11,468]\u001b[0m Trial 168 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:11,929]\u001b[0m Trial 169 finished with value: 0.6736512672604336 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:12,372]\u001b[0m Trial 170 finished with value: 0.6699003682771585 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:12,843]\u001b[0m Trial 171 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:13,307]\u001b[0m Trial 172 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:13,933]\u001b[0m Trial 173 finished with value: 0.6702685146239664 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:14,390]\u001b[0m Trial 174 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:14,869]\u001b[0m Trial 175 finished with value: 0.6736512672604336 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:15,319]\u001b[0m Trial 176 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:15,773]\u001b[0m Trial 177 finished with value: 0.6736512672604336 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:16,231]\u001b[0m Trial 178 finished with value: 0.6617617243589973 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 80}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:16,651]\u001b[0m Trial 179 finished with value: 0.6401869774121804 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:17,119]\u001b[0m Trial 180 finished with value: 0.6736512672604336 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:17,574]\u001b[0m Trial 181 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:18,019]\u001b[0m Trial 182 finished with value: 0.6699003682771585 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:18,460]\u001b[0m Trial 183 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:18,880]\u001b[0m Trial 184 finished with value: 0.6699003682771585 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:23:19,356]\u001b[0m Trial 185 finished with value: 0.6736512672604336 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:19,810]\u001b[0m Trial 186 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:20,266]\u001b[0m Trial 187 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:20,726]\u001b[0m Trial 188 finished with value: 0.6736512672604336 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:21,174]\u001b[0m Trial 189 finished with value: 0.6699003682771585 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:21,617]\u001b[0m Trial 190 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:22,049]\u001b[0m Trial 191 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:22,540]\u001b[0m Trial 192 finished with value: 0.6699003682771585 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:22,983]\u001b[0m Trial 193 finished with value: 0.6736512672604336 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:23,397]\u001b[0m Trial 194 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:23,826]\u001b[0m Trial 195 finished with value: 0.6744684716934607 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:24,304]\u001b[0m Trial 196 finished with value: 0.6736512672604336 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:24,748]\u001b[0m Trial 197 finished with value: 0.664562761872698 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 86}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:25,231]\u001b[0m Trial 198 finished with value: 0.6699003682771585 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:25,700]\u001b[0m Trial 199 finished with value: 0.6736512672604336 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7040\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 83\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.658472    0.663120    0.695179    0.754528\n",
      "1                    TP   47.000000   44.000000   46.000000   41.000000\n",
      "2                    TN  199.000000  200.000000  195.000000  200.000000\n",
      "3                    FP    2.000000    1.000000    6.000000    0.000000\n",
      "4                    FN   20.000000   23.000000   21.000000   27.000000\n",
      "5              Accuracy    0.917910    0.910448    0.899254    0.899254\n",
      "6             Precision    0.959184    0.977778    0.884615    1.000000\n",
      "7           Sensitivity    0.701493    0.656716    0.686567    0.602941\n",
      "8           Specificity    0.990000    0.995000    0.970100    1.000000\n",
      "9              F1 score    0.810345    0.785714    0.773109    0.752294\n",
      "10  F1 score (weighted)    0.913300    0.903976    0.894716    0.889961\n",
      "11     F1 score (macro)    0.878982    0.864555    0.854181    0.844531\n",
      "12    Balanced Accuracy    0.845771    0.825871    0.828358    0.801471\n",
      "13                  MCC    0.774701    0.755009    0.719092    0.728852\n",
      "14                  NPV    0.908700    0.896900    0.902800    0.881100\n",
      "15              ROC_AUC    0.845771    0.825871    0.828358    0.801471\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=24,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_knn_3)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet3_cat = np.where(((Y_testSet3>=2) | (Y_testSet3<=-2)), 1, 0) \n",
    "y_pred_knn_3_cat = np.where(((y_pred_knn_3 >= 2) | (y_pred_knn_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_knn_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:23:26,219]\u001b[0m Trial 200 finished with value: 0.6650355238253385 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:26,654]\u001b[0m Trial 201 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:27,101]\u001b[0m Trial 202 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:27,557]\u001b[0m Trial 203 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:27,992]\u001b[0m Trial 204 finished with value: 0.5744971257106896 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:28,486]\u001b[0m Trial 205 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:28,962]\u001b[0m Trial 206 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:29,402]\u001b[0m Trial 207 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:29,850]\u001b[0m Trial 208 finished with value: 0.6526019520532096 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 84}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:30,316]\u001b[0m Trial 209 finished with value: 0.6677507146294899 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:30,787]\u001b[0m Trial 210 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:31,279]\u001b[0m Trial 211 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:31,765]\u001b[0m Trial 212 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:32,211]\u001b[0m Trial 213 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:32,660]\u001b[0m Trial 214 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:33,122]\u001b[0m Trial 215 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:33,601]\u001b[0m Trial 216 finished with value: 0.6677507146294899 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:34,034]\u001b[0m Trial 217 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:34,494]\u001b[0m Trial 218 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:34,926]\u001b[0m Trial 219 finished with value: 0.6424614895578133 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:35,397]\u001b[0m Trial 220 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:35,861]\u001b[0m Trial 221 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:36,416]\u001b[0m Trial 222 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:36,910]\u001b[0m Trial 223 finished with value: 0.5521717748776609 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:37,366]\u001b[0m Trial 224 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:37,850]\u001b[0m Trial 225 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:38,298]\u001b[0m Trial 226 finished with value: 0.6677507146294899 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:38,746]\u001b[0m Trial 227 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:39,271]\u001b[0m Trial 228 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:39,732]\u001b[0m Trial 229 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:40,198]\u001b[0m Trial 230 finished with value: 0.6718868032523878 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 87}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:40,635]\u001b[0m Trial 231 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:41,053]\u001b[0m Trial 232 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:41,473]\u001b[0m Trial 233 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:41,891]\u001b[0m Trial 234 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:23:42,380]\u001b[0m Trial 235 finished with value: 0.6677507146294899 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:42,837]\u001b[0m Trial 236 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:43,287]\u001b[0m Trial 237 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:43,745]\u001b[0m Trial 238 finished with value: 0.6677507146294899 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:44,213]\u001b[0m Trial 239 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:44,786]\u001b[0m Trial 240 finished with value: 0.6758375544841926 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:45,279]\u001b[0m Trial 241 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:45,729]\u001b[0m Trial 242 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:46,199]\u001b[0m Trial 243 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:46,645]\u001b[0m Trial 244 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:47,076]\u001b[0m Trial 245 finished with value: 0.6594192536582739 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:47,524]\u001b[0m Trial 246 finished with value: 0.6423697650057145 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:47,991]\u001b[0m Trial 247 finished with value: 0.6685062480855999 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 75}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:48,464]\u001b[0m Trial 248 finished with value: 0.6770440364348841 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:48,907]\u001b[0m Trial 249 finished with value: 0.6760862248234097 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7040\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 83\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.658472    0.663120    0.695179    0.754528   \n",
      "1                    TP   47.000000   44.000000   46.000000   41.000000   \n",
      "2                    TN  199.000000  200.000000  195.000000  200.000000   \n",
      "3                    FP    2.000000    1.000000    6.000000    0.000000   \n",
      "4                    FN   20.000000   23.000000   21.000000   27.000000   \n",
      "5              Accuracy    0.917910    0.910448    0.899254    0.899254   \n",
      "6             Precision    0.959184    0.977778    0.884615    1.000000   \n",
      "7           Sensitivity    0.701493    0.656716    0.686567    0.602941   \n",
      "8           Specificity    0.990000    0.995000    0.970100    1.000000   \n",
      "9              F1 score    0.810345    0.785714    0.773109    0.752294   \n",
      "10  F1 score (weighted)    0.913300    0.903976    0.894716    0.889961   \n",
      "11     F1 score (macro)    0.878982    0.864555    0.854181    0.844531   \n",
      "12    Balanced Accuracy    0.845771    0.825871    0.828358    0.801471   \n",
      "13                  MCC    0.774701    0.755009    0.719092    0.728852   \n",
      "14                  NPV    0.908700    0.896900    0.902800    0.881100   \n",
      "15              ROC_AUC    0.845771    0.825871    0.828358    0.801471   \n",
      "\n",
      "          Set4  \n",
      "0     0.690113  \n",
      "1    41.000000  \n",
      "2   198.000000  \n",
      "3     5.000000  \n",
      "4    24.000000  \n",
      "5     0.891791  \n",
      "6     0.891304  \n",
      "7     0.630769  \n",
      "8     0.975400  \n",
      "9     0.738739  \n",
      "10    0.884949  \n",
      "11    0.835252  \n",
      "12    0.803069  \n",
      "13    0.689003  \n",
      "14    0.891900  \n",
      "15    0.803069  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=24,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_knn_4)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet4_cat = np.where(((Y_testSet4>=2) | (Y_testSet4<=-2)), 1, 0) \n",
    "y_pred_knn_4_cat = np.where(((y_pred_knn_4 >= 2) | (y_pred_knn_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_knn_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:23:49,477]\u001b[0m Trial 250 finished with value: 0.6892294851140616 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:49,900]\u001b[0m Trial 251 finished with value: 0.6932560521405441 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:50,354]\u001b[0m Trial 252 finished with value: 0.6892294851140616 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:50,840]\u001b[0m Trial 253 finished with value: 0.6372596547272538 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 50 with value: 0.7040062642130659.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:51,312]\u001b[0m Trial 254 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:51,770]\u001b[0m Trial 255 finished with value: 0.6932560521405441 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:52,213]\u001b[0m Trial 256 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:52,684]\u001b[0m Trial 257 finished with value: 0.6932560521405441 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:53,200]\u001b[0m Trial 258 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:53,694]\u001b[0m Trial 259 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:54,261]\u001b[0m Trial 260 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:54,695]\u001b[0m Trial 261 finished with value: 0.6892294851140616 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:55,121]\u001b[0m Trial 262 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:55,547]\u001b[0m Trial 263 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:55,973]\u001b[0m Trial 264 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:56,455]\u001b[0m Trial 265 finished with value: 0.6994943948523581 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:56,911]\u001b[0m Trial 266 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:57,365]\u001b[0m Trial 267 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:57,795]\u001b[0m Trial 268 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:58,214]\u001b[0m Trial 269 finished with value: 0.6842850398626299 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:58,615]\u001b[0m Trial 270 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:59,052]\u001b[0m Trial 271 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:59,532]\u001b[0m Trial 272 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:23:59,964]\u001b[0m Trial 273 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:00,376]\u001b[0m Trial 274 finished with value: 0.6994943948523581 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:00,807]\u001b[0m Trial 275 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:01,310]\u001b[0m Trial 276 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:01,783]\u001b[0m Trial 277 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:02,294]\u001b[0m Trial 278 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:02,781]\u001b[0m Trial 279 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:03,241]\u001b[0m Trial 280 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:03,692]\u001b[0m Trial 281 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:04,108]\u001b[0m Trial 282 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:04,564]\u001b[0m Trial 283 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:04,986]\u001b[0m Trial 284 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:24:05,472]\u001b[0m Trial 285 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:05,954]\u001b[0m Trial 286 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:06,381]\u001b[0m Trial 287 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:06,821]\u001b[0m Trial 288 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:07,310]\u001b[0m Trial 289 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:07,802]\u001b[0m Trial 290 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:08,276]\u001b[0m Trial 291 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:08,733]\u001b[0m Trial 292 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:09,204]\u001b[0m Trial 293 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:09,661]\u001b[0m Trial 294 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:10,086]\u001b[0m Trial 295 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:10,561]\u001b[0m Trial 296 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:11,006]\u001b[0m Trial 297 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:11,453]\u001b[0m Trial 298 finished with value: 0.7062222360032736 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:11,879]\u001b[0m Trial 299 finished with value: 0.6842850398626299 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7062\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 98\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.658472    0.663120    0.695179    0.754528   \n",
      "1                    TP   47.000000   44.000000   46.000000   41.000000   \n",
      "2                    TN  199.000000  200.000000  195.000000  200.000000   \n",
      "3                    FP    2.000000    1.000000    6.000000    0.000000   \n",
      "4                    FN   20.000000   23.000000   21.000000   27.000000   \n",
      "5              Accuracy    0.917910    0.910448    0.899254    0.899254   \n",
      "6             Precision    0.959184    0.977778    0.884615    1.000000   \n",
      "7           Sensitivity    0.701493    0.656716    0.686567    0.602941   \n",
      "8           Specificity    0.990000    0.995000    0.970100    1.000000   \n",
      "9              F1 score    0.810345    0.785714    0.773109    0.752294   \n",
      "10  F1 score (weighted)    0.913300    0.903976    0.894716    0.889961   \n",
      "11     F1 score (macro)    0.878982    0.864555    0.854181    0.844531   \n",
      "12    Balanced Accuracy    0.845771    0.825871    0.828358    0.801471   \n",
      "13                  MCC    0.774701    0.755009    0.719092    0.728852   \n",
      "14                  NPV    0.908700    0.896900    0.902800    0.881100   \n",
      "15              ROC_AUC    0.845771    0.825871    0.828358    0.801471   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.690113    0.640676  \n",
      "1    41.000000   38.000000  \n",
      "2   198.000000  197.000000  \n",
      "3     5.000000    4.000000  \n",
      "4    24.000000   29.000000  \n",
      "5     0.891791    0.876866  \n",
      "6     0.891304    0.904762  \n",
      "7     0.630769    0.567164  \n",
      "8     0.975400    0.980100  \n",
      "9     0.738739    0.697248  \n",
      "10    0.884949    0.866349  \n",
      "11    0.835252    0.809982  \n",
      "12    0.803069    0.773632  \n",
      "13    0.689003    0.651858  \n",
      "14    0.891900    0.871700  \n",
      "15    0.803069    0.773632  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=24,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_knn_5)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet5_cat = np.where(((Y_testSet5>=2) | (Y_testSet5<=-2)), 1, 0) \n",
    "y_pred_knn_5_cat = np.where(((y_pred_knn_5 >= 2) | (y_pred_knn_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_knn_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:24:12,398]\u001b[0m Trial 300 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:12,856]\u001b[0m Trial 301 finished with value: 0.5589653940302352 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:13,505]\u001b[0m Trial 302 finished with value: 0.6718207886187623 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 20}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:13,972]\u001b[0m Trial 303 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:14,427]\u001b[0m Trial 304 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:14,921]\u001b[0m Trial 305 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:15,350]\u001b[0m Trial 306 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:15,778]\u001b[0m Trial 307 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:16,198]\u001b[0m Trial 308 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:16,660]\u001b[0m Trial 309 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:17,144]\u001b[0m Trial 310 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:17,639]\u001b[0m Trial 311 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:18,070]\u001b[0m Trial 312 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:18,546]\u001b[0m Trial 313 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:19,006]\u001b[0m Trial 314 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:19,479]\u001b[0m Trial 315 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:19,916]\u001b[0m Trial 316 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:20,375]\u001b[0m Trial 317 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:20,821]\u001b[0m Trial 318 finished with value: 0.6537830654080536 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:21,289]\u001b[0m Trial 319 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:21,740]\u001b[0m Trial 320 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:22,245]\u001b[0m Trial 321 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:22,699]\u001b[0m Trial 322 finished with value: 0.6710617591129602 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:23,146]\u001b[0m Trial 323 finished with value: 0.5861911824979004 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:23,596]\u001b[0m Trial 324 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:24,073]\u001b[0m Trial 325 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:24,510]\u001b[0m Trial 326 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:25,023]\u001b[0m Trial 327 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:25,457]\u001b[0m Trial 328 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:25,916]\u001b[0m Trial 329 finished with value: 0.6096941356073432 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:26,382]\u001b[0m Trial 330 finished with value: 0.657882805109417 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:26,854]\u001b[0m Trial 331 finished with value: 0.6302552779321925 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:27,506]\u001b[0m Trial 332 finished with value: 0.6764830853793196 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:27,946]\u001b[0m Trial 333 finished with value: 0.6764418824434747 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:28,429]\u001b[0m Trial 334 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:24:28,884]\u001b[0m Trial 335 finished with value: 0.6764418824434747 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:29,295]\u001b[0m Trial 336 finished with value: 0.6537830654080536 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:29,756]\u001b[0m Trial 337 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:30,192]\u001b[0m Trial 338 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:30,653]\u001b[0m Trial 339 finished with value: 0.6764418824434747 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:31,118]\u001b[0m Trial 340 finished with value: 0.6764418824434747 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:31,553]\u001b[0m Trial 341 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:32,052]\u001b[0m Trial 342 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:32,497]\u001b[0m Trial 343 finished with value: 0.6764418824434747 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:32,939]\u001b[0m Trial 344 finished with value: 0.5624650378714764 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:33,335]\u001b[0m Trial 345 finished with value: 0.6777208270703096 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:33,849]\u001b[0m Trial 346 finished with value: 0.6764830853793196 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:34,290]\u001b[0m Trial 347 finished with value: 0.6764418824434747 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:34,704]\u001b[0m Trial 348 finished with value: 0.6710617591129602 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 78}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:35,123]\u001b[0m Trial 349 finished with value: 0.6764418824434747 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7062\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 98\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.658472    0.663120    0.695179    0.754528   \n",
      "1                    TP   47.000000   44.000000   46.000000   41.000000   \n",
      "2                    TN  199.000000  200.000000  195.000000  200.000000   \n",
      "3                    FP    2.000000    1.000000    6.000000    0.000000   \n",
      "4                    FN   20.000000   23.000000   21.000000   27.000000   \n",
      "5              Accuracy    0.917910    0.910448    0.899254    0.899254   \n",
      "6             Precision    0.959184    0.977778    0.884615    1.000000   \n",
      "7           Sensitivity    0.701493    0.656716    0.686567    0.602941   \n",
      "8           Specificity    0.990000    0.995000    0.970100    1.000000   \n",
      "9              F1 score    0.810345    0.785714    0.773109    0.752294   \n",
      "10  F1 score (weighted)    0.913300    0.903976    0.894716    0.889961   \n",
      "11     F1 score (macro)    0.878982    0.864555    0.854181    0.844531   \n",
      "12    Balanced Accuracy    0.845771    0.825871    0.828358    0.801471   \n",
      "13                  MCC    0.774701    0.755009    0.719092    0.728852   \n",
      "14                  NPV    0.908700    0.896900    0.902800    0.881100   \n",
      "15              ROC_AUC    0.845771    0.825871    0.828358    0.801471   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.690113    0.640676    0.679932  \n",
      "1    41.000000   38.000000   40.000000  \n",
      "2   198.000000  197.000000  197.000000  \n",
      "3     5.000000    4.000000    5.000000  \n",
      "4    24.000000   29.000000   26.000000  \n",
      "5     0.891791    0.876866    0.884328  \n",
      "6     0.891304    0.904762    0.888889  \n",
      "7     0.630769    0.567164    0.606061  \n",
      "8     0.975400    0.980100    0.975200  \n",
      "9     0.738739    0.697248    0.720721  \n",
      "10    0.884949    0.866349    0.876244  \n",
      "11    0.835252    0.809982    0.823890  \n",
      "12    0.803069    0.773632    0.790654  \n",
      "13    0.689003    0.651858    0.670032  \n",
      "14    0.891900    0.871700    0.883400  \n",
      "15    0.803069    0.773632    0.790654  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=24,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_knn_6)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet6_cat = np.where(((Y_testSet6>=2) | (Y_testSet6<=-2)), 1, 0) \n",
    "y_pred_knn_6_cat = np.where(((y_pred_knn_6 >= 2) | (y_pred_knn_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_knn_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:24:35,632]\u001b[0m Trial 350 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:36,105]\u001b[0m Trial 351 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:36,586]\u001b[0m Trial 352 finished with value: 0.6859686167402219 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:37,096]\u001b[0m Trial 353 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:37,535]\u001b[0m Trial 354 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:37,985]\u001b[0m Trial 355 finished with value: 0.6859686167402219 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:38,460]\u001b[0m Trial 356 finished with value: 0.6596456840023563 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:38,901]\u001b[0m Trial 357 finished with value: 0.6859686167402219 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:39,342]\u001b[0m Trial 358 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:39,773]\u001b[0m Trial 359 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:40,337]\u001b[0m Trial 360 finished with value: 0.6876383523846391 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:40,764]\u001b[0m Trial 361 finished with value: 0.5825736110429087 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:41,228]\u001b[0m Trial 362 finished with value: 0.6859686167402219 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:41,682]\u001b[0m Trial 363 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:42,164]\u001b[0m Trial 364 finished with value: 0.6859686167402219 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:42,651]\u001b[0m Trial 365 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:43,172]\u001b[0m Trial 366 finished with value: 0.6859686167402219 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:43,625]\u001b[0m Trial 367 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:44,087]\u001b[0m Trial 368 finished with value: 0.6777198455674768 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:44,534]\u001b[0m Trial 369 finished with value: 0.6752544163987582 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:44,979]\u001b[0m Trial 370 finished with value: 0.6859686167402219 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:45,428]\u001b[0m Trial 371 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:45,882]\u001b[0m Trial 372 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:46,332]\u001b[0m Trial 373 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:46,792]\u001b[0m Trial 374 finished with value: 0.6543333859879246 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:47,223]\u001b[0m Trial 375 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:47,718]\u001b[0m Trial 376 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:48,207]\u001b[0m Trial 377 finished with value: 0.6859686167402219 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:48,646]\u001b[0m Trial 378 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:49,086]\u001b[0m Trial 379 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:49,612]\u001b[0m Trial 380 finished with value: 0.6859686167402219 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:50,063]\u001b[0m Trial 381 finished with value: 0.6859686167402219 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:50,463]\u001b[0m Trial 382 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:50,985]\u001b[0m Trial 383 finished with value: 0.6876383523846391 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:51,410]\u001b[0m Trial 384 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:24:51,925]\u001b[0m Trial 385 finished with value: 0.6859686167402219 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:52,381]\u001b[0m Trial 386 finished with value: 0.6859686167402219 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:52,865]\u001b[0m Trial 387 finished with value: 0.5570848836443115 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:53,314]\u001b[0m Trial 388 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:53,817]\u001b[0m Trial 389 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:54,329]\u001b[0m Trial 390 finished with value: 0.6752544163987582 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:54,799]\u001b[0m Trial 391 finished with value: 0.6777198455674768 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:55,274]\u001b[0m Trial 392 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:55,718]\u001b[0m Trial 393 finished with value: 0.6543333859879246 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:56,214]\u001b[0m Trial 394 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:56,662]\u001b[0m Trial 395 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:57,115]\u001b[0m Trial 396 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:57,591]\u001b[0m Trial 397 finished with value: 0.6859686167402219 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:58,036]\u001b[0m Trial 398 finished with value: 0.6862418898073787 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:58,505]\u001b[0m Trial 399 finished with value: 0.6859686167402219 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7062\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 98\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.658472    0.663120    0.695179    0.754528   \n",
      "1                    TP   47.000000   44.000000   46.000000   41.000000   \n",
      "2                    TN  199.000000  200.000000  195.000000  200.000000   \n",
      "3                    FP    2.000000    1.000000    6.000000    0.000000   \n",
      "4                    FN   20.000000   23.000000   21.000000   27.000000   \n",
      "5              Accuracy    0.917910    0.910448    0.899254    0.899254   \n",
      "6             Precision    0.959184    0.977778    0.884615    1.000000   \n",
      "7           Sensitivity    0.701493    0.656716    0.686567    0.602941   \n",
      "8           Specificity    0.990000    0.995000    0.970100    1.000000   \n",
      "9              F1 score    0.810345    0.785714    0.773109    0.752294   \n",
      "10  F1 score (weighted)    0.913300    0.903976    0.894716    0.889961   \n",
      "11     F1 score (macro)    0.878982    0.864555    0.854181    0.844531   \n",
      "12    Balanced Accuracy    0.845771    0.825871    0.828358    0.801471   \n",
      "13                  MCC    0.774701    0.755009    0.719092    0.728852   \n",
      "14                  NPV    0.908700    0.896900    0.902800    0.881100   \n",
      "15              ROC_AUC    0.845771    0.825871    0.828358    0.801471   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.690113    0.640676    0.679932    0.696736  \n",
      "1    41.000000   38.000000   40.000000   45.000000  \n",
      "2   198.000000  197.000000  197.000000  200.000000  \n",
      "3     5.000000    4.000000    5.000000    3.000000  \n",
      "4    24.000000   29.000000   26.000000   20.000000  \n",
      "5     0.891791    0.876866    0.884328    0.914179  \n",
      "6     0.891304    0.904762    0.888889    0.937500  \n",
      "7     0.630769    0.567164    0.606061    0.692308  \n",
      "8     0.975400    0.980100    0.975200    0.985200  \n",
      "9     0.738739    0.697248    0.720721    0.796460  \n",
      "10    0.884949    0.866349    0.876244    0.909448  \n",
      "11    0.835252    0.809982    0.823890    0.871043  \n",
      "12    0.803069    0.773632    0.790654    0.838765  \n",
      "13    0.689003    0.651858    0.670032    0.757357  \n",
      "14    0.891900    0.871700    0.883400    0.909100  \n",
      "15    0.803069    0.773632    0.790654    0.838765  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=24,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_knn_7)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet7_cat = np.where(((Y_testSet7>=2) | (Y_testSet7<=-2)), 1, 0) \n",
    "y_pred_knn_7_cat = np.where(((y_pred_knn_7 >= 2) | (y_pred_knn_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_knn_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:24:59,094]\u001b[0m Trial 400 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:24:59,548]\u001b[0m Trial 401 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:00,016]\u001b[0m Trial 402 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:00,473]\u001b[0m Trial 403 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:00,924]\u001b[0m Trial 404 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:01,366]\u001b[0m Trial 405 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:01,817]\u001b[0m Trial 406 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:02,269]\u001b[0m Trial 407 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:02,734]\u001b[0m Trial 408 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:03,191]\u001b[0m Trial 409 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:03,655]\u001b[0m Trial 410 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:04,089]\u001b[0m Trial 411 finished with value: 0.6888481675095749 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:04,629]\u001b[0m Trial 412 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:05,105]\u001b[0m Trial 413 finished with value: 0.6711782669724229 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 86}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:05,556]\u001b[0m Trial 414 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:06,004]\u001b[0m Trial 415 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:06,445]\u001b[0m Trial 416 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:06,885]\u001b[0m Trial 417 finished with value: 0.6076590707765523 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:07,375]\u001b[0m Trial 418 finished with value: 0.685542225034492 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:07,864]\u001b[0m Trial 419 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:08,301]\u001b[0m Trial 420 finished with value: 0.6771229448751401 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:08,731]\u001b[0m Trial 421 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:09,147]\u001b[0m Trial 422 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:09,585]\u001b[0m Trial 423 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 97}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:10,068]\u001b[0m Trial 424 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:10,505]\u001b[0m Trial 425 finished with value: 0.6615190601045379 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:11,002]\u001b[0m Trial 426 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:11,440]\u001b[0m Trial 427 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:11,885]\u001b[0m Trial 428 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:12,347]\u001b[0m Trial 429 finished with value: 0.6447079165258278 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:12,803]\u001b[0m Trial 430 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:13,236]\u001b[0m Trial 431 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:13,681]\u001b[0m Trial 432 finished with value: 0.6686158612274458 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:14,146]\u001b[0m Trial 433 finished with value: 0.6888481675095749 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:14,613]\u001b[0m Trial 434 finished with value: 0.6717004582504456 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:25:15,096]\u001b[0m Trial 435 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:15,562]\u001b[0m Trial 436 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:16,032]\u001b[0m Trial 437 finished with value: 0.6888481675095749 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 87}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:16,510]\u001b[0m Trial 438 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:16,984]\u001b[0m Trial 439 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:17,466]\u001b[0m Trial 440 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:17,969]\u001b[0m Trial 441 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:18,419]\u001b[0m Trial 442 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:18,937]\u001b[0m Trial 443 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:19,393]\u001b[0m Trial 444 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 99}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:19,881]\u001b[0m Trial 445 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:20,339]\u001b[0m Trial 446 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:20,827]\u001b[0m Trial 447 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:21,292]\u001b[0m Trial 448 finished with value: 0.6955082179859016 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:21,751]\u001b[0m Trial 449 finished with value: 0.6967738509509743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7062\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 98\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.658472    0.663120    0.695179    0.754528   \n",
      "1                    TP   47.000000   44.000000   46.000000   41.000000   \n",
      "2                    TN  199.000000  200.000000  195.000000  200.000000   \n",
      "3                    FP    2.000000    1.000000    6.000000    0.000000   \n",
      "4                    FN   20.000000   23.000000   21.000000   27.000000   \n",
      "5              Accuracy    0.917910    0.910448    0.899254    0.899254   \n",
      "6             Precision    0.959184    0.977778    0.884615    1.000000   \n",
      "7           Sensitivity    0.701493    0.656716    0.686567    0.602941   \n",
      "8           Specificity    0.990000    0.995000    0.970100    1.000000   \n",
      "9              F1 score    0.810345    0.785714    0.773109    0.752294   \n",
      "10  F1 score (weighted)    0.913300    0.903976    0.894716    0.889961   \n",
      "11     F1 score (macro)    0.878982    0.864555    0.854181    0.844531   \n",
      "12    Balanced Accuracy    0.845771    0.825871    0.828358    0.801471   \n",
      "13                  MCC    0.774701    0.755009    0.719092    0.728852   \n",
      "14                  NPV    0.908700    0.896900    0.902800    0.881100   \n",
      "15              ROC_AUC    0.845771    0.825871    0.828358    0.801471   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.690113    0.640676    0.679932    0.696736    0.586564  \n",
      "1    41.000000   38.000000   40.000000   45.000000   32.000000  \n",
      "2   198.000000  197.000000  197.000000  200.000000  199.000000  \n",
      "3     5.000000    4.000000    5.000000    3.000000    3.000000  \n",
      "4    24.000000   29.000000   26.000000   20.000000   34.000000  \n",
      "5     0.891791    0.876866    0.884328    0.914179    0.861940  \n",
      "6     0.891304    0.904762    0.888889    0.937500    0.914286  \n",
      "7     0.630769    0.567164    0.606061    0.692308    0.484848  \n",
      "8     0.975400    0.980100    0.975200    0.985200    0.985100  \n",
      "9     0.738739    0.697248    0.720721    0.796460    0.633663  \n",
      "10    0.884949    0.866349    0.876244    0.909448    0.845672  \n",
      "11    0.835252    0.809982    0.823890    0.871043    0.774303  \n",
      "12    0.803069    0.773632    0.790654    0.838765    0.734998  \n",
      "13    0.689003    0.651858    0.670032    0.757357    0.600940  \n",
      "14    0.891900    0.871700    0.883400    0.909100    0.854100  \n",
      "15    0.803069    0.773632    0.790654    0.838765    0.734998  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=24,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_knn_8)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet8_cat = np.where(((Y_testSet8>=2) | (Y_testSet8<=-2)), 1, 0) \n",
    "y_pred_knn_8_cat = np.where(((y_pred_knn_8 >= 2) | (y_pred_knn_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_knn_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:25:22,288]\u001b[0m Trial 450 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:22,983]\u001b[0m Trial 451 finished with value: 0.6897455100019748 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:23,467]\u001b[0m Trial 452 finished with value: 0.6490445740895505 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:23,939]\u001b[0m Trial 453 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:24,425]\u001b[0m Trial 454 finished with value: 0.6849587752387423 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:24,905]\u001b[0m Trial 455 finished with value: 0.6808828891155152 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 65}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:25,422]\u001b[0m Trial 456 finished with value: 0.6849587752387423 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:25,886]\u001b[0m Trial 457 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:26,363]\u001b[0m Trial 458 finished with value: 0.6808828891155152 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 86}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:26,782]\u001b[0m Trial 459 finished with value: 0.6849587752387423 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:27,203]\u001b[0m Trial 460 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:27,638]\u001b[0m Trial 461 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:28,062]\u001b[0m Trial 462 finished with value: 0.6849587752387423 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:28,532]\u001b[0m Trial 463 finished with value: 0.6819444287209894 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:28,970]\u001b[0m Trial 464 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:29,441]\u001b[0m Trial 465 finished with value: 0.6162410238230449 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:29,924]\u001b[0m Trial 466 finished with value: 0.6849587752387423 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:30,357]\u001b[0m Trial 467 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:30,807]\u001b[0m Trial 468 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:31,307]\u001b[0m Trial 469 finished with value: 0.6849587752387423 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:31,769]\u001b[0m Trial 470 finished with value: 0.6598616568076866 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:32,223]\u001b[0m Trial 471 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:32,681]\u001b[0m Trial 472 finished with value: 0.6849587752387423 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:33,168]\u001b[0m Trial 473 finished with value: 0.6819444287209894 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:33,673]\u001b[0m Trial 474 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:34,145]\u001b[0m Trial 475 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:34,602]\u001b[0m Trial 476 finished with value: 0.6849587752387423 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:35,052]\u001b[0m Trial 477 finished with value: 0.6808828891155152 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:35,493]\u001b[0m Trial 478 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:36,047]\u001b[0m Trial 479 finished with value: 0.6863124376977062 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:36,484]\u001b[0m Trial 480 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:36,972]\u001b[0m Trial 481 finished with value: 0.5949975590909931 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:37,583]\u001b[0m Trial 482 finished with value: 0.5007368137205721 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:38,026]\u001b[0m Trial 483 finished with value: 0.6849587752387423 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:38,510]\u001b[0m Trial 484 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:25:39,034]\u001b[0m Trial 485 finished with value: 0.6849587752387423 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:39,524]\u001b[0m Trial 486 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:39,989]\u001b[0m Trial 487 finished with value: 0.6849587752387423 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:40,487]\u001b[0m Trial 488 finished with value: 0.6819444287209894 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:40,959]\u001b[0m Trial 489 finished with value: 0.6598616568076866 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:41,427]\u001b[0m Trial 490 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:41,898]\u001b[0m Trial 491 finished with value: 0.6849587752387423 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:42,358]\u001b[0m Trial 492 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:42,870]\u001b[0m Trial 493 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:43,344]\u001b[0m Trial 494 finished with value: 0.6010466077222613 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:43,775]\u001b[0m Trial 495 finished with value: 0.6849587752387423 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:44,207]\u001b[0m Trial 496 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:44,649]\u001b[0m Trial 497 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:45,206]\u001b[0m Trial 498 finished with value: 0.6863124376977062 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:25:45,657]\u001b[0m Trial 499 finished with value: 0.6904874355064744 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 254 with value: 0.7062222360032736.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7062\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 98\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.658472    0.663120    0.695179    0.754528   \n",
      "1                    TP   47.000000   44.000000   46.000000   41.000000   \n",
      "2                    TN  199.000000  200.000000  195.000000  200.000000   \n",
      "3                    FP    2.000000    1.000000    6.000000    0.000000   \n",
      "4                    FN   20.000000   23.000000   21.000000   27.000000   \n",
      "5              Accuracy    0.917910    0.910448    0.899254    0.899254   \n",
      "6             Precision    0.959184    0.977778    0.884615    1.000000   \n",
      "7           Sensitivity    0.701493    0.656716    0.686567    0.602941   \n",
      "8           Specificity    0.990000    0.995000    0.970100    1.000000   \n",
      "9              F1 score    0.810345    0.785714    0.773109    0.752294   \n",
      "10  F1 score (weighted)    0.913300    0.903976    0.894716    0.889961   \n",
      "11     F1 score (macro)    0.878982    0.864555    0.854181    0.844531   \n",
      "12    Balanced Accuracy    0.845771    0.825871    0.828358    0.801471   \n",
      "13                  MCC    0.774701    0.755009    0.719092    0.728852   \n",
      "14                  NPV    0.908700    0.896900    0.902800    0.881100   \n",
      "15              ROC_AUC    0.845771    0.825871    0.828358    0.801471   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.690113    0.640676    0.679932    0.696736    0.586564    0.748406  \n",
      "1    41.000000   38.000000   40.000000   45.000000   32.000000   44.000000  \n",
      "2   198.000000  197.000000  197.000000  200.000000  199.000000  200.000000  \n",
      "3     5.000000    4.000000    5.000000    3.000000    3.000000    2.000000  \n",
      "4    24.000000   29.000000   26.000000   20.000000   34.000000   22.000000  \n",
      "5     0.891791    0.876866    0.884328    0.914179    0.861940    0.910448  \n",
      "6     0.891304    0.904762    0.888889    0.937500    0.914286    0.956522  \n",
      "7     0.630769    0.567164    0.606061    0.692308    0.484848    0.666667  \n",
      "8     0.975400    0.980100    0.975200    0.985200    0.985100    0.990100  \n",
      "9     0.738739    0.697248    0.720721    0.796460    0.633663    0.785714  \n",
      "10    0.884949    0.866349    0.876244    0.909448    0.845672    0.904564  \n",
      "11    0.835252    0.809982    0.823890    0.871043    0.774303    0.864555  \n",
      "12    0.803069    0.773632    0.790654    0.838765    0.734998    0.828383  \n",
      "13    0.689003    0.651858    0.670032    0.757357    0.600940    0.750417  \n",
      "14    0.891900    0.871700    0.883400    0.909100    0.854100    0.900900  \n",
      "15    0.803069    0.773632    0.790654    0.838765    0.734998    0.828383  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=24,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_knn_9)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet9_cat = np.where(((Y_testSet9>=2) | (Y_testSet9<=-2)), 1, 0) \n",
    "y_pred_knn_9_cat = np.where(((y_pred_knn_9 >= 2) | (y_pred_knn_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_knn_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.710253</td>\n",
       "      <td>0.058495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>2.584140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>98.100000</td>\n",
       "      <td>1.286684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.173788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>2.830391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.902188</td>\n",
       "      <td>0.027290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.903045</td>\n",
       "      <td>0.049349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.680258</td>\n",
       "      <td>0.082239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.976120</td>\n",
       "      <td>0.011671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.774640</td>\n",
       "      <td>0.067849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.896815</td>\n",
       "      <td>0.029938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.856078</td>\n",
       "      <td>0.042413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.828189</td>\n",
       "      <td>0.044964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.726569</td>\n",
       "      <td>0.079507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.902030</td>\n",
       "      <td>0.024389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.828189</td>\n",
       "      <td>0.044964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.710253     0.058495\n",
       "1                    TP        22.700000     2.584140\n",
       "2                    TN        98.100000     1.286684\n",
       "3                    FP         2.400000     1.173788\n",
       "4                    FN        10.700000     2.830391\n",
       "5              Accuracy         0.902188     0.027290\n",
       "6             Precision         0.903045     0.049349\n",
       "7           Sensitivity         0.680258     0.082239\n",
       "8           Specificity         0.976120     0.011671\n",
       "9              F1 score         0.774640     0.067849\n",
       "10  F1 score (weighted)         0.896815     0.029938\n",
       "11     F1 score (macro)         0.856078     0.042413\n",
       "12    Balanced Accuracy         0.828189     0.044964\n",
       "13                  MCC         0.726569     0.079507\n",
       "14                  NPV         0.902030     0.024389\n",
       "15              ROC_AUC         0.828189     0.044964"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.658472</td>\n",
       "      <td>0.663120</td>\n",
       "      <td>0.695179</td>\n",
       "      <td>0.754528</td>\n",
       "      <td>0.690113</td>\n",
       "      <td>0.640676</td>\n",
       "      <td>0.679932</td>\n",
       "      <td>0.696736</td>\n",
       "      <td>0.586564</td>\n",
       "      <td>0.748406</td>\n",
       "      <td>0.681373</td>\n",
       "      <td>0.049276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>41.800000</td>\n",
       "      <td>4.467164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>198.500000</td>\n",
       "      <td>1.715938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1.911951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>4.477102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.917910</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.914179</td>\n",
       "      <td>0.861940</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>0.896642</td>\n",
       "      <td>0.018028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.931484</td>\n",
       "      <td>0.040693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.629553</td>\n",
       "      <td>0.067142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.970100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975400</td>\n",
       "      <td>0.980100</td>\n",
       "      <td>0.975200</td>\n",
       "      <td>0.985200</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.990100</td>\n",
       "      <td>0.984620</td>\n",
       "      <td>0.009480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.773109</td>\n",
       "      <td>0.752294</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.697248</td>\n",
       "      <td>0.720721</td>\n",
       "      <td>0.796460</td>\n",
       "      <td>0.633663</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.749401</td>\n",
       "      <td>0.053942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.913300</td>\n",
       "      <td>0.903976</td>\n",
       "      <td>0.894716</td>\n",
       "      <td>0.889961</td>\n",
       "      <td>0.884949</td>\n",
       "      <td>0.866349</td>\n",
       "      <td>0.876244</td>\n",
       "      <td>0.909448</td>\n",
       "      <td>0.845672</td>\n",
       "      <td>0.904564</td>\n",
       "      <td>0.888918</td>\n",
       "      <td>0.021287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.878982</td>\n",
       "      <td>0.864555</td>\n",
       "      <td>0.854181</td>\n",
       "      <td>0.844531</td>\n",
       "      <td>0.835252</td>\n",
       "      <td>0.809982</td>\n",
       "      <td>0.823890</td>\n",
       "      <td>0.871043</td>\n",
       "      <td>0.774303</td>\n",
       "      <td>0.864555</td>\n",
       "      <td>0.842127</td>\n",
       "      <td>0.032257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.825871</td>\n",
       "      <td>0.828358</td>\n",
       "      <td>0.801471</td>\n",
       "      <td>0.803069</td>\n",
       "      <td>0.773632</td>\n",
       "      <td>0.790654</td>\n",
       "      <td>0.838765</td>\n",
       "      <td>0.734998</td>\n",
       "      <td>0.828383</td>\n",
       "      <td>0.807097</td>\n",
       "      <td>0.033981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.774701</td>\n",
       "      <td>0.755009</td>\n",
       "      <td>0.719092</td>\n",
       "      <td>0.728852</td>\n",
       "      <td>0.689003</td>\n",
       "      <td>0.651858</td>\n",
       "      <td>0.670032</td>\n",
       "      <td>0.757357</td>\n",
       "      <td>0.600940</td>\n",
       "      <td>0.750417</td>\n",
       "      <td>0.709726</td>\n",
       "      <td>0.055616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.908700</td>\n",
       "      <td>0.896900</td>\n",
       "      <td>0.902800</td>\n",
       "      <td>0.881100</td>\n",
       "      <td>0.891900</td>\n",
       "      <td>0.871700</td>\n",
       "      <td>0.883400</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.854100</td>\n",
       "      <td>0.900900</td>\n",
       "      <td>0.890060</td>\n",
       "      <td>0.017622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.825871</td>\n",
       "      <td>0.828358</td>\n",
       "      <td>0.801471</td>\n",
       "      <td>0.803069</td>\n",
       "      <td>0.773632</td>\n",
       "      <td>0.790654</td>\n",
       "      <td>0.838765</td>\n",
       "      <td>0.734998</td>\n",
       "      <td>0.828383</td>\n",
       "      <td>0.807097</td>\n",
       "      <td>0.033981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.658472    0.663120    0.695179    0.754528   \n",
       "1                    TP   47.000000   44.000000   46.000000   41.000000   \n",
       "2                    TN  199.000000  200.000000  195.000000  200.000000   \n",
       "3                    FP    2.000000    1.000000    6.000000    0.000000   \n",
       "4                    FN   20.000000   23.000000   21.000000   27.000000   \n",
       "5              Accuracy    0.917910    0.910448    0.899254    0.899254   \n",
       "6             Precision    0.959184    0.977778    0.884615    1.000000   \n",
       "7           Sensitivity    0.701493    0.656716    0.686567    0.602941   \n",
       "8           Specificity    0.990000    0.995000    0.970100    1.000000   \n",
       "9              F1 score    0.810345    0.785714    0.773109    0.752294   \n",
       "10  F1 score (weighted)    0.913300    0.903976    0.894716    0.889961   \n",
       "11     F1 score (macro)    0.878982    0.864555    0.854181    0.844531   \n",
       "12    Balanced Accuracy    0.845771    0.825871    0.828358    0.801471   \n",
       "13                  MCC    0.774701    0.755009    0.719092    0.728852   \n",
       "14                  NPV    0.908700    0.896900    0.902800    0.881100   \n",
       "15              ROC_AUC    0.845771    0.825871    0.828358    0.801471   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.690113    0.640676    0.679932    0.696736    0.586564    0.748406   \n",
       "1    41.000000   38.000000   40.000000   45.000000   32.000000   44.000000   \n",
       "2   198.000000  197.000000  197.000000  200.000000  199.000000  200.000000   \n",
       "3     5.000000    4.000000    5.000000    3.000000    3.000000    2.000000   \n",
       "4    24.000000   29.000000   26.000000   20.000000   34.000000   22.000000   \n",
       "5     0.891791    0.876866    0.884328    0.914179    0.861940    0.910448   \n",
       "6     0.891304    0.904762    0.888889    0.937500    0.914286    0.956522   \n",
       "7     0.630769    0.567164    0.606061    0.692308    0.484848    0.666667   \n",
       "8     0.975400    0.980100    0.975200    0.985200    0.985100    0.990100   \n",
       "9     0.738739    0.697248    0.720721    0.796460    0.633663    0.785714   \n",
       "10    0.884949    0.866349    0.876244    0.909448    0.845672    0.904564   \n",
       "11    0.835252    0.809982    0.823890    0.871043    0.774303    0.864555   \n",
       "12    0.803069    0.773632    0.790654    0.838765    0.734998    0.828383   \n",
       "13    0.689003    0.651858    0.670032    0.757357    0.600940    0.750417   \n",
       "14    0.891900    0.871700    0.883400    0.909100    0.854100    0.900900   \n",
       "15    0.803069    0.773632    0.790654    0.838765    0.734998    0.828383   \n",
       "\n",
       "           ave       std  \n",
       "0     0.681373  0.049276  \n",
       "1    41.800000  4.467164  \n",
       "2   198.500000  1.715938  \n",
       "3     3.100000  1.911951  \n",
       "4    24.600000  4.477102  \n",
       "5     0.896642  0.018028  \n",
       "6     0.931484  0.040693  \n",
       "7     0.629553  0.067142  \n",
       "8     0.984620  0.009480  \n",
       "9     0.749401  0.053942  \n",
       "10    0.888918  0.021287  \n",
       "11    0.842127  0.032257  \n",
       "12    0.807097  0.033981  \n",
       "13    0.709726  0.055616  \n",
       "14    0.890060  0.017622  \n",
       "15    0.807097  0.033981  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test['std'] = mat_met_knn_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_knn0</th>\n",
       "      <th>y_pred_knn1</th>\n",
       "      <th>y_pred_knn2</th>\n",
       "      <th>y_pred_knn3</th>\n",
       "      <th>y_pred_knn4</th>\n",
       "      <th>y_pred_knn_ave</th>\n",
       "      <th>y_pred_knn_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4635479</td>\n",
       "      <td>0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.914435</td>\n",
       "      <td>0.918282</td>\n",
       "      <td>0.918282</td>\n",
       "      <td>0.942416</td>\n",
       "      <td>0.087025</td>\n",
       "      <td>0.700073</td>\n",
       "      <td>0.330197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4299417</td>\n",
       "      <td>1</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.740447</td>\n",
       "      <td>0.740447</td>\n",
       "      <td>0.740447</td>\n",
       "      <td>0.740447</td>\n",
       "      <td>1.032701</td>\n",
       "      <td>0.829081</td>\n",
       "      <td>0.126268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4225331</td>\n",
       "      <td>2</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.679473</td>\n",
       "      <td>1.704118</td>\n",
       "      <td>1.679473</td>\n",
       "      <td>1.679473</td>\n",
       "      <td>1.346038</td>\n",
       "      <td>1.488096</td>\n",
       "      <td>0.315370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL1094710</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.252342</td>\n",
       "      <td>0.199999</td>\n",
       "      <td>0.252342</td>\n",
       "      <td>0.274627</td>\n",
       "      <td>0.274627</td>\n",
       "      <td>0.283990</td>\n",
       "      <td>0.078307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3287256</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.092652</td>\n",
       "      <td>0.010387</td>\n",
       "      <td>0.092652</td>\n",
       "      <td>0.092652</td>\n",
       "      <td>0.092652</td>\n",
       "      <td>0.008499</td>\n",
       "      <td>0.154333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>CHEMBL3769491</td>\n",
       "      <td>1334</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.584428</td>\n",
       "      <td>0.206291</td>\n",
       "      <td>-0.584428</td>\n",
       "      <td>-0.013141</td>\n",
       "      <td>-0.584428</td>\n",
       "      <td>-0.145023</td>\n",
       "      <td>0.486023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>CHEMBL482095</td>\n",
       "      <td>1335</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.087628</td>\n",
       "      <td>0.884399</td>\n",
       "      <td>1.092275</td>\n",
       "      <td>1.048993</td>\n",
       "      <td>1.087628</td>\n",
       "      <td>0.940154</td>\n",
       "      <td>0.235141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>CHEMBL4095596</td>\n",
       "      <td>1336</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.626439</td>\n",
       "      <td>1.305801</td>\n",
       "      <td>1.305801</td>\n",
       "      <td>1.626439</td>\n",
       "      <td>1.626439</td>\n",
       "      <td>1.378486</td>\n",
       "      <td>0.303643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>CHEMBL4072925</td>\n",
       "      <td>1337</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.425249</td>\n",
       "      <td>0.533988</td>\n",
       "      <td>0.533988</td>\n",
       "      <td>0.533988</td>\n",
       "      <td>0.279343</td>\n",
       "      <td>0.487759</td>\n",
       "      <td>0.108968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>CHEMBL3774993</td>\n",
       "      <td>1338</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.478063</td>\n",
       "      <td>0.305444</td>\n",
       "      <td>0.478063</td>\n",
       "      <td>0.478063</td>\n",
       "      <td>0.478063</td>\n",
       "      <td>0.531283</td>\n",
       "      <td>0.206076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1339 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_knn0  y_pred_knn1  \\\n",
       "0         CHEMBL4635479            0     0.42     0.914435     0.918282   \n",
       "1         CHEMBL4299417            1     0.98     0.740447     0.740447   \n",
       "2         CHEMBL4225331            2     0.84     1.679473     1.704118   \n",
       "3         CHEMBL1094710            3     0.45     0.252342     0.199999   \n",
       "4         CHEMBL3287256            4    -0.33     0.092652     0.010387   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "1334      CHEMBL3769491         1334     0.69    -0.584428     0.206291   \n",
       "1335       CHEMBL482095         1335     0.44     1.087628     0.884399   \n",
       "1336      CHEMBL4095596         1336     0.78     1.626439     1.305801   \n",
       "1337      CHEMBL4072925         1337     0.62     0.425249     0.533988   \n",
       "1338      CHEMBL3774993         1338     0.97     0.478063     0.305444   \n",
       "\n",
       "      y_pred_knn2  y_pred_knn3  y_pred_knn4  y_pred_knn_ave  y_pred_knn_std  \n",
       "0        0.918282     0.942416     0.087025        0.700073        0.330197  \n",
       "1        0.740447     0.740447     1.032701        0.829081        0.126268  \n",
       "2        1.679473     1.679473     1.346038        1.488096        0.315370  \n",
       "3        0.252342     0.274627     0.274627        0.283990        0.078307  \n",
       "4        0.092652     0.092652     0.092652        0.008499        0.154333  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "1334    -0.584428    -0.013141    -0.584428       -0.145023        0.486023  \n",
       "1335     1.092275     1.048993     1.087628        0.940154        0.235141  \n",
       "1336     1.305801     1.626439     1.626439        1.378486        0.303643  \n",
       "1337     0.533988     0.533988     0.279343        0.487759        0.108968  \n",
       "1338     0.478063     0.478063     0.478063        0.531283        0.206076  \n",
       "\n",
       "[1339 rows x 10 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=24,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_knn_cat = np.where(((y_pred_optimized_knn >= 2) | (y_pred_optimized_knn <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_knn_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_knn))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "knn_run0 = data_knn[['y_test_idx0', 'y_test0', 'y_pred_knn0']]\n",
    "knn_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "knn_run0.reset_index(inplace=True, drop=True)\n",
    "knn_run1 = data_knn[['y_test_idx1', 'y_test1', 'y_pred_knn1']]\n",
    "knn_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "knn_run1.reset_index(inplace=True, drop=True)\n",
    "knn_run2 = data_knn[['y_test_idx2', 'y_test2', 'y_pred_knn2']]\n",
    "knn_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "knn_run2.reset_index(inplace=True, drop=True)\n",
    "knn_run3 = data_knn[['y_test_idx3', 'y_test3', 'y_pred_knn3']]\n",
    "knn_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "knn_run3.reset_index(inplace=True, drop=True)\n",
    "knn_run4 = data_knn[['y_test_idx4', 'y_test4', 'y_pred_knn4']]\n",
    "knn_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "knn_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "knn_5preds = pd.concat([chembl_id, knn_run0, knn_run1, knn_run2, knn_run3, knn_run4], axis=1)\n",
    "knn_5preds = knn_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_knn0', 'y_pred_knn1', 'y_pred_knn2', 'y_pred_knn3', 'y_pred_knn4']]\n",
    "knn_5preds['y_pred_knn_ave'] = knn_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "knn_5preds['y_pred_knn_std'] = knn_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "knn_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c149767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_knn.to_csv(output/'mat_met_knn_opt.csv')\n",
    "knn_5preds.to_csv(output/'knn_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0bc43db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model r2_score 0.6764 with a standard deviation of 0.0519\n",
      "KNN optimized model r2_score 0.7045 with a standard deviation of 0.0427\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"KNN baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_knn_noSemiSel.joblib']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_reg, output/\"knn_reg.joblib\")\n",
    "#joblib.dump(optimized_knn, output/\"optimized_knn.joblib\")\n",
    "joblib.dump(optimizedCV_knn, output/\"optimizedCV_knn.joblib\")\n",
    "#loaded_rf = joblib.load(output/\"optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.687690     0.042416\n",
      "1                    TP        15.300000     3.164034\n",
      "2                    TN        99.500000     1.433721\n",
      "3                    FP         1.000000     1.414214\n",
      "4                    FN        18.100000     3.142893\n",
      "5              Accuracy         0.857356     0.027800\n",
      "6             Precision         0.941111     0.081826\n",
      "7           Sensitivity         0.458005     0.093422\n",
      "8           Specificity         0.990060     0.014095\n",
      "9              F1 score         0.611482     0.089867\n",
      "10  F1 score (weighted)         0.837472     0.034548\n",
      "11     F1 score (macro)         0.762018     0.052845\n",
      "12    Balanced Accuracy         0.724032     0.048776\n",
      "13                  MCC         0.592132     0.089655\n",
      "14                  NPV         0.846560     0.023497\n",
      "15              ROC_AUC         0.724032     0.048776\n",
      "CPU times: user 9.52 s, sys: 3.99 ms, total: 9.53 s\n",
      "Wall time: 9.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    \n",
    "    svm_reg.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:26:07,236]\u001b[0m A new study created in memory with name: SVM_regressor_CV\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:26:13,691]\u001b[0m Trial 0 finished with value: -0.019257078224670265 and parameters: {'C': 0.0078125, 'gamma': 3.0517578125e-05}. Best is trial 0 with value: -0.019257078224670265.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:26:20,149]\u001b[0m Trial 1 finished with value: 0.10961877399417279 and parameters: {'C': 0.0625, 'gamma': 0.001953125}. Best is trial 1 with value: 0.10961877399417279.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:26:26,576]\u001b[0m Trial 2 finished with value: -0.01862556712736063 and parameters: {'C': 0.0078125, 'gamma': 0.25}. Best is trial 1 with value: 0.10961877399417279.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:26:33,087]\u001b[0m Trial 3 finished with value: -0.008202643248183806 and parameters: {'C': 0.0078125, 'gamma': 0.0009765625}. Best is trial 1 with value: 0.10961877399417279.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:26:39,770]\u001b[0m Trial 4 finished with value: 0.06932723807489813 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 1 with value: 0.10961877399417279.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:26:46,255]\u001b[0m Trial 5 finished with value: -0.018870016801938914 and parameters: {'C': 0.015625, 'gamma': 3.0517578125e-05}. Best is trial 1 with value: 0.10961877399417279.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:26:52,674]\u001b[0m Trial 6 finished with value: 0.328984692905146 and parameters: {'C': 4.0, 'gamma': 0.0001220703125}. Best is trial 6 with value: 0.328984692905146.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:26:59,193]\u001b[0m Trial 7 finished with value: 0.02475437492684964 and parameters: {'C': 0.125, 'gamma': 0.000244140625}. Best is trial 6 with value: 0.328984692905146.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:27:05,736]\u001b[0m Trial 8 finished with value: 0.5974440784862953 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 8 with value: 0.5974440784862953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:27:12,241]\u001b[0m Trial 9 finished with value: -0.010857901159966243 and parameters: {'C': 0.125, 'gamma': 1.0}. Best is trial 8 with value: 0.5974440784862953.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:27:18,578]\u001b[0m Trial 10 finished with value: 0.6189416691585135 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 10 with value: 0.6189416691585135.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:27:24,868]\u001b[0m Trial 11 finished with value: 0.6189416691585135 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 10 with value: 0.6189416691585135.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:27:31,158]\u001b[0m Trial 12 finished with value: 0.6189416691585135 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 10 with value: 0.6189416691585135.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:27:37,538]\u001b[0m Trial 13 finished with value: 0.3246140598116976 and parameters: {'C': 1.0, 'gamma': 0.00048828125}. Best is trial 10 with value: 0.6189416691585135.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:27:43,739]\u001b[0m Trial 14 finished with value: 0.6892330291825535 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 14 with value: 0.6892330291825535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:27:49,827]\u001b[0m Trial 15 finished with value: 0.6827464179889061 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 14 with value: 0.6892330291825535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:27:55,991]\u001b[0m Trial 16 finished with value: 0.6827464179889061 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 14 with value: 0.6892330291825535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:28:02,140]\u001b[0m Trial 17 finished with value: 0.6827464179889061 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 14 with value: 0.6892330291825535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:28:08,644]\u001b[0m Trial 18 finished with value: 0.15555255363516185 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 14 with value: 0.6892330291825535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:28:14,962]\u001b[0m Trial 19 finished with value: 0.5178307239190583 and parameters: {'C': 0.5, 'gamma': 0.00390625}. Best is trial 14 with value: 0.6892330291825535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:28:21,266]\u001b[0m Trial 20 finished with value: 0.5927859150058563 and parameters: {'C': 64.0, 'gamma': 6.103515625e-05}. Best is trial 14 with value: 0.6892330291825535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:28:27,308]\u001b[0m Trial 21 finished with value: 0.6827464179889061 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 14 with value: 0.6892330291825535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:28:34,021]\u001b[0m Trial 22 finished with value: 0.036909528789647705 and parameters: {'C': 2.0, 'gamma': 0.5}. Best is trial 14 with value: 0.6892330291825535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:28:40,606]\u001b[0m Trial 23 finished with value: 0.2655759459148146 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 14 with value: 0.6892330291825535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:28:47,321]\u001b[0m Trial 24 finished with value: 0.027719801532262556 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 14 with value: 0.6892330291825535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:28:53,713]\u001b[0m Trial 25 finished with value: 0.49014223906798593 and parameters: {'C': 0.25, 'gamma': 0.0078125}. Best is trial 14 with value: 0.6892330291825535.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:28:59,923]\u001b[0m Trial 26 finished with value: 0.6894882019239421 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:29:06,649]\u001b[0m Trial 27 finished with value: 0.0262866824314405 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:29:13,599]\u001b[0m Trial 28 finished with value: 0.026080027553332975 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:29:19,712]\u001b[0m Trial 29 finished with value: 0.6797387869253806 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:29:26,104]\u001b[0m Trial 30 finished with value: 0.06812706347244427 and parameters: {'C': 0.0625, 'gamma': 0.0625}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:29:32,143]\u001b[0m Trial 31 finished with value: 0.6827464179889061 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:29:38,193]\u001b[0m Trial 32 finished with value: 0.6827464179889061 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:29:44,379]\u001b[0m Trial 33 finished with value: 0.6892834510003721 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:29:50,849]\u001b[0m Trial 34 finished with value: 0.6149017646116494 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:29:56,990]\u001b[0m Trial 35 finished with value: 0.6892834510003721 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:30:03,117]\u001b[0m Trial 36 finished with value: 0.6892834510003721 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:30:09,240]\u001b[0m Trial 37 finished with value: 0.6892834510003721 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:30:15,614]\u001b[0m Trial 38 finished with value: 0.33013107661828506 and parameters: {'C': 16.0, 'gamma': 3.0517578125e-05}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:30:21,874]\u001b[0m Trial 39 finished with value: 0.5322868843803845 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:30:28,275]\u001b[0m Trial 40 finished with value: -0.01862556712736063 and parameters: {'C': 0.0078125, 'gamma': 0.25}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:30:34,401]\u001b[0m Trial 41 finished with value: 0.6892834510003721 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:30:40,537]\u001b[0m Trial 42 finished with value: 0.6892834510003721 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:30:46,848]\u001b[0m Trial 43 finished with value: 0.5931635895427824 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:30:53,286]\u001b[0m Trial 44 finished with value: -0.018263234287448603 and parameters: {'C': 0.015625, 'gamma': 1.0}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:31:00,308]\u001b[0m Trial 45 finished with value: 0.026075537378603665 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:31:06,640]\u001b[0m Trial 46 finished with value: 0.6245674096807596 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:31:12,703]\u001b[0m Trial 47 finished with value: 0.6681235462898368 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:31:19,164]\u001b[0m Trial 48 finished with value: 0.01669420879906488 and parameters: {'C': 0.125, 'gamma': 0.125}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:31:25,941]\u001b[0m Trial 49 finished with value: 0.02628656856290158 and parameters: {'C': 64.0, 'gamma': 4.0}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6895\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_regressor_CV\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.718703\n",
      "1                    TP   39.000000\n",
      "2                    TN  199.000000\n",
      "3                    FP    2.000000\n",
      "4                    FN   28.000000\n",
      "5              Accuracy    0.888060\n",
      "6             Precision    0.951220\n",
      "7           Sensitivity    0.582090\n",
      "8           Specificity    0.990000\n",
      "9              F1 score    0.722222\n",
      "10  F1 score (weighted)    0.877985\n",
      "11     F1 score (macro)    0.826064\n",
      "12    Balanced Accuracy    0.786070\n",
      "13                  MCC    0.688228\n",
      "14                  NPV    0.876700\n",
      "15              ROC_AUC    0.786070\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_svm_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_svm_0_cat = np.where(((y_pred_svm_0 >= 2) | (y_pred_svm_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_svm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:31:33,213]\u001b[0m Trial 50 finished with value: 0.025613242497355294 and parameters: {'C': 0.03125, 'gamma': 0.0625}. Best is trial 26 with value: 0.6894882019239421.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:31:39,315]\u001b[0m Trial 51 finished with value: 0.7062268722272902 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.7062268722272902.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:31:45,411]\u001b[0m Trial 52 finished with value: 0.7062268722272902 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.7062268722272902.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:31:51,854]\u001b[0m Trial 53 finished with value: 0.6625008496173039 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 51 with value: 0.7062268722272902.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:31:58,070]\u001b[0m Trial 54 finished with value: 0.5312022280805698 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 51 with value: 0.7062268722272902.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:32:04,604]\u001b[0m Trial 55 finished with value: 0.02579887824617829 and parameters: {'C': 0.5, 'gamma': 6.103515625e-05}. Best is trial 51 with value: 0.7062268722272902.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:32:10,706]\u001b[0m Trial 56 finished with value: 0.707509091271446 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:32:17,415]\u001b[0m Trial 57 finished with value: 0.029114319243875152 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:32:23,524]\u001b[0m Trial 58 finished with value: 0.6867348387234312 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:32:30,017]\u001b[0m Trial 59 finished with value: -0.01855871237496043 and parameters: {'C': 0.0625, 'gamma': 2.0}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:32:36,314]\u001b[0m Trial 60 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:32:42,585]\u001b[0m Trial 61 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:32:48,862]\u001b[0m Trial 62 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:32:55,119]\u001b[0m Trial 63 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:33:01,394]\u001b[0m Trial 64 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:33:07,631]\u001b[0m Trial 65 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:33:13,856]\u001b[0m Trial 66 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:33:20,077]\u001b[0m Trial 67 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:33:26,297]\u001b[0m Trial 68 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:33:32,520]\u001b[0m Trial 69 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:33:38,772]\u001b[0m Trial 70 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:33:44,985]\u001b[0m Trial 71 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:33:51,238]\u001b[0m Trial 72 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:33:57,507]\u001b[0m Trial 73 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:34:03,776]\u001b[0m Trial 74 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:34:10,280]\u001b[0m Trial 75 finished with value: 0.019644049535666765 and parameters: {'C': 0.015625, 'gamma': 0.001953125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:34:16,668]\u001b[0m Trial 76 finished with value: 0.4658198203696909 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:34:23,272]\u001b[0m Trial 77 finished with value: 0.042332498687659106 and parameters: {'C': 1.0, 'gamma': 0.25}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:34:29,635]\u001b[0m Trial 78 finished with value: 0.5424193898982352 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:34:36,140]\u001b[0m Trial 79 finished with value: -0.022012830064437994 and parameters: {'C': 0.0078125, 'gamma': 3.0517578125e-05}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:34:42,455]\u001b[0m Trial 80 finished with value: 0.685830327139698 and parameters: {'C': 32.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:34:48,737]\u001b[0m Trial 81 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:34:55,106]\u001b[0m Trial 82 finished with value: 0.5428782849236198 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:35:01,366]\u001b[0m Trial 83 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:35:07,879]\u001b[0m Trial 84 finished with value: -0.014971907450463862 and parameters: {'C': 0.125, 'gamma': 1.0}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:35:14,392]\u001b[0m Trial 85 finished with value: 0.022788962240739653 and parameters: {'C': 0.03125, 'gamma': 0.0009765625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:35:20,709]\u001b[0m Trial 86 finished with value: 0.6859254095373966 and parameters: {'C': 64.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:35:27,055]\u001b[0m Trial 87 finished with value: 0.5293161175156137 and parameters: {'C': 0.5, 'gamma': 0.00390625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:35:33,426]\u001b[0m Trial 88 finished with value: 0.34827361787505506 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:35:39,851]\u001b[0m Trial 89 finished with value: 0.04498401151062572 and parameters: {'C': 0.25, 'gamma': 0.125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:35:46,809]\u001b[0m Trial 90 finished with value: 0.018293342298719106 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:35:53,065]\u001b[0m Trial 91 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:35:59,312]\u001b[0m Trial 92 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:36:05,587]\u001b[0m Trial 93 finished with value: 0.69094511652316 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:36:12,316]\u001b[0m Trial 94 finished with value: 0.018376857250573954 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:36:18,818]\u001b[0m Trial 95 finished with value: -0.0174808303784161 and parameters: {'C': 0.0625, 'gamma': 0.5}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:36:25,553]\u001b[0m Trial 96 finished with value: 0.01900604110041928 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:36:32,018]\u001b[0m Trial 97 finished with value: -0.009798879180083885 and parameters: {'C': 0.0078125, 'gamma': 0.0625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:36:38,343]\u001b[0m Trial 98 finished with value: 0.6864848138097287 and parameters: {'C': 128.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:36:44,859]\u001b[0m Trial 99 finished with value: 0.07562727609920641 and parameters: {'C': 0.015625, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7075\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.718703    0.680756\n",
      "1                    TP   39.000000   35.000000\n",
      "2                    TN  199.000000  199.000000\n",
      "3                    FP    2.000000    2.000000\n",
      "4                    FN   28.000000   32.000000\n",
      "5              Accuracy    0.888060    0.873134\n",
      "6             Precision    0.951220    0.945946\n",
      "7           Sensitivity    0.582090    0.522388\n",
      "8           Specificity    0.990000    0.990000\n",
      "9              F1 score    0.722222    0.673077\n",
      "10  F1 score (weighted)    0.877985    0.859241\n",
      "11     F1 score (macro)    0.826064    0.797187\n",
      "12    Balanced Accuracy    0.786070    0.756219\n",
      "13                  MCC    0.688228    0.643235\n",
      "14                  NPV    0.876700    0.861500\n",
      "15              ROC_AUC    0.786070    0.756219\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_svm_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet1_cat = np.where(((Y_testSet1>=2) | (Y_testSet1<=-2)), 1, 0) \n",
    "y_pred_svm_1_cat = np.where(((y_pred_svm_1 >= 2) | (y_pred_svm_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_svm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:36:51,964]\u001b[0m Trial 100 finished with value: 0.674731930388473 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:36:58,298]\u001b[0m Trial 101 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:37:04,626]\u001b[0m Trial 102 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:37:10,934]\u001b[0m Trial 103 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:37:17,235]\u001b[0m Trial 104 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:37:23,396]\u001b[0m Trial 105 finished with value: 0.6600402085406809 and parameters: {'C': 4.0, 'gamma': 0.03125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:37:29,769]\u001b[0m Trial 106 finished with value: 0.4574209063045682 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:37:35,904]\u001b[0m Trial 107 finished with value: 0.6300083812500703 and parameters: {'C': 1.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:37:42,383]\u001b[0m Trial 108 finished with value: 0.0525718291732587 and parameters: {'C': 0.03125, 'gamma': 0.001953125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:37:48,713]\u001b[0m Trial 109 finished with value: 0.40746085954594646 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:37:55,094]\u001b[0m Trial 110 finished with value: 0.4579452480154583 and parameters: {'C': 32.0, 'gamma': 3.0517578125e-05}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:38:01,374]\u001b[0m Trial 111 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:38:07,659]\u001b[0m Trial 112 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:38:14,382]\u001b[0m Trial 113 finished with value: 0.05469139493815221 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:38:20,683]\u001b[0m Trial 114 finished with value: 0.613693040446335 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:38:27,017]\u001b[0m Trial 115 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:38:33,508]\u001b[0m Trial 116 finished with value: 0.12814920552489287 and parameters: {'C': 0.5, 'gamma': 0.000244140625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:38:39,850]\u001b[0m Trial 117 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:38:46,293]\u001b[0m Trial 118 finished with value: 0.6496307936753837 and parameters: {'C': 64.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:38:52,728]\u001b[0m Trial 119 finished with value: 0.2120884884504548 and parameters: {'C': 0.25, 'gamma': 0.0009765625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:38:58,952]\u001b[0m Trial 120 finished with value: 0.674731930388473 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:39:05,250]\u001b[0m Trial 121 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:39:11,994]\u001b[0m Trial 122 finished with value: 0.015815565883870508 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:39:18,101]\u001b[0m Trial 123 finished with value: 0.6512931125406313 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:39:24,384]\u001b[0m Trial 124 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:39:30,834]\u001b[0m Trial 125 finished with value: 0.6271126357327509 and parameters: {'C': 16.0, 'gamma': 0.00390625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:39:37,147]\u001b[0m Trial 126 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:39:43,905]\u001b[0m Trial 127 finished with value: 0.012043737785426622 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:39:50,324]\u001b[0m Trial 128 finished with value: -0.0074193700962272065 and parameters: {'C': 0.0625, 'gamma': 0.125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:39:57,288]\u001b[0m Trial 129 finished with value: 0.01190041503538658 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:40:03,564]\u001b[0m Trial 130 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:40:09,848]\u001b[0m Trial 131 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:40:16,144]\u001b[0m Trial 132 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:40:22,540]\u001b[0m Trial 133 finished with value: 0.34683822532828085 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:40:29,284]\u001b[0m Trial 134 finished with value: 0.024079118101317132 and parameters: {'C': 128.0, 'gamma': 0.5}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:40:35,753]\u001b[0m Trial 135 finished with value: 0.08114277601222344 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:40:42,265]\u001b[0m Trial 136 finished with value: 0.027044788331043755 and parameters: {'C': 0.0078125, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:40:48,683]\u001b[0m Trial 137 finished with value: 0.6533922998215791 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:40:55,028]\u001b[0m Trial 138 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:41:01,282]\u001b[0m Trial 139 finished with value: 0.674731930388473 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:41:07,615]\u001b[0m Trial 140 finished with value: 0.5407830166995747 and parameters: {'C': 4.0, 'gamma': 0.0625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:41:13,955]\u001b[0m Trial 141 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:41:20,688]\u001b[0m Trial 142 finished with value: 0.01294646141344602 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:41:26,998]\u001b[0m Trial 143 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:41:33,160]\u001b[0m Trial 144 finished with value: 0.6278507261861871 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:41:39,493]\u001b[0m Trial 145 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:41:45,911]\u001b[0m Trial 146 finished with value: 0.6504462541224447 and parameters: {'C': 32.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:41:52,242]\u001b[0m Trial 147 finished with value: 0.656957346233848 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:41:58,692]\u001b[0m Trial 148 finished with value: 0.20057317957044846 and parameters: {'C': 0.125, 'gamma': 0.001953125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:42:05,205]\u001b[0m Trial 149 finished with value: -0.017883868844243356 and parameters: {'C': 0.03125, 'gamma': 0.0001220703125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7075\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.718703    0.680756    0.734606\n",
      "1                    TP   39.000000   35.000000   35.000000\n",
      "2                    TN  199.000000  199.000000  199.000000\n",
      "3                    FP    2.000000    2.000000    2.000000\n",
      "4                    FN   28.000000   32.000000   32.000000\n",
      "5              Accuracy    0.888060    0.873134    0.873134\n",
      "6             Precision    0.951220    0.945946    0.945946\n",
      "7           Sensitivity    0.582090    0.522388    0.522388\n",
      "8           Specificity    0.990000    0.990000    0.990000\n",
      "9              F1 score    0.722222    0.673077    0.673077\n",
      "10  F1 score (weighted)    0.877985    0.859241    0.859241\n",
      "11     F1 score (macro)    0.826064    0.797187    0.797187\n",
      "12    Balanced Accuracy    0.786070    0.756219    0.756219\n",
      "13                  MCC    0.688228    0.643235    0.643235\n",
      "14                  NPV    0.876700    0.861500    0.861500\n",
      "15              ROC_AUC    0.786070    0.756219    0.756219\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_svm_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet2_cat = np.where(((Y_testSet2>=2) | (Y_testSet2<=-2)), 1, 0) \n",
    "y_pred_svm_2_cat = np.where(((y_pred_svm_2 >= 2) | (y_pred_svm_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_svm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:42:12,548]\u001b[0m Trial 150 finished with value: 0.3508189232816316 and parameters: {'C': 16.0, 'gamma': 3.0517578125e-05}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:42:18,805]\u001b[0m Trial 151 finished with value: 0.6809448526968239 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:42:25,064]\u001b[0m Trial 152 finished with value: 0.6809448526968239 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:42:31,328]\u001b[0m Trial 153 finished with value: 0.6809448526968239 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:42:38,056]\u001b[0m Trial 154 finished with value: 0.05457579715479015 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:42:44,325]\u001b[0m Trial 155 finished with value: 0.6809448526968239 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:42:50,463]\u001b[0m Trial 156 finished with value: 0.6027014093499321 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:42:57,057]\u001b[0m Trial 157 finished with value: 0.6050364240592983 and parameters: {'C': 64.0, 'gamma': 0.00048828125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:43:03,373]\u001b[0m Trial 158 finished with value: 0.5316155212450211 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:43:09,460]\u001b[0m Trial 159 finished with value: 0.6671066987679165 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:43:16,164]\u001b[0m Trial 160 finished with value: 0.020223221162310755 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:43:22,368]\u001b[0m Trial 161 finished with value: 0.6809448526968239 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:43:28,715]\u001b[0m Trial 162 finished with value: 0.4917215283600692 and parameters: {'C': 0.25, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:43:34,976]\u001b[0m Trial 163 finished with value: 0.6809448526968239 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:43:41,231]\u001b[0m Trial 164 finished with value: 0.6809448526968239 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:43:47,470]\u001b[0m Trial 165 finished with value: 0.6809448526968239 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:43:53,849]\u001b[0m Trial 166 finished with value: 0.27218892464994565 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:44:00,168]\u001b[0m Trial 167 finished with value: 0.6808851289933003 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:44:06,477]\u001b[0m Trial 168 finished with value: 0.6209774113909561 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:44:12,756]\u001b[0m Trial 169 finished with value: 0.6588438939753122 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:44:19,350]\u001b[0m Trial 170 finished with value: 0.2508972800477998 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:44:25,595]\u001b[0m Trial 171 finished with value: 0.6809448526968239 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:44:31,820]\u001b[0m Trial 172 finished with value: 0.6809448526968239 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:44:38,045]\u001b[0m Trial 173 finished with value: 0.6809448526968239 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:44:45,016]\u001b[0m Trial 174 finished with value: 0.015625029234453934 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:44:51,361]\u001b[0m Trial 175 finished with value: 0.6809160891689984 and parameters: {'C': 128.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:44:57,877]\u001b[0m Trial 176 finished with value: -0.01711604069186423 and parameters: {'C': 0.0078125, 'gamma': 6.103515625e-05}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:45:04,645]\u001b[0m Trial 177 finished with value: 0.01580198479646039 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:45:10,933]\u001b[0m Trial 178 finished with value: 0.6809448526968239 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:45:17,472]\u001b[0m Trial 179 finished with value: -0.016860478088501107 and parameters: {'C': 0.015625, 'gamma': 0.5}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:45:23,677]\u001b[0m Trial 180 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:45:29,863]\u001b[0m Trial 181 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:45:36,066]\u001b[0m Trial 182 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:45:42,358]\u001b[0m Trial 183 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:45:48,599]\u001b[0m Trial 184 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:45:54,784]\u001b[0m Trial 185 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:46:00,928]\u001b[0m Trial 186 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:46:07,111]\u001b[0m Trial 187 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:46:13,295]\u001b[0m Trial 188 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:46:19,484]\u001b[0m Trial 189 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:46:25,679]\u001b[0m Trial 190 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:46:31,896]\u001b[0m Trial 191 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:46:38,114]\u001b[0m Trial 192 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:46:44,314]\u001b[0m Trial 193 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:46:50,526]\u001b[0m Trial 194 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:46:56,756]\u001b[0m Trial 195 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:47:02,992]\u001b[0m Trial 196 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:47:09,177]\u001b[0m Trial 197 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:47:15,417]\u001b[0m Trial 198 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:47:21,652]\u001b[0m Trial 199 finished with value: 0.7004569328124866 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7075\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.718703    0.680756    0.734606    0.718255\n",
      "1                    TP   39.000000   35.000000   35.000000   35.000000\n",
      "2                    TN  199.000000  199.000000  199.000000  198.000000\n",
      "3                    FP    2.000000    2.000000    2.000000    2.000000\n",
      "4                    FN   28.000000   32.000000   32.000000   33.000000\n",
      "5              Accuracy    0.888060    0.873134    0.873134    0.869403\n",
      "6             Precision    0.951220    0.945946    0.945946    0.945946\n",
      "7           Sensitivity    0.582090    0.522388    0.522388    0.514706\n",
      "8           Specificity    0.990000    0.990000    0.990000    0.990000\n",
      "9              F1 score    0.722222    0.673077    0.673077    0.666667\n",
      "10  F1 score (weighted)    0.877985    0.859241    0.859241    0.854821\n",
      "11     F1 score (macro)    0.826064    0.797187    0.797187    0.792730\n",
      "12    Balanced Accuracy    0.786070    0.756219    0.756219    0.752353\n",
      "13                  MCC    0.688228    0.643235    0.643235    0.636650\n",
      "14                  NPV    0.876700    0.861500    0.861500    0.857100\n",
      "15              ROC_AUC    0.786070    0.756219    0.756219    0.752353\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_svm_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet3_cat = np.where(((Y_testSet3>=2) | (Y_testSet3<=-2)), 1, 0) \n",
    "y_pred_svm_3_cat = np.where(((y_pred_svm_3 >= 2) | (y_pred_svm_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_svm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:47:28,686]\u001b[0m Trial 200 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:47:34,826]\u001b[0m Trial 201 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:47:40,937]\u001b[0m Trial 202 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:47:47,090]\u001b[0m Trial 203 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:47:53,242]\u001b[0m Trial 204 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:47:59,391]\u001b[0m Trial 205 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:48:05,542]\u001b[0m Trial 206 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:48:11,690]\u001b[0m Trial 207 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:48:17,835]\u001b[0m Trial 208 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:48:23,970]\u001b[0m Trial 209 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:48:30,083]\u001b[0m Trial 210 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:48:36,194]\u001b[0m Trial 211 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:48:42,301]\u001b[0m Trial 212 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:48:48,409]\u001b[0m Trial 213 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:48:54,520]\u001b[0m Trial 214 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:49:00,626]\u001b[0m Trial 215 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:49:06,737]\u001b[0m Trial 216 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:49:12,846]\u001b[0m Trial 217 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:49:18,994]\u001b[0m Trial 218 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:49:25,149]\u001b[0m Trial 219 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:49:31,302]\u001b[0m Trial 220 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:49:37,464]\u001b[0m Trial 221 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:49:43,583]\u001b[0m Trial 222 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:49:49,683]\u001b[0m Trial 223 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:49:55,789]\u001b[0m Trial 224 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:50:01,957]\u001b[0m Trial 225 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:50:08,078]\u001b[0m Trial 226 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:50:14,195]\u001b[0m Trial 227 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:50:20,337]\u001b[0m Trial 228 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:50:26,444]\u001b[0m Trial 229 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:50:32,607]\u001b[0m Trial 230 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:50:38,764]\u001b[0m Trial 231 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:50:44,920]\u001b[0m Trial 232 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:50:51,060]\u001b[0m Trial 233 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:50:57,195]\u001b[0m Trial 234 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:51:03,345]\u001b[0m Trial 235 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:51:09,452]\u001b[0m Trial 236 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:51:15,559]\u001b[0m Trial 237 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:51:21,668]\u001b[0m Trial 238 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:51:27,777]\u001b[0m Trial 239 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:51:33,929]\u001b[0m Trial 240 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:51:40,093]\u001b[0m Trial 241 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:51:46,257]\u001b[0m Trial 242 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:51:52,422]\u001b[0m Trial 243 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:51:59,164]\u001b[0m Trial 244 finished with value: 0.02056794294490083 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:52:05,306]\u001b[0m Trial 245 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:52:11,446]\u001b[0m Trial 246 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:52:17,606]\u001b[0m Trial 247 finished with value: 0.687994335685319 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:52:23,910]\u001b[0m Trial 248 finished with value: 0.5541776716264813 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:52:29,981]\u001b[0m Trial 249 finished with value: 0.6891879038179878 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7075\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.718703    0.680756    0.734606    0.718255   \n",
      "1                    TP   39.000000   35.000000   35.000000   35.000000   \n",
      "2                    TN  199.000000  199.000000  199.000000  198.000000   \n",
      "3                    FP    2.000000    2.000000    2.000000    2.000000   \n",
      "4                    FN   28.000000   32.000000   32.000000   33.000000   \n",
      "5              Accuracy    0.888060    0.873134    0.873134    0.869403   \n",
      "6             Precision    0.951220    0.945946    0.945946    0.945946   \n",
      "7           Sensitivity    0.582090    0.522388    0.522388    0.514706   \n",
      "8           Specificity    0.990000    0.990000    0.990000    0.990000   \n",
      "9              F1 score    0.722222    0.673077    0.673077    0.666667   \n",
      "10  F1 score (weighted)    0.877985    0.859241    0.859241    0.854821   \n",
      "11     F1 score (macro)    0.826064    0.797187    0.797187    0.792730   \n",
      "12    Balanced Accuracy    0.786070    0.756219    0.756219    0.752353   \n",
      "13                  MCC    0.688228    0.643235    0.643235    0.636650   \n",
      "14                  NPV    0.876700    0.861500    0.861500    0.857100   \n",
      "15              ROC_AUC    0.786070    0.756219    0.756219    0.752353   \n",
      "\n",
      "          Set4  \n",
      "0     0.723604  \n",
      "1    36.000000  \n",
      "2   201.000000  \n",
      "3     2.000000  \n",
      "4    29.000000  \n",
      "5     0.884328  \n",
      "6     0.947368  \n",
      "7     0.553846  \n",
      "8     0.990100  \n",
      "9     0.699029  \n",
      "10    0.872774  \n",
      "11    0.813718  \n",
      "12    0.771997  \n",
      "13    0.668410  \n",
      "14    0.873900  \n",
      "15    0.771997  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_svm_4)\n",
    "# now convert the resuls to binary with cutoff 6.4\n",
    "Y_testSet4_cat = np.where(((Y_testSet4>=2) | (Y_testSet4<=-2)), 1, 0) \n",
    "y_pred_svm_4_cat = np.where(((y_pred_svm_4 >= 2) | (y_pred_svm_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_svm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:52:36,892]\u001b[0m Trial 250 finished with value: 0.6921416579051758 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 56 with value: 0.707509091271446.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:52:42,939]\u001b[0m Trial 251 finished with value: 0.714053086907222 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:52:49,057]\u001b[0m Trial 252 finished with value: 0.714053086907222 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:52:55,160]\u001b[0m Trial 253 finished with value: 0.714053086907222 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:53:01,251]\u001b[0m Trial 254 finished with value: 0.714053086907222 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:53:07,356]\u001b[0m Trial 255 finished with value: 0.714053086907222 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:53:13,419]\u001b[0m Trial 256 finished with value: 0.714053086907222 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:53:19,544]\u001b[0m Trial 257 finished with value: 0.714053086907222 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:53:25,656]\u001b[0m Trial 258 finished with value: 0.714053086907222 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:53:31,771]\u001b[0m Trial 259 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:53:38,076]\u001b[0m Trial 260 finished with value: 0.6013087740580643 and parameters: {'C': 32.0, 'gamma': 0.0001220703125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:53:44,602]\u001b[0m Trial 261 finished with value: -0.015735507959094553 and parameters: {'C': 0.125, 'gamma': 3.0517578125e-05}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:53:50,649]\u001b[0m Trial 262 finished with value: 0.6794624962630544 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:53:57,416]\u001b[0m Trial 263 finished with value: 0.054319310065615044 and parameters: {'C': 64.0, 'gamma': 0.25}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:54:03,507]\u001b[0m Trial 264 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:54:10,149]\u001b[0m Trial 265 finished with value: 0.6581189362172982 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:54:16,232]\u001b[0m Trial 266 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:54:22,402]\u001b[0m Trial 267 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:54:29,163]\u001b[0m Trial 268 finished with value: 0.012635610599186919 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:54:35,362]\u001b[0m Trial 269 finished with value: 0.6421264991706875 and parameters: {'C': 32.0, 'gamma': 0.000244140625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:54:41,429]\u001b[0m Trial 270 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:54:47,962]\u001b[0m Trial 271 finished with value: 0.6542769222249528 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:54:54,033]\u001b[0m Trial 272 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:55:00,102]\u001b[0m Trial 273 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:55:06,175]\u001b[0m Trial 274 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:55:12,805]\u001b[0m Trial 275 finished with value: 0.2605933724147693 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:55:19,144]\u001b[0m Trial 276 finished with value: 0.6524055353092468 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:55:25,218]\u001b[0m Trial 277 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:55:31,786]\u001b[0m Trial 278 finished with value: 0.6712385286776866 and parameters: {'C': 32.0, 'gamma': 0.00390625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:55:38,872]\u001b[0m Trial 279 finished with value: 0.00984829408673149 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:55:44,947]\u001b[0m Trial 280 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:55:51,236]\u001b[0m Trial 281 finished with value: 0.5412384246996269 and parameters: {'C': 32.0, 'gamma': 6.103515625e-05}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:55:58,028]\u001b[0m Trial 282 finished with value: 0.009934705482970619 and parameters: {'C': 32.0, 'gamma': 4.0}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:56:04,141]\u001b[0m Trial 283 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:56:10,253]\u001b[0m Trial 284 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:56:17,010]\u001b[0m Trial 285 finished with value: 0.019938791522820354 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:56:23,076]\u001b[0m Trial 286 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:56:29,175]\u001b[0m Trial 287 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:56:35,258]\u001b[0m Trial 288 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:56:41,331]\u001b[0m Trial 289 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:56:47,420]\u001b[0m Trial 290 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:56:53,540]\u001b[0m Trial 291 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:56:59,629]\u001b[0m Trial 292 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:57:05,723]\u001b[0m Trial 293 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:57:11,825]\u001b[0m Trial 294 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:57:17,920]\u001b[0m Trial 295 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:57:24,006]\u001b[0m Trial 296 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:57:30,300]\u001b[0m Trial 297 finished with value: 0.5679695177280607 and parameters: {'C': 32.0, 'gamma': 0.0625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:57:36,371]\u001b[0m Trial 298 finished with value: 0.7139914868412571 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:57:43,105]\u001b[0m Trial 299 finished with value: 0.010522252188538195 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7141\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.718703    0.680756    0.734606    0.718255   \n",
      "1                    TP   39.000000   35.000000   35.000000   35.000000   \n",
      "2                    TN  199.000000  199.000000  199.000000  198.000000   \n",
      "3                    FP    2.000000    2.000000    2.000000    2.000000   \n",
      "4                    FN   28.000000   32.000000   32.000000   33.000000   \n",
      "5              Accuracy    0.888060    0.873134    0.873134    0.869403   \n",
      "6             Precision    0.951220    0.945946    0.945946    0.945946   \n",
      "7           Sensitivity    0.582090    0.522388    0.522388    0.514706   \n",
      "8           Specificity    0.990000    0.990000    0.990000    0.990000   \n",
      "9              F1 score    0.722222    0.673077    0.673077    0.666667   \n",
      "10  F1 score (weighted)    0.877985    0.859241    0.859241    0.854821   \n",
      "11     F1 score (macro)    0.826064    0.797187    0.797187    0.792730   \n",
      "12    Balanced Accuracy    0.786070    0.756219    0.756219    0.752353   \n",
      "13                  MCC    0.688228    0.643235    0.643235    0.636650   \n",
      "14                  NPV    0.876700    0.861500    0.861500    0.857100   \n",
      "15              ROC_AUC    0.786070    0.756219    0.756219    0.752353   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.723604    0.692007  \n",
      "1    36.000000   30.000000  \n",
      "2   201.000000  200.000000  \n",
      "3     2.000000    1.000000  \n",
      "4    29.000000   37.000000  \n",
      "5     0.884328    0.858209  \n",
      "6     0.947368    0.967742  \n",
      "7     0.553846    0.447761  \n",
      "8     0.990100    0.995000  \n",
      "9     0.699029    0.612245  \n",
      "10    0.872774    0.837993  \n",
      "11    0.813718    0.762743  \n",
      "12    0.771997    0.721393  \n",
      "13    0.668410    0.599480  \n",
      "14    0.873900    0.843900  \n",
      "15    0.771997    0.721393  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_svm_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "Y_testSet5_cat = np.where(((Y_testSet5>=2) | (Y_testSet5<=-2)), 1, 0) \n",
    "y_pred_svm_5_cat = np.where(((y_pred_svm_5 >= 2) | (y_pred_svm_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_svm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 03:57:50,167]\u001b[0m Trial 300 finished with value: 0.6756416123788325 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:57:56,399]\u001b[0m Trial 301 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:58:02,624]\u001b[0m Trial 302 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:58:08,871]\u001b[0m Trial 303 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:58:15,107]\u001b[0m Trial 304 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:58:21,360]\u001b[0m Trial 305 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:58:27,661]\u001b[0m Trial 306 finished with value: 0.5808814830905071 and parameters: {'C': 32.0, 'gamma': 0.0001220703125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:58:34,295]\u001b[0m Trial 307 finished with value: 0.6257113064114024 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:58:40,511]\u001b[0m Trial 308 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:58:46,937]\u001b[0m Trial 309 finished with value: 0.4501959373167111 and parameters: {'C': 32.0, 'gamma': 3.0517578125e-05}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:58:53,603]\u001b[0m Trial 310 finished with value: 0.06452150538439377 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:58:59,827]\u001b[0m Trial 311 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:59:06,157]\u001b[0m Trial 312 finished with value: 0.628795143600416 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:59:12,377]\u001b[0m Trial 313 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:59:18,606]\u001b[0m Trial 314 finished with value: 0.6167392400458422 and parameters: {'C': 32.0, 'gamma': 0.000244140625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:59:25,055]\u001b[0m Trial 315 finished with value: 0.165562259047877 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:59:31,769]\u001b[0m Trial 316 finished with value: 0.02076002947876502 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:59:37,990]\u001b[0m Trial 317 finished with value: 0.6041372522883701 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:59:44,136]\u001b[0m Trial 318 finished with value: 0.6860387185432385 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:59:50,765]\u001b[0m Trial 319 finished with value: 0.6244000586045222 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 03:59:57,012]\u001b[0m Trial 320 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:00:03,249]\u001b[0m Trial 321 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:00:09,614]\u001b[0m Trial 322 finished with value: 0.5169430494445103 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:00:16,226]\u001b[0m Trial 323 finished with value: 0.6425982809762614 and parameters: {'C': 32.0, 'gamma': 0.00390625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:00:23,203]\u001b[0m Trial 324 finished with value: 0.016386599440799086 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:00:29,728]\u001b[0m Trial 325 finished with value: 0.0014449173469779143 and parameters: {'C': 0.0625, 'gamma': 0.125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:00:35,968]\u001b[0m Trial 326 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:00:42,241]\u001b[0m Trial 327 finished with value: 0.615403715030933 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:00:48,829]\u001b[0m Trial 328 finished with value: -0.01646887035405995 and parameters: {'C': 0.015625, 'gamma': 4.0}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:00:55,074]\u001b[0m Trial 329 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:01:01,800]\u001b[0m Trial 330 finished with value: 0.029143306513549783 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:01:08,342]\u001b[0m Trial 331 finished with value: 0.04197464855476783 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:01:14,541]\u001b[0m Trial 332 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:01:20,787]\u001b[0m Trial 333 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:01:27,009]\u001b[0m Trial 334 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:01:33,216]\u001b[0m Trial 335 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:01:39,460]\u001b[0m Trial 336 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:01:45,584]\u001b[0m Trial 337 finished with value: 0.69580485157917 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:01:51,678]\u001b[0m Trial 338 finished with value: 0.6624436516212453 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:01:58,371]\u001b[0m Trial 339 finished with value: 0.017345504353164332 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:02:04,802]\u001b[0m Trial 340 finished with value: 0.40661596699313113 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:02:11,019]\u001b[0m Trial 341 finished with value: 0.6940586930121044 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:02:17,252]\u001b[0m Trial 342 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:02:23,666]\u001b[0m Trial 343 finished with value: 0.165562259047877 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 04:02:29,854]\u001b[0m Trial 344 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:02:36,077]\u001b[0m Trial 345 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:02:42,234]\u001b[0m Trial 346 finished with value: 0.6756416123788325 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:02:48,508]\u001b[0m Trial 347 finished with value: 0.5499500530623513 and parameters: {'C': 2.0, 'gamma': 0.0625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:02:54,761]\u001b[0m Trial 348 finished with value: 0.6935436516804071 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:03:01,267]\u001b[0m Trial 349 finished with value: 0.06974962032050205 and parameters: {'C': 0.5, 'gamma': 0.0001220703125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7141\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.718703    0.680756    0.734606    0.718255   \n",
      "1                    TP   39.000000   35.000000   35.000000   35.000000   \n",
      "2                    TN  199.000000  199.000000  199.000000  198.000000   \n",
      "3                    FP    2.000000    2.000000    2.000000    2.000000   \n",
      "4                    FN   28.000000   32.000000   32.000000   33.000000   \n",
      "5              Accuracy    0.888060    0.873134    0.873134    0.869403   \n",
      "6             Precision    0.951220    0.945946    0.945946    0.945946   \n",
      "7           Sensitivity    0.582090    0.522388    0.522388    0.514706   \n",
      "8           Specificity    0.990000    0.990000    0.990000    0.990000   \n",
      "9              F1 score    0.722222    0.673077    0.673077    0.666667   \n",
      "10  F1 score (weighted)    0.877985    0.859241    0.859241    0.854821   \n",
      "11     F1 score (macro)    0.826064    0.797187    0.797187    0.792730   \n",
      "12    Balanced Accuracy    0.786070    0.756219    0.756219    0.752353   \n",
      "13                  MCC    0.688228    0.643235    0.643235    0.636650   \n",
      "14                  NPV    0.876700    0.861500    0.861500    0.857100   \n",
      "15              ROC_AUC    0.786070    0.756219    0.756219    0.752353   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.723604    0.692007    0.718489  \n",
      "1    36.000000   30.000000   38.000000  \n",
      "2   201.000000  200.000000  200.000000  \n",
      "3     2.000000    1.000000    2.000000  \n",
      "4    29.000000   37.000000   28.000000  \n",
      "5     0.884328    0.858209    0.888060  \n",
      "6     0.947368    0.967742    0.950000  \n",
      "7     0.553846    0.447761    0.575758  \n",
      "8     0.990100    0.995000    0.990100  \n",
      "9     0.699029    0.612245    0.716981  \n",
      "10    0.872774    0.837993    0.877715  \n",
      "11    0.813718    0.762743    0.823607  \n",
      "12    0.771997    0.721393    0.782928  \n",
      "13    0.668410    0.599480    0.684158  \n",
      "14    0.873900    0.843900    0.877200  \n",
      "15    0.771997    0.721393    0.782928  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_svm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "Y_testSet6_cat = np.where(((Y_testSet6>=2) | (Y_testSet6<=-2)), 1, 0) \n",
    "y_pred_svm_6_cat = np.where(((y_pred_svm_6 >= 2) | (y_pred_svm_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_svm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 04:03:08,600]\u001b[0m Trial 350 finished with value: -0.008476313846273776 and parameters: {'C': 0.25, 'gamma': 3.0517578125e-05}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:03:15,291]\u001b[0m Trial 351 finished with value: 0.6065443045571164 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:03:21,702]\u001b[0m Trial 352 finished with value: -0.013164526830011158 and parameters: {'C': 0.0625, 'gamma': 0.25}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:03:27,870]\u001b[0m Trial 353 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:03:34,235]\u001b[0m Trial 354 finished with value: 0.6109035965023389 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:03:40,441]\u001b[0m Trial 355 finished with value: 0.6994136966673217 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:03:46,652]\u001b[0m Trial 356 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:03:53,064]\u001b[0m Trial 357 finished with value: -0.019683081511618995 and parameters: {'C': 0.015625, 'gamma': 1.0}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:03:59,262]\u001b[0m Trial 358 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:04:05,727]\u001b[0m Trial 359 finished with value: -0.01807395598231174 and parameters: {'C': 0.0078125, 'gamma': 0.000244140625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:04:11,902]\u001b[0m Trial 360 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:04:18,432]\u001b[0m Trial 361 finished with value: 0.6061913916470196 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:04:24,581]\u001b[0m Trial 362 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:04:30,752]\u001b[0m Trial 363 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:04:36,986]\u001b[0m Trial 364 finished with value: 0.6964357808142703 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:04:44,120]\u001b[0m Trial 365 finished with value: 0.021810703517837926 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:04:50,471]\u001b[0m Trial 366 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:04:57,243]\u001b[0m Trial 367 finished with value: 0.2554300093744044 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:05:03,828]\u001b[0m Trial 368 finished with value: 0.06621079156789576 and parameters: {'C': 1.0, 'gamma': 6.103515625e-05}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:05:10,489]\u001b[0m Trial 369 finished with value: 0.6495766217189389 and parameters: {'C': 32.0, 'gamma': 0.00390625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:05:16,930]\u001b[0m Trial 370 finished with value: 0.163017687954771 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:05:23,412]\u001b[0m Trial 371 finished with value: -0.012373237774438217 and parameters: {'C': 0.125, 'gamma': 4.0}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:05:30,208]\u001b[0m Trial 372 finished with value: 0.03420196884456076 and parameters: {'C': 64.0, 'gamma': 0.5}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:05:36,508]\u001b[0m Trial 373 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:05:42,798]\u001b[0m Trial 374 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:05:48,969]\u001b[0m Trial 375 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:05:55,158]\u001b[0m Trial 376 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:06:01,618]\u001b[0m Trial 377 finished with value: 0.5246653485528707 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:06:07,869]\u001b[0m Trial 378 finished with value: 0.6073124092242215 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:06:14,065]\u001b[0m Trial 379 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:06:20,410]\u001b[0m Trial 380 finished with value: 0.27392130734564846 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:06:27,129]\u001b[0m Trial 381 finished with value: 0.023277577047285614 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:06:33,245]\u001b[0m Trial 382 finished with value: 0.6848616593695422 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:06:39,406]\u001b[0m Trial 383 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:06:45,649]\u001b[0m Trial 384 finished with value: 0.5513492706750294 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:06:51,809]\u001b[0m Trial 385 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:06:58,209]\u001b[0m Trial 386 finished with value: 0.08633310534166097 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:07:04,541]\u001b[0m Trial 387 finished with value: 0.019039461190751672 and parameters: {'C': 0.0078125, 'gamma': 0.03125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:07:10,752]\u001b[0m Trial 388 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:07:17,109]\u001b[0m Trial 389 finished with value: 0.579358809170109 and parameters: {'C': 32.0, 'gamma': 0.0001220703125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:07:23,316]\u001b[0m Trial 390 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:07:29,746]\u001b[0m Trial 391 finished with value: 0.4574868357088703 and parameters: {'C': 32.0, 'gamma': 3.0517578125e-05}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:07:36,427]\u001b[0m Trial 392 finished with value: 0.6065443045571164 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:07:42,631]\u001b[0m Trial 393 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 04:07:49,395]\u001b[0m Trial 394 finished with value: 0.06494791727408679 and parameters: {'C': 4.0, 'gamma': 0.25}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:07:55,601]\u001b[0m Trial 395 finished with value: 0.6992093730112141 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:08:01,992]\u001b[0m Trial 396 finished with value: 0.6109035965023389 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:08:08,421]\u001b[0m Trial 397 finished with value: 0.41010443537598507 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:08:14,841]\u001b[0m Trial 398 finished with value: 0.6067054332850873 and parameters: {'C': 64.0, 'gamma': 0.000244140625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:08:20,990]\u001b[0m Trial 399 finished with value: 0.6594181143020689 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7141\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.718703    0.680756    0.734606    0.718255   \n",
      "1                    TP   39.000000   35.000000   35.000000   35.000000   \n",
      "2                    TN  199.000000  199.000000  199.000000  198.000000   \n",
      "3                    FP    2.000000    2.000000    2.000000    2.000000   \n",
      "4                    FN   28.000000   32.000000   32.000000   33.000000   \n",
      "5              Accuracy    0.888060    0.873134    0.873134    0.869403   \n",
      "6             Precision    0.951220    0.945946    0.945946    0.945946   \n",
      "7           Sensitivity    0.582090    0.522388    0.522388    0.514706   \n",
      "8           Specificity    0.990000    0.990000    0.990000    0.990000   \n",
      "9              F1 score    0.722222    0.673077    0.673077    0.666667   \n",
      "10  F1 score (weighted)    0.877985    0.859241    0.859241    0.854821   \n",
      "11     F1 score (macro)    0.826064    0.797187    0.797187    0.792730   \n",
      "12    Balanced Accuracy    0.786070    0.756219    0.756219    0.752353   \n",
      "13                  MCC    0.688228    0.643235    0.643235    0.636650   \n",
      "14                  NPV    0.876700    0.861500    0.861500    0.857100   \n",
      "15              ROC_AUC    0.786070    0.756219    0.756219    0.752353   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.723604    0.692007    0.718489    0.696068  \n",
      "1    36.000000   30.000000   38.000000   36.000000  \n",
      "2   201.000000  200.000000  200.000000  200.000000  \n",
      "3     2.000000    1.000000    2.000000    3.000000  \n",
      "4    29.000000   37.000000   28.000000   29.000000  \n",
      "5     0.884328    0.858209    0.888060    0.880597  \n",
      "6     0.947368    0.967742    0.950000    0.923077  \n",
      "7     0.553846    0.447761    0.575758    0.553846  \n",
      "8     0.990100    0.995000    0.990100    0.985200  \n",
      "9     0.699029    0.612245    0.716981    0.692308  \n",
      "10    0.872774    0.837993    0.877715    0.869265  \n",
      "11    0.813718    0.762743    0.823607    0.809117  \n",
      "12    0.771997    0.721393    0.782928    0.769534  \n",
      "13    0.668410    0.599480    0.684158    0.655236  \n",
      "14    0.873900    0.843900    0.877200    0.873400  \n",
      "15    0.771997    0.721393    0.782928    0.769534  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_svm_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "Y_testSet7_cat = np.where(((Y_testSet7>=2) | (Y_testSet7<=-2)), 1, 0) \n",
    "y_pred_svm_7_cat = np.where(((y_pred_svm_7 >= 2) | (y_pred_svm_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_svm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 04:08:28,505]\u001b[0m Trial 400 finished with value: 0.024077790883004525 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 251 with value: 0.714053086907222.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:08:34,639]\u001b[0m Trial 401 finished with value: 0.717809344679767 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:08:41,007]\u001b[0m Trial 402 finished with value: 0.1713149067687747 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:08:47,556]\u001b[0m Trial 403 finished with value: 0.6692200417679647 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:08:53,645]\u001b[0m Trial 404 finished with value: 0.717809344679767 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:08:59,749]\u001b[0m Trial 405 finished with value: 0.717809344679767 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:09:06,008]\u001b[0m Trial 406 finished with value: 0.5514781316723197 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:09:12,513]\u001b[0m Trial 407 finished with value: 0.6890291532935852 and parameters: {'C': 32.0, 'gamma': 0.00390625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:09:19,073]\u001b[0m Trial 408 finished with value: 0.24682826648622624 and parameters: {'C': 2.0, 'gamma': 0.125}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:09:25,203]\u001b[0m Trial 409 finished with value: 0.6408242463353256 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:09:31,554]\u001b[0m Trial 410 finished with value: 0.28747520885918565 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:09:38,424]\u001b[0m Trial 411 finished with value: 0.01921912155786093 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:09:45,132]\u001b[0m Trial 412 finished with value: 0.019389812254811 and parameters: {'C': 32.0, 'gamma': 4.0}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:09:51,440]\u001b[0m Trial 413 finished with value: 0.5682788030197907 and parameters: {'C': 32.0, 'gamma': 6.103515625e-05}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:09:57,530]\u001b[0m Trial 414 finished with value: 0.717809344679767 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:10:03,606]\u001b[0m Trial 415 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:10:10,131]\u001b[0m Trial 416 finished with value: -0.017949910523579082 and parameters: {'C': 0.015625, 'gamma': 0.5}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:10:16,224]\u001b[0m Trial 417 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:10:22,313]\u001b[0m Trial 418 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:10:28,406]\u001b[0m Trial 419 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:10:34,502]\u001b[0m Trial 420 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:10:40,616]\u001b[0m Trial 421 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:10:46,745]\u001b[0m Trial 422 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:10:52,881]\u001b[0m Trial 423 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:10:58,986]\u001b[0m Trial 424 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:11:05,098]\u001b[0m Trial 425 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:11:11,202]\u001b[0m Trial 426 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:11:17,829]\u001b[0m Trial 427 finished with value: 0.020512562191627205 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:11:23,933]\u001b[0m Trial 428 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:11:30,193]\u001b[0m Trial 429 finished with value: 0.5771801220355466 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:11:36,288]\u001b[0m Trial 430 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:11:42,403]\u001b[0m Trial 431 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:11:48,540]\u001b[0m Trial 432 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:11:54,647]\u001b[0m Trial 433 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:12:00,754]\u001b[0m Trial 434 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:12:06,853]\u001b[0m Trial 435 finished with value: 0.6954272362120082 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:12:12,962]\u001b[0m Trial 436 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:12:19,089]\u001b[0m Trial 437 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:12:25,209]\u001b[0m Trial 438 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:12:31,332]\u001b[0m Trial 439 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:12:37,791]\u001b[0m Trial 440 finished with value: 0.6598142274464197 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:12:44,695]\u001b[0m Trial 441 finished with value: 0.6510183651840855 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:12:50,813]\u001b[0m Trial 442 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:12:56,901]\u001b[0m Trial 443 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 04:13:02,984]\u001b[0m Trial 444 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:13:09,253]\u001b[0m Trial 445 finished with value: 0.6263275706106968 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:13:16,243]\u001b[0m Trial 446 finished with value: 0.6366231103361774 and parameters: {'C': 128.0, 'gamma': 0.00048828125}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:13:22,907]\u001b[0m Trial 447 finished with value: 0.06309390561937323 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:13:29,016]\u001b[0m Trial 448 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:13:35,126]\u001b[0m Trial 449 finished with value: 0.7173082340769616 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7178\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.718703    0.680756    0.734606    0.718255   \n",
      "1                    TP   39.000000   35.000000   35.000000   35.000000   \n",
      "2                    TN  199.000000  199.000000  199.000000  198.000000   \n",
      "3                    FP    2.000000    2.000000    2.000000    2.000000   \n",
      "4                    FN   28.000000   32.000000   32.000000   33.000000   \n",
      "5              Accuracy    0.888060    0.873134    0.873134    0.869403   \n",
      "6             Precision    0.951220    0.945946    0.945946    0.945946   \n",
      "7           Sensitivity    0.582090    0.522388    0.522388    0.514706   \n",
      "8           Specificity    0.990000    0.990000    0.990000    0.990000   \n",
      "9              F1 score    0.722222    0.673077    0.673077    0.666667   \n",
      "10  F1 score (weighted)    0.877985    0.859241    0.859241    0.854821   \n",
      "11     F1 score (macro)    0.826064    0.797187    0.797187    0.792730   \n",
      "12    Balanced Accuracy    0.786070    0.756219    0.756219    0.752353   \n",
      "13                  MCC    0.688228    0.643235    0.643235    0.636650   \n",
      "14                  NPV    0.876700    0.861500    0.861500    0.857100   \n",
      "15              ROC_AUC    0.786070    0.756219    0.756219    0.752353   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.723604    0.692007    0.718489    0.696068    0.608059  \n",
      "1    36.000000   30.000000   38.000000   36.000000   29.000000  \n",
      "2   201.000000  200.000000  200.000000  200.000000  201.000000  \n",
      "3     2.000000    1.000000    2.000000    3.000000    1.000000  \n",
      "4    29.000000   37.000000   28.000000   29.000000   37.000000  \n",
      "5     0.884328    0.858209    0.888060    0.880597    0.858209  \n",
      "6     0.947368    0.967742    0.950000    0.923077    0.966667  \n",
      "7     0.553846    0.447761    0.575758    0.553846    0.439394  \n",
      "8     0.990100    0.995000    0.990100    0.985200    0.995000  \n",
      "9     0.699029    0.612245    0.716981    0.692308    0.604167  \n",
      "10    0.872774    0.837993    0.877715    0.869265    0.837424  \n",
      "11    0.813718    0.762743    0.823607    0.809117    0.758902  \n",
      "12    0.771997    0.721393    0.782928    0.769534    0.717222  \n",
      "13    0.668410    0.599480    0.684158    0.655236    0.593652  \n",
      "14    0.873900    0.843900    0.877200    0.873400    0.844500  \n",
      "15    0.771997    0.721393    0.782928    0.769534    0.717222  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_svm_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "Y_testSet8_cat = np.where(((Y_testSet8>=2) | (Y_testSet8<=-2)), 1, 0) \n",
    "y_pred_svm_8_cat = np.where(((y_pred_svm_8 >= 2) | (y_pred_svm_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_svm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 04:13:42,684]\u001b[0m Trial 450 finished with value: 0.02627736383693028 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:13:48,937]\u001b[0m Trial 451 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:13:55,662]\u001b[0m Trial 452 finished with value: 0.5767963938381958 and parameters: {'C': 128.0, 'gamma': 0.000244140625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:14:01,888]\u001b[0m Trial 453 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:14:08,138]\u001b[0m Trial 454 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:14:15,401]\u001b[0m Trial 455 finished with value: 0.5211830315419961 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:14:21,631]\u001b[0m Trial 456 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:14:28,223]\u001b[0m Trial 457 finished with value: 0.2489484760991041 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:14:34,440]\u001b[0m Trial 458 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:14:41,098]\u001b[0m Trial 459 finished with value: 0.6424525681099972 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:14:47,977]\u001b[0m Trial 460 finished with value: 0.02134322830786963 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:14:54,207]\u001b[0m Trial 461 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:15:00,414]\u001b[0m Trial 462 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:15:06,631]\u001b[0m Trial 463 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:15:13,302]\u001b[0m Trial 464 finished with value: 0.034924717146880224 and parameters: {'C': 128.0, 'gamma': 0.5}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:15:19,671]\u001b[0m Trial 465 finished with value: 0.6024708906177699 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:15:25,931]\u001b[0m Trial 466 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:15:32,176]\u001b[0m Trial 467 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:15:38,398]\u001b[0m Trial 468 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:15:44,620]\u001b[0m Trial 469 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:15:50,842]\u001b[0m Trial 470 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:15:57,094]\u001b[0m Trial 471 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:16:03,779]\u001b[0m Trial 472 finished with value: 0.022691193256678555 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:16:09,991]\u001b[0m Trial 473 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:16:16,633]\u001b[0m Trial 474 finished with value: 0.02151018400492971 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:16:22,871]\u001b[0m Trial 475 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:16:29,215]\u001b[0m Trial 476 finished with value: 0.5551497289229699 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:16:35,482]\u001b[0m Trial 477 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:16:41,784]\u001b[0m Trial 478 finished with value: 0.6839159713020095 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:16:47,983]\u001b[0m Trial 479 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:16:54,195]\u001b[0m Trial 480 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:17:00,432]\u001b[0m Trial 481 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:17:06,667]\u001b[0m Trial 482 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:17:13,636]\u001b[0m Trial 483 finished with value: 0.5859679221010917 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:17:19,901]\u001b[0m Trial 484 finished with value: 0.5702646895298846 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:17:26,112]\u001b[0m Trial 485 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:17:32,496]\u001b[0m Trial 486 finished with value: 0.6002164280087636 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:17:38,719]\u001b[0m Trial 487 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:17:45,737]\u001b[0m Trial 488 finished with value: 0.5354387857745296 and parameters: {'C': 128.0, 'gamma': 0.00048828125}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:17:52,376]\u001b[0m Trial 489 finished with value: 0.06442945557433842 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:17:58,601]\u001b[0m Trial 490 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:18:05,320]\u001b[0m Trial 491 finished with value: 0.5767963938381958 and parameters: {'C': 128.0, 'gamma': 0.000244140625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:18:11,532]\u001b[0m Trial 492 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:18:17,754]\u001b[0m Trial 493 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-22 04:18:25,035]\u001b[0m Trial 494 finished with value: 0.5211830315419961 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:18:31,284]\u001b[0m Trial 495 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:18:37,924]\u001b[0m Trial 496 finished with value: 0.02627736383693028 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:18:44,157]\u001b[0m Trial 497 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:18:50,376]\u001b[0m Trial 498 finished with value: 0.7036072432485503 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n",
      "\u001b[32m[I 2023-02-22 04:18:56,963]\u001b[0m Trial 499 finished with value: 0.2489484760991041 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 401 with value: 0.717809344679767.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7178\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.718703    0.680756    0.734606    0.718255   \n",
      "1                    TP   39.000000   35.000000   35.000000   35.000000   \n",
      "2                    TN  199.000000  199.000000  199.000000  198.000000   \n",
      "3                    FP    2.000000    2.000000    2.000000    2.000000   \n",
      "4                    FN   28.000000   32.000000   32.000000   33.000000   \n",
      "5              Accuracy    0.888060    0.873134    0.873134    0.869403   \n",
      "6             Precision    0.951220    0.945946    0.945946    0.945946   \n",
      "7           Sensitivity    0.582090    0.522388    0.522388    0.514706   \n",
      "8           Specificity    0.990000    0.990000    0.990000    0.990000   \n",
      "9              F1 score    0.722222    0.673077    0.673077    0.666667   \n",
      "10  F1 score (weighted)    0.877985    0.859241    0.859241    0.854821   \n",
      "11     F1 score (macro)    0.826064    0.797187    0.797187    0.792730   \n",
      "12    Balanced Accuracy    0.786070    0.756219    0.756219    0.752353   \n",
      "13                  MCC    0.688228    0.643235    0.643235    0.636650   \n",
      "14                  NPV    0.876700    0.861500    0.861500    0.857100   \n",
      "15              ROC_AUC    0.786070    0.756219    0.756219    0.752353   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.723604    0.692007    0.718489    0.696068    0.608059    0.741383  \n",
      "1    36.000000   30.000000   38.000000   36.000000   29.000000   35.000000  \n",
      "2   201.000000  200.000000  200.000000  200.000000  201.000000  201.000000  \n",
      "3     2.000000    1.000000    2.000000    3.000000    1.000000    1.000000  \n",
      "4    29.000000   37.000000   28.000000   29.000000   37.000000   31.000000  \n",
      "5     0.884328    0.858209    0.888060    0.880597    0.858209    0.880597  \n",
      "6     0.947368    0.967742    0.950000    0.923077    0.966667    0.972222  \n",
      "7     0.553846    0.447761    0.575758    0.553846    0.439394    0.530303  \n",
      "8     0.990100    0.995000    0.990100    0.985200    0.995000    0.995000  \n",
      "9     0.699029    0.612245    0.716981    0.692308    0.604167    0.686275  \n",
      "10    0.872774    0.837993    0.877715    0.869265    0.837424    0.867165  \n",
      "11    0.813718    0.762743    0.823607    0.809117    0.758902    0.806271  \n",
      "12    0.771997    0.721393    0.782928    0.769534    0.717222    0.762676  \n",
      "13    0.668410    0.599480    0.684158    0.655236    0.593652    0.663748  \n",
      "14    0.873900    0.843900    0.877200    0.873400    0.844500    0.866400  \n",
      "15    0.771997    0.721393    0.782928    0.769534    0.717222    0.762676  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_svm_9)\n",
    "# now convert the resuls to binary with cutoff 6.9\n",
    "Y_testSet9_cat = np.where(((Y_testSet9>=2) | (Y_testSet9<=-2)), 1, 0) \n",
    "y_pred_svm_9_cat = np.where(((y_pred_svm_9 >= 2) | (y_pred_svm_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_svm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7b0e56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7178\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "#plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.721627</td>\n",
       "      <td>0.052918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>17.700000</td>\n",
       "      <td>3.164034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>99.400000</td>\n",
       "      <td>1.349897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.286684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>3.164034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.874537</td>\n",
       "      <td>0.029791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.938904</td>\n",
       "      <td>0.072239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.529952</td>\n",
       "      <td>0.094297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.989060</td>\n",
       "      <td>0.012838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.674584</td>\n",
       "      <td>0.090901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.860467</td>\n",
       "      <td>0.035854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.798404</td>\n",
       "      <td>0.054184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.759506</td>\n",
       "      <td>0.050853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.644486</td>\n",
       "      <td>0.095950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.864060</td>\n",
       "      <td>0.024577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.759506</td>\n",
       "      <td>0.050853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.721627     0.052918\n",
       "1                    TP        17.700000     3.164034\n",
       "2                    TN        99.400000     1.349897\n",
       "3                    FP         1.100000     1.286684\n",
       "4                    FN        15.700000     3.164034\n",
       "5              Accuracy         0.874537     0.029791\n",
       "6             Precision         0.938904     0.072239\n",
       "7           Sensitivity         0.529952     0.094297\n",
       "8           Specificity         0.989060     0.012838\n",
       "9              F1 score         0.674584     0.090901\n",
       "10  F1 score (weighted)         0.860467     0.035854\n",
       "11     F1 score (macro)         0.798404     0.054184\n",
       "12    Balanced Accuracy         0.759506     0.050853\n",
       "13                  MCC         0.644486     0.095950\n",
       "14                  NPV         0.864060     0.024577\n",
       "15              ROC_AUC         0.759506     0.050853"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.718703</td>\n",
       "      <td>0.680756</td>\n",
       "      <td>0.734606</td>\n",
       "      <td>0.718255</td>\n",
       "      <td>0.723604</td>\n",
       "      <td>0.692007</td>\n",
       "      <td>0.718489</td>\n",
       "      <td>0.696068</td>\n",
       "      <td>0.608059</td>\n",
       "      <td>0.741383</td>\n",
       "      <td>0.703193</td>\n",
       "      <td>0.038432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>34.800000</td>\n",
       "      <td>3.119829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>199.800000</td>\n",
       "      <td>1.032796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.632456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>3.339993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.869403</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.875373</td>\n",
       "      <td>0.011013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.014310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.553846</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.553846</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.524248</td>\n",
       "      <td>0.048231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990100</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.990100</td>\n",
       "      <td>0.985200</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.991040</td>\n",
       "      <td>0.003113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.673077</td>\n",
       "      <td>0.673077</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.699029</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.674605</td>\n",
       "      <td>0.039479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.877985</td>\n",
       "      <td>0.859241</td>\n",
       "      <td>0.859241</td>\n",
       "      <td>0.854821</td>\n",
       "      <td>0.872774</td>\n",
       "      <td>0.837993</td>\n",
       "      <td>0.877715</td>\n",
       "      <td>0.869265</td>\n",
       "      <td>0.837424</td>\n",
       "      <td>0.867165</td>\n",
       "      <td>0.861362</td>\n",
       "      <td>0.014686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.826064</td>\n",
       "      <td>0.797187</td>\n",
       "      <td>0.797187</td>\n",
       "      <td>0.792730</td>\n",
       "      <td>0.813718</td>\n",
       "      <td>0.762743</td>\n",
       "      <td>0.823607</td>\n",
       "      <td>0.809117</td>\n",
       "      <td>0.758902</td>\n",
       "      <td>0.806271</td>\n",
       "      <td>0.798753</td>\n",
       "      <td>0.022797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.786070</td>\n",
       "      <td>0.756219</td>\n",
       "      <td>0.756219</td>\n",
       "      <td>0.752353</td>\n",
       "      <td>0.771997</td>\n",
       "      <td>0.721393</td>\n",
       "      <td>0.782928</td>\n",
       "      <td>0.769534</td>\n",
       "      <td>0.717222</td>\n",
       "      <td>0.762676</td>\n",
       "      <td>0.757661</td>\n",
       "      <td>0.023094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.688228</td>\n",
       "      <td>0.643235</td>\n",
       "      <td>0.643235</td>\n",
       "      <td>0.636650</td>\n",
       "      <td>0.668410</td>\n",
       "      <td>0.599480</td>\n",
       "      <td>0.684158</td>\n",
       "      <td>0.655236</td>\n",
       "      <td>0.593652</td>\n",
       "      <td>0.663748</td>\n",
       "      <td>0.647603</td>\n",
       "      <td>0.031839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.876700</td>\n",
       "      <td>0.861500</td>\n",
       "      <td>0.861500</td>\n",
       "      <td>0.857100</td>\n",
       "      <td>0.873900</td>\n",
       "      <td>0.843900</td>\n",
       "      <td>0.877200</td>\n",
       "      <td>0.873400</td>\n",
       "      <td>0.844500</td>\n",
       "      <td>0.866400</td>\n",
       "      <td>0.863610</td>\n",
       "      <td>0.012340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.786070</td>\n",
       "      <td>0.756219</td>\n",
       "      <td>0.756219</td>\n",
       "      <td>0.752353</td>\n",
       "      <td>0.771997</td>\n",
       "      <td>0.721393</td>\n",
       "      <td>0.782928</td>\n",
       "      <td>0.769534</td>\n",
       "      <td>0.717222</td>\n",
       "      <td>0.762676</td>\n",
       "      <td>0.757661</td>\n",
       "      <td>0.023094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.718703    0.680756    0.734606    0.718255   \n",
       "1                    TP   39.000000   35.000000   35.000000   35.000000   \n",
       "2                    TN  199.000000  199.000000  199.000000  198.000000   \n",
       "3                    FP    2.000000    2.000000    2.000000    2.000000   \n",
       "4                    FN   28.000000   32.000000   32.000000   33.000000   \n",
       "5              Accuracy    0.888060    0.873134    0.873134    0.869403   \n",
       "6             Precision    0.951220    0.945946    0.945946    0.945946   \n",
       "7           Sensitivity    0.582090    0.522388    0.522388    0.514706   \n",
       "8           Specificity    0.990000    0.990000    0.990000    0.990000   \n",
       "9              F1 score    0.722222    0.673077    0.673077    0.666667   \n",
       "10  F1 score (weighted)    0.877985    0.859241    0.859241    0.854821   \n",
       "11     F1 score (macro)    0.826064    0.797187    0.797187    0.792730   \n",
       "12    Balanced Accuracy    0.786070    0.756219    0.756219    0.752353   \n",
       "13                  MCC    0.688228    0.643235    0.643235    0.636650   \n",
       "14                  NPV    0.876700    0.861500    0.861500    0.857100   \n",
       "15              ROC_AUC    0.786070    0.756219    0.756219    0.752353   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.723604    0.692007    0.718489    0.696068    0.608059    0.741383   \n",
       "1    36.000000   30.000000   38.000000   36.000000   29.000000   35.000000   \n",
       "2   201.000000  200.000000  200.000000  200.000000  201.000000  201.000000   \n",
       "3     2.000000    1.000000    2.000000    3.000000    1.000000    1.000000   \n",
       "4    29.000000   37.000000   28.000000   29.000000   37.000000   31.000000   \n",
       "5     0.884328    0.858209    0.888060    0.880597    0.858209    0.880597   \n",
       "6     0.947368    0.967742    0.950000    0.923077    0.966667    0.972222   \n",
       "7     0.553846    0.447761    0.575758    0.553846    0.439394    0.530303   \n",
       "8     0.990100    0.995000    0.990100    0.985200    0.995000    0.995000   \n",
       "9     0.699029    0.612245    0.716981    0.692308    0.604167    0.686275   \n",
       "10    0.872774    0.837993    0.877715    0.869265    0.837424    0.867165   \n",
       "11    0.813718    0.762743    0.823607    0.809117    0.758902    0.806271   \n",
       "12    0.771997    0.721393    0.782928    0.769534    0.717222    0.762676   \n",
       "13    0.668410    0.599480    0.684158    0.655236    0.593652    0.663748   \n",
       "14    0.873900    0.843900    0.877200    0.873400    0.844500    0.866400   \n",
       "15    0.771997    0.721393    0.782928    0.769534    0.717222    0.762676   \n",
       "\n",
       "           ave       std  \n",
       "0     0.703193  0.038432  \n",
       "1    34.800000  3.119829  \n",
       "2   199.800000  1.032796  \n",
       "3     1.800000  0.632456  \n",
       "4    31.600000  3.339993  \n",
       "5     0.875373  0.011013  \n",
       "6     0.951613  0.014310  \n",
       "7     0.524248  0.048231  \n",
       "8     0.991040  0.003113  \n",
       "9     0.674605  0.039479  \n",
       "10    0.861362  0.014686  \n",
       "11    0.798753  0.022797  \n",
       "12    0.757661  0.023094  \n",
       "13    0.647603  0.031839  \n",
       "14    0.863610  0.012340  \n",
       "15    0.757661  0.023094  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test['std'] = mat_met_svm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>y_test_idx0</th>\n",
       "      <th>y_test0</th>\n",
       "      <th>y_pred_svm0</th>\n",
       "      <th>y_pred_svm1</th>\n",
       "      <th>y_pred_svm2</th>\n",
       "      <th>y_pred_svm3</th>\n",
       "      <th>y_pred_svm4</th>\n",
       "      <th>y_pred_svm_ave</th>\n",
       "      <th>y_pred_svm_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4635479</td>\n",
       "      <td>0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.805779</td>\n",
       "      <td>0.779109</td>\n",
       "      <td>0.811309</td>\n",
       "      <td>0.875031</td>\n",
       "      <td>0.061997</td>\n",
       "      <td>0.625538</td>\n",
       "      <td>0.292305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4299417</td>\n",
       "      <td>1</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.833693</td>\n",
       "      <td>0.843639</td>\n",
       "      <td>0.855277</td>\n",
       "      <td>0.904510</td>\n",
       "      <td>0.946105</td>\n",
       "      <td>0.893871</td>\n",
       "      <td>0.054611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4225331</td>\n",
       "      <td>2</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.175726</td>\n",
       "      <td>1.278796</td>\n",
       "      <td>1.056998</td>\n",
       "      <td>1.507110</td>\n",
       "      <td>1.242024</td>\n",
       "      <td>1.183442</td>\n",
       "      <td>0.204648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL1094710</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.059069</td>\n",
       "      <td>-0.055019</td>\n",
       "      <td>0.067725</td>\n",
       "      <td>-0.049667</td>\n",
       "      <td>-0.097548</td>\n",
       "      <td>0.042737</td>\n",
       "      <td>0.189132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3287256</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.036604</td>\n",
       "      <td>0.076714</td>\n",
       "      <td>0.130514</td>\n",
       "      <td>0.057204</td>\n",
       "      <td>0.104161</td>\n",
       "      <td>0.012533</td>\n",
       "      <td>0.156172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>CHEMBL3769491</td>\n",
       "      <td>1334</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.117411</td>\n",
       "      <td>-0.141687</td>\n",
       "      <td>-0.146953</td>\n",
       "      <td>-0.165922</td>\n",
       "      <td>-0.254585</td>\n",
       "      <td>-0.022760</td>\n",
       "      <td>0.321660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>CHEMBL482095</td>\n",
       "      <td>1335</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.643770</td>\n",
       "      <td>0.562889</td>\n",
       "      <td>0.696936</td>\n",
       "      <td>0.598154</td>\n",
       "      <td>0.504957</td>\n",
       "      <td>0.574451</td>\n",
       "      <td>0.085058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>CHEMBL4095596</td>\n",
       "      <td>1336</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.256897</td>\n",
       "      <td>0.987371</td>\n",
       "      <td>0.773645</td>\n",
       "      <td>1.287640</td>\n",
       "      <td>1.255758</td>\n",
       "      <td>1.056885</td>\n",
       "      <td>0.221557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>CHEMBL4072925</td>\n",
       "      <td>1337</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.500069</td>\n",
       "      <td>0.547592</td>\n",
       "      <td>0.576781</td>\n",
       "      <td>0.488154</td>\n",
       "      <td>0.318990</td>\n",
       "      <td>0.508598</td>\n",
       "      <td>0.095762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>CHEMBL3774993</td>\n",
       "      <td>1338</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.902916</td>\n",
       "      <td>0.878625</td>\n",
       "      <td>0.835389</td>\n",
       "      <td>0.855395</td>\n",
       "      <td>0.866660</td>\n",
       "      <td>0.884831</td>\n",
       "      <td>0.043321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1339 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id  y_test_idx0  y_test0  y_pred_svm0  y_pred_svm1  \\\n",
       "0         CHEMBL4635479            0     0.42     0.805779     0.779109   \n",
       "1         CHEMBL4299417            1     0.98     0.833693     0.843639   \n",
       "2         CHEMBL4225331            2     0.84     1.175726     1.278796   \n",
       "3         CHEMBL1094710            3     0.45    -0.059069    -0.055019   \n",
       "4         CHEMBL3287256            4    -0.33     0.036604     0.076714   \n",
       "...                 ...          ...      ...          ...          ...   \n",
       "1334      CHEMBL3769491         1334     0.69    -0.117411    -0.141687   \n",
       "1335       CHEMBL482095         1335     0.44     0.643770     0.562889   \n",
       "1336      CHEMBL4095596         1336     0.78     1.256897     0.987371   \n",
       "1337      CHEMBL4072925         1337     0.62     0.500069     0.547592   \n",
       "1338      CHEMBL3774993         1338     0.97     0.902916     0.878625   \n",
       "\n",
       "      y_pred_svm2  y_pred_svm3  y_pred_svm4  y_pred_svm_ave  y_pred_svm_std  \n",
       "0        0.811309     0.875031     0.061997        0.625538        0.292305  \n",
       "1        0.855277     0.904510     0.946105        0.893871        0.054611  \n",
       "2        1.056998     1.507110     1.242024        1.183442        0.204648  \n",
       "3        0.067725    -0.049667    -0.097548        0.042737        0.189132  \n",
       "4        0.130514     0.057204     0.104161        0.012533        0.156172  \n",
       "...           ...          ...          ...             ...             ...  \n",
       "1334    -0.146953    -0.165922    -0.254585       -0.022760        0.321660  \n",
       "1335     0.696936     0.598154     0.504957        0.574451        0.085058  \n",
       "1336     0.773645     1.287640     1.255758        1.056885        0.221557  \n",
       "1337     0.576781     0.488154     0.318990        0.508598        0.095762  \n",
       "1338     0.835389     0.855395     0.866660        0.884831        0.043321  \n",
       "\n",
       "[1339 rows x 10 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_svm_cat = np.where(((y_pred_optimized_svm >= 2) | (y_pred_optimized_svm <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_svm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_svm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "svm_run0 = data_svm[['y_test_idx0', 'y_test0', 'y_pred_svm0']]\n",
    "svm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "svm_run0.reset_index(inplace=True, drop=True)\n",
    "svm_run1 = data_svm[['y_test_idx1', 'y_test1', 'y_pred_svm1']]\n",
    "svm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "svm_run1.reset_index(inplace=True, drop=True)\n",
    "svm_run2 = data_svm[['y_test_idx2', 'y_test2', 'y_pred_svm2']]\n",
    "svm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "svm_run2.reset_index(inplace=True, drop=True)\n",
    "svm_run3 = data_svm[['y_test_idx3', 'y_test3', 'y_pred_svm3']]\n",
    "svm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "svm_run3.reset_index(inplace=True, drop=True)\n",
    "svm_run4 = data_svm[['y_test_idx4', 'y_test4', 'y_pred_svm4']]\n",
    "svm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "svm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "svm_5preds = pd.concat([chembl_id,svm_run0, svm_run1, svm_run2, svm_run3, svm_run4], axis=1)\n",
    "svm_5preds = svm_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_svm0', 'y_pred_svm1', 'y_pred_svm2', 'y_pred_svm3', 'y_pred_svm4']]\n",
    "svm_5preds['y_pred_svm_ave'] = svm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "svm_5preds['y_pred_svm_std'] = svm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "svm_5preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6394fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_met_optimized_svm.to_csv(output/'mat_met_svm_opt.csv')\n",
    "svm_5preds.to_csv(output/'svm_5test_CV_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2869d8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEJCAYAAAB2T0usAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABMh0lEQVR4nO2dd3xUVfr/P/fOJAQCJGQS0oMgRBZpgoAgqEB0WXRFVqWIfEUXpKgUkRJFEViqooIg7gIKYkNQdtl1f4tERaQoRSIo0sGE9EwaKZNkcs/vjzN3ermTTEvyvF8vXmTu3PLcAznPOU8VGGMMBEEQRLNG9LcABEEQhP8hZUAQBEGQMiAIgiBIGRAEQRAgZUAQBEGAlAFBEAQBQO1vARpCdna2X58fGRmJwsJCv8oQKNBYmKCxMEFjYSJQxiIuLs7ucdoZEARBEKQMCIIgCFIGBEEQBEgZEARBECBlQBAEQYCUAUEQBAFSBgRBEARIGRAEQRAgZUAQBEGAlAFBEAQBUgYEQRAESBkQBEEQIGVAEARBgJRBg0lMTMS9996LYcOG4YknnkBpaanF9xUVFfjTn/6EgQMHIjc31+K7Z599FkOGDMGwYcPw/PPPo7a2tsHyZGRk4IEHHsCdd96JadOmoaamxu55f/vb3zB06FDcfffdePnll8EYc3p9WVkZnnjiCaSkpGDo0KHYuXNng2UlCCJwIGXQQEJCQrB//3588803CA8Px7Zt24zf6fV6TJs2DQ8//DAWLVqEp556Cjdu3DB+P3r0aBw8eBBff/01dDodPv744wbLs3z5ckyZMgWHDx9GWFgYPvnkE5tzjh8/juPHjyMtLQ3ffPMN0tPTcfToUafXb9u2DcnJyUhLS8Pu3buxdOlSh4qGIIjGBykDD9K3b1+L1f+CBQswdOhQTJ48Gffffz9mzpyJGTNmGHcAw4cPhyAIEAQBvXv3Rk5OToOezxjD4cOHcf/99wMAHn30Uezbt8/mPEEQUF1djZqaGtTU1ECv1yMqKsrp9YIgoLy8HIwxVFRUIDw8HGp1o26HQRCEGfTb7CHq6upw6NAhjB8/3nhs7dq1FueMGDECI0aMsLm2trYWn3/+OZYuXWrz3aVLlzB9+nS7z9y9ezfCwsKMn4uLixEWFmacpGNjY21MUwBw++23Y9CgQejTpw8YY5g0aRK6dOmCoqIih9c/+eSTmDRpEvr06YPy8nJs2rQJokhrCYJoKgSUMpAkCQsXLkRERAQWLlzob3EUodPpcO+99+L69evo0aMH7rrrLrfv8eKLL2LAgAEYMGCAzXedO3fG/v37Fd1HtvubIwiCzbGrV6/i4sWLOHHiBABg3Lhx+OGHH9ClSxeH1x84cAC33nordu3ahWvXrmH8+PEYMGAA2rRpo0g2giACm4Ba2v33v/9FfHy8v8VwC9ln8OOPP6K2ttbCZ6CEN954A1qtFq+++qrd7y9duoR7773X7h9rZ3VERARKS0uh1+sBADk5OYiOjra55//+9z/06dMHoaGhCA0NxbBhw/DTTz85vX7nzp0YOXIkBEFAx44dkZiYiEuXLrn1rgRBBC4Bowy0Wi1++uknDB8+3N+i1Iu2bdti2bJlePfddxVHBX388cc4cOAANm7c6NDkIu8M7P0xNxEBfBU/aNAgfPnllwCAXbt24b777rO5Z1xcHH744Qfo9XrU1tbi6NGj6Ny5s9Pr4+PjcejQIQBAQUEBrly5gg4dOigbHIIgAh6B2bMt+IG1a9di9OjRqKqqwr///W+7ZqK0tDSkpaUBAFatWuX3aBa1Wo22bduiqKjIeGz06NF45JFHMGHCBJfXt2rVCklJSUZTy0MPPYSXXnqpQTJduXIFEydORFFREXr37o1t27ahRYsWOHnyJDZv3ox3330XdXV1eO6553Do0CEIgoD77rsPr732mtPrs7OzMXnyZOTm5oIxhnnz5uGxxx6zGAt5R9HcobEwQWNhIlDGIjg42O7xgFAGJ0+exKlTpzB58mT8+uuvDpWBNdnZ2T6QzjGRkZEoLCz0qwyBAo2FCRoLEzQWJgJlLOLi4uweDwgH8vnz53HixAmcOnUKNTU1qKqqwvr16zFz5kx/i0YQBNEsCAhl8NhjjxlNDvLOgBQBQRCehOkqgawMID4JQkgrf4sTcChSBoWFhfj9999RUVGB0NBQdOjQAZGRkd6WjSAIwiMwXSWk1QuB7EwgLhHiglX1UgiOFEpTUDQOlYFer0daWhr279+P/Px8xMTEICQkBDqdDrm5uWjfvj3uvfdepKSkeDQT9dZbb8Wtt97qsfsRBEEgK4MrAqkOyLnOP9/c1a1bOFIonlI0/sbhLD5v3jx0794dTz/9NLp06WIR+ihJEi5duoTvv/8e8+fPxxtvvOETYQmCIOpFfBIQl8gVQWwC/+wujhSKBxRNIOBQGbz66qs2cewyoigiOTkZycnJKCsr85pwBEEQnkAIaQVxwaqGmXIcKRSr40wTBVw+1+hMRg6VgSNFYE3btm09JgxBEIS3EEJaNXjFLox5CmCA0CnZONGbKxqmiQJbtwSsEZqMnBr733nnHZc3mDFjhseEIQiCCEQs/AIx8cDYyYCVQmDxScCx7/nug0mNzmTkVBl89913iIuLQ9++falcMUEQzRdzv0B2Bthbr4LFJ9l3IqtUgCTU3zfhJ5zO8HPnzsXBgwdx8OBB9OvXD3fffTeSk5N9JRtBEIRLfBLWKfsFsjMASTKu/KXTJ4DMq0BiR5OyEABhwjQI/Yc0GhMR4EIZ9O/fH/3790d5eTmOHDmC7du3o7y8HHfddRdGjBiB0NBQX8lJEARhg6/COmW/ALt6EezTzUBeNqBpD2x+3XRSdDxQmAfEJjQ6RQAoTDpr3bo17rvvPgwePBhffPEFdu3ahVtuuQXdu3f3tnwEQRCO8UFYp7zzYJooCMEtgDlLIGgLIJ34Hkgzq4/Woy/E2wc3uigiGZfKQJIk/Pzzz/juu+9w9uxZ9OnTB6+88gq6devmC/kIgiAc4yJ/QIkJSSrRAqdPAD1vhxiusbleWr2Q30OlApMkkwP5jqFA2r9NJ989AkJMgsdf0Vc4VQYffPABjh49iqSkJNx1112YMWOGw/KnBEEQvsZZ/oASE5JUogVLfRrQ1wLqIEgr/wExXGPaDVTr+PVMAvQSv8jgQEZ8ErDoDeDEYeDO4VA1YkUAuFAGX375JaKjo1FVVYWvvvoKX331lc05S5Ys8ZpwBEEQrnCYP+DEhMR0lWBXL4CdPsEVAQDoa8FOHoEUmwi2cwuQm8V3ATHx/GdR5PcycyCLej2Eh5/w4dt6D6fKwFEjdoIgCH+iKILIgQmJ6SohrZzPI4PMUQcBB/4fWF42n+wB7iievhBCaTFY8q0QirUmB3IjCx11hVNlcM899/hIDIIgCGUoMf/IykKYtZgrA/MeXlkZQO5102dBAP74F6BzV+CdVSZFIIpAdBzwxQdguVlAXCKEBasgpK5p9BVK7eHSgcwYQ2lpKcLCwiAIAtLT0/HTTz8hKSkJKSkpvpCRIAjChIsIIptsYQDIzQIzKA7EJwExCaadQXQ8hD/0AuISweSdRHQchHFTAMbA1i2xeJZwc9dGk1XsDk6VwdmzZ7F27VqUl5ejffv2GDt2LHbs2IFbbrkFP/74IwoLCzFu3DhfyUoQBOG6Aqm5ssjNAsC4nd9sMhdT1/CcgZpqvvJft4Sv/GcthqAtMK76ma7SpCCamFnIGqfKYMeOHZgwYQIGDx6MAwcO4N1338WqVauQkJCArKwsrFixgpQBQRA+xWUFUnNlEW3o92tm4zeakDp2gZCVASk3y7Dyz4SgLeArf6XPakI4VQbZ2dkYNmwYACAlJQXbt29HQgIPn4qPj8eNGze8LyFBEI0SpqtEzbkzYK3DGjyJWjuMnVUgtZ7Ama7KmEcAwNKEdP8YU5SQIPLEMjv3a4pmIWsUV58TRdEmx0AQrIeNIAjCZLcvNphXlJaJsBclVJ+SE/IEznSV3ASUnQl8mwg8+pRFwTlsXgvA4FyWJG4isko8cyZXU9oxOFUGtbW12Llzp/FzTU2NxWe9Xu89yQiCaLzUo0yEw0nfzr1YfJKxRARyMm16DDiUQxD4jiAn0xBhZFAEoshNS3Z8AvbkAtAkWl2a41QZDB48GFqt1vj5zjvvtPlMEARhg2y3z73OI3eUOF4dKRArH4BUXgqsnG9KBDMkjbG4JIipaywnZWtnc6whS1gQAJWaO5Zj4iGMm8J9CPYmdHtyAU2i1aU5TpUBNa4hCKI+yHb7sIoylIa2VbZqttM+kp1N5yWhDfkC7NPNwDsr+SQO8MlYJjsD0vlfIAQFg9VUQwhuAaFTsoX/AFkZPGdAkgBBUFZq2lH0UhOLMnLpM9Dr9cbGNufOnYMk/yMAuOWWW6BSqbwnHUEQjRvzZC8X2LSPfHOxKRcgNpHH/csTOcB3BaIImJurNywHM5h+GMx2C3IZCquJXUmpaUcRRU0tysipMvjqq69w/vx5PPfccwCAv/3tb2jTpg0AoLq6Go8//rgx2oggCEKmvg5kI9mZ/I9MTiZYbY2hTtB1nig2bgrYpd+Af39i/mTL++RcBzv2PWCY9OsbKmovoqipRRm5bHs5ZcoU4+egoCBs2rQJAHDt2jVs3ryZlAFBELa46UBmukpI588An24Bigr5pB+h4T9bIzGgphosIhI4ccjyO0E0lZMAAFEE+3AT2P8+hzB/JcRwjeJJXGm0UFOJKnKqDPLz83HTTTcZP8s5BgDQoUMH5Ofne00wgiAaMeYO5PZxYDU6QFdpd7K0WzguNwt4YCxw4L/AjVIgMgY4d9p0jjYfWPsyUFbCP4siMPr/INxxN5Bt2EUU5gE7t3LlUJALtuZFsFfeVBziqiRayN2w10BWHKKzL3U6HXQ6nfHzsmXLjD9XV1dbfEcQBCEjm2PCX34DAMDeWgJp9UI+GVpjXTiOXwHs/ZhP9owBBTlA2l7LU0qLeOtJlRqIS4J4zwgIIS0htAiBeEt3iHcOByLbm87X5oMd+96+DPZkshdBVN/zAEhVFZBWL4S0JtXxWLiA6SrBLp+r17WucKoMkpKScPr0abvfpaenIzEx0eMCEQTRNBBCWvE2kcZyD7aTJdNV8l1D+zjTQVE0OYmdIaqAgUOB6QstYv+lNamQls4B01VBmL8SiIrl5xpMRkomYqaJMigalfNoIXkHpFK7jCrS/35FseKwK5NhF9IQZeIMp8pg5MiR2LJlC44dO2aMIpIkCceOHcN7772HkSNHelQYgiA8jzdXk65Qd+hkMVkyTZRRFnlyY28t4XH/Dz8BPPiY8gikOj3fPby7mpecMF+lF+SArUmFENIS4itvQpgwDairMzalcTQRM10lpN/SeSRTYR4QEcXDWgGwy+d4ZzSzsZR3QOK8FS5NRNZj4XY4qhu7kPrg1Gdw5513oqioCG+//Tb0ej3atm2LsrIyBAUF4ZFHHsHgwYM9KgxBEJ6lPqUcPInYMtQyXHTdEjC5LtDAodwHIEk8I/iLD/iuwJ4yiIwB7hwOfPMlcKPE8jt9La891H8IX80X5PDj2gJTyen+Q8C+/dIihwGXzzkoe5Fh2pkUFQLZmZB2vcfHUBTB6up4WOrYyaasZwUOafOxqJfPwFW11gbiMs/gz3/+M4YPH44LFy7gxo0baNOmDZKTk9GqVWA5PwiCsEM9ykIowR1HqLFO0Nl0fg2T+IT7+XarmzK+erdGFIG/TAT27LBVBACgUoOFtIQAQJi/AmxNKlcEZuUlbHIYZKVkt+yFQREIIp90BZjGUE5yM/RBZvFJbinYhoSjeruCqqJCdRs2bMD8+fNtjr/++ut44YUXPCoQQRAexAuryfrsNpiukvcVZgp8AeYIArf579zKHcbmiCLQMhSouAFsfh2SJhrCwlUQX3nL7oQphLTiSWfHvjcpJeuyFzHxpoglg4lICGlp6GmQyRVEnZ4rLuvrfYA3cxsUKYNff/3VreMEQQQGXllN1mO3IZ07w88F+CQergGKClw/S44kMncoy1UPIsxMQgCgzeN+glfesuhJIO9i5B0BsjP5PSTBQkEKIa0gjJ0M9tarfKIv0Rr7G5jvKoxlMZpYH2SnykCuUKrX6y2qlQJAXl4eoqKivCcZQRAeweOrSTd3G1KJFthk1ltYEIAZqdzOv/dj18+zjiyKjIEwYRpYq1Bg5TxL01JhPtiVCxC69QZgtYvRRHHzkVTH6x3ZqUskdErmuwerd5PHUACAcA1YE+yD7FQZyBVKJUmyqFYKAJGRkRgzZoz3JCMIIiBxe7dx+oRlQbm6OuDtZcDdf6qfAAU5YL9fAr77n62PgUlgO7eAydVLzXcxhflAWDvud4hNtFuXSOm7NbVSFIDCqqXJyclISUnxmhCFhYXYuHEjSkpKIAgCUlJSKGyVIBwQCFmsSidDqaoCrG04j/M3VwilxcChfbbHW7cByq06KLYJ41nIxptKts5n8+vysm3LXxsigVBaDES2N/gCms9ErwRFPoOUlBRUVlYiOzvbJuu4e/fuDRZCpVJh4sSJ6NSpE6qqqrBw4UL07NnTovwFQRD+DxU1l8OVQmK6ShQvfx7IuMpX5MVWdYaKtNxkJBNqRxEAQHkZoA4y9i2wQBC4U7eigp8jSTbmHXHBKp55/NG73FRVVOiwo1lzRpEyOHDgALZu3YqQkBCL1peCIGDDhg0NFqJdu3Zo164dAKBly5aIj49HUVERKQOi2aB4te+lUFF3UKyQsjKgz7zKZbVWBMabmeUUVDjoqc6YSRGo1DyaBwCi44HKClO4qV4Cxk2BeOdwmygi6zyD+jp9A2FX5i0UKYNPPvkEzz//PG677TZvy4P8/HxcvXoVnTt39vqzCCIQcGu17+XEI3OZHE56ChUS00RxM5C93AF3EAST0mAMGDcFQlwSwBjPFDY/NTbR7th5IqoqUHZl3kKRMpAkCb169fK2LNDpdFi7di0mTZpkN6ktLS0NaWlpAIBVq1YhMjLS6zI5Q61W+12GQIHGwoS7Y1Fz7gyv+y/VAbnXEVZRhuAEx5O8tGYz9BlXoU7qCLFlqCdEtrx/VQWKlz8PfeY1qBNvQrsVmyyeI4XehuKkjtBfvwZ1wk1o1/M2u3LUFOaguL6KQB2MkD+OgrpjMip2bgUryAUAqBI6IOLBMRBbhkKqqkBRUkfUZVwBAIhxSdD0GwiA1wFSd+hkK5eTcXWFu/9ONq8U4L8jipTBqFGj8Pnnn+Phhx+GKDotZ1Rv9Ho91q5diyFDhmDAgAF2z0lJSbFwZBcWOth6+ojIyEi/yxAo0FiYcHcsWOswvsrP4f2CS0PbQnB1vSYGqKjifzwMu3wOUgY37+gzr0F7+pRF3D4AsLl/g5iVgTpNFLSnT9ldbbPWYRBi4sHMS1PbQxCBu0cAZ05wc5IkAfoa6L7cZdOfoO7Bx1Bk9t7Scy8DqxYA2gJI1dUouHwR+Psar6ze6/XvZEag/I7ExcXZPe5QGUyfPt3ic0lJCfbu3YvWrVtbHJeb3TQExhjeffddxMfH44EHHmjw/Qgi0HBmdlFqwvCZvdpOL2LrOj4AwKp1YG8u5j2F7Uy8TFcFZh4F5AiVCji4D2gfCyR3530LjDexyjHYvQ3slu7G5wjaArBiLQAGaPOAtYuAslKnJqz6jqO3y0H4G4fKQG516QvOnz+PgwcPIikpCfPmzQMAjB8/Hn369PGZDAThLZTYml2FMzakiQoAiwnM1WToqo4PwEtFG0s6ADYTL9NVgq150TIk1BGyczj3up2+BlaYh40C/P3Mi9OVlgCR0Ty72Y5PpaF2/6YcdupQGXTr1s1nQnTt2hWfffaZz55HEN5EqqoAM19JeyICyI17WEx4MfH8oGH1LsxabCrJYDYZWisI46R3+RxXBNZlk7MzTYpAFG0n3qwM3o3M02iibHYq1sXphFmLeehoA5zfzRFFPgPrUhQyQUFBiIiIQO/evREeHu5JuQgi4HAntl7KuGpaScsF0HKvA9Fx9YsAcieKyHzCy80CwAxloq/zbGCryZDFJzleLTt6rnwsOg7CuCkQOnaxHBPz64KDgSo7vRTM/QGCALQN50lh9hBFICIKmLXYYJridnsxdQ3EcA2YVXE6FtLS+G4O5VIQjdWUQ0mtUaQMcnJycOzYMXTu3BkajQZarRaXLl1C3759cfLkSWzduhVz585F7969vSwuQfgH92Lrr1muPOUJR2HPFnu4Za82n/CiDc5Cuahaz9uBby0nQ3blgv0qnk6e60oWIaQVX6G//hKkvGxbGTXtuTIozDV9njCNl6mw1+Vs2AMQRz0GdvWCySGdnQF29SKEP/SyMN9IJVq+UyjMB6xKTLszjg01KTU2RaI4tHT27Nno37+/8djx48dx6NAhLF++HAcOHMBHH31EyoBouig1L8QnQZ14E1cI8mR79QLfFTDJ1uYN5ZOGUnu19YQny880UdzhOnU+hAu/csUAWJaWtrNzsfdcJbII2gJIBXlmBwRgygsQI6Ig3SgFNi43fVdUAGxY7rjdZdeefMVvrVCtDhh9FYZQVGRn2oy3Yrt/A0xK9hRJoKNIGfz888+YPXu2xbG+ffsas4/vuusuvPfeex4XjiACBoXmBSGkFdqt2GQMtwQA9ukW0yRnNdla2/ctumdZwXSVfBUvAEJH++eYy4GbuxqvYTXVwAcbeOSP3K0rPgkY85TBlARAFLnJx8Gz3V7lxidBldDBmAeA2ESIPfry69N/tDxXkgCYKQLzPsiRMRCCg7kMcYmm0hRqNVh5GaTf0k3jYe2r0EQ5/Ldy+U4NSfCzp0gakOPgCxQpg5iYGHz11VcYMWKE8dhXX32F6OhoAEBZWRlatGjhHQkJIgBwx7wgtgw1xuWzy+fMJlsVn+zNrzWfNMy6Z1k7QZmuEtLK+cbGKywuCaJcmdMB1tcYkQvD5VznpiuzCU/o2MVmkqyvuUQIaYWIVX9H4fGjYLU1EAylbJiuknctk1GrAYlxBVBXB0S2B6bOB347DdRUAycOgb21hDeYeWiCKaNZrwc2vw5mNh6mCTwTiGgPYf4Kh8pNSYRXvUNJfZQp7kkUKYOpU6di7dq1+Ne//oWIiAgUFRVBFEXMnTsXAJCdnY2xY8d6VVCC8Df1Ciu0mhSETskWXzNNFLeXa/P4SphJQHYm2JoXwbT5JhNDVoapOQzAzU6uzBbW1wB8whVVxoJuQqdkCFYmJRvzRgPMJWLLUN4jYOV8sNzrYDG8d7BRQQoCMHIsEBQEaCIhGBK72JuLbZVYdgbw4Sb73dIM42HeiMbpBK7wneobStoYcxIUKYNOnTph3bp1uHDhAkpKShAeHo7k5GSo1fzybt26+TQUlSAaC8aqmVcv2rdvr1sCFOZxc4Y6iNu6IyK5qcPaCR2bYJogo+PBqnWArtKxSalGx81Scux+bCKEcVN4+KV16KX5TsZ6kmzgKpddvWCSOzsDrLbGVFZaEIC9H5nOjUuCMG6ybb6BKPJzSwytLwUBaBdp6pYWY9uIxik+WLk3tpwERcoA4HU1aMInGgOBGMXBPtvKnbiR7SHMXwkxXGNanTIJKNZy01BwC1N7RrOJSghpBWHOErCTR3gI5t5PuEkpNsFoLpLfW1KruTO2rJhPks+8BKFFiGX4Z7iGKwxDvD4AU1tHq0myoatcVlNt8VkICoawYBWkw18Dn262PDn3OlhNDZdbViDt44Bh91ueK6qAOUsgFBUCgmAb2uqCxrhy9zYOlcGcOXPw5ptvArAtTWGOJ8pREISnCMjKklkZptDNglxuAnrlTVsTkmFCEwAwq4nKuIvIzuQrYq0hSic7gzuVOyXz977+OyxiWHOzILYJs60t5CIxjfsTTPext8pVonSlqgpgt1kjmpgEo4NciEu0jbaVJGD3NuCZF4H1S3gimVoN3HobEB5h2hkwBrGi3Njesj40tpW7t3GoDKZOnWr82ZelKQiiQQRihml8EneKyuGO2jywY99D6D/E7urU7iRr/l5FVpm9gmD63np6bd3GbkkGdux7s8Q0gyNZzjPIzgTb9R7/24FCVaJ0ma4SVT98Y2nyuXO4SeyOyWBxSfz71m2BshL+Re514JefAK2haF1eFrB+qWVCmihCKi+F6MBM1lACcXfpbRwqg65dTb9A9sxDkiRh165dZDoiAgsv2YIbMjnwkgkrefy7Ng8QVWAfvQv27ZfcNGT1HGnlfNNuYc4SICeTm1raxxombmaIwJGAqFiAMUihrbnfQVY4Mno9mK7KQtEY6wqpVIAA3iQGMCWmCXCtUF0oXfk55XL5CpnPt0M6uM9oKhNT14BdvQh27SLwxQem86JiTFFBbcKBgjxYKDp9LfDOSkhxSR7f/QXk7tIHKPYZWFNXV4cvvviCooiIgMIbtuCGTA6yEhHikyC88qap/aJUZzdqiF2xcrauWmCKm4+IAp+pGQ+rhAAU5vHIG5Whl3CbcENxOMPEWVkOtnwupJfW2vopJAHChGkQ+g/hshpyGBCb6FShGp3TMfEmBWKtdM2fY01BLtiahWCvrOOVTbe/zd9RpebnxyRAvKU72KzFpkxitZq/n9wsh0mmEhue3v0F4u7SB9RbGRBEoOJxW3A9Jwe7SsS8/aK9qCHB6iZytAwAlGi5ualQzuplphaQ8t83SoCQlnzSrCw3XFdk6aeIief+gfaxQFS0SV6DachZsTebJLmZrxjDZS2K85nXYxIE225nBXmQzv8CfLLZpOzq9JZtK7MywLQFfOJnAoQJ03nWdHYmz5p2pIgaSiPMEfAEpAwIwhVuTA5MV4mac2d4IxQrJcKuXOBRPYaJ1l7UEHRVfAcgKwE5EQvgUTWOyjWYozM0vDEvBFeYZ1meAsywq+ARSRhyn0V9IkFbYON0BmD5TnnZEFqEALCTm2AcEOZY5sI8S2UHQIgzUz7WDvb+Q/h34Rqw1DVes+k310gjp8rgl19+cfidXq/3uDAEEYg4yxUwR141F8uT16zFplV4VAzYzi3GRjDCglUQQ1pZRA0B4MqhyKwbVl0d0CYMuO8hrjA2rlQuuLmJRgCkG6UQrlwwRApJpkk6OwPY9Z7B1CQ4V3j2FKO9nRPA31seL6uOZdBEQ+g7COzA/7PMg+jYxSSyk0nZ25FAzTHSyKkycBU2Gsj9PAnC07DPtjqNsLGZFI3ZvwyoqwXyCmwqg5pPOsaEL+uIoBul3Lkam8izlQutnMRKkCRg00qw6HjDbkOemA0+CEni8foGH4LTukdjngKrroYgl6BxVeY6rB1/pWLDLqBtODB7MYSQlmByMpmmPYTnl7rd9IfwHE6VwcaNG30lB0F4HI+GByrwG/DSElGmLluM8dWxJPHVfmR7/rehlST7Ld1YG0g2GyEyGsjP5hOoeSglY3wFLTWgDrYcpinfQhCB4Q/wMM6CXAtTjL2xs45EYpIEJvsXHn3KJvlLmLUY4tqXUSf3VJApKwE2rgB77GnT7qFYy30U4Zr6vx/RIMhnQDRJPB4e6MJvYEwK0xZAjIqGNGoCP1++JiqGx9hr2gNBwYYyy4ZWjeogMEmyTCYrLQbC2/E2joLIJ8w24UBpkfuyP/wEcPRb7nCV+xsYqpfimy/5sfvHAkFqMIO/wTjpO8qY1ssmpkyw5XO5nPFJvM6RAUFbgLqCXNht5JB7Hay6umFlLpy09iTcx6EySE1NxYMPPoh+/foZaxCZo9frcezYMfznP//BihUrvCokQbiNh8MDXToVzZ4n5WXzlW9cIi8h8ftl4N3VwOfb7d9c7gGszbM8XmLYGagEviMwOGvrxfSFxh4GQkhLyxDXnExjfSD2z4/Api2wzJhevRBs8Tq+cxFFU9VTUWVZL8i6d4B5b4eoGKCinEc7GRBahFgUyXOadGeFswzq5pIX4GkcKoNnnnkGO3fuxJYtW9CxY0fExcUhJCQEOp0OOTk5uHLlCrp3744ZM2b4Ul6CcImiOPh64NR+Le8csjMMjlnG8wSuXeL9euUJvz7IwRr5djqGKeHz7cA/PwJjDPiWO6+NIa5GeQ3U6XkJCE2UKYS1MI93FAtuwfsgAHy3MvR+4Jt/m6616h1g3duB6arAVi/kZrQYU/kNe8lqSjrKOcygbiZ5AZ7GoTJISEjA3LlzUVJSgtOnTyMjIwM3btxAaGgo7rrrLjz77LMICwvzpawE4RJHcfDOVphKWyC6avMo2iu+tnMLn1zdJbydaWfgCeQ8hJxMk2ll1ATg43+YHLsyt/YGIqN4sTsZxvg18Ummdpq//mSKFoqMhjB/pc3YmPd2EEJagS1e53y8HezobMbfWWvPZpIX4Glc+gzCw8Nx1113+UIWgmg4duLgFZkanKxCnfXUNUcIacUTpo58jbrr1wyr6wL7WbiCCPQdBJw4ZP89PKkIrJ4rqdXA0tm2JR4AQFRBrCgHbukBKS6Jm15i4o0KVTaVsRod2FtLjNcIE5/hfgUHWEzmzlbtdnwzjv6d7LX2JJ9B/SEHMtG0cCd71IlfQZ68mCbKsqduVgYv8mYn/FK+Jnzxmyi+eJ5f++Zivhq3zk+IjAZuuwM49YNp1e4NQtsAlRUmhVSn5yv+Eq3tuYLAx04uW20nscto1tFV8s5jDpr2mOOOM9+eb8ZujwWrsFwAZBpqIKQMiCaFW9mjDhSHxeSlMS//AEAUjEXmzCc182tKkzoCc/8mR/DbT1SrLAe2vqEso7ghVJTzshelxVwRiCLvc2B8H5ELGR0H4bGplqGhTnwkbo2zm858m+c20/IQvoaUAdHkUJqo5HBCsy4XHdme189p1RooLzOew65cgNCtt6kktKGXgP73K8D5X/huwrp1o0zFDc+8rEt4DL9xZyBJXAEAPOM49TWIer3dGkSuJnrFCWHm9ZCi49yezJtreQhfQ8qAaNbYndAsmqpHAiPHAP/5FCg2KxPBJLBPN0N6fqmh6UwGjPZ3JgHvrDSFYPobJvGWmta5DHV1EOw0iPFeCef6J8xRJrL3EZWcdOjQIVy/zlPrs7OzsXjxYixZsgRZWVleFY4g6ovc0pHpKt2+VghpxesKtYvkTtbt6w3VRa1MOnlZYD8c4DsC6+8CRRHIMAnChGnAuMmWh3MybcfIUa2h+pKVwe8jSXx30ND7EV5BkTLYuXMnWrduDQD44IMPcPPNN+MPf/gDtmzZ4lXhCKI+yCtbaU0qpNUL7SoEV8pC0BbYVNS0oXVb4F8foSErXq+jUvE/sYm8s1rXHkBcEjcVqYOAT7dAWjobkrlDWd4ZqdQesdEzTRSXAQBEkX8mAg5FZqKysjKEh4ejpqYG58+fx9y5c6FSqfDXv/7V2/IRhBHFtYaUdOGy6iYm1+6Xr2eaKMum7NaIKlObxkAmtA2EyXMtHMNi6hqegbzjHQDMoi+zENLK4zZ6QVvAy20AfHdw+gSYi2J4hO9RpAzatm2L3NxcZGRk4Oabb0ZQUBCqq6u9LRthh+bYmxVw047tqo6QTTexhWDFhTZlDTB9IbBuCY8mkrtwaaKBhA48JDRQCGkFODKH3SjlLTOvXAATeN9hIaQV0H8I2P8+N4XMFuVbKE2P2ujNfTCCCPbhJptoLML/KFIGDz/8MBYsWABRFDFnzhwAwJkzZ9ChQwevCkdY0lx7swJwKzzR1cqW1VgtZIoMiWFydU1J4sriu/+ZunAxBvx5PLD3E1OBuUDBmV+EMWDTKjBDLgOLS+L5AwAw5q/AZ1sMlVQTvRayaewHcex7sA83UdmIAEWRMrjnnnswcOBAAEALQw3zLl26YPbs2V4TjLBDM+3NCsDtWHNHK1umqwT27DAdiI7nE2ZBLu8wVqLlykCSgLS9ZufFAd/8x342caBjntSWe503oDf0ZkBMPIRZr1qYkbyBcTcit/ykfIGAQ3FoaU1NDU6dOoXi4mKMGjUKdXV1vPAV4TuacfKNp+zY7OoFU2ctUQU8OB54fx2f5IsLHWcDt4viZo7GiKgyRTfFGPosmJfsCG7hkx0m5QsENoqUwdmzZ7F27Vp06tQJ58+fx6hRo5Cbm4u9e/di4cKF3paRMNBcfpkc+UUaasdmukqwT7eYwkBj4nlJZbmiqJyhay8r+OxP9X6uX2nbDpj5MoSKcmPzGQAWpSR8uaigfIHARZEy2LZtG2bPno0ePXrgySefBAB07twZly9f9qpwhC1N/ZfJ034RiwYoWRkGvwB4cbWxk8EiIrlzuE5v+rspUV4KbFvPy1abjWNzWFQQ7qFIGRQUFKBHjx6WF6rVqKvzXGJNeno63n//fUiShOHDh+Ohhx7y2L2JRoQH/SLWikWYtdjSzBaXyKOF6uqAVm2AkJY8qqYxY+ikhvwcPoaSfWdtU19UEO6jSBkkJCQgPT0dvXv3Nh47c+YMkpI8s72UJAlbt27FokWLoNFokJqaittvvx0JCQkeuT/BaRRhqZ70i9hpUG/eqxdZGWBZhjISlTf4n8aMKALjn4YQFAxWWwN88QHV+CcUo0gZTJw4EatXr8Ztt92Gmpoa/OMf/8DJkycxb948jwhx6dIlxMTEIDo6GgAwaNAgHD9+nJSBB2ksYal2Sxi70YCGXbkAGOLprRugsE83G3MIhAWrTG0c6wI0Qqh9HFBdxSuOyoS2AR56HPjqC940x9y/EREFfPEBmPyOZsl0gfhvTQQWipRBcnIyXnvtNXz//fcICQlBZGQkVqxYAY3GcTMLdygqKrK4l0ajwcWLF23OS0tLQ1paGgBg1apViIyM9Mjz64tarfa7DEqpOXcGxTnXjW0CwyrKEJxgWi1KVRXQ/34F6g6dILYMdfv+Hh8Lg2xSVQWKlz8PfeY1qBNvQrsVm+zKJ1VVoGjZbEgZVwAAYlInRKz6O7BmM/QZV8F0VShZNtfY8zf07CmoouNQYh4RJ4gBEToqRMUg7NlUBHXpBqmqEiUvPYO6vGxAJQJVlVAf3o+wVf9A3fVruLF1HeqyfocqKhatn3wOpWteMv4bh9fVInjAYL++S2P6HfE2gT4WikNLIyIiMGrUKK8IYS9EVRAEm2MpKSlISUkxfi4sLLQ5x5dERkb6XQalsNZh3FyQcx2ISUBpaFsIBtk9sWvw1liwy+cgZVwFpDroM69Be/qU3U5Z7PI5SJnXjJ/rrpudq4nhu4vYBP6Ogogbf3+dRxPFxAN5WUBEe2DqPODvrwGFufZ7EPgCTTQwfyXKQloCht7BeOl1CGYN7PWZ11B88TyEm7uCzVsBMSsDLD4JNwCH/8b+ojH9jnibQBmLuLg4u8cVKYO3337b7uQMAM8++2z9pTKg0Wig1ZoKZWm1WrRr167B9yVMGCtxnj4B9LzdcrIP5GQ2BT4EpqsEq9YBUTGmpvGiCkwTBfl/rUUWrGFSRW4W8OiTEOL4MwRtAdj8FWAnj1j2MfYFbdsBE2fwQnKARe0kMXWNqYG91ThYO4IpSoioL4qUQUxMjMXnkpIS/PDDDxgyZIhHhLj55puRk5OD/Px8RERE4MiRI5g5c6ZH7k1wmK7SUHc/E/g2Ecx89d9Apy3TVaLm3Bmw1mEen4BclpYw39VERPLWjYwBej3YtUtAb43FvYxZsNmZ3F+w6z2wiChAHQSWn8N3CvePAdqEATdKPfouDgltA8xbznsPw07tpCsXIHbrbeo/rInif9sZD4oSIuqLImXw6KOP2hwbNmwYdu3a5REhVCoVnnrqKSxfvhySJGHo0KFITEz0yL0JA05W/w1JZpMn42J5FesFx7TTCc78vbQFQOswnkgGBvx9NeqmLoDYtYdFK0dh1mKwfXt4eQlJsmxrmZ0BbH7do/K7pM8g4O9rIBnMdBg1wfJ7QbDsybxuCViABwIQjY96dzq76aab8Ntvv3lMkD59+qBPnz4eux9hhYvVf71XlP42McnvJa/0y81W83o9sHEFpJh44O4REG6/E0JIS96k3lFp6gYhAJooXtbCOovZvCSENV17AIfT+PfZhpIXcUm8bEZMAhCbYNuTmYq9ER5GkTL45ZdfLD5XV1fj8OHDFPrZiPBaKQt5MpYnLh/Hs9v4Amwcv4zLtnML2O73wf46x1SbyOPCCPYVAcDlCtfwZvSCyDOdW7UBnl8CIawdz4QuzOMKYc8Oi7BQdtVgNpIkU0/mokLKHyA8iiJlsGnTJovPISEh6NChA2bNmuUVoQjv4A17sjwZh1WU8egVP5gsjL6Ar//NJ/qoWOCOe4Dv9/FJU6auDti5lRedk/sACyLQTuO6q5kSmASIah4CKgimEheSZMyAFrQF3LFt1kxHWr3Q8vl52RC0BTxayLqeUnQ85Q8QXkGRMti4caO35SAaMUJIKwQnJPksjNFhElpdHSAx/veh/UBxkcFsU2Qy0ZQWc3OS8WYS0GegZblqpYgqIKwd3w0YZdCjxd1/RHX/uyHkZoF1uBnC75eBnrdDDNcA4Roe4RTOHdvs8jmDmU0y3TM6jkdHye9pVU9Jvg9BeBKHykCyt9W1g2j+i0UQXsYieigmHsK4yRA6JkM6d4bnCwA8T0CmSAv86WHgv2bBDub/twWhfooA4Armj6OBnVsszFPV3+0DvtvHOyOrVGAS4+aeBav4CeaKzCpLGn/5P55FvG4JmJ16SkKn5PrJShAucKgMxo8fr+gGO3fu9JgwROPEm6GlNpg7rLMzwN56FSwuCRhwtwPhJN6iMiaBF2+LjOYmGblsdUOSy8LaAbfeBsR34L0OWoYC5WWW58jFHLMzLZvKmEUCmftykJUBKTfL6JAXtAUQKHeA8AEOlcGGDRt8KQfRSPFFaKkF5tFDclXO7AweZWMesSPnGwBAfg631we3AOKTIJUUAXs/Bo4fcv28mATg4SeAXe/zhDaVit9XEIHSEuDva4y+AEmtBlbOMykAczRRlk1lzCKBzH05zE7Ulzu+nkZRjJAISBwqg6ioKF/KQTRWfBxaaoweunKBF57LMdjb937CM5DzsgEBliv+yGiAMZ6sdfUC8OG7pkxlZzz4GHfi1lSbGsczAPeNBvbt4R8Mq3fEJwGrFwKMQYyIhPSnR4Cv9gDaQkDTHsL8FTys1UVynydyPgK9GCERmCjOMzhx4gTOnj2LsjLLbbAnylEQjQfz5Cc5MsbXoaVCSCsI3XpDGjcF7K1XuSkoPwd8pmb8L3N0lfw8lYpH+Cg1DR35Guw/O7nDWS5gFxMPYfgDYL+ctJzUjUpRglRUCHz8D27jn23ZX1jJRN9ocz6IRo0iZbBr1y7s378fgwYNwg8//ICUlBQcPnwYAwcO9LZ8hAdoqOnAOvsVWRkGx6gpZDK8rtbroaXW7yF0SuZmFdn5KkmGyBuryV4uAa13oyKpSs0zmplkMj2JIoRxUyCGa7gz2LzMttF8ZcgHYJLd/sJeLRfRjHtkEw1HkTL49ttvsWjRIiQlJeHAgQOYNGkSBg8ejM8//9zb8hENpKGmA4vrNVGmCVKeWA1mkuABgxscWupMaTl6D/N6Pez1RbDdFsCyr7EgcN+C3N7SvGx1eARQUsR/rqvjyV3FhfwcWfEZeghbT+pG89XVixB3v4+6rIx613mqr+JuLj2yCe+gSBlUVFQYu5qp1Wro9Xp07twZZ8+e9apwzQGvO/zqaTpgukqwqxd4DRzz2j+a9oA232Q68dAK1KXSysrgf5jEz7FyvkrpP5pCS62xDpNuGwYUy1VymUkh3CjlDWKKCvhxdRCEWa9yc4+CJC8hpBWEP/RCxKq/Q2soP+224rWuVloPhUCmIaI+KK5ampmZicTERCQmJuKrr75C69at0bp1a2/L16TxpMPPkVJhmig+gRflK564jZOSXL9Hpeb29ljHWbQNDi11obSYJorLoJcAUbQoTw2AKyhzevYDTh+383IMKDHrHBYRBbQI4Y7naENnMZn8HG7mCdeAhbR0WCnUGrFlqEXPBcWd2uxUKxW69Xb6LILwFIqUwdixY3HjBu8PO2HCBKxbtw46nQ6TJ0/2qnBNHg85/BwpFWPZ6sI8ILI9n8gVTNbsygUepSMjSRAenw6h/xB+vVkWrcdCS10V0tMWcB8FADCJKyLzLNxbb7O839CR/B20BTzKqCCXj7NKDbSP5dFEEaYoH2RlgFXr+HjJaKJ4slgDlLZb11q3DHHQQ4QgvIEiZWBeTbRz5854++23vSZQs8JTDj9HSkU+ziSgqNB2ArUD01WCWWXUIjbBpAiUPtvOfZ2tjl3au12MlVhRDsk8t2DDcv7emmgIL/yNHzM29mlp+5ybu4KVaE1mME0kMOavbr2jXdy4VuiYzBPoDJFZsn+CIHyBImWwZs0aDBkyBH379kVwcLC3ZWo2eMzh52iirI+ysaiFIwJj/grxzuGOZVNQtVTp6tiRvVtWJLKJyqGyiIwBCnL4Z9lBXFRgLPqGu/5oOt/qORa7KE0UoAoCNq2CpGkPzHy5/krbjX8DIaQVxNQ15AAm/ILA7DUgtuI///kPDh8+jOzsbPTr1w+DBw9Gz549/V6XKDtbQeKQFwmUnqaAE5+Bmw5q48TthtmH6SqdVi1ll89BWpNqNNOI81bY7WPsVB4FZhapRAu2JhUozDdkCktArPNrjGGzsolIquPmGUEwOZ6jYiDMX6m4Uqj1/4vmnBUcSL8j/iZQxsJRD2RFykAmJycHhw4dwpEjR1BeXo6BAwfiqaee8piQ7kLKwD6eyitw53pnY1EfBWO81k1FYp0U5+wdrIve8TwFQ68Dldq0uxBVEOevVKTAmK4SYeWlKPVFnaZGQKD+jviDQBkLR8rArU5nsbGxePTRR9GvXz98+OGH2Ldvn1+VAWGLJyKUXIUnMl0ldzIL3M7t6v4NMoe5aeqSZTcvE+0Qc3t+Xjbw6JO83wGTuGII1wBlJfz5SqOw3HCmN+cdAxF4KFYGubm5OHz4MA4fPowbN25gwIABeOSRR7wpG1EfvFySwDrslMUlcTu3C+ob/+7VRCorRSP0HQR2aL/pszMfhT3cGHuqI0QEGoqUQWpqqtFfMHHiRPTq1cvv/gLCAd4uSZCVwe8tk2uY9BK8V/rAniLxxKranqKxLjPhVhMZd1qAUh0hIsBQpAz+/Oc/4/bbb6dIokaA10sSxCdxJSMnR/mh77EnV9X2ykrUd1J2qwUo1REiAgxFymDQoEHeloPwIN4sSSCHP3KfgWBRkdNn2FlVM7lyqJkCtLd78LadXmkLUKojRAQabjmQieaFo4lTLiHtN6xW1UwTBWa1UwBgs3uwd8yfkzDVESICCVIGhF0C2cFpr1Uks7a/A7Y2eXvHaDImCAAAeYEJu7ArF/hkaT2ZBghCSCsIctVSeaegUpvs70qPEQQBwMnOIC8vT9ENoqOjPSYM4Xsc2dV5fSJDBm50XEBPnI7s70qPEQThRBnMnDlT0Q127tzpMWEI3+LQFGRVn0gYNyXgJ0579nelxwiCcKIMzCf5b7/9FmfOnMGjjz6KqKgoFBQUYPfu3ejRo4dPhCS8hKNYd+uwx9gEsMvnaDXtJpRhTDQmFDmQd+7cifXr1xvzDGJjY/H0009j1qxZuOeee7wpH+FNHMS627STXLeEO2gDzJEcyASyA54g7KFIGTDGkJ+fj4SEBOOxgoICSNbtBIlGhbNYd2OT92Pfm9pNUgSOcijDmGhkKFIG999/P5YuXYp77rnHWHnvu+++w/333+9t+QgvI9vQma7SwhRksbJVqQBJoAgcd6AMY6KRoUgZPPjgg0hKSsLRo0dx7do1hIeHY/r06ejdu7eXxSN8gT2ThsXKVgCECdMcdzsjbKAMY6KxoTjprHfv3jT5N1XsmTSsK3qSInAbilwiGhOKlEFtbS12795tLF+9fft2/Pzzz8jJycGIESO8LSPhbeyYNGhlSxDNC0XKYPv27SgqKsLMmTOxYsUKAEBiYiK2b9/eYGWwY8cOnDx5Emq1GtHR0ZgxYwZCQ0MbdE/CPRxN/LSyJYjmg6JyFMeOHcPMmTORnJwMQRAAABERESgqKmqwAD179sTatWvx+uuvIzY2Fnv27GnwPQlbpBItpIP7IJVo7X5vUd6BIIhmh6KdgVqttgkjLSsrQ5s2bRosQK9evYw/Jycn44cffmjwPQlLpBItWOrTgL4WUAdBWvkPiO40bSEIosmjaGdwxx13YMOGDcjPzwcAFBcXY+vWrR7vc/DNN9+Qk9obnD7BFQHA/z59wr/yEAQRcAiMMebqJL1ejw8//BBff/01ampqEBwcjOHDh2PChAkICgpy+ZBly5ahpKTE5vi4cePQr18/AMAXX3yBy5cv44UXXjCaoqxJS0tDWloaAGDVqlWoqalx+Wxvolarodfr/SqDEvRFBdBOexSorQGCgqF5dxfUEVEefUZjGQtfQGNhgsbCRKCMhaOOlYqUgTmyecjRhF0fDhw4gP379+OVV15BixYtFF+XnZ3tMRnqg5yAZ06g1qORSrR8R9Dzdq+YiOyNRXOFxsIEjYWJQBmLuLg4u8cVmYmefPJJ489t27Y1KoLJkyc3WLD09HT861//woIFC9xSBIGInLwlrUmFtHohVwwBghiugXjXH50qAjkLWanc7p5PEETgosiBXFdXZ3NMr9d7pDbR1q1bodfrsWzZMgBAly5d8PTTTzf4vn6hEdejcbewmvX50prNPpSWIAhP41QZvPLKKxAEAbW1tVi8eLHFd1qtFsnJyQ0W4O23327wPQIGP9ajabB5SqEik5/DqnUW5+szrgKaGA+8iWcIVHMdQQQqTpXBsGHDAACXLl3C0KFDjccFQUBYWBi6d+/uXekaGe5m7dZ3wrK+rj7lkm2erUCRWTwnJp7/ycsGYhOgTuoIVFQpfgdvQuWjCcJ9nCoDuVdBly5dEB8f7wt5Gj1Ks3brO2G5LCqnwDzl6NkuFZn5c/KyIcxaDCG4BRCfBLFlaMAog8ZsriMIf6HIgbxv3z6cP3/e4tj58+exbds2b8jUPLA3YUGBU9ZZUTmljd4dPNtlFrLVc4SOXQIza5ka3xOE2yhyIB8+fBj/93//Z3GsU6dOeO211zBp0iRvyNX0sWOWYbpKSCvnG4+JqWtsJ9p6FJWrj0nIHo2leF1jkZMgAglFykAQBJvIIUmS4GaKAgHLidl6wpLOpgPZfJWO7AywKxcgdOttcb3SonJGR6+hbaUrk5BS/0VjKV7XWOQkiEBBkTLo2rUrPv30Uzz++OMQRRGSJGHXrl3o2pV+2dzBrq3efMKyzuNzkNjnaqKzeI4mCtAW2NjPze9hPD8rA4hsD2H+SqpdRBDNDMVJZ2fOnMHUqVORmpqKqVOn4vTp03jqqae8LV/TwoGtXkbomAzEJQGiCMQlQejYpf7PycrgzynIA9qGA6IKiIgC09iWoWBXLwBZv/M+xwW5YGtepEQygmhmKNoZaDQarF69GpcuXYJWq4VGo0Hnzp0hiop0CSHjwlYvhLSCmLqmwbZuponifYv1EgAGlGj554JcsDWpkMxW/kxXCfbpFsDc5FeUTxE4BNHMUNz2UhRFjySZNWeUODY9YesWtAVg1tnhchZ5QS7Yyvmo6zMQuHsExIpyIDfLdJ4oArGJFIFDEM0Mh8pgzpw5ePPNNwEA06dPd3iDTZs2eV6qJoxPHJvGHUgmIIhcEYiCSSEUFQBpe4G0vZAWvWHarUTHQRg3he9asjLA3HQuEwTReHGoDKZOnWr8+bnnnvOJMET9MY8eErQFPCFMW2D8LIW2BtYvAwpzLU1CJw5b7FYA8CJ7Bie3MGux3WgkgiCaFg6VgXmkULdu3XwiDFE/LKKHRBGsro6v4hesghjSCgjXQAVAmr8CbMV8oLjAdPGdwy0jiy6fs3Rynz5B2bwE0QxwqAx27typ6AZjx471mDBEPTGPUpIMpiA7E7egLQArNfStFgRgygtQxSRY3svayd3zduBb/xTfIwjCdzhUBlqtqXF6TU0NfvzxR3Tu3NnYoOHSpUsYMGCAT4QkHMN0lbyCaEw8kJfFfQSSZH/itproxR59be5nNyGNsnkJosnjUBnMmDHD+PNbb72FWbNm4Y477jAe+/HHH3H06FHvStfMceW4ta4iKsx6ldcM0hbYvUZpmQZrJzdl8xJE00dRosCpU6fQv39/i2P9+vXDqVOnvCIUwdtUSktn885pjrqmWVcRDW4BMVzjtHicy2J0BEE0SxQpg5iYGPzvf/+zOLZv3z7ExAROM5PGir0qpUxXCbbmRaAgl2cFZ/E6RTZQdU6CIDyEoqSzadOm4fXXX8fevXsRERGBoqIiqFQqzJ0719vyNWkc9jTIygC0+WYnSmA7t4BZVTGl6pwEQXgKRcqgY8eOWLduHS5evIji4mKEh4cjOTkZarXiBGbCHo6asMgr/uwM7gwGeEcxO2GdZM8nCMIT1Ku4ULdu3aDX66HT6TwtT/PCgZlHXvELs5fwwnVkBiIIwssoWtpnZGRg9erVCAoKglarxaBBg3D27Fl89913mDNnjrdlbLI4M/MIIa0g/KEXmAcK1xEEQbhC0c5g8+bNGDt2LN566y2jaahbt244d+6cV4VrDriK7qHoH4IgfIEiZXD9+nUMGTLE4lhISAhqamq8IhRBEAThWxQpg6ioKFy5csXi2KVLlyi0tB64bHhPEAThBxT5DMaOHYtVq1bh3nvvhV6vx549e7B//36LyqaEaxyGkhIEQfgZRTuDvn37IjU1FWVlZejWrRsKCgrwwgsvoFevXt6Wr2nhou0lQRCEv3C5M5AkCbNmzcIbb7yByZMn+0KmpouLtpcEQRD+wqUyEEURoiiitrYWQUFBvpCpyUIZwwRBBCqKfAYjR47Em2++idGjRyMiIgKCIBi/i46O9ppwTRHKGCYIIhBRpAzee+89AMDp06dtvlPaBIcgCIIIXBQpA5rwCYIgmjZOlUF1dTU+//xzZGZmomPHjhg9ejT5DQiCIJogTkNLt27dipMnTyI+Ph4//vgjduzY4Su5CIIgCB/iVBmkp6dj0aJFePzxx5GamoqTJ0/6Si6CIAjChzhVBtXV1WjXrh0AIDIyEpWVVEKBIAiiKeLUZ1BXV4dffvnF+FmSJIvPANC9e3ePCLJ37158+OGH2LJlC9q2beuRexIEQRDKcKoMwsLCsGnTJuPn1q1bW3wWBAEbNmxosBCFhYU4c+YMIiMjG3wvgiAIwn2cKoONGzf6RIjt27djwoQJeO2113zyPIIgCMISvzcxPnHiBCIiInDTTTe5PDctLQ1paWkAgFWrVvl9J6FWq/0uQ6BAY2GCxsIEjYWJQB8LnyiDZcuWoaSkxOb4uHHjsGfPHixatEjRfVJSUpCSkmL8XFhY6CkR60VkZKTfZQgUaCxM0FiYoLEwEShjERcXZ/e4T5TByy+/bPd4RkYG8vPzMW/ePACAVqvFggULsHLlSoSHh/tCNIIgCAJ+NhMlJSVhy5Ytxs/PPPMMVq5cSdFEBEEQPkZRcxuCIAiiaeN3B7I5vopeIgiCICyhnQFBEARByoAgCIIgZUAQBEGAlAFBEAQBUgYEQRAESBkQBEEQIGVAEARBgJQBQRAEAVIGBEEQBEgZEARBECBlQBAEQYCUAUEQBAFSBgRBEARIGRAEQRAgZUAQBEGAlAFBEAQBQGCMMX8LQRAEQfgX2hk0gIULF/pbhICBxsIEjYUJGgsTgT4WpAwIgiAIUgYEQRAEKYMGkZKS4m8RAgYaCxM0FiZoLEwE+liQA5kgCIKgnQFBEARByoAgCIIAoPa3AE2FvXv34sMPP8SWLVvQtm1bf4vjF3bs2IGTJ09CrVYjOjoaM2bMQGhoqL/F8inp6el4//33IUkShg8fjoceesjfIvmFwsJCbNy4ESUlJRAEASkpKRg5cqS/xfIrkiRh4cKFiIiICMgwU1IGHqCwsBBnzpxBZGSkv0XxKz179sRjjz0GlUqFDz/8EHv27MHjjz/ub7F8hiRJ2Lp1KxYtWgSNRoPU1FTcfvvtSEhI8LdoPkelUmHixIno1KkTqqqqsHDhQvTs2bNZjoXMf//7X8THx6OqqsrfotiFzEQeYPv27ZgwYQIEQfC3KH6lV69eUKlUAIDk5GQUFRX5WSLfcunSJcTExCA6OhpqtRqDBg3C8ePH/S2WX2jXrh06deoEAGjZsiXi4+Ob3f8Hc7RaLX766ScMHz7c36I4hJRBAzlx4gQiIiJw0003+VuUgOKbb75B7969/S2GTykqKoJGozF+1mg0zXoClMnPz8fVq1fRuXNnf4viN7Zt24bHH388oBeMZCZSwLJly1BSUmJzfNy4cdizZw8WLVrke6H8hLOx6NevHwDgiy++gEqlwpAhQ3wsnX+xF6UdyL/8vkCn02Ht2rWYNGkSWrVq5W9x/MLJkycRFhaGTp064ddff/W3OA6hPIMGkJGRgaVLl6JFixYA+FawXbt2WLlyJcLDw/0rnJ84cOAA9u/fj1deecU4Ls2FCxcuYNeuXXjppZcAAHv27AEAjB492p9i+Q29Xo/Vq1ejV69eeOCBB/wtjt/4+OOPcfDgQahUKtTU1KCqqgr9+/fHzJkz/S2aJYzwGDNmzGClpaX+FsNvnDp1is2ePbvZjoFer2fPPPMMy8vLY7W1teyFF15gGRkZ/hbLL0iSxN5++232/vvv+1uUgOKXX35hK1eu9LcYdiEzEeExtm7dCr1ej2XLlgEAunTpgqefftrPUvkOlUqFp556CsuXL4ckSRg6dCgSExP9LZZfOH/+PA4ePIikpCTMmzcPADB+/Hj06dPHz5IRjiAzEUEQBEHRRARBEAQpA4IgCAKkDAiCIAiQMiAIgiBAyoAgCIIAKQOC8Cuvvvoqvv76a3+LQRBUjoJoOkycONH4c01NDdRqNUSRr3eefvrpZlcegyDcgZQB0WTYsWOH8ednnnkGU6dORc+ePW3Oq6urM1ZXJQiCQ8qAaPL8+uuvePvttzFixAh8+eWX6NmzJ3r06IGvv/7amC0NAGPGjMH69esRExOD2tpafPLJJzh69Cj0ej369euHSZMmITg42OLetbW1mDJlCpYuXYqkpCQAQFlZGaZPn4533nkHKpUKGzZswMWLFyFJEm655RZMmTLForqpzGeffYbc3FxjzZr8/Hw8++yz+OSTT6BSqVBZWYnt27fj1KlTEAQBQ4cOxZgxYyCKInJzc7Fp0yZcu3YNarUa3bt3x5w5c7w4qkRTg3wGRLOgpKQE5eXleOeddzB16lSX53/00UfIycnBa6+9hvXr16OoqAi7d++2OS8oKAj9+/fH4cOHjceOHDmCbt26ISwsDIwx3HPPPXjnnXfwzjvvIDg4GFu3bq3XO2zYsAEqlQrr16/HmjVr8PPPPxv9DZ9++il69eqF999/H5s2bcKf/vSnej2DaL6QMiCaBYIgYMyYMQgKCrJZ3VvDGMPXX3+NJ554Aq1bt0bLli3xl7/8xWLCN2fw4MEW3x0+fBiDBw8GALRp0wZ33HEHWrRoYbzPb7/95rb8JSUlSE9Px6RJkxASEoKwsDDcf//9OHLkCABArVajoKAAxcXFCA4ORteuXd1+BtG8ITMR0Sxo27atSyUgU1ZWhurqaos+tYwxSJJk9/zu3bujpqYGFy9eRHh4OK5du4b+/fsDAKqrq7F9+3akp6ejoqICAFBVVQVJkozObSUUFhairq7OovAfY8xobnr88cfx6aef4sUXX0RoaCgeeOABDBs2TPH9CYKUAdEssG4y06JFC9TU1Bg/mzfsadOmDYKDg/HGG28gIiLC5b1FUcTAgQNx+PBhhIWFoU+fPmjZsiUA4N///jeys7OxYsUKo6KYP3++3UY4ISEhDmXSaDRQq9XYunWrXed3eHg4pk2bBgA4d+4cli1bhm7duiEmJsal/AQBkJmIaKZ06NABmZmZuHbtGmpqavDZZ58ZvxNFEcOHD8e2bdtQWloKgLe0TE9Pd3i/wYMH48iRIzh06JDRRATwTl/BwcFo1aoVysvLsWvXLof3uOmmm/Dbb7+hsLAQlZWV+Oc//2n8rl27dujVqxc++OADVFZWQpIk5Obm4uzZswCAo0ePQqvVAgBCQ0ON70EQSqGdAdEsiYuLwyOPPIJly5YhODgY48ePR1pamvH7CRMmYPfu3XjppZdw48YNRERE4N5773XY17lLly5o0aIFioqKcNtttxmPjxw5EuvXr8df//pXRERE4IEHHsDx48ft3qNnz54YOHAgXnjhBbRp0wajRo3CiRMnjN8/++yz+Oijj/D888+jqqoK0dHRGDVqFADg8uXL2LZtGyorKxEeHo4nn3wS7du398BIEc0F6mdAEARBkJmIIAiCIGVAEARBgJQBQRAEAVIGBEEQBEgZEARBECBlQBAEQYCUAUEQBAFSBgRBEASA/w+nDxy0KVjlFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model r2_score 0.6829 with a standard deviation of 0.0513\n",
      "SVM optimized model r2_score 0.7166 with a standard deviation of 0.0462\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVR \n",
    "svm_baseline_CVscore = cross_val_score(svm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"SVM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./optimizedCV_svm_noSemiSel.joblib']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_reg, output/\"svm_reg.joblib\")\n",
    "#joblib.dump(optimized_svm, output/\"optimized_svm.joblib\")\n",
    "joblib.dump(optimizedCV_svm, output/\"optimizedCV_svm.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
